<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>懒羊羊</title>
  
  <subtitle>xxlila</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://xxlaila.github.io/"/>
  <updated>2019-09-23T06:06:26.809Z</updated>
  <id>https://xxlaila.github.io/</id>
  
  <author>
    <name>xxlaila</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>k8s v1.14 prometheus</title>
    <link href="https://xxlaila.github.io/2019/09/20/k8s-v1-14-prometheus/"/>
    <id>https://xxlaila.github.io/2019/09/20/k8s-v1-14-prometheus/</id>
    <published>2019-09-20T08:12:48.000Z</published>
    <updated>2019-09-23T06:06:26.809Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Prometheus、Grafana-部署"><a href="#Prometheus、Grafana-部署" class="headerlink" title="Prometheus、Grafana 部署"></a>Prometheus、Grafana 部署</h1><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Grafana是一个开源的度量分析与可视化套件。经常被用作基础设施的时间序列数据和应用程序分析的可视化，我们这里用它来做Kubernetes集群监控数据的可视化。</p><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;截至当前，prometheus、grafana均采用最新的镜像包，在在第一次部署的时候grafana报了一个错误<code>mkdir: cannot create directory &#39;/var/lib/grafana/plugins&#39;: No such file or directory</code>,这是因为Grafana启动使用的用户和用户组都是472，造成对外挂存储没有权限。<a href="https://grafana.com/docs/installation/docker/#migration-from-a-previous-version-of-the-docker-container-to-5-1-or-later" target="_blank" rel="noopener">参考官方</a></p><a id="more"></a><h3 id="开始部署"><a href="#开始部署" class="headerlink" title="开始部署"></a>开始部署</h3><p>新建yaml文件</p><ul><li>monitor-namespace.yaml<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat  monitor-namespace.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Namespace</span><br><span class="line">metadata:</span><br><span class="line">  name: monitoring</span><br></pre></td></tr></table></figure></li></ul><p>其他的文件均采用以前历史的，然后稍加修改，其他<a href="https://github.com/xxlaila/kubernetes-yaml.git" target="_blank" rel="noopener">yaml</a>文件,移除<code>grafana-ingress.yaml</code>、<code>prometheus-ingress.yaml</code></p><h3 id="文件修改"><a href="#文件修改" class="headerlink" title="文件修改"></a>文件修改</h3><ul><li><p>grafana-deploy.yaml</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: grafana-core</span><br><span class="line">  namespace: monitoring</span><br><span class="line">  labels:</span><br><span class="line">    app: grafana</span><br><span class="line">    component: core</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: grafana</span><br><span class="line">        component: core</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - image: grafana/grafana:latest</span><br><span class="line">        name: grafana-core</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        <span class="comment"># env:</span></span><br><span class="line">        resources:</span><br><span class="line">          <span class="comment"># keep request = limit to keep this container in guaranteed class</span></span><br><span class="line">          limits:</span><br><span class="line">            cpu: 100m</span><br><span class="line">            memory: 100Mi</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 100m</span><br><span class="line">            memory: 100Mi</span><br><span class="line">        env:</span><br><span class="line">          <span class="comment"># The following env variables set up basic auth twith the default admin user and admin password.</span></span><br><span class="line">          - name: GF_AUTH_BASIC_ENABLED</span><br><span class="line">            value: <span class="string">"true"</span></span><br><span class="line">          - name: GF_AUTH_ANONYMOUS_ENABLED</span><br><span class="line">            value: <span class="string">"false"</span></span><br><span class="line">          <span class="comment"># - name: GF_AUTH_ANONYMOUS_ORG_ROLE</span></span><br><span class="line">          <span class="comment">#   value: Admin</span></span><br><span class="line">          <span class="comment"># does not really work, because of template variables in exported dashboards:</span></span><br><span class="line">          <span class="comment"># - name: GF_DASHBOARDS_JSON_ENABLED</span></span><br><span class="line">          <span class="comment">#   value: "true"</span></span><br><span class="line">        readinessProbe:</span><br><span class="line">          httpGet:</span><br><span class="line">            path: /login</span><br><span class="line">            port: 3000</span><br><span class="line">          <span class="comment"># initialDelaySeconds: 30</span></span><br><span class="line">          <span class="comment"># timeoutSeconds: 1</span></span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: grafana-persistent-storage</span><br><span class="line">          mountPath: /var/lib/grafana</span><br><span class="line">      volumes:</span><br><span class="line">      - name: grafana-persistent-storage</span><br><span class="line">        emptyDir: &#123;&#125;</span><br></pre></td></tr></table></figure></li><li><p>prometheus-deploy.yaml</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spec:</span><br><span class="line">      containers:</span><br><span class="line">      - image: prom/prometheus:latest</span><br><span class="line">        name: prometheus</span><br><span class="line">        <span class="built_in">command</span>:</span><br><span class="line">        - <span class="string">"/bin/prometheus"</span></span><br></pre></td></tr></table></figure></li><li><p>prometheus-svc.yaml</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">kind: Service</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: prometheus</span><br><span class="line">  name: prometheus</span><br><span class="line">  namespace: monitoring</span><br><span class="line">spec:</span><br><span class="line">  <span class="comment">#type: NodePort</span></span><br><span class="line">  ports:</span><br><span class="line">  - port: 9090</span><br><span class="line">    targetPort: 9090</span><br><span class="line">    <span class="comment">#nodePort: 30005</span></span><br><span class="line">  selector:</span><br><span class="line">    app: prometheus</span><br></pre></td></tr></table></figure></li><li><p>grafana-svc.yaml</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat grafana-svc.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: grafana</span><br><span class="line">  namespace: monitoring</span><br><span class="line">  labels:</span><br><span class="line">    app: grafana</span><br><span class="line">    component: core</span><br><span class="line">spec:</span><br><span class="line">  <span class="comment">#type: NodePort</span></span><br><span class="line">  ports:</span><br><span class="line">    - port: 3000</span><br><span class="line">  selector:</span><br><span class="line">    app: grafana</span><br><span class="line">    component: core</span><br></pre></td></tr></table></figure></li></ul><h3 id="执行创建"><a href="#执行创建" class="headerlink" title="执行创建"></a>执行创建</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl apply -f ./</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 多执行几次</span></span><br></pre></td></tr></table></figure><h3 id="检查部署"><a href="#检查部署" class="headerlink" title="检查部署"></a>检查部署</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get pod,svc,deploy -n monitoring</span></span><br><span class="line">NAME                                READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod/grafana-core-7b5989cf9d-snbk5   1/1     Running   0          2m31s</span><br><span class="line">pod/node-exporter-dddv7             1/1     Running   0          12m</span><br><span class="line">pod/node-exporter-fhfp6             1/1     Running   0          12m</span><br><span class="line">pod/node-exporter-m46bf             1/1     Running   0          12m</span><br><span class="line">pod/node-exporter-xkrzp             1/1     Running   0          12m</span><br><span class="line">pod/node-exporter-zfcxh             1/1     Running   0          12m</span><br><span class="line">pod/prometheus-67bcf457db-999ns     1/1     Running   0          12m</span><br><span class="line"></span><br><span class="line">NAME                    TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">service/grafana         ClusterIP   10.254.95.151    &lt;none&gt;        3000/TCP         12m</span><br><span class="line">service/node-exporter   ClusterIP   10.254.114.12    &lt;none&gt;        9100/TCP         12m</span><br><span class="line">service/prometheus      ClusterIP   10.254.104.216   &lt;none&gt;        9090/TCP         12m</span><br><span class="line"></span><br><span class="line">NAME                                 READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.extensions/grafana-core   1/1     1            1           12m</span><br><span class="line">deployment.extensions/prometheus     1/1     1            1           12m</span><br></pre></td></tr></table></figure><h3 id="创建Ingress"><a href="#创建Ingress" class="headerlink" title="创建Ingress"></a>创建Ingress</h3><h4 id="prometheus-Ingress"><a href="#prometheus-Ingress" class="headerlink" title="prometheus Ingress"></a>prometheus Ingress</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat prometheus-Ingress.yaml </span></span><br><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: prometheus-web-ui</span><br><span class="line">  namespace: monitoring</span><br><span class="line">  annotations:</span><br><span class="line">    kubernetes.io/ingress.class: traefik</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: prometheus.xxlaila.cn</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: /</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: prometheus</span><br><span class="line">          servicePort: 9090</span><br></pre></td></tr></table></figure><h4 id="grafana-Ingress"><a href="#grafana-Ingress" class="headerlink" title="grafana Ingress"></a>grafana Ingress</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat grafana-Ingress.yaml </span></span><br><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: grafana-web-ui</span><br><span class="line">  namespace: monitoring</span><br><span class="line">  annotations:</span><br><span class="line">    kubernetes.io/ingress.class: traefik</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: grafana.xxlaila.cn</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: /</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: grafana</span><br><span class="line">          servicePort: 3000</span><br></pre></td></tr></table></figure><h4 id="执行创建-1"><a href="#执行创建-1" class="headerlink" title="执行创建"></a>执行创建</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl apply -f prometheus-Ingress.yaml </span></span><br><span class="line">ingress.extensions/prometheus-web-ui created</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f grafana-Ingress.yaml </span></span><br><span class="line">ingress.extensions/grafana-web-ui created</span><br></pre></td></tr></table></figure><p>在浏览器输入prometheus.xxlaila.cn访问prometheus，输入grafana.xxlaila.cn访问grafana。</p><h3 id="访问prometheus"><a href="#访问prometheus" class="headerlink" title="访问prometheus"></a>访问prometheus</h3><p><img src="http://zxc.kingxunlian.com/1569218750254.jpg" alt="img"></p><h3 id="配置grafana"><a href="#配置grafana" class="headerlink" title="配置grafana"></a>配置grafana</h3><p><img src="http://zxc.kingxunlian.com/1568968344227.jpg" alt="img"></p><p>到grafana的官方下载对应的模版文件导入，就可以出图啦<br><img src="http://zxc.kingxunlian.com/1568968420655.jpg" alt="img"></p><p><a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/prometheus" target="_blank" rel="noopener">后续利用pvc</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Prometheus、Grafana-部署&quot;&gt;&lt;a href=&quot;#Prometheus、Grafana-部署&quot; class=&quot;headerlink&quot; title=&quot;Prometheus、Grafana 部署&quot;&gt;&lt;/a&gt;Prometheus、Grafana 部署&lt;/h1&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Grafana是一个开源的度量分析与可视化套件。经常被用作基础设施的时间序列数据和应用程序分析的可视化，我们这里用它来做Kubernetes集群监控数据的可视化。&lt;/p&gt;
&lt;h3 id=&quot;准备工作&quot;&gt;&lt;a href=&quot;#准备工作&quot; class=&quot;headerlink&quot; title=&quot;准备工作&quot;&gt;&lt;/a&gt;准备工作&lt;/h3&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;截至当前，prometheus、grafana均采用最新的镜像包，在在第一次部署的时候grafana报了一个错误&lt;code&gt;mkdir: cannot create directory &amp;#39;/var/lib/grafana/plugins&amp;#39;: No such file or directory&lt;/code&gt;,这是因为Grafana启动使用的用户和用户组都是472，造成对外挂存储没有权限。&lt;a href=&quot;https://grafana.com/docs/installation/docker/#migration-from-a-previous-version-of-the-docker-container-to-5-1-or-later&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;参考官方&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="kubenertes" scheme="https://xxlaila.github.io/categories/kubenertes/"/>
    
    
      <category term="k8s v1.14,prometheus" scheme="https://xxlaila.github.io/tags/k8s-v1-14-prometheus/"/>
    
  </entry>
  
  <entry>
    <title>k8s v1.14 weave-scope</title>
    <link href="https://xxlaila.github.io/2019/09/20/k8s-v1-14-weave-scope/"/>
    <id>https://xxlaila.github.io/2019/09/20/k8s-v1-14-weave-scope/</id>
    <published>2019-09-20T03:49:59.000Z</published>
    <updated>2019-09-20T06:24:24.126Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前沿"><a href="#前沿" class="headerlink" title="前沿"></a>前沿</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Kubernetes 集群并部署容器化应用只是第一步。一旦集群运行起来，我们需要确保一起正常，所有必要组件就位并各司其职，有足够的资源满足应用的需求。Kubernetes 是一个复杂系统，运维团队需要有一套工具帮助他们获知集群的实时状态，并为故障排查提供及时和准确的数据支持。</p><h3 id="weave-scope-介绍"><a href="#weave-scope-介绍" class="headerlink" title="weave scope 介绍"></a>weave scope 介绍</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Weave Scope是Docker和Kubernetes的可视化和监控工具。它提供了一个自上而下的应用程序以及整个基础架构视图，并允许您在部署到云提供商时实时诊断分布式容器化应用程序的任何问题。</p><a id="more"></a><h3 id="功能介绍"><a href="#功能介绍" class="headerlink" title="功能介绍"></a>功能介绍</h3><ul><li>pod拓扑映射</li><li>图形或表格模式</li><li>灵活过滤</li><li>强大的搜索功能</li><li>实时应用和容器指标</li><li>排除故障并管理容器</li><li>使用Plugin API生成自定义指标</li></ul><p><a href="https://www.weave.works/docs/scope/latest/features/" target="_blank" rel="noopener">介绍参考</a></p><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>在 K8s 集群中安装 Scope 的方法很简单，使用下面的命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl apply -f "https://cloud.weave.works/k8s/scope.yaml?k8s-version=$(kubectl version | base64 | tr -d '\n')"</span></span><br><span class="line">namespace/weave created</span><br><span class="line">serviceaccount/weave-scope created</span><br><span class="line">clusterrole.rbac.authorization.k8s.io/weave-scope created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/weave-scope created</span><br><span class="line">deployment.apps/weave-scope-app created</span><br><span class="line">service/weave-scope-app created</span><br><span class="line">deployment.apps/weave-scope-cluster-agent created</span><br><span class="line">daemonset.extensions/weave-scope-agent created</span><br></pre></td></tr></table></figure><h3 id="检查"><a href="#检查" class="headerlink" title="检查"></a>检查</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get pod,svc,deploy -n weave</span></span><br><span class="line">NAME                                             READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod/weave-scope-agent-2t4m5                      1/1     Running   0          15m</span><br><span class="line">pod/weave-scope-agent-6tfp5                      1/1     Running   0          15m</span><br><span class="line">pod/weave-scope-agent-fxj5f                      1/1     Running   0          15m</span><br><span class="line">pod/weave-scope-agent-gkxc6                      1/1     Running   0          15m</span><br><span class="line">pod/weave-scope-agent-qnbbv                      1/1     Running   0          15m</span><br><span class="line">pod/weave-scope-app-b99fb9585-wld6n              1/1     Running   0          15m</span><br><span class="line">pod/weave-scope-cluster-agent-77bc946585-8fcjj   1/1     Running   0          15m</span><br><span class="line"></span><br><span class="line">NAME                      TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">service/weave-scope-app   ClusterIP   10.254.184.106   &lt;none&gt;        80/TCP    15m</span><br><span class="line"></span><br><span class="line">NAME                                              READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.extensions/weave-scope-app             1/1     1            1           15m</span><br><span class="line">deployment.extensions/weave-scope-cluster-agent   1/1     1            1           15m</span><br></pre></td></tr></table></figure><h3 id="创建weave-scope-ingress"><a href="#创建weave-scope-ingress" class="headerlink" title="创建weave-scope ingress"></a>创建weave-scope ingress</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat weave-scope.yaml </span></span><br><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: weave-web-ui</span><br><span class="line">  namespace: weave</span><br><span class="line">  annotations:</span><br><span class="line">    kubernetes.io/ingress.class: traefik</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: weave-scope.xxlaila.cn</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: /</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: weave-scope-app</span><br><span class="line">          servicePort: 80</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f weave-scope.yaml </span></span><br><span class="line">ingress.extensions/weave-web-ui created</span><br></pre></td></tr></table></figure><p>在浏览输入<code>weave-scope.xxlaila.cn</code>即可访问<br><img src="http://zxc.kingxunlian.com/1568958836846.jpg" alt="img"></p><h4 id="拓扑结构"><a href="#拓扑结构" class="headerlink" title="拓扑结构"></a>拓扑结构</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Scope 会自动构建应用和集群的逻辑拓扑。比如点击顶部 Pods，会显示所有 Pod 以及 Pod 之间的依赖关系<br><img src="http://zxc.kingxunlian.com/1568958666089.jpg" alt="img"><br>点击 Hosts，会显示各个节点之间的关系，可以在 Scope 中查看资源的 CPU 和内存使用情况。<br><img src="http://zxc.kingxunlian.com/1568958913275.jpg" alt="img"></p><h3 id="在线操作"><a href="#在线操作" class="headerlink" title="在线操作"></a>在线操作</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Scope 还提供了便捷的在线操作功能，比如选中某个 Host，点击 &gt;_按钮可以直接在浏览器中打开节点的命令行终端：<br><img src="http://zxc.kingxunlian.com/1568959004395.jpg" alt="img"></p><ul><li><p>点击 Deployment 的 + 可以执行新增一个pod实列<br><img src="http://zxc.kingxunlian.com/1568959269040.jpg" alt="img"></p></li><li><p>查看pod的日志<br><img src="http://zxc.kingxunlian.com/1568959359334.jpg" alt="img"></p></li><li><p>attach、restart、stop 容器，以及直接在 Scope 中排查问题<br><img src="http://zxc.kingxunlian.com/1568959467442.jpg" alt="img"></p></li></ul><p>更多功呢个请<a href="https://www.weave.works/docs/scope/latest/plugins/" target="_blank" rel="noopener">参考官方</a>,或者实操</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;前沿&quot;&gt;&lt;a href=&quot;#前沿&quot; class=&quot;headerlink&quot; title=&quot;前沿&quot;&gt;&lt;/a&gt;前沿&lt;/h3&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Kubernetes 集群并部署容器化应用只是第一步。一旦集群运行起来，我们需要确保一起正常，所有必要组件就位并各司其职，有足够的资源满足应用的需求。Kubernetes 是一个复杂系统，运维团队需要有一套工具帮助他们获知集群的实时状态，并为故障排查提供及时和准确的数据支持。&lt;/p&gt;
&lt;h3 id=&quot;weave-scope-介绍&quot;&gt;&lt;a href=&quot;#weave-scope-介绍&quot; class=&quot;headerlink&quot; title=&quot;weave scope 介绍&quot;&gt;&lt;/a&gt;weave scope 介绍&lt;/h3&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Weave Scope是Docker和Kubernetes的可视化和监控工具。它提供了一个自上而下的应用程序以及整个基础架构视图，并允许您在部署到云提供商时实时诊断分布式容器化应用程序的任何问题。&lt;/p&gt;
    
    </summary>
    
      <category term="kubernetes" scheme="https://xxlaila.github.io/categories/kubernetes/"/>
    
    
      <category term="k8s v1.14, weave-scope" scheme="https://xxlaila.github.io/tags/k8s-v1-14-weave-scope/"/>
    
  </entry>
  
  <entry>
    <title>k8s v1.14 traefik部署</title>
    <link href="https://xxlaila.github.io/2019/09/20/k8s-v1-14-traefik%E9%83%A8%E7%BD%B2/"/>
    <id>https://xxlaila.github.io/2019/09/20/k8s-v1-14-traefik部署/</id>
    <published>2019-09-20T01:20:25.000Z</published>
    <updated>2019-09-20T06:27:47.673Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;traefix 前篇是可以使用，这里k8s v1.14 之前的拿来用不上，然后折腾了一下，参考官方的折腾起来了</p><h3 id="基于角色的访问控制配置（仅限Kubernetes-1-6-）"><a href="#基于角色的访问控制配置（仅限Kubernetes-1-6-）" class="headerlink" title="基于角色的访问控制配置（仅限Kubernetes 1.6+）"></a>基于角色的访问控制配置（仅限Kubernetes 1.6+）</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Kubernetes在1.6+中引入了基于角色的访问控制（RBAC），以允许对Kubernetes资源和API进行细粒度控制。群集配置了RBAC，则需要授权Traefik使用Kubernetes API。有两种方法可以设置适当的权限：通过特定于命名空间的RoleBindings或单个全局ClusterRoleBinding。</p><a id="more"></a><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;每个命名空间的RoleBinding可以限制授予权限，只有Traefik正在监视的名称空间才能使用，从而遵循最小权限原则。如果Traefik不应该监视所有名称空间，并且名称空间集不会动态更改，那么这是首选方法。否则，必须使用单个ClusterRoleBinding。</p><p><a href="https://xxlaila.github.io/2019/09/05/traefik-ingress%E4%BD%BF%E7%94%A8/">traefik学习</a><br><a href="https://docs.traefik.io/v1.7/user-guide/kubernetes/" target="_blank" rel="noopener">traefik官方</a></p><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><p>下载trarfix代码，然后切换到v1.7的分支</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># git clone https://github.com/containous/traefik.git</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># git branch --all</span></span><br><span class="line">* master</span><br><span class="line">  remotes/origin/HEAD -&gt; origin/master</span><br><span class="line">  remotes/origin/add-plugin-support</span><br><span class="line">  remotes/origin/gh-pages</span><br><span class="line">  remotes/origin/master</span><br><span class="line">  remotes/origin/v1.0</span><br><span class="line">  remotes/origin/v1.1</span><br><span class="line">  remotes/origin/v1.2</span><br><span class="line">  remotes/origin/v1.3</span><br><span class="line">  remotes/origin/v1.4</span><br><span class="line">  remotes/origin/v1.5</span><br><span class="line">  remotes/origin/v1.6</span><br><span class="line">  remotes/origin/v1.7</span><br><span class="line">  remotes/origin/v2.0</span><br><span class="line"></span><br><span class="line"><span class="comment"># git checkout v1.7</span></span><br><span class="line">Branch <span class="string">'v1.7'</span> <span class="built_in">set</span> up to track remote branch <span class="string">'v1.7'</span> from <span class="string">'origin'</span>.</span><br><span class="line">Switched to a new branch <span class="string">'v1.7'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># /root/traefik/examples/k8s</span></span><br></pre></td></tr></table></figure><h3 id="安装部署"><a href="#安装部署" class="headerlink" title="安装部署"></a>安装部署</h3><h4 id="使用ClusterRoleBinding"><a href="#使用ClusterRoleBinding" class="headerlink" title="使用ClusterRoleBinding"></a>使用ClusterRoleBinding</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl apply -f traefik-rbac.yaml </span></span><br><span class="line">clusterrole.rbac.authorization.k8s.io/traefik-ingress-controller created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/traefik-ingress-controller created</span><br></pre></td></tr></table></figure><p>对于命名空间限制，每个监视命名空间需要一个RoleBinding以及Traefik kubernetes.namespaces参数的相应配置。</p><h4 id="使用Deployments部署或部署DaemonSet"><a href="#使用Deployments部署或部署DaemonSet" class="headerlink" title="使用Deployments部署或部署DaemonSet"></a>使用Deployments部署或部署DaemonSet</h4><p>可以将Traefik与Deployment或DaemonSet对象一起使用，而这两个选项各有利弊：</p><ul><li>使用部署时，可伸缩性可以更好，因为在使用DaemonSet时您将拥有每个节点的Single-Pod模型，而在使用部署时，可能需要更少的基于环境的副本。</li><li>当节点加入群集时，DaemonSet会自动扩展到新节点，而部署窗格仅在需要时在新节点上进行调度。</li><li>DaemonSets确保只有一个pod副本在任何单个节点上运行。如果要确保两个pod不在同一节点上，则部署需要关联设置</li><li>可以使用该NET_BIND_SERVICE功能运行DaemonSet ，这将允许它绑定到每个主机上的端口80/443 / etc。这将允许绕过kube-proxy，并减少流量跳跃。请注意，这违反了Kubernetes最佳实践指南，并提出了调度/扩展问题的可能性。尽管存在潜在问题，但这仍然是大多数入口控制器的选择。</li></ul><h4 id="Deployments部署"><a href="#Deployments部署" class="headerlink" title="Deployments部署"></a>Deployments部署</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl  apply -f  traefik-deployment.yaml</span></span><br><span class="line">serviceaccount/traefik-ingress-controller created</span><br><span class="line">deployment.extensions/traefik-ingress-controller created</span><br><span class="line">service/traefik-ingress-service created</span><br></pre></td></tr></table></figure><h4 id="DaemonSets-部署-可选"><a href="#DaemonSets-部署-可选" class="headerlink" title="DaemonSets 部署(可选)"></a>DaemonSets 部署(可选)</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl apply -f traefik-ds.yaml</span></span><br></pre></td></tr></table></figure><ul><li>Deployments和DaemonSets之间存在一些显着差异:<ul><li>部署具有更容易的向上和向下扩展可能性。它可以实现完整的pod生命周期，并支持Kubernetes 1.2的滚动更新。运行部署至少需要一个Pod。</li><li>DaemonSet会自动扩展到满足特定选择器的所有节点，并保证一次填充一个节点。Kubernetes 1.7也完全支持滚动更新，适用于DaemonSets</li></ul></li></ul><h3 id="检查部署"><a href="#检查部署" class="headerlink" title="检查部署"></a>检查部署</h3><ul><li><p>查看pod</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl --namespace=kube-system get pods</span></span><br><span class="line">NAME                                         READY   STATUS    RESTARTS   AGE</span><br><span class="line">coredns-5579b8778b-xw8m9                     1/1     Running   2          3d21h</span><br><span class="line">kubernetes-dashboard-65dfbf6f4f-hcgbb        1/1     Running   0          2d16h</span><br><span class="line">metrics-server-94ff5d4cc-b97l5               1/1     Running   1          3d</span><br><span class="line">tiller-deploy-5cbcf75545-rbzld               1/1     Running   0          17h</span><br><span class="line">traefik-ingress-controller-c595665d6-cm7kh   1/1     Running   0          3m20s</span><br></pre></td></tr></table></figure></li><li><p>查看services</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get services --namespace=kube-system</span></span><br><span class="line">NAME                      TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                       AGE</span><br><span class="line">kube-dns                  ClusterIP   10.254.0.2       &lt;none&gt;        53/UDP,53/TCP,9153/TCP        3d21h</span><br><span class="line">kubernetes-dashboard      NodePort    10.254.214.153   &lt;none&gt;        443:32533/TCP                 3d21h</span><br><span class="line">metrics-server            ClusterIP   10.254.61.132    &lt;none&gt;        443/TCP                       3d</span><br><span class="line">tiller-deploy             ClusterIP   10.254.207.227   &lt;none&gt;        44134/TCP                     17h</span><br><span class="line">traefik-ingress-service   NodePort    10.254.246.158   &lt;none&gt;        80:32146/TCP,8080:30455/TCP   3m53s</span><br></pre></td></tr></table></figure></li></ul><p>这里使用的是nodeport模式进行部署的，可以看到端口为32146，这里访问会返回<code>404 page not found</code>,那是因为我们还没有给Traefik任何配置。</p><h3 id="创建一个服务和一个将公开Traefik-Web-UI的Ingres"><a href="#创建一个服务和一个将公开Traefik-Web-UI的Ingres" class="headerlink" title="创建一个服务和一个将公开Traefik Web UI的Ingres"></a>创建一个服务和一个将公开Traefik Web UI的Ingres</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl apply -f ui.yaml </span></span><br><span class="line">service/traefik-web-ui created</span><br><span class="line">ingress.extensions/traefik-web-ui created</span><br></pre></td></tr></table></figure><p>在/etc/hosts 文件设置一个路由条目<code>traefik-ui.minikube</code></p><p>在浏览器进行访问可以看到Traefik Web UI</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;traefix 前篇是可以使用，这里k8s v1.14 之前的拿来用不上，然后折腾了一下，参考官方的折腾起来了&lt;/p&gt;
&lt;h3 id=&quot;基于角色的访问控制配置（仅限Kubernetes-1-6-）&quot;&gt;&lt;a href=&quot;#基于角色的访问控制配置（仅限Kubernetes-1-6-）&quot; class=&quot;headerlink&quot; title=&quot;基于角色的访问控制配置（仅限Kubernetes 1.6+）&quot;&gt;&lt;/a&gt;基于角色的访问控制配置（仅限Kubernetes 1.6+）&lt;/h3&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Kubernetes在1.6+中引入了基于角色的访问控制（RBAC），以允许对Kubernetes资源和API进行细粒度控制。群集配置了RBAC，则需要授权Traefik使用Kubernetes API。有两种方法可以设置适当的权限：通过特定于命名空间的RoleBindings或单个全局ClusterRoleBinding。&lt;/p&gt;
    
    </summary>
    
      <category term="kubernetes" scheme="https://xxlaila.github.io/categories/kubernetes/"/>
    
    
      <category term="k8s v1.14, traefik" scheme="https://xxlaila.github.io/tags/k8s-v1-14-traefik/"/>
    
  </entry>
  
  <entry>
    <title>k8s v1.14 metrics-server</title>
    <link href="https://xxlaila.github.io/2019/09/17/k8s-v1-14-metrics-server/"/>
    <id>https://xxlaila.github.io/2019/09/17/k8s-v1-14-metrics-server/</id>
    <published>2019-09-17T01:06:18.000Z</published>
    <updated>2019-09-20T05:47:25.885Z</updated>
    
    <content type="html"><![CDATA[<p>metrics-server这里不详细介绍，可以参考<a href="https://xxlaila.github.io/2019/09/04/metrics-server安装季/">metrics-server安装季</a></p><h3 id="安装metrics-server"><a href="#安装metrics-server" class="headerlink" title="安装metrics-server"></a>安装metrics-server</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里安装和之前的<strong>metrics-server安装季</strong>稍微有点不一样，之前集群安装没有使用https证书，后面去各种生成的证书和踩坑，这里是在安装的时候一开始就使用了https全证书,所有稍微有一点区别，这里只列出有区别的地方，其他的完全可以参考<a href="https://xxlaila.github.io/2019/09/04/metrics-server安装季/">metrics-server安装季</a>，这里https证书<strong>不需要</strong>重新生成；</p><a id="more"></a><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;配置文件也不需要添加，在v1.14安装的时候就已经吧配置文件添加进去了，所以这里配置文件也不需要增加</p><h3 id="文件的修改"><a href="#文件的修改" class="headerlink" title="文件的修改"></a>文件的修改</h3><ul><li><p>修改 metrics-server-deployment.yaml</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat metrics-server-deployment.yaml</span></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: metrics-server</span><br><span class="line">  namespace: kube-system</span><br><span class="line">---</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: metrics-server</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: metrics-server</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      k8s-app: metrics-server</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: metrics-server</span><br><span class="line">      labels:</span><br><span class="line">        k8s-app: metrics-server</span><br><span class="line">    spec:</span><br><span class="line">      serviceAccountName: metrics-server</span><br><span class="line">      hostNetwork: <span class="literal">true</span> 这个还是需要增加</span><br><span class="line">      volumes:</span><br><span class="line">      <span class="comment"># mount in tmp so we can safely use from-scratch images and/or read-only containers</span></span><br><span class="line">      - name: tmp-dir</span><br><span class="line">        emptyDir: &#123;&#125;</span><br><span class="line">      containers:</span><br><span class="line">      - name: metrics-server</span><br><span class="line">        image: mirrorgooglecontainers/metrics-server-amd64:v0.3.4</span><br><span class="line">        imagePullPolicy: Always</span><br><span class="line">        args:  <span class="comment"># 这里不一样</span></span><br><span class="line">        - --metric-resolution=30s</span><br><span class="line">        - --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: tmp-dir</span><br><span class="line">          mountPath: /tmp</span><br></pre></td></tr></table></figure></li><li><p>–metric-resolution=30s：从 kubelet 采集数据的周期；</p></li><li><p>–kubelet-preferred-address-types：优先使用 InternalIP 来访问 kubelet，这样可以避免节点名称没有 DNS 解析记录时，通过节点名称调用节点 kubelet API 失败的情况（未配置时默认的情况）；</p></li><li><p><strong>hostNetwork: true:</strong> 这个不增加的会提示：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Error from server (ServiceUnavailable): the server is currently unable to handle the request</span><br></pre></td></tr></table></figure></li><li><p>修改 resource-reader.yaml</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat resource-reader.yaml </span></span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRole</span><br><span class="line">metadata:</span><br><span class="line">  name: system:metrics-server</span><br><span class="line">rules:</span><br><span class="line">- apiGroups:</span><br><span class="line">  - <span class="string">""</span></span><br><span class="line">  resources:</span><br><span class="line">  - pods</span><br><span class="line">  - nodes</span><br><span class="line">  - nodes/stats</span><br><span class="line">  - namespaces</span><br><span class="line">  verbs:</span><br><span class="line">  - get</span><br><span class="line">  - list</span><br><span class="line">  - watch</span><br><span class="line">- apiGroups: <span class="comment"># 增加</span></span><br><span class="line">  - <span class="string">"extensions"</span></span><br><span class="line">  resources:</span><br><span class="line">  - deployments</span><br><span class="line">  verbs:</span><br><span class="line">  - get</span><br><span class="line">  - list</span><br><span class="line">  - update</span><br><span class="line">  - watch</span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: system:metrics-server</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: system:metrics-server</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: metrics-server</span><br><span class="line">  namespace: kube-system</span><br></pre></td></tr></table></figure></li></ul><h3 id="执行创建"><a href="#执行创建" class="headerlink" title="执行创建"></a>执行创建</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl apply -f ./</span></span><br></pre></td></tr></table></figure><h3 id="查看运行情况"><a href="#查看运行情况" class="headerlink" title="查看运行情况"></a>查看运行情况</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl -n kube-system get pods -l k8s-app=metrics-server</span></span><br><span class="line">NAME                             READY   STATUS    RESTARTS   AGE</span><br><span class="line">metrics-server-94ff5d4cc-b97l5   1/1     Running   0          21m</span><br><span class="line"></span><br><span class="line"><span class="comment">#  kubectl get svc -n kube-system  metrics-server</span></span><br><span class="line">NAME             TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">metrics-server   ClusterIP   10.254.61.132   &lt;none&gt;        443/TCP   27m</span><br></pre></td></tr></table></figure><h3 id="获取v1beta1-metrics-k8s-io并验证"><a href="#获取v1beta1-metrics-k8s-io并验证" class="headerlink" title="获取v1beta1.metrics.k8s.io并验证"></a>获取v1beta1.metrics.k8s.io并验证</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get apiservice</span></span><br><span class="line">NAME                                   SERVICE                      AVAILABLE   AGE</span><br><span class="line">v1.                                    Local                        True        23h</span><br><span class="line">v1.apps                                Local                        True        23h</span><br><span class="line">v1.authentication.k8s.io               Local                        True        23h</span><br><span class="line">v1.authorization.k8s.io                Local                        True        23h</span><br><span class="line">v1.autoscaling                         Local                        True        23h</span><br><span class="line">v1.batch                               Local                        True        23h</span><br><span class="line">v1.coordination.k8s.io                 Local                        True        23h</span><br><span class="line">v1.networking.k8s.io                   Local                        True        23h</span><br><span class="line">v1.rbac.authorization.k8s.io           Local                        True        23h</span><br><span class="line">v1.scheduling.k8s.io                   Local                        True        23h</span><br><span class="line">v1.storage.k8s.io                      Local                        True        23h</span><br><span class="line">v1alpha1.auditregistration.k8s.io      Local                        True        23h</span><br><span class="line">v1alpha1.node.k8s.io                   Local                        True        23h</span><br><span class="line">v1alpha1.rbac.authorization.k8s.io     Local                        True        23h</span><br><span class="line">v1alpha1.scheduling.k8s.io             Local                        True        23h</span><br><span class="line">v1alpha1.settings.k8s.io               Local                        True        23h</span><br><span class="line">v1alpha1.storage.k8s.io                Local                        True        23h</span><br><span class="line">v1beta1.admissionregistration.k8s.io   Local                        True        23h</span><br><span class="line">v1beta1.apiextensions.k8s.io           Local                        True        23h</span><br><span class="line">v1beta1.apps                           Local                        True        23h</span><br><span class="line">v1beta1.authentication.k8s.io          Local                        True        23h</span><br><span class="line">v1beta1.authorization.k8s.io           Local                        True        23h</span><br><span class="line">v1beta1.batch                          Local                        True        23h</span><br><span class="line">v1beta1.certificates.k8s.io            Local                        True        23h</span><br><span class="line">v1beta1.coordination.k8s.io            Local                        True        23h</span><br><span class="line">v1beta1.events.k8s.io                  Local                        True        23h</span><br><span class="line">v1beta1.extensions                     Local                        True        23h</span><br><span class="line">v1beta1.metrics.k8s.io                 kube-system/metrics-server   True        27m</span><br><span class="line">v1beta1.networking.k8s.io              Local                        True        23h</span><br><span class="line">v1beta1.node.k8s.io                    Local                        True        23h</span><br><span class="line">v1beta1.policy                         Local                        True        23h</span><br><span class="line">v1beta1.rbac.authorization.k8s.io      Local                        True        23h</span><br><span class="line">v1beta1.scheduling.k8s.io              Local                        True        23h</span><br><span class="line">v1beta1.storage.k8s.io                 Local                        True        23h</span><br><span class="line">v1beta2.apps                           Local                        True        23h</span><br><span class="line">v2alpha1.batch                         Local                        True        23h</span><br><span class="line">v2beta1.autoscaling                    Local                        True        23h</span><br><span class="line">v2beta2.autoscaling                    Local                        True        23h</span><br></pre></td></tr></table></figure><h3 id="metrics-server-的命令行参数"><a href="#metrics-server-的命令行参数" class="headerlink" title="metrics-server 的命令行参数"></a>metrics-server 的命令行参数</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl exec --namespace kube-system -it metrics-server-94ff5d4cc-b97l5 -- /metrics-server --help</span></span><br><span class="line">Launch metrics-server</span><br><span class="line"></span><br><span class="line">Usage:</span><br><span class="line">   [flags]</span><br><span class="line"></span><br><span class="line">Flags:</span><br><span class="line">      --alsologtostderr                                         <span class="built_in">log</span> to standard error as well as files</span><br><span class="line">      --authentication-kubeconfig string                        kubeconfig file pointing at the <span class="string">'core'</span> kubernetes server with enough rights to create tokenaccessreviews.authentication.k8s.io.</span><br><span class="line">      --authentication-skip-lookup                              If <span class="literal">false</span>, the authentication-kubeconfig will be used to lookup missing authentication configuration from the cluster.</span><br><span class="line">      --authentication-token-webhook-cache-ttl duration         The duration to cache responses from the webhook token authenticator. (default 10s)</span><br><span class="line">      --authentication-tolerate-lookup-failure                  If <span class="literal">true</span>, failures to look up missing authentication configuration from the cluster are not considered fatal. Note that this can result <span class="keyword">in</span> authentication that treats all requests as anonymous.</span><br><span class="line">      --authorization-always-allow-paths strings                A list of HTTP paths to skip during authorization, i.e. these are authorized without contacting the <span class="string">'core'</span> kubernetes server.</span><br><span class="line">      --authorization-kubeconfig string                         kubeconfig file pointing at the <span class="string">'core'</span> kubernetes server with enough rights to create subjectaccessreviews.authorization.k8s.io.</span><br><span class="line">      --authorization-webhook-cache-authorized-ttl duration     The duration to cache <span class="string">'authorized'</span> responses from the webhook authorizer. (default 10s)</span><br><span class="line">      --authorization-webhook-cache-unauthorized-ttl duration   The duration to cache <span class="string">'unauthorized'</span> responses from the webhook authorizer. (default 10s)</span><br><span class="line">      --<span class="built_in">bind</span>-address ip                                         The IP address on <span class="built_in">which</span> to listen <span class="keyword">for</span> the --secure-port port. The associated interface(s) must be reachable by the rest of the cluster, and by CLI/web clients. If blank, all interfaces will be used (0.0.0.0 <span class="keyword">for</span> all IPv4 interfaces and :: <span class="keyword">for</span> all IPv6 interfaces). (default 0.0.0.0)</span><br><span class="line">      --cert-dir string                                         The directory <span class="built_in">where</span> the TLS certs are located. If --tls-cert-file and --tls-private-key-file are provided, this flag will be ignored. (default <span class="string">"apiserver.local.config/certificates"</span>)</span><br><span class="line">      --client-ca-file string                                   If <span class="built_in">set</span>, any request presenting a client certificate signed by one of the authorities <span class="keyword">in</span> the client-ca-file is authenticated with an identity corresponding to the CommonName of the client certificate.</span><br><span class="line">      --contention-profiling                                    Enable lock contention profiling, <span class="keyword">if</span> profiling is enabled</span><br><span class="line">  -h, --<span class="built_in">help</span>                                                    <span class="built_in">help</span> <span class="keyword">for</span> this <span class="built_in">command</span></span><br><span class="line">      --http2-max-streams-per-connection int                    The <span class="built_in">limit</span> that the server gives to clients <span class="keyword">for</span> the maximum number of streams <span class="keyword">in</span> an HTTP/2 connection. Zero means to use golang<span class="string">'s default.</span></span><br><span class="line"><span class="string">      --kubeconfig string                                       The path to the kubeconfig used to connect to the Kubernetes API server and the Kubelets (defaults to in-cluster config)</span></span><br><span class="line"><span class="string">      --kubelet-certificate-authority string                    Path to the CA to use to validate the Kubelet'</span>s serving certificates.</span><br><span class="line">      --kubelet-insecure-tls                                    Do not verify CA of serving certificates presented by Kubelets.  For testing purposes only.</span><br><span class="line">      --kubelet-port int                                        The port to use to connect to Kubelets. (default 10250)</span><br><span class="line">      --kubelet-preferred-address-types strings                 The priority of node address types to use when determining <span class="built_in">which</span> address to use to connect to a particular node (default [Hostname,InternalDNS,InternalIP,ExternalDNS,ExternalIP])</span><br><span class="line">      --<span class="built_in">log</span>-flush-frequency duration                            Maximum number of seconds between <span class="built_in">log</span> flushes (default 5s)</span><br><span class="line">      --log_backtrace_at traceLocation                          when logging hits line file:N, emit a stack trace (default :0)</span><br><span class="line">      --log_dir string                                          If non-empty, write <span class="built_in">log</span> files <span class="keyword">in</span> this directory</span><br><span class="line">      --log_file string                                         If non-empty, use this <span class="built_in">log</span> file</span><br><span class="line">      --logtostderr                                             <span class="built_in">log</span> to standard error instead of files (default <span class="literal">true</span>)</span><br><span class="line">      --metric-resolution duration                              The resolution at <span class="built_in">which</span> metrics-server will retain metrics. (default 1m0s)</span><br><span class="line">      --profiling                                               Enable profiling via web interface host:port/debug/pprof/ (default <span class="literal">true</span>)</span><br><span class="line">      --requestheader-allowed-names strings                     List of client certificate common names to allow to provide usernames <span class="keyword">in</span> headers specified by --requestheader-username-headers. If empty, any client certificate validated by the authorities <span class="keyword">in</span> --requestheader-client-ca-file is allowed.</span><br><span class="line">      --requestheader-client-ca-file string                     Root certificate bundle to use to verify client certificates on incoming requests before trusting usernames <span class="keyword">in</span> headers specified by --requestheader-username-headers. WARNING: generally <span class="keyword">do</span> not depend on authorization being already <span class="keyword">done</span> <span class="keyword">for</span> incoming requests.</span><br><span class="line">      --requestheader-extra-headers-prefix strings              List of request header prefixes to inspect. X-Remote-Extra- is suggested. (default [x-remote-extra-])</span><br><span class="line">      --requestheader-group-headers strings                     List of request headers to inspect <span class="keyword">for</span> groups. X-Remote-Group is suggested. (default [x-remote-group])</span><br><span class="line">      --requestheader-username-headers strings                  List of request headers to inspect <span class="keyword">for</span> usernames. X-Remote-User is common. (default [x-remote-user])</span><br><span class="line">      --secure-port int                                         The port on <span class="built_in">which</span> to serve HTTPS with authentication and authorization.If 0, don<span class="string">'t serve HTTPS at all. (default 443)</span></span><br><span class="line"><span class="string">      --skip_headers                                            If true, avoid header prefixes in the log messages</span></span><br><span class="line"><span class="string">      --stderrthreshold severity                                logs at or above this threshold go to stderr</span></span><br><span class="line"><span class="string">      --tls-cert-file string                                    File containing the default x509 Certificate for HTTPS. (CA cert, if any, concatenated after server cert). If HTTPS serving is enabled, and --tls-cert-file and --tls-private-key-file are not provided, a self-signed certificate and key are generated for the public address and saved to the directory specified by --cert-dir.</span></span><br><span class="line"><span class="string">      --tls-cipher-suites strings                               Comma-separated list of cipher suites for the server. If omitted, the default Go cipher suites will be use.  Possible values: TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_RC4_128_SHA,TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_RC4_128_SHA,TLS_RSA_WITH_3DES_EDE_CBC_SHA,TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_RSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_256_CBC_SHA,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_RC4_128_SHA</span></span><br><span class="line"><span class="string">      --tls-min-version string                                  Minimum TLS version supported. Possible values: VersionTLS10, VersionTLS11, VersionTLS12</span></span><br><span class="line"><span class="string">      --tls-private-key-file string                             File containing the default x509 private key matching --tls-cert-file.</span></span><br><span class="line"><span class="string">      --tls-sni-cert-key namedCertKey                           A pair of x509 certificate and private key file paths, optionally suffixed with a list of domain patterns which are fully qualified domain names, possibly with prefixed wildcard segments. If no domain patterns are provided, the names of the certificate are extracted. Non-wildcard matches trump over wildcard matches, explicit domain patterns trump over extracted names. For multiple key/certificate pairs, use the --tls-sni-cert-key multiple times. Examples: "example.crt,example.key" or "foo.crt,foo.key:*.foo.com,foo.com". (default [])</span></span><br><span class="line"><span class="string">  -v, --v Level                                                 number for the log level verbosity</span></span><br><span class="line"><span class="string">      --vmodule moduleSpec                                      comma-separated list of pattern=N settings for file-filtered logging</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;metrics-server这里不详细介绍，可以参考&lt;a href=&quot;https://xxlaila.github.io/2019/09/04/metrics-server安装季/&quot;&gt;metrics-server安装季&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;安装metrics-server&quot;&gt;&lt;a href=&quot;#安装metrics-server&quot; class=&quot;headerlink&quot; title=&quot;安装metrics-server&quot;&gt;&lt;/a&gt;安装metrics-server&lt;/h3&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;这里安装和之前的&lt;strong&gt;metrics-server安装季&lt;/strong&gt;稍微有点不一样，之前集群安装没有使用https证书，后面去各种生成的证书和踩坑，这里是在安装的时候一开始就使用了https全证书,所有稍微有一点区别，这里只列出有区别的地方，其他的完全可以参考&lt;a href=&quot;https://xxlaila.github.io/2019/09/04/metrics-server安装季/&quot;&gt;metrics-server安装季&lt;/a&gt;，这里https证书&lt;strong&gt;不需要&lt;/strong&gt;重新生成；&lt;/p&gt;
    
    </summary>
    
      <category term="kubernetes" scheme="https://xxlaila.github.io/categories/kubernetes/"/>
    
    
      <category term="metrics-server" scheme="https://xxlaila.github.io/tags/metrics-server/"/>
    
  </entry>
  
  <entry>
    <title>k8s v1.14 dashboard</title>
    <link href="https://xxlaila.github.io/2019/09/16/k8s-v1-14-dashboard/"/>
    <id>https://xxlaila.github.io/2019/09/16/k8s-v1-14-dashboard/</id>
    <published>2019-09-16T09:46:10.000Z</published>
    <updated>2019-09-20T06:37:46.785Z</updated>
    
    <content type="html"><![CDATA[<p>kuberntes 自带插件的 manifests yaml 文件使用 gcr.io 的 docker registry，国内被墙，需要手动替换为其它 registry 地址</p><h3 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h3><p>将下载的 kubernetes-server-linux-amd64.tar.gz 解压后，再解压其中的 kubernetes-src.tar.gz 文件。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cd kubernetes</span></span><br><span class="line"><span class="comment"># tar -xzvf kubernetes-src.tar.gz</span></span><br></pre></td></tr></table></figure><p>dashboard 对应的目录是：cluster/addons/dashboard：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cd cluster/addons/dashboard</span></span><br></pre></td></tr></table></figure><p>修改 service 定义，指定端口类型为 NodePort，这样外界可以通过地址 NodeIP:NodePort 访问 dashboard；</p><a id="more"></a><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat dashboard-service.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: kubernetes-dashboard</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: kubernetes-dashboard</span><br><span class="line">    kubernetes.io/cluster-service: <span class="string">"true"</span></span><br><span class="line">    addonmanager.kubernetes.io/mode: Reconcile</span><br><span class="line">spec:</span><br><span class="line">  <span class="built_in">type</span>: NodePort <span class="comment"># 增加这一行</span></span><br><span class="line">  selector:</span><br><span class="line">    k8s-app: kubernetes-dashboard</span><br><span class="line">  ports:</span><br><span class="line">  - port: 443</span><br><span class="line">    targetPort: 8443</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat dashboard-controller.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: kubernetes-dashboard</span><br><span class="line">    addonmanager.kubernetes.io/mode: Reconcile</span><br><span class="line">  name: kubernetes-dashboard</span><br><span class="line">  namespace: kube-system</span><br><span class="line">---</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: kubernetes-dashboard</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: kubernetes-dashboard</span><br><span class="line">    kubernetes.io/cluster-service: <span class="string">"true"</span></span><br><span class="line">    addonmanager.kubernetes.io/mode: Reconcile</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      k8s-app: kubernetes-dashboard</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        k8s-app: kubernetes-dashboard</span><br><span class="line">      annotations:</span><br><span class="line">        scheduler.alpha.kubernetes.io/critical-pod: <span class="string">''</span></span><br><span class="line">        seccomp.security.alpha.kubernetes.io/pod: <span class="string">'docker/default'</span></span><br><span class="line">    spec:</span><br><span class="line">      priorityClassName: system-cluster-critical</span><br><span class="line">      containers:</span><br><span class="line">      - name: kubernetes-dashboard</span><br><span class="line">        image: docker.io/xxlaila/kubernetes-dashboard-amd64:v1.10.0  <span class="comment">#修改这一行</span></span><br><span class="line">        resources:</span><br><span class="line">          limits:</span><br><span class="line">            cpu: 100m</span><br><span class="line">            memory: 300Mi</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 50m</span><br><span class="line">            memory: 100Mi</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 8443</span><br><span class="line">          protocol: TCP</span><br></pre></td></tr></table></figure><h3 id="执行所有定义文件"><a href="#执行所有定义文件" class="headerlink" title="执行所有定义文件"></a>执行所有定义文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ls *.yaml</span></span><br><span class="line">dashboard-configmap.yaml  dashboard-controller.yaml  dashboard-rbac.yaml  dashboard-secret.yaml  dashboard-service.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f  .</span></span><br></pre></td></tr></table></figure><h3 id="查看分配的-NodePort"><a href="#查看分配的-NodePort" class="headerlink" title="查看分配的 NodePort"></a>查看分配的 NodePort</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get deployment kubernetes-dashboard  -n kube-system</span></span><br><span class="line">NAME                   READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">kubernetes-dashboard   1/1     1            1           5h10m</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl --namespace kube-system get pods -o wide</span></span><br><span class="line">NAME                                    READY   STATUS    RESTARTS   AGE     IP             NODE            NOMINATED NODE   READINESS GATES</span><br><span class="line">coredns-5579b8778b-xw8m9                1/1     Running   1          5h15m   172.30.232.3   172.21.16.204   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kubernetes-dashboard-6cc78dfc99-hb4l5   1/1     Running   0          5h10m   172.30.176.3   172.21.16.240   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl get services kubernetes-dashboard -n kube-system</span></span><br><span class="line">NAME                   TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)         AGE</span><br><span class="line">kubernetes-dashboard   NodePort   10.254.214.153   &lt;none&gt;        443:32533/TCP   5h10m</span><br></pre></td></tr></table></figure><ul><li>NodePort 32533 映射到 dashboard pod 443 端口；</li></ul><h3 id="查看-dashboard-支持的命令行参数"><a href="#查看-dashboard-支持的命令行参数" class="headerlink" title="查看 dashboard 支持的命令行参数"></a>查看 dashboard 支持的命令行参数</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl exec --namespace kube-system -it kubernetes-dashboard-6cc78dfc99-hb4l5  -- /dashboard --help</span></span><br><span class="line">2019/09/16 09:51:33 Starting overwatch</span><br><span class="line">Usage of /dashboard:</span><br><span class="line">      --alsologtostderr                  <span class="built_in">log</span> to standard error as well as files</span><br><span class="line">      --api-log-level string             Level of API request logging. Should be one of <span class="string">'INFO|NONE|DEBUG'</span>. Default: <span class="string">'INFO'</span>. (default <span class="string">"INFO"</span>)</span><br><span class="line">      --apiserver-host string            The address of the Kubernetes Apiserver to connect to <span class="keyword">in</span> the format of protocol://address:port, e.g., http://localhost:8080. If not specified, the assumption is that the binary runs inside a Kubernetes cluster and <span class="built_in">local</span> discovery is attempted.</span><br><span class="line">      --authentication-mode strings      Enables authentication options that will be reflected on login screen. Supported values: token, basic. Default: token.Note that basic option should only be used <span class="keyword">if</span> apiserver has <span class="string">'--authorization-mode=ABAC'</span> and <span class="string">'--basic-auth-file'</span> flags <span class="built_in">set</span>. (default [token])</span><br><span class="line">      --auto-generate-certificates       When <span class="built_in">set</span> to <span class="literal">true</span>, Dashboard will automatically generate certificates used to serve HTTPS. Default: <span class="literal">false</span>.</span><br><span class="line">      --<span class="built_in">bind</span>-address ip                  The IP address on <span class="built_in">which</span> to serve the --secure-port (<span class="built_in">set</span> to 0.0.0.0 <span class="keyword">for</span> all interfaces). (default 0.0.0.0)</span><br><span class="line">      --default-cert-dir string          Directory path containing <span class="string">'--tls-cert-file'</span> and <span class="string">'--tls-key-file'</span> files. Used also when auto-generating certificates flag is <span class="built_in">set</span>. (default <span class="string">"/certs"</span>)</span><br><span class="line">      --<span class="built_in">disable</span>-settings-authorizer      When enabled, Dashboard settings page will not require user to be logged <span class="keyword">in</span> and authorized to access settings page.</span><br><span class="line">      --<span class="built_in">disable</span>-skip                     When enabled, the skip button on the login page will not be shown. Default: <span class="literal">false</span>.</span><br><span class="line">      --<span class="built_in">enable</span>-insecure-login            When enabled, Dashboard login view will also be shown when Dashboard is not served over HTTPS. Default: <span class="literal">false</span>.</span><br><span class="line">      --heapster-host string             The address of the Heapster Apiserver to connect to <span class="keyword">in</span> the format of protocol://address:port, e.g., http://localhost:8082. If not specified, the assumption is that the binary runs inside a Kubernetes cluster and service proxy will be used.</span><br><span class="line">      --insecure-bind-address ip         The IP address on <span class="built_in">which</span> to serve the --port (<span class="built_in">set</span> to 0.0.0.0 <span class="keyword">for</span> all interfaces). (default 127.0.0.1)</span><br><span class="line">      --insecure-port int                The port to listen to <span class="keyword">for</span> incoming HTTP requests. (default 9090)</span><br><span class="line">      --kubeconfig string                Path to kubeconfig file with authorization and master location information.</span><br><span class="line">      --log_backtrace_at traceLocation   when logging hits line file:N, emit a stack trace (default :0)</span><br><span class="line">      --log_dir string                   If non-empty, write <span class="built_in">log</span> files <span class="keyword">in</span> this directory</span><br><span class="line">      --logtostderr                      <span class="built_in">log</span> to standard error instead of files</span><br><span class="line">      --metric-client-check-period int   Time <span class="keyword">in</span> seconds that defines how often configured metric client health check should be run. Default: 30 seconds. (default 30)</span><br><span class="line">      --port int                         The secure port to listen to <span class="keyword">for</span> incoming HTTPS requests. (default 8443)</span><br><span class="line">      --stderrthreshold severity         logs at or above this threshold go to stderr (default 2)</span><br><span class="line">      --system-banner string             When non-empty displays message to Dashboard users. Accepts simple HTML tags. Default: <span class="string">''</span>.</span><br><span class="line">      --system-banner-severity string    Severity of system banner. Should be one of <span class="string">'INFO|WARNING|ERROR'</span>. Default: <span class="string">'INFO'</span>. (default <span class="string">"INFO"</span>)</span><br><span class="line">      --tls-cert-file string             File containing the default x509 Certificate <span class="keyword">for</span> HTTPS.</span><br><span class="line">      --tls-key-file string              File containing the default x509 private key matching --tls-cert-file.</span><br><span class="line">      --token-ttl int                    Expiration time (<span class="keyword">in</span> seconds) of JWE tokens generated by dashboard. Default: 15 min. 0 - never expires (default 900)</span><br><span class="line">  -v, --v Level                          <span class="built_in">log</span> level <span class="keyword">for</span> V logs</span><br><span class="line">      --vmodule moduleSpec               comma-separated list of pattern=N settings <span class="keyword">for</span> file-filtered logging</span><br><span class="line">pflag: <span class="built_in">help</span> requested</span><br><span class="line"><span class="built_in">command</span> terminated with <span class="built_in">exit</span> code 2</span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dashboard 的 –authentication-mode 支持 token、basic，默认为 token。如果使用 basic，则 kube-apiserver 必须配置 –authorization-mode=ABAC 和 –basic-auth-file 参数</p><h3 id="访问-dashboard"><a href="#访问-dashboard" class="headerlink" title="访问 dashboard"></a>访问 dashboard</h3><p>使用https协议，在浏览器输入任意node的ip加端口即可访问<br><img src="http://zxc.kingxunlian.com/1568961339763.jpg" alt="img"></p><h3 id="创建登录-Dashboard-的-token-和-kubeconfig-配置文件"><a href="#创建登录-Dashboard-的-token-和-kubeconfig-配置文件" class="headerlink" title="创建登录 Dashboard 的 token 和 kubeconfig 配置文件"></a>创建登录 Dashboard 的 token 和 kubeconfig 配置文件</h3><p>dashboard 默认只支持 token 认证（不支持 client 证书认证），所以如果使用 Kubeconfig 文件，需要将 token 写入到该文件。</p><h4 id="创建登录-token"><a href="#创建登录-token" class="headerlink" title="创建登录 token"></a>创建登录 token</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl create sa dashboard-admin -n kube-system</span></span><br><span class="line"><span class="comment"># kubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin</span></span><br><span class="line"><span class="comment"># ADMIN_SECRET=$(kubectl get secrets -n kube-system | grep dashboard-admin | awk '&#123;print $1&#125;')</span></span><br><span class="line"><span class="comment"># DASHBOARD_LOGIN_TOKEN=$(kubectl describe secret -n kube-system $&#123;ADMIN_SECRET&#125; | grep -E '^token' | awk '&#123;print $2&#125;')</span></span><br><span class="line"><span class="comment"># echo $&#123;DASHBOARD_LOGIN_TOKEN&#125;</span></span><br></pre></td></tr></table></figure><p>使用输出的 token 登录 Dashboard。</p><h3 id="创建使用-token-的-KubeConfig-文件"><a href="#创建使用-token-的-KubeConfig-文件" class="headerlink" title="创建使用 token 的 KubeConfig 文件"></a>创建使用 token 的 KubeConfig 文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置集群参数</span></span><br><span class="line">kubectl config <span class="built_in">set</span>-cluster kubernetes \</span><br><span class="line">  --certificate-authority=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --embed-certs=<span class="literal">true</span> \</span><br><span class="line">  --server=<span class="variable">$&#123;KUBE_APISERVER&#125;</span> \</span><br><span class="line">  --kubeconfig=dashboard.kubeconfig</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置客户端认证参数，使用上面创建的 Token</span></span><br><span class="line">kubectl config <span class="built_in">set</span>-credentials dashboard_user \</span><br><span class="line">  --token=<span class="variable">$&#123;DASHBOARD_LOGIN_TOKEN&#125;</span> \</span><br><span class="line">  --kubeconfig=dashboard.kubeconfig</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置上下文参数</span></span><br><span class="line">kubectl config <span class="built_in">set</span>-context default \</span><br><span class="line">  --cluster=kubernetes \</span><br><span class="line">  --user=dashboard_user \</span><br><span class="line">  --kubeconfig=dashboard.kubeconfig</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置默认上下文</span></span><br><span class="line">kubectl config use-context default --kubeconfig=dashboard.kubeconfig</span><br></pre></td></tr></table></figure><p>如图:<br><img src="http://zxc.kingxunlian.com/1568961447890.jpg" alt="img"></p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;用生成的 dashboard.kubeconfig 登录 Dashboard。由于k8s 默认的Dashboard 15分钟后就会弹出，又要重新登录和获取token麻烦，可以参考之前的<a href="https://xxlaila.github.io/2019/08/29/k8s配置Dashboard/">k8s配置Dashboard</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;kuberntes 自带插件的 manifests yaml 文件使用 gcr.io 的 docker registry，国内被墙，需要手动替换为其它 registry 地址&lt;/p&gt;
&lt;h3 id=&quot;修改配置文件&quot;&gt;&lt;a href=&quot;#修改配置文件&quot; class=&quot;headerlink&quot; title=&quot;修改配置文件&quot;&gt;&lt;/a&gt;修改配置文件&lt;/h3&gt;&lt;p&gt;将下载的 kubernetes-server-linux-amd64.tar.gz 解压后，再解压其中的 kubernetes-src.tar.gz 文件。&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# cd kubernetes&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# tar -xzvf kubernetes-src.tar.gz&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;p&gt;dashboard 对应的目录是：cluster/addons/dashboard：&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# cd cluster/addons/dashboard&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;p&gt;修改 service 定义，指定端口类型为 NodePort，这样外界可以通过地址 NodeIP:NodePort 访问 dashboard；&lt;/p&gt;
    
    </summary>
    
      <category term="kubernetes" scheme="https://xxlaila.github.io/categories/kubernetes/"/>
    
    
      <category term="v1.14 dashboard" scheme="https://xxlaila.github.io/tags/v1-14-dashboard/"/>
    
  </entry>
  
  <entry>
    <title>k8s v1.14 dns插件</title>
    <link href="https://xxlaila.github.io/2019/09/16/k8s-v1-14-dns%E6%8F%92%E4%BB%B6/"/>
    <id>https://xxlaila.github.io/2019/09/16/k8s-v1-14-dns插件/</id>
    <published>2019-09-16T09:37:06.000Z</published>
    <updated>2019-09-20T05:47:25.878Z</updated>
    
    <content type="html"><![CDATA[<h3 id="部署-coredns-插件"><a href="#部署-coredns-插件" class="headerlink" title="部署 coredns 插件"></a>部署 coredns 插件</h3><p><strong>注意:</strong></p><ul><li>kuberntes 自带插件的 manifests yaml 文件使用 gcr.io 的 docker registry，国内被墙，需要手动替换为其它 registry 地址;</li></ul><h3 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h3><p>将下载的 kubernetes-server-linux-amd64.tar.gz 解压后，再解压其中的 kubernetes-src.tar.gz 文件。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cd kubernetes</span></span><br><span class="line"><span class="comment"># tar -xzvf kubernetes-src.tar.gz</span></span><br></pre></td></tr></table></figure><a id="more"></a><ul><li>coredns 目录是 cluster/addons/dns<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cd cluster/addons/dns/coredns</span></span><br><span class="line"><span class="comment"># cp coredns.yaml.base coredns.yaml</span></span><br><span class="line"><span class="comment"># sed -i -e "s/__PILLAR__DNS__DOMAIN__/cluster.local/" -e "s/__PILLAR__DNS__SERVER__/10.254.0.2/" coredns.yaml</span></span><br><span class="line"><span class="comment"># sed -i "s/k8s.gcr.io/coredns/" coredns.yaml</span></span><br><span class="line"><span class="comment"># kubectl create -f coredns.yaml</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="检查-coredns-功能"><a href="#检查-coredns-功能" class="headerlink" title="检查 coredns 功能"></a>检查 coredns 功能</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get all -n kube-system</span></span><br><span class="line">NAME                                        READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod/coredns-5579b8778b-xw8m9                1/1     Running   1          5h7m</span><br><span class="line"></span><br><span class="line">NAME                           TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                  AGE</span><br><span class="line">service/kube-dns               ClusterIP   10.254.0.2       &lt;none&gt;        53/UDP,53/TCP,9153/TCP   5h7m</span><br><span class="line"></span><br><span class="line">NAME                                   READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps/coredns                1/1     1            1           5h7m</span><br><span class="line"></span><br><span class="line">NAME                                              DESIRED   CURRENT   READY   AGE</span><br><span class="line">replicaset.apps/coredns-5579b8778b                1         1         1       5h7m</span><br></pre></td></tr></table></figure><h3 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get pods</span></span><br><span class="line">NAME             READY   STATUS    RESTARTS   AGE</span><br><span class="line">nginx-ds-9fb46   1/1     Running   0          5h14m</span><br><span class="line">nginx-ds-bgfzt   1/1     Running   0          5h14m</span><br><span class="line">nginx-ds-t22wj   1/1     Running   0          5h14m</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl -it exec nginx-ds-9fb46 bash</span></span><br><span class="line">root@nginx-ds-9fb46:/<span class="comment"># cat /etc/resolv.conf</span></span><br><span class="line">nameserver 10.254.0.2</span><br><span class="line">search default.svc.cluster.local svc.cluster.local cluster.local openstacklocal novalocal</span><br><span class="line">options ndots:5</span><br><span class="line"></span><br><span class="line">root@nginx-ds-9fb46:/<span class="comment"># ping www.baidu.com</span></span><br><span class="line">PING www.wshifen.com (104.193.88.77): 48 data bytes</span><br><span class="line">56 bytes from 104.193.88.77: icmp_seq=0 ttl=45 time=191.953 ms</span><br><span class="line">56 bytes from 104.193.88.77: icmp_seq=1 ttl=45 time=191.680 ms</span><br><span class="line">^C--- www.wshifen.com ping statistics ---</span><br><span class="line">2 packets transmitted, 2 packets received, 0% packet loss</span><br><span class="line">round-trip min/avg/max/stddev = 191.680/191.817/191.953/0.137 ms</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">root@nginx-ds-9fb46:/<span class="comment"># ping kube-dns.kube-system.svc</span></span><br><span class="line">PING kube-dns.kube-system.svc.cluster.local (10.254.0.2): 48 data bytes</span><br><span class="line">56 bytes from 10.254.0.2: icmp_seq=0 ttl=64 time=0.120 ms</span><br><span class="line">56 bytes from 10.254.0.2: icmp_seq=1 ttl=64 time=0.116 ms</span><br><span class="line">^C--- kube-dns.kube-system.svc.cluster.local ping statistics ---</span><br><span class="line">2 packets transmitted, 2 packets received, 0% packet loss</span><br><span class="line">round-trip min/avg/max/stddev = 0.116/0.118/0.120/0.000 ms</span><br><span class="line"></span><br><span class="line">root@nginx-ds-9fb46:/<span class="comment"># ping kube-dns.kube-system.svc.cluster.local</span></span><br><span class="line">PING kube-dns.kube-system.svc.cluster.local (10.254.0.2): 48 data bytes</span><br><span class="line">56 bytes from 10.254.0.2: icmp_seq=0 ttl=64 time=0.079 ms</span><br><span class="line">56 bytes from 10.254.0.2: icmp_seq=1 ttl=64 time=0.152 ms</span><br><span class="line">^C--- kube-dns.kube-system.svc.cluster.local ping statistics ---</span><br><span class="line">2 packets transmitted, 2 packets received, 0% packet loss</span><br><span class="line">round-trip min/avg/max/stddev = 0.079/0.115/0.152/0.037 ms</span><br><span class="line"></span><br><span class="line">root@nginx-ds-9fb46:/<span class="comment"># ping kube-dns.kube-system.svc.cluster.local.</span></span><br><span class="line">PING kube-dns.kube-system.svc.cluster.local (10.254.0.2): 48 data bytes</span><br><span class="line">56 bytes from 10.254.0.2: icmp_seq=0 ttl=64 time=0.080 ms</span><br><span class="line">^C--- kube-dns.kube-system.svc.cluster.local ping statistics ---</span><br><span class="line">1 packets transmitted, 1 packets received, 0% packet loss</span><br><span class="line">round-trip min/avg/max/stddev = 0.080/0.080/0.080/0.000 ms</span><br><span class="line">`</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;部署-coredns-插件&quot;&gt;&lt;a href=&quot;#部署-coredns-插件&quot; class=&quot;headerlink&quot; title=&quot;部署 coredns 插件&quot;&gt;&lt;/a&gt;部署 coredns 插件&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;注意:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;kuberntes 自带插件的 manifests yaml 文件使用 gcr.io 的 docker registry，国内被墙，需要手动替换为其它 registry 地址;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;修改配置文件&quot;&gt;&lt;a href=&quot;#修改配置文件&quot; class=&quot;headerlink&quot; title=&quot;修改配置文件&quot;&gt;&lt;/a&gt;修改配置文件&lt;/h3&gt;&lt;p&gt;将下载的 kubernetes-server-linux-amd64.tar.gz 解压后，再解压其中的 kubernetes-src.tar.gz 文件。&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# cd kubernetes&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# tar -xzvf kubernetes-src.tar.gz&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="kubernetes" scheme="https://xxlaila.github.io/categories/kubernetes/"/>
    
    
      <category term="coredns" scheme="https://xxlaila.github.io/tags/coredns/"/>
    
  </entry>
  
  <entry>
    <title>k8s v1.14集群验证</title>
    <link href="https://xxlaila.github.io/2019/09/16/k8s-v1-14%E9%9B%86%E7%BE%A4%E9%AA%8C%E8%AF%81/"/>
    <id>https://xxlaila.github.io/2019/09/16/k8s-v1-14集群验证/</id>
    <published>2019-09-16T09:21:22.000Z</published>
    <updated>2019-09-20T05:47:25.896Z</updated>
    
    <content type="html"><![CDATA[<h3 id="验证集群功能"><a href="#验证集群功能" class="headerlink" title="验证集群功能"></a>验证集群功能</h3><h3 id="检查节点状态"><a href="#检查节点状态" class="headerlink" title="检查节点状态"></a>检查节点状态</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get nodes</span></span><br><span class="line">NAME            STATUS   ROLES    AGE     VERSION</span><br><span class="line">172.21.16.204   Ready    &lt;none&gt;   5h50m   v1.14.6</span><br><span class="line">172.21.16.240   Ready    &lt;none&gt;   5h48m   v1.14.6</span><br><span class="line">172.21.16.87    Ready    &lt;none&gt;   5h45m   v1.14.6</span><br></pre></td></tr></table></figure><p>都为 Ready 时正常。</p><a id="more"></a><h3 id="创建测试文件"><a href="#创建测试文件" class="headerlink" title="创建测试文件"></a>创建测试文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat nginx-ds.yml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ds</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx-ds</span><br><span class="line">spec:</span><br><span class="line">  <span class="built_in">type</span>: NodePort</span><br><span class="line">  selector:</span><br><span class="line">    app: nginx-ds</span><br><span class="line">  ports:</span><br><span class="line">  - name: http</span><br><span class="line">    port: 80</span><br><span class="line">    targetPort: 80</span><br><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: DaemonSet</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ds</span><br><span class="line">  labels:</span><br><span class="line">    addonmanager.kubernetes.io/mode: Reconcile</span><br><span class="line">spec:</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx-ds</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: my-nginx</span><br><span class="line">        image: nginx:1.7.9</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl create -f nginx-ds.yml</span></span><br></pre></td></tr></table></figure><h3 id="检查各节点的-Pod-IP-连通性"><a href="#检查各节点的-Pod-IP-连通性" class="headerlink" title="检查各节点的 Pod IP 连通性"></a>检查各节点的 Pod IP 连通性</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get pods  -o wide|grep nginx-ds</span></span><br><span class="line">nginx-ds-9fb46   1/1     Running   0          5h2m   172.30.232.2   172.21.16.204   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">nginx-ds-bgfzt   1/1     Running   0          5h2m   172.30.128.2   172.21.16.87    &lt;none&gt;           &lt;none&gt;</span><br><span class="line">nginx-ds-t22wj   1/1     Running   0          5h2m   172.30.176.2   172.21.16.240   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><h3 id="检查服务-IP-和端口可达性"><a href="#检查服务-IP-和端口可达性" class="headerlink" title="检查服务 IP 和端口可达性"></a>检查服务 IP 和端口可达性</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get svc |grep nginx-ds</span></span><br><span class="line">nginx-ds     NodePort    10.254.232.104   &lt;none&gt;        80:30349/TCP   5h2m</span><br></pre></td></tr></table></figure><p>在浏览器在30349进行访问可以看到neinx的欢迎界面</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;验证集群功能&quot;&gt;&lt;a href=&quot;#验证集群功能&quot; class=&quot;headerlink&quot; title=&quot;验证集群功能&quot;&gt;&lt;/a&gt;验证集群功能&lt;/h3&gt;&lt;h3 id=&quot;检查节点状态&quot;&gt;&lt;a href=&quot;#检查节点状态&quot; class=&quot;headerlink&quot; title=&quot;检查节点状态&quot;&gt;&lt;/a&gt;检查节点状态&lt;/h3&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# kubectl get nodes&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;NAME            STATUS   ROLES    AGE     VERSION&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;172.21.16.204   Ready    &amp;lt;none&amp;gt;   5h50m   v1.14.6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;172.21.16.240   Ready    &amp;lt;none&amp;gt;   5h48m   v1.14.6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;172.21.16.87    Ready    &amp;lt;none&amp;gt;   5h45m   v1.14.6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;p&gt;都为 Ready 时正常。&lt;/p&gt;
    
    </summary>
    
      <category term="kubernetes" scheme="https://xxlaila.github.io/categories/kubernetes/"/>
    
    
      <category term="v1.14" scheme="https://xxlaila.github.io/tags/v1-14/"/>
    
  </entry>
  
  <entry>
    <title>kubernetes-v1.14 node安装</title>
    <link href="https://xxlaila.github.io/2019/09/16/kubernetes-v1-14-node%E5%AE%89%E8%A3%85/"/>
    <id>https://xxlaila.github.io/2019/09/16/kubernetes-v1-14-node安装/</id>
    <published>2019-09-16T07:42:55.000Z</published>
    <updated>2019-09-23T06:03:46.377Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1、安装docker"><a href="#1、安装docker" class="headerlink" title="1、安装docker"></a>1、安装docker</h3><h4 id="1-1、增加docker-源"><a href="#1-1、增加docker-源" class="headerlink" title="1.1、增加docker 源"></a>1.1、增加docker 源</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yum-config-manager \</span><br><span class="line">  --add-repo \</span><br><span class="line">  https://download.docker.com/linux/centos/docker-ce.repo</span><br></pre></td></tr></table></figure><h4 id="1-2、安装docker"><a href="#1-2、安装docker" class="headerlink" title="1.2、安装docker"></a>1.2、安装docker</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  yum -y install docker-ce</span></span><br></pre></td></tr></table></figure><a id="more"></a><h4 id="1-3、修改docker-systemd-unit-文件"><a href="#1-3、修改docker-systemd-unit-文件" class="headerlink" title="1.3、修改docker systemd unit 文件"></a>1.3、修改docker systemd unit 文件</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat /usr/lib/systemd/system/docker.service |egrep -Ev "^$|^#"</span></span><br><span class="line">[Unit]</span><br><span class="line">Description=Docker Application Container Engine</span><br><span class="line">Documentation=https://docs.docker.com</span><br><span class="line">BindsTo=containerd.service</span><br><span class="line">After=network-online.target firewalld.service containerd.service</span><br><span class="line">Wants=network-online.target</span><br><span class="line">Requires=docker.socket</span><br><span class="line">[Service]</span><br><span class="line">Type=notify</span><br><span class="line">EnvironmentFile=-/run/flannel/docker</span><br><span class="line">ExecStart=/usr/bin/dockerd <span class="variable">$DOCKER_NETWORK_OPTIONS</span></span><br><span class="line">ExecReload=/bin/<span class="built_in">kill</span> -s HUP <span class="variable">$MAINPID</span></span><br><span class="line">TimeoutSec=0</span><br><span class="line">RestartSec=2</span><br><span class="line">Restart=always</span><br><span class="line">StartLimitBurst=3</span><br><span class="line">StartLimitInterval=60s</span><br><span class="line">LimitNOFILE=infinity</span><br><span class="line">LimitNPROC=infinity</span><br><span class="line">LimitCORE=infinity</span><br><span class="line">TasksMax=infinity</span><br><span class="line">Delegate=yes</span><br><span class="line">KillMode=process</span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><ul><li>dockerd 运行时会调用其它 docker 命令，如 docker-proxy，所以需要将 docker 命令所在的目录加到 PATH 环境变量中；</li><li>flanneld 启动时将网络配置写入 /run/flannel/docker 文件中，dockerd 启动前读取该文件中的环境变量 DOCKER_NETWORK_OPTIONS ，然后设置 docker0 网桥网段；</li><li>如果指定了多个 EnvironmentFile 选项，则必须将 /run/flannel/docker 放在最后(确保 docker0 使用 flanneld 生成的 bip 参数)；</li><li>docker 需要以 root 用于运行；</li></ul><h4 id="1-4、启动-docker-服务"><a href="#1-4、启动-docker-服务" class="headerlink" title="1.4、启动 docker 服务"></a>1.4、启动 docker 服务</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># systemctl daemon-reload &amp;&amp; systemctl enable docker &amp;&amp; systemctl restart docker &amp;&amp; systemctl status docker</span></span><br></pre></td></tr></table></figure><h4 id="1-5、检查-docker0-网桥"><a href="#1-5、检查-docker0-网桥" class="headerlink" title="1.5、检查 docker0 网桥"></a>1.5、检查 docker0 网桥</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># /usr/sbin/ip addr show flannel.1 &amp;&amp; /usr/sbin/ip addr show docker0</span></span><br><span class="line">3: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN </span><br><span class="line">    link/ether 8a:be:12:b9:ab:b8 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.30.128.0/32 scope global flannel.1</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">4: docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP </span><br><span class="line">    link/ether 02:42:eb:ec:ae:94 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.30.128.1/21 brd 172.30.135.255 scope global docker0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure><h4 id="1-5、查看-docker-的状态信息"><a href="#1-5、查看-docker-的状态信息" class="headerlink" title="1.5、查看 docker 的状态信息"></a>1.5、查看 docker 的状态信息</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ps -elfH|grep docker</span></span><br><span class="line">0 S root      1436   975  0  80   0 - 28167 -      10:54 pts/0    00:00:00                 grep --color=auto docker</span><br><span class="line">4 S root      1265     1  1  80   0 - 122095 futex_ 10:54 ?       00:00:00   /usr/bin/dockerd --bip=172.30.112.1/21 --ip-masq=<span class="literal">false</span> --mtu=1450</span><br><span class="line"></span><br><span class="line"><span class="comment"># docker info</span></span><br><span class="line">vClient:</span><br><span class="line"> Debug Mode: <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">Server:</span><br><span class="line"> Containers: 0</span><br><span class="line">  Running: 0</span><br><span class="line">  Paused: 0</span><br><span class="line">  Stopped: 0</span><br><span class="line"> Images: 0</span><br><span class="line"> Server Version: 18.09.6</span><br><span class="line"> Storage Driver: overlay2</span><br><span class="line">  Backing Filesystem: xfs</span><br><span class="line">  Supports d_type: <span class="literal">true</span></span><br><span class="line">  Native Overlay Diff: <span class="literal">true</span></span><br><span class="line"> Logging Driver: json-file</span><br><span class="line"> Cgroup Driver: cgroupfs</span><br><span class="line"> Plugins:</span><br><span class="line">  Volume: <span class="built_in">local</span></span><br><span class="line">  Network: bridge host macvlan null overlay</span><br><span class="line">  Log: awslogs fluentd gcplogs gelf journald json-file <span class="built_in">local</span> logentries splunk syslog</span><br><span class="line"> Swarm: inactive</span><br><span class="line"> Runtimes: runc</span><br><span class="line"> Default Runtime: runc</span><br><span class="line"> Init Binary: docker-init</span><br><span class="line"> containerd version: 894b81a4b802e4eb2a91d1ce216b8817763c29fb</span><br><span class="line"> runc version: 425e105d5a03fabd737a126ad93d62a9eeede87f</span><br><span class="line"> init version: fec3683</span><br><span class="line"> Security Options:</span><br><span class="line">  seccomp</span><br><span class="line">   Profile: default</span><br><span class="line"> Kernel Version: 4.4.193-1.el7.elrepo.x86_64</span><br><span class="line"> Operating System: CentOS Linux 7 (Core)</span><br><span class="line"> OSType: linux</span><br><span class="line"> Architecture: x86_64</span><br><span class="line"> CPUs: 4</span><br><span class="line"> Total Memory: 7.796GiB</span><br><span class="line"> Name: k8s-node-2.kxl</span><br><span class="line"> ID: GJEA:U6PT:NMHM:KWD2:DOIJ:U6XW:6N3U:4QZN:F5PT:CQXH:MZKU:VATL</span><br><span class="line"> Docker Root Dir: /var/lib/docker</span><br><span class="line"> Debug Mode: <span class="literal">false</span></span><br><span class="line"> Registry: https://index.docker.io/v1/</span><br><span class="line"> Labels:</span><br><span class="line"> Experimental: <span class="literal">false</span></span><br><span class="line"> Insecure Registries:</span><br><span class="line">  127.0.0.0/8</span><br><span class="line"> Live Restore Enabled: <span class="literal">false</span></span><br><span class="line"> Product License: Community Engine</span><br></pre></td></tr></table></figure><h3 id="2、部署-kubelet-组件"><a href="#2、部署-kubelet-组件" class="headerlink" title="2、部署 kubelet 组件"></a>2、部署 kubelet 组件</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kubelet 运行在每个 worker 节点上，接收 kube-apiserver 发送的请求，管理 Pod 容器，执行交互式命令，如 exec、run、logs 等。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kubelet 启动时自动向 kube-apiserver 注册节点信息，内置的 cadvisor 统计和监控节点的资源使用情况。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为确保安全，部署时关闭了 kubelet 的非安全 http 端口，对请求进行认证和授权，拒绝未授权的访问(如 apiserver、heapster 的请求)。</p><h4 id="2-1、创建-kubelet-bootstrap-kubeconfig-文件"><a href="#2-1、创建-kubelet-bootstrap-kubeconfig-文件" class="headerlink" title="2.1、创建 kubelet bootstrap kubeconfig 文件"></a>2.1、创建 kubelet bootstrap kubeconfig 文件</h4><p>NODE_NAMES 里面的值是node的主机名</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># NODE_NAMES=(node-01 node-02 node-03)</span></span><br><span class="line"><span class="keyword">for</span> node_name <span class="keyword">in</span> <span class="variable">$&#123;NODE_NAMES[@]&#125;</span></span><br><span class="line">  <span class="keyword">do</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"&gt;&gt;&gt; <span class="variable">$&#123;node_name&#125;</span>"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建 token</span></span><br><span class="line">    <span class="built_in">export</span> BOOTSTRAP_TOKEN=$(kubeadm token create \</span><br><span class="line">      --description kubelet-bootstrap-token \</span><br><span class="line">      --groups system:bootstrappers:<span class="variable">$&#123;node_name&#125;</span> \</span><br><span class="line">      --kubeconfig ~/.kube/config)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置集群参数</span></span><br><span class="line">    kubectl config <span class="built_in">set</span>-cluster kubernetes \</span><br><span class="line">      --certificate-authority=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">      --embed-certs=<span class="literal">true</span> \</span><br><span class="line">      --server=<span class="variable">$&#123;KUBE_APISERVER&#125;</span> \</span><br><span class="line">      --kubeconfig=kubelet-bootstrap-<span class="variable">$&#123;node_name&#125;</span>.kubeconfig</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置客户端认证参数</span></span><br><span class="line">    kubectl config <span class="built_in">set</span>-credentials kubelet-bootstrap \</span><br><span class="line">      --token=<span class="variable">$&#123;BOOTSTRAP_TOKEN&#125;</span> \</span><br><span class="line">      --kubeconfig=kubelet-bootstrap-<span class="variable">$&#123;node_name&#125;</span>.kubeconfig</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置上下文参数</span></span><br><span class="line">    kubectl config <span class="built_in">set</span>-context default \</span><br><span class="line">      --cluster=kubernetes \</span><br><span class="line">      --user=kubelet-bootstrap \</span><br><span class="line">      --kubeconfig=kubelet-bootstrap-<span class="variable">$&#123;node_name&#125;</span>.kubeconfig</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置默认上下文</span></span><br><span class="line">    kubectl config use-context default --kubeconfig=kubelet-bootstrap-<span class="variable">$&#123;node_name&#125;</span>.kubeconfig</span><br><span class="line">  <span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># for node_name in $&#123;NODE_NAMES[@]&#125;</span></span><br><span class="line">  <span class="keyword">do</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"&gt;&gt;&gt; <span class="variable">$&#123;node_name&#125;</span>"</span></span><br><span class="line">    scp kubelet-bootstrap-<span class="variable">$&#123;node_name&#125;</span>.kubeconfig root@<span class="variable">$&#123;node_name&#125;</span>:/etc/kubernetes/kubelet-bootstrap.kubeconfig</span><br><span class="line">  <span class="keyword">done</span></span><br></pre></td></tr></table></figure><ul><li><p>向 kubeconfig 写入的是 token，bootstrap 结束后 kube-controller-manager 为 kubelet 创建 client 和 server 证书；</p></li><li><p>查看 kubeadm 为各节点创建的 token:<br>master 节点查看</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubeadm token list --kubeconfig ~/.kube/config</span></span><br><span class="line">TOKEN                     TTL       EXPIRES                     USAGES                   DESCRIPTION               EXTRA GROUPS</span><br><span class="line">016e9x.306t91l832suzg8i   19h       2019-09-17T11:29:43+08:00   authentication,signing   kubelet-bootstrap-token   system:bootstrappers:node-03</span><br><span class="line">4l4tcx.juy6qs9rmrnfpbig   19h       2019-09-17T11:29:43+08:00   authentication,signing   kubelet-bootstrap-token   system:bootstrappers:node-01</span><br><span class="line">64pk36.vbhvbmtojpskyclt   19h       2019-09-17T11:29:43+08:00   authentication,signing   kubelet-bootstrap-token   system:bootstrappers:node-02</span><br></pre></td></tr></table></figure><ul><li>token 有效期为 1 天，超期后将不能再被用来 boostrap kubelet，且会被 kube-controller-manager 的 tokencleaner 清理；</li><li>kube-apiserver 接收 kubelet 的 bootstrap token 后，将请求的 user 设置为 system:bootstrap:<token id>，group 设置为 system:bootstrappers，后续将为这个 group 设置 ClusterRoleBinding；</token></li></ul></li><li><p>查看各 token 关联的 Secret：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get secrets  -n kube-system|grep bootstrap-token</span></span><br><span class="line">bootstrap-token-016e9x                           bootstrap.kubernetes.io/token         7      4h25m</span><br><span class="line">bootstrap-token-4l4tcx                           bootstrap.kubernetes.io/token         7      4h25m</span><br><span class="line">bootstrap-token-64pk36                           bootstrap.kubernetes.io/token         7      4h25m</span><br></pre></td></tr></table></figure></li></ul><h4 id="2-2、创建和分发-kubelet-参数配置文件"><a href="#2-2、创建和分发-kubelet-参数配置文件" class="headerlink" title="2.2、创建和分发 kubelet 参数配置文件"></a>2.2、创建和分发 kubelet 参数配置文件</h4><p>从 v1.10 开始，部分 kubelet 参数需在配置文件中配置，kubelet –help 会提示：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DEPRECATED: This parameter should be <span class="built_in">set</span> via the config file specified by the Kubelet<span class="string">'s --config flag</span></span><br></pre></td></tr></table></figure><ul><li><p>创建 kubelet 参数配置文件模板</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat /etc/kubernetes/kubelet-config.yaml</span></span><br><span class="line">kind: KubeletConfiguration</span><br><span class="line">apiVersion: kubelet.config.k8s.io/v1beta1</span><br><span class="line">address: <span class="string">"0.0.0.0"</span></span><br><span class="line">staticPodPath: <span class="string">""</span></span><br><span class="line">syncFrequency: 1m</span><br><span class="line">fileCheckFrequency: 20s</span><br><span class="line">httpCheckFrequency: 20s</span><br><span class="line">staticPodURL: <span class="string">""</span></span><br><span class="line">port: 10250</span><br><span class="line">readOnlyPort: 0</span><br><span class="line">rotateCertificates: <span class="literal">true</span></span><br><span class="line">serverTLSBootstrap: <span class="literal">true</span></span><br><span class="line">authentication:</span><br><span class="line">  anonymous:</span><br><span class="line">    enabled: <span class="literal">false</span></span><br><span class="line">  webhook:</span><br><span class="line">    enabled: <span class="literal">true</span></span><br><span class="line">  x509:</span><br><span class="line">    clientCAFile: <span class="string">"/etc/kubernetes/ssl/ca.pem"</span></span><br><span class="line">authorization:</span><br><span class="line">  mode: Webhook</span><br><span class="line">registryPullQPS: 0</span><br><span class="line">registryBurst: 20</span><br><span class="line">eventRecordQPS: 0</span><br><span class="line">eventBurst: 20</span><br><span class="line">enableDebuggingHandlers: <span class="literal">true</span></span><br><span class="line">enableContentionProfiling: <span class="literal">true</span></span><br><span class="line">healthzPort: 10248</span><br><span class="line">healthzBindAddress: <span class="string">"172.21.16.87"</span> <span class="comment"># node节点ip</span></span><br><span class="line">clusterDomain: <span class="string">"cluster.local"</span></span><br><span class="line">clusterDNS:</span><br><span class="line">  - <span class="string">"10.254.0.2"</span></span><br><span class="line">nodeStatusUpdateFrequency: 10s</span><br><span class="line">nodeStatusReportFrequency: 1m</span><br><span class="line">imageMinimumGCAge: 2m</span><br><span class="line">imageGCHighThresholdPercent: 85</span><br><span class="line">imageGCLowThresholdPercent: 80</span><br><span class="line">volumeStatsAggPeriod: 1m</span><br><span class="line">kubeletCgroups: <span class="string">""</span></span><br><span class="line">systemCgroups: <span class="string">""</span></span><br><span class="line">cgroupRoot: <span class="string">""</span></span><br><span class="line">cgroupsPerQOS: <span class="literal">true</span></span><br><span class="line">cgroupDriver: cgroupfs</span><br><span class="line">runtimeRequestTimeout: 10m</span><br><span class="line">hairpinMode: promiscuous-bridge</span><br><span class="line">maxPods: 100</span><br><span class="line">podCIDR: <span class="string">"172.30.0.0/16"</span></span><br><span class="line">podPidsLimit: -1</span><br><span class="line">resolvConf: /etc/resolv.conf</span><br><span class="line">maxOpenFiles: 1000000</span><br><span class="line">kubeAPIQPS: 1000</span><br><span class="line">kubeAPIBurst: 2000</span><br><span class="line">serializeImagePulls: <span class="literal">false</span></span><br><span class="line">evictionHard:</span><br><span class="line">  memory.available:  <span class="string">"100Mi"</span></span><br><span class="line">nodefs.available:  <span class="string">"10%"</span></span><br><span class="line">nodefs.inodesFree: <span class="string">"5%"</span></span><br><span class="line">imagefs.available: <span class="string">"15%"</span></span><br><span class="line">evictionSoft: &#123;&#125;</span><br><span class="line">enableControllerAttachDetach: <span class="literal">true</span></span><br><span class="line">failSwapOn: <span class="literal">true</span></span><br><span class="line">containerLogMaxSize: 20Mi</span><br><span class="line">containerLogMaxFiles: 10</span><br><span class="line">systemReserved: &#123;&#125;</span><br><span class="line">kubeReserved: &#123;&#125;</span><br><span class="line">systemReservedCgroup: <span class="string">""</span></span><br><span class="line">kubeReservedCgroup: <span class="string">""</span></span><br><span class="line">enforceNodeAllocatable: [<span class="string">"pods"</span>]</span><br></pre></td></tr></table></figure><ul><li>address：kubelet 安全端口（https，10250）监听的地址，不能为 127.0.0.1，否则 kube-apiserver、heapster 等不能调用 kubelet 的 API；</li><li>readOnlyPort=0：关闭只读端口(默认 10255)，等效为未指定；</li><li>authentication.anonymous.enabled：设置为 false，不允许匿名�访问 10250 端口；</li><li>authentication.x509.clientCAFile：指定签名客户端证书的 CA 证书，开启 HTTP 证书认证；</li><li>authentication.webhook.enabled=true：开启 HTTPs bearer token 认证；</li><li>对于未通过 x509 证书和 webhook 认证的请求(kube-apiserver 或其他客户端)，将被拒绝，提示 Unauthorized；</li><li>authroization.mode=Webhook：kubelet 使用 SubjectAccessReview API 查询 kube-apiserver 某 user、group 是否具有操作资源的权限(RBAC)；</li><li>featureGates.RotateKubeletClientCertificate、featureGates.RotateKubeletServerCertificate：自动 rotate 证书，证书的有效期取决于 kube-controller-manager 的 –experimental-cluster-signing-duration 参数；</li><li>需要 root 账户运行；</li></ul></li></ul><h4 id="2-3、创建kubelet-systemd-unit-文件"><a href="#2-3、创建kubelet-systemd-unit-文件" class="headerlink" title="2.3、创建kubelet systemd unit 文件"></a>2.3、创建kubelet systemd unit 文件</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat /usr/lib/systemd/system/kubelet.service </span></span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes Kubelet</span><br><span class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</span><br><span class="line">After=docker.service</span><br><span class="line">Requires=docker.service</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">WorkingDirectory=/var/<span class="built_in">log</span>/k8s/kubelet</span><br><span class="line">ExecStart=/usr/bin/kubelet \</span><br><span class="line">  --allow-privileged=<span class="literal">true</span> \</span><br><span class="line">  --bootstrap-kubeconfig=/etc/kubernetes/kubelet-bootstrap.kubeconfig \</span><br><span class="line">  --cert-dir=/etc/kubernetes/ssl \</span><br><span class="line">  --cni-conf-dir=/etc/cni/net.d \</span><br><span class="line">  --container-runtime=docker \</span><br><span class="line">  --container-runtime-endpoint=unix:///var/run/dockershim.sock \</span><br><span class="line">  --root-dir=/var/lib/kubelet \</span><br><span class="line">  --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \</span><br><span class="line">  --config=/etc/kubernetes/kubelet-config.yaml \</span><br><span class="line">  --hostname-override=172.21.16.87 \ <span class="comment"># node 节点ip</span></span><br><span class="line">  --pod-infra-container-image=registry.cn-beijing.aliyuncs.com/images_k8s/pause-amd64:3.1 \</span><br><span class="line">  --image-pull-progress-deadline=15m \</span><br><span class="line">  --volume-plugin-dir=/var/lib/kubelet/kubelet-plugins/volume/<span class="built_in">exec</span>/ \</span><br><span class="line">  --logtostderr=<span class="literal">true</span> \</span><br><span class="line">  --v=2</span><br><span class="line">Restart=always</span><br><span class="line">RestartSec=5</span><br><span class="line">StartLimitInterval=0</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><ul><li>如果设置了 –hostname-override 选项，则 kube-proxy 也需要设置该选项，否则会出现找不到 Node 的情况；</li><li>–bootstrap-kubeconfig：指向 bootstrap kubeconfig 文件，kubelet 使用该文件中的用户名和 token 向 kube-apiserver 发送 TLS Bootstrapping 请求；</li><li>K8S approve kubelet 的 csr 请求后，在 –cert-dir 目录创建证书和私钥文件，然后写入 –kubeconfig 文件；</li><li>–pod-infra-container-image 不使用 redhat 的 pod-infrastructure:latest 镜像，它不能回收容器的僵尸；</li></ul><h4 id="2-4、Bootstrap-Token-Auth-和授予权限"><a href="#2-4、Bootstrap-Token-Auth-和授予权限" class="headerlink" title="2.4、Bootstrap Token Auth 和授予权限"></a>2.4、Bootstrap Token Auth 和授予权限</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kubelet 启动时查找 –kubeletconfig 参数对应的文件是否存在，如果不存在则使用 –bootstrap-kubeconfig 指定的 kubeconfig 文件向 kube-apiserver 发送证书签名请求 (CSR)。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kube-apiserver 收到 CSR 请求后，对其中的 Token 进行认证，认证通过后将请求的 user 设置为 system:bootstrap:<token id>，group 设置为 system:bootstrappers，这一过程称为 Bootstrap Token Auth。</token></p><p>默认情况下，这个 user 和 group 没有创建 CSR 的权限，kubelet 启动失败，错误日志如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Sep 16 11:39:43 node-02 kubelet[20385]: E0916 11:39:43.858672   20385 reflector.go:126] k8s.io/client-go/informers/factory.go:133: Failed to list *v1beta1.RuntimeClass: Unauthorized</span><br><span class="line">Sep 16 11:39:43 node-02 kubelet[20385]: E0916 11:39:43.860429   20385 reflector.go:126] k8s.io/client-go/informers/factory.go:133: Failed to list *v1beta1.CSIDriver: Unauthorized</span><br><span class="line">Sep 16 11:39:43 node-02 kubelet[20385]: E0916 11:39:43.903098   20385 kubelet.go:2244] node <span class="string">"172.21.16.240"</span> not found</span><br><span class="line">Sep 16 11:39:43 node-02 kubelet[20385]: E0916 11:39:43.985568   20385 reflector.go:126] k8s.io/kubernetes/pkg/kubelet/kubelet.go:442: Failed to list *v1.Service: Unauthorized</span><br><span class="line">Sep 16 11:39:43 node-02 kubelet[20385]: E0916 11:39:43.986781   20385 reflector.go:126] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Unauthorized</span><br><span class="line">Sep 16 11:39:43 node-02 kubelet[20385]: E0916 11:39:43.987454   20385 reflector.go:126] k8s.io/kubernetes/pkg/kubelet/kubelet.go:451: Failed to list *v1.Node: Unauthorized</span><br></pre></td></tr></table></figure><p>解决办法是：创建一个 clusterrolebinding，将 group system:bootstrappers 和 clusterrole system:node-bootstrapper 绑定：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --group=system:bootstrappers</span></span><br></pre></td></tr></table></figure><h4 id="2-5、启动-kubelet-服务"><a href="#2-5、启动-kubelet-服务" class="headerlink" title="2.5、启动 kubelet 服务"></a>2.5、启动 kubelet 服务</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># mkdir -p /var/log/k8s/kubelet</span></span><br><span class="line"><span class="comment"># systemctl daemon-reload &amp;&amp; systemctl enable kubelet &amp;&amp; systemctl restart kubelet &amp;&amp; systemctl status kubelet</span></span><br></pre></td></tr></table></figure><ul><li>启动服务前必须先创建工作目录；</li><li>关闭 swap 分区，否则 kubelet 会启动失败；</li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kubelet 启动后使用 –bootstrap-kubeconfig 向 kube-apiserver 发送 CSR 请求，当这个 CSR 被 approve 后，kube-controller-manager 为 kubelet 创建 TLS 客户端证书、私钥和 –kubeletconfig 文件。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>注意</strong>：kube-controller-manager 需要配置 –cluster-signing-cert-file 和 –cluster-signing-key-file 参数，才会为 TLS Bootstrap 创建证书和私钥。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get csr</span></span><br><span class="line">NAME        AGE     REQUESTOR                 CONDITION</span><br><span class="line">csr-bwcbm   82s     system:bootstrap:016e9x   Pending</span><br><span class="line">csr-gqdhf   105s    system:bootstrap:64pk36   Pending</span><br><span class="line">csr-q995g   6m57s   system:bootstrap:4l4tcx   Pending</span><br><span class="line">csr-xx45v   7m33s   system:bootstrap:4l4tcx   Pending</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl get nodes</span></span><br><span class="line">No resources found.</span><br></pre></td></tr></table></figure><h4 id="2-6、自动-approve-CSR-请求"><a href="#2-6、自动-approve-CSR-请求" class="headerlink" title="2.6、自动 approve CSR 请求"></a>2.6、自动 approve CSR 请求</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat /etc/kubernetes/csr-crb.yaml</span></span><br><span class="line"><span class="comment"># Approve all CSRs for the group "system:bootstrappers"</span></span><br><span class="line"> kind: ClusterRoleBinding</span><br><span class="line"> apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line"> metadata:</span><br><span class="line">   name: auto-approve-csrs-for-group</span><br><span class="line"> subjects:</span><br><span class="line"> - kind: Group</span><br><span class="line">   name: system:bootstrappers</span><br><span class="line">   apiGroup: rbac.authorization.k8s.io</span><br><span class="line"> roleRef:</span><br><span class="line">   kind: ClusterRole</span><br><span class="line">   name: system:certificates.k8s.io:certificatesigningrequests:nodeclient</span><br><span class="line">   apiGroup: rbac.authorization.k8s.io</span><br><span class="line">---</span><br><span class="line"> <span class="comment"># To let a node of the group "system:nodes" renew its own credentials</span></span><br><span class="line"> kind: ClusterRoleBinding</span><br><span class="line"> apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line"> metadata:</span><br><span class="line">   name: node-client-cert-renewal</span><br><span class="line"> subjects:</span><br><span class="line"> - kind: Group</span><br><span class="line">   name: system:nodes</span><br><span class="line">   apiGroup: rbac.authorization.k8s.io</span><br><span class="line"> roleRef:</span><br><span class="line">   kind: ClusterRole</span><br><span class="line">   name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient</span><br><span class="line">   apiGroup: rbac.authorization.k8s.io</span><br><span class="line">---</span><br><span class="line"><span class="comment"># A ClusterRole which instructs the CSR approver to approve a node requesting a</span></span><br><span class="line"><span class="comment"># serving cert matching its client cert.</span></span><br><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: approve-node-server-renewal-csr</span><br><span class="line">rules:</span><br><span class="line">- apiGroups: [<span class="string">"certificates.k8s.io"</span>]</span><br><span class="line">  resources: [<span class="string">"certificatesigningrequests/selfnodeserver"</span>]</span><br><span class="line">  verbs: [<span class="string">"create"</span>]</span><br><span class="line">---</span><br><span class="line"> <span class="comment"># To let a node of the group "system:nodes" renew its own server credentials</span></span><br><span class="line"> kind: ClusterRoleBinding</span><br><span class="line"> apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line"> metadata:</span><br><span class="line">   name: node-server-cert-renewal</span><br><span class="line"> subjects:</span><br><span class="line"> - kind: Group</span><br><span class="line">   name: system:nodes</span><br><span class="line">   apiGroup: rbac.authorization.k8s.io</span><br><span class="line"> roleRef:</span><br><span class="line">   kind: ClusterRole</span><br><span class="line">   name: approve-node-server-renewal-csr</span><br><span class="line">   apiGroup: rbac.authorization.k8s.io</span><br></pre></td></tr></table></figure><ul><li>auto-approve-csrs-for-group：自动 approve node 的第一次 CSR； 注意第一次 CSR 时，请求的 Group 为 system:bootstrappers；</li><li>node-client-cert-renewal：自动 approve node 后续过期的 client 证书，自动生成的证书 Group 为 system:nodes;</li><li>node-server-cert-renewal：自动 approve node 后续过期的 server 证书，自动生成的证书 Group 为 system:nodes;</li></ul><h4 id="2-6、等查看-kubelet-的情况"><a href="#2-6、等查看-kubelet-的情况" class="headerlink" title="2.6、等查看 kubelet 的情况"></a>2.6、等查看 kubelet 的情况</h4><p>待一段时间(1-10 分钟)，三个节点的 CSR 都被自动 approved：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get csr</span></span><br><span class="line">NAME        AGE     REQUESTOR                   CONDITION</span><br><span class="line">csr-2t4bj   2m58s   system:node:172.21.16.240   Pending</span><br><span class="line">csr-2z2mq   4m14s   system:node:172.21.16.204   Pending</span><br><span class="line">csr-bwcbm   6m6s    system:bootstrap:016e9x     Approved,Issued</span><br><span class="line">csr-gqdhf   6m29s   system:bootstrap:64pk36     Approved,Issued</span><br><span class="line">csr-q995g   11m     system:bootstrap:4l4tcx     Approved,Issued</span><br><span class="line">csr-xx45v   12m     system:bootstrap:4l4tcx     Pending</span><br></pre></td></tr></table></figure><ul><li>所有节点均 ready：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get nodes</span></span><br><span class="line">NAME            STATUS   ROLES    AGE     VERSION</span><br><span class="line">172.21.16.204   Ready    &lt;none&gt;   4m17s   v1.14.6</span><br><span class="line">172.21.16.240   Ready    &lt;none&gt;   3m2s    v1.14.6</span><br><span class="line">172.21.16.87    Ready    &lt;none&gt;   3s      v1.14.6</span><br></pre></td></tr></table></figure></li></ul><p>kube-controller-manager 为各 node 生成了 kubeconfig 文件和公私钥：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ls -l /etc/kubernetes/kubelet.kubeconfig</span></span><br><span class="line">-rw------- 1 root root 2311 Sep 16 11:31 /etc/kubernetes/kubelet.kubeconfig</span><br><span class="line"></span><br><span class="line"><span class="comment"># ls -l /etc/kubernetes/ssl/|grep kubelet</span></span><br><span class="line">-rw------- 1 root root 1281 Sep 16 11:43 kubelet-client-2019-09-16-11-43-20.pem</span><br><span class="line">lrwxrwxrwx 1 root root   58 Sep 16 11:43 kubelet-client-current.pem -&gt; /etc/kubernetes/ssl/kubelet-client-2019-09-16-11-43-20.pem</span><br></pre></td></tr></table></figure><p>没有自动生成 kubelet server 证书；</p><h4 id="2-8、手动-approve-server-cert-csr"><a href="#2-8、手动-approve-server-cert-csr" class="headerlink" title="2.8、手动 approve server cert csr"></a>2.8、手动 approve server cert csr</h4><p>基于<a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/#kubelet-configuratio" target="_blank" rel="noopener">安全性考虑</a>，CSR approving controllers 不会自动 approve kubelet server 证书签名请求，需要手动 approve：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get csr</span></span><br><span class="line">NAME        AGE     REQUESTOR                   CONDITION</span><br><span class="line">csr-2t4bj   3m5s    system:node:172.21.16.240   Pending</span><br><span class="line">csr-2z2mq   4m21s   system:node:172.21.16.204   Pending</span><br><span class="line">csr-bwcbm   6m13s   system:bootstrap:016e9x     Approved,Issued</span><br><span class="line">csr-gqdhf   6m36s   system:bootstrap:64pk36     Approved,Issued</span><br><span class="line">csr-gtkrt   7s      system:node:172.21.16.87    Pending</span><br><span class="line">csr-q995g   11m     system:bootstrap:4l4tcx     Approved,Issued</span><br><span class="line">csr-xx45v   12m     system:bootstrap:4l4tcx     Pending</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl certificate approve csr-2t4bj</span></span><br><span class="line">certificatesigningrequest.certificates.k8s.io/csr-2t4bj approved</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl certificate approve csr-2z2mq </span></span><br><span class="line">certificatesigningrequest.certificates.k8s.io/csr-2z2mq approved</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl certificate approve csr-gtkrt</span></span><br><span class="line">certificatesigningrequest.certificates.k8s.io/csr-gtkrt approved</span><br><span class="line"></span><br><span class="line"><span class="comment"># ls -l /etc/kubernetes/ssl/|grep kubelet</span></span><br><span class="line">-rw------- 1 root root 1281 Sep 16 11:43 kubelet-client-2019-09-16-11-43-20.pem</span><br><span class="line">lrwxrwxrwx 1 root root   58 Sep 16 11:43 kubelet-client-current.pem -&gt; /etc/kubernetes/ssl/kubelet-client-2019-09-16-11-43-20.pem</span><br><span class="line">-rw------- 1 root root 1305 Sep 16 11:44 kubelet-server-2019-09-16-11-44-12.pem</span><br><span class="line">lrwxrwxrwx 1 root root   58 Sep 16 11:44 kubelet-server-current.pem -&gt; /etc/kubernetes/ssl/kubelet-server-2019-09-16-11-44-12.pem</span><br></pre></td></tr></table></figure><h4 id="2-9、kubelet-提供的-API-接口"><a href="#2-9、kubelet-提供的-API-接口" class="headerlink" title="2.9、kubelet 提供的 API 接口"></a>2.9、kubelet 提供的 API 接口</h4><p>kubelet 启动后监听多个端口，用于接收 kube-apiserver 或其它客户端发送的请求：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># netstat -lnpt|grep kubelet</span></span><br><span class="line">tcp        0      0 127.0.0.1:43042         0.0.0.0:*               LISTEN      22726/kubelet       </span><br><span class="line">tcp        0      0 172.21.16.87:10248      0.0.0.0:*               LISTEN      22726/kubelet       </span><br><span class="line">tcp6       0      0 :::10250                :::*                    LISTEN      22726/kubelet</span><br></pre></td></tr></table></figure><ul><li>10248: healthz http 服务；</li><li>10250: https 服务，访问该端口时需要认证和授权（即使访问 /healthz 也需要）；</li><li>未开启只读端口 10255；</li><li>从 K8S v1.10 开始，去除了 –cadvisor-port 参数（默认 4194 端口），不支持访问 cAdvisor UI &amp; API。</li></ul><p>由于关闭了匿名认证，同时开启了 webhook 授权，所有访问 10250 端口 https API 的请求都需要被认证和授权。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;预定义的 ClusterRole system:kubelet-api-admin 授予访问 kubelet 所有 API 的权限(kube-apiserver 使用的 kubernetes 证书 User 授予了该权限)：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl describe clusterrole system:kubelet-api-admin</span></span><br><span class="line">Name:         system:kubelet-api-admin</span><br><span class="line">Labels:       kubernetes.io/bootstrapping=rbac-defaults</span><br><span class="line">Annotations:  rbac.authorization.kubernetes.io/autoupdate: <span class="literal">true</span></span><br><span class="line">PolicyRule:</span><br><span class="line">  Resources      Non-Resource URLs  Resource Names  Verbs</span><br><span class="line">  ---------      -----------------  --------------  -----</span><br><span class="line">  nodes/<span class="built_in">log</span>      []                 []              [*]</span><br><span class="line">  nodes/metrics  []                 []              [*]</span><br><span class="line">  nodes/proxy    []                 []              [*]</span><br><span class="line">  nodes/spec     []                 []              [*]</span><br><span class="line">  nodes/stats    []                 []              [*]</span><br><span class="line">  nodes          []                 []              [get list watch proxy]</span><br></pre></td></tr></table></figure><h4 id="2-10、kubelet-api-认证和授权"><a href="#2-10、kubelet-api-认证和授权" class="headerlink" title="2.10、kubelet api 认证和授权"></a>2.10、kubelet api 认证和授权</h4><p>kubelet 配置了如下认证参数:</p><ul><li>authentication.anonymous.enabled：设置为 false，不允许匿名访问 10250 端口；</li><li>authentication.x509.clientCAFile：指定签名客户端证书的 CA 证书，开启 HTTPs 证书认证；</li><li>authentication.webhook.enabled=true：开启 HTTPs bearer token 认证；</li></ul><p>同时配置了如下授权参数:</p><ul><li>authroization.mode=Webhook：开启 RBAC 授权</li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kubelet 收到请求后，使用 clientCAFile 对证书签名进行认证，或者查询 bearer token 是否有效。如果两者都没通过，则拒绝请求，提示 Unauthorized：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># curl -s --cacert /etc/kubernetes/ssl/ca.pem https://172.21.16.87:10250/metrics</span></span><br><span class="line">Unauthorized</span><br><span class="line"></span><br><span class="line"><span class="comment"># curl -s --cacert /etc/kubernetes/ssl/ca.pem -H "Authorization: Bearer 123456"  https://172.21.16.87:10250/metrics</span></span><br><span class="line">Unauthorized</span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过认证后，kubelet 使用 SubjectAccessReview API 向 kube-apiserver 发送请求，查询证书或 token 对应的 user、group 是否有操作资源的权限(RBAC)；</p><h4 id="2-11、证书认证和授权"><a href="#2-11、证书认证和授权" class="headerlink" title="2.11、证书认证和授权"></a>2.11、证书认证和授权</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 权限不足的证书；</span></span><br><span class="line"><span class="comment"># curl -s --cacert /etc/kubernetes/ssl/ca.pem --cert /etc/kubernetes/ssl/kube-controller-manager.pem --key /etc/kubernetes/ssl/kube-controller-manager-key.pem https://172.21.16.87:10250/metrics</span></span><br><span class="line">Forbidden (user=system:kube-controller-manager, verb=get, resource=nodes, subresource=metrics)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用部署 kubectl 命令行工具时创建的、具有最高权限的 admin 证书；</span></span><br><span class="line"><span class="comment"># curl -s --cacert /etc/kubernetes/ssl/ca.pem --cert /etc/kubernetes/ssl/admin.pem --key /etc/kubernetes/ssl/admin-key.pem https://172.21.16.87:10250/metrics</span></span><br><span class="line"><span class="comment"># HELP apiserver_audit_event_total Counter of audit events generated and sent to the audit backend.</span></span><br><span class="line"><span class="comment"># TYPE apiserver_audit_event_total counter</span></span><br><span class="line">apiserver_audit_event_total 0</span><br><span class="line"><span class="comment"># HELP apiserver_audit_requests_rejected_total Counter of apiserver requests rejected due to an error in audit logging backend.</span></span><br><span class="line"><span class="comment"># TYPE apiserver_audit_requests_rejected_total counter</span></span><br><span class="line">apiserver_audit_requests_rejected_total 0</span><br><span class="line"><span class="comment"># HELP apiserver_client_certificate_expiration_seconds Distribution of the remaining lifetime on the certificate used to authenticate a request.</span></span><br><span class="line"><span class="comment"># TYPE apiserver_client_certificate_expiration_seconds histogram</span></span><br><span class="line">apiserver_client_certificate_expiration_seconds_bucket&#123;le=<span class="string">"0"</span>&#125; 0</span><br><span class="line">apiserver_client_certificate_expiration_seconds_bucket&#123;le=<span class="string">"1800"</span>&#125; 0</span><br></pre></td></tr></table></figure><ul><li>–cacert、–cert、–key 的参数值必须是文件路径，如上面的 ./admin.pem 不能省略 ./，否则返回 401 Unauthorized；</li></ul><h4 id="2-12、bear-token-认证和授权"><a href="#2-12、bear-token-认证和授权" class="headerlink" title="2.12、bear token 认证和授权"></a>2.12、bear token 认证和授权</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;创建一个 ServiceAccount，将它和 ClusterRole system:kubelet-api-admin 绑定，从而具有调用 kubelet API 的权限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl create sa kubelet-api-test</span></span><br><span class="line"><span class="comment"># kubectl create clusterrolebinding kubelet-api-test --clusterrole=system:kubelet-api-admin --serviceaccount=default:kubelet-api-test</span></span><br><span class="line"><span class="comment"># SECRET=$(kubectl get secrets | grep kubelet-api-test | awk '&#123;print $1&#125;')</span></span><br><span class="line"><span class="comment"># TOKEN=$(kubectl describe secret $&#123;SECRET&#125; | grep -E '^token' | awk '&#123;print $2&#125;')</span></span><br><span class="line"><span class="comment"># echo $&#123;TOKEN&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># curl -s --cacert /etc/kubernetes/ssl/ca.pem -H "Authorization: Bearer $&#123;TOKEN&#125;" https://172.21.16.87:10250/metrics|head</span></span><br></pre></td></tr></table></figure><h3 id="3、cadvisor-和-metrics"><a href="#3、cadvisor-和-metrics" class="headerlink" title="3、cadvisor 和 metrics"></a>3、cadvisor 和 metrics</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;cadvisor 是内嵌在 kubelet 二进制中的，统计所在节点各容器的资源(CPU、内存、磁盘、网卡)使用情况的服务。<br>浏览器访问 <a href="https://172.21.16.87:10250/metrics" target="_blank" rel="noopener">https://172.21.16.87:10250/metrics</a> 和 <a href="https://172.21.16.87:10250/metrics/cadvisor" target="_blank" rel="noopener">https://172.21.16.87:10250/metrics/cadvisor</a> 分别返回 kubelet 和 cadvisor 的 metrics。<br><img src="http://zxc.kingxunlian.com/1568624798589.jpg" alt="img"></p><p><strong>注意:</strong></p><ul><li>kubelet.config.json 设置 authentication.anonymous.enabled 为 false，不允许匿名证书访问 10250 的 https 服务；</li><li>参考<a href="https://xxlaila.github.io/2019/09/04/kubelet提供api请求接口/">kubelet提供api请求接口</a>，创建和导入相关证书，然后访问上面的 10250 端口；</li></ul><h4 id="3-1、获取-kubelet-的配置"><a href="#3-1、获取-kubelet-的配置" class="headerlink" title="3.1、获取 kubelet 的配置"></a>3.1、获取 kubelet 的配置</h4><p>从 kube-apiserver 获取各节点 kubelet 的配置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用部署 kubectl 命令行工具时创建的、具有最高权限的 admin 证书；</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># curl -sSL --cacert /etc/kubernetes/ssl/ca.pem --cert /etc/kubernetes/ssl/admin.pem --key /etc/kubernetes/ssl/admin-key.pem $&#123;KUBE_APISERVER&#125;/api/v1/nodes/172.21.16.87/proxy/configz | jq  '.kubeletconfig|.kind="KubeletConfiguration"|.apiVersion="kubelet.config.k8s.io/v1beta1"'</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"syncFrequency"</span>: <span class="string">"1m0s"</span>,</span><br><span class="line">  <span class="string">"fileCheckFrequency"</span>: <span class="string">"20s"</span>,</span><br><span class="line">  <span class="string">"httpCheckFrequency"</span>: <span class="string">"20s"</span>,</span><br><span class="line">  <span class="string">"address"</span>: <span class="string">"0.0.0.0"</span>,</span><br><span class="line">  <span class="string">"port"</span>: 10250,</span><br><span class="line">  <span class="string">"rotateCertificates"</span>: <span class="literal">true</span>,</span><br><span class="line">  <span class="string">"serverTLSBootstrap"</span>: <span class="literal">true</span>,</span><br><span class="line">  <span class="string">"authentication"</span>: &#123;</span><br><span class="line">    <span class="string">"x509"</span>: &#123;</span><br><span class="line">      <span class="string">"clientCAFile"</span>: <span class="string">"/etc/kubernetes/ssl/ca.pem"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"webhook"</span>: &#123;</span><br><span class="line">      <span class="string">"enabled"</span>: <span class="literal">true</span>,</span><br><span class="line">      <span class="string">"cacheTTL"</span>: <span class="string">"2m0s"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"anonymous"</span>: &#123;</span><br><span class="line">      <span class="string">"enabled"</span>: <span class="literal">false</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"authorization"</span>: &#123;</span><br><span class="line">    <span class="string">"mode"</span>: <span class="string">"Webhook"</span>,</span><br><span class="line">    <span class="string">"webhook"</span>: &#123;</span><br><span class="line">      <span class="string">"cacheAuthorizedTTL"</span>: <span class="string">"5m0s"</span>,</span><br><span class="line">      <span class="string">"cacheUnauthorizedTTL"</span>: <span class="string">"30s"</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"registryPullQPS"</span>: 0,</span><br><span class="line">  <span class="string">"registryBurst"</span>: 20,</span><br><span class="line">  <span class="string">"eventRecordQPS"</span>: 0,</span><br><span class="line">  <span class="string">"eventBurst"</span>: 20,</span><br><span class="line">  <span class="string">"enableDebuggingHandlers"</span>: <span class="literal">true</span>,</span><br><span class="line">  <span class="string">"enableContentionProfiling"</span>: <span class="literal">true</span>,</span><br><span class="line">  <span class="string">"healthzPort"</span>: 10248,</span><br><span class="line">  <span class="string">"healthzBindAddress"</span>: <span class="string">"172.21.16.87"</span>,</span><br><span class="line">  <span class="string">"oomScoreAdj"</span>: -999,</span><br><span class="line">  <span class="string">"clusterDomain"</span>: <span class="string">"cluster.local"</span>,</span><br><span class="line">  <span class="string">"clusterDNS"</span>: [</span><br><span class="line">    <span class="string">"10.254.0.2"</span></span><br><span class="line">  ],</span><br><span class="line">  <span class="string">"streamingConnectionIdleTimeout"</span>: <span class="string">"4h0m0s"</span>,</span><br><span class="line">  <span class="string">"nodeStatusUpdateFrequency"</span>: <span class="string">"10s"</span>,</span><br><span class="line">  <span class="string">"nodeStatusReportFrequency"</span>: <span class="string">"1m0s"</span>,</span><br><span class="line">  <span class="string">"nodeLeaseDurationSeconds"</span>: 40,</span><br><span class="line">  <span class="string">"imageMinimumGCAge"</span>: <span class="string">"2m0s"</span>,</span><br><span class="line">  <span class="string">"imageGCHighThresholdPercent"</span>: 85,</span><br><span class="line">  <span class="string">"imageGCLowThresholdPercent"</span>: 80,</span><br><span class="line">  <span class="string">"volumeStatsAggPeriod"</span>: <span class="string">"1m0s"</span>,</span><br><span class="line">  <span class="string">"cgroupsPerQOS"</span>: <span class="literal">true</span>,</span><br><span class="line">  <span class="string">"cgroupDriver"</span>: <span class="string">"cgroupfs"</span>,</span><br><span class="line">  <span class="string">"cpuManagerPolicy"</span>: <span class="string">"none"</span>,</span><br><span class="line">  <span class="string">"cpuManagerReconcilePeriod"</span>: <span class="string">"10s"</span>,</span><br><span class="line">  <span class="string">"runtimeRequestTimeout"</span>: <span class="string">"10m0s"</span>,</span><br><span class="line">  <span class="string">"hairpinMode"</span>: <span class="string">"promiscuous-bridge"</span>,</span><br><span class="line">  <span class="string">"maxPods"</span>: 100,</span><br><span class="line">  <span class="string">"podCIDR"</span>: <span class="string">"172.30.0.0/16"</span>,</span><br><span class="line">  <span class="string">"podPidsLimit"</span>: -1,</span><br><span class="line">  <span class="string">"resolvConf"</span>: <span class="string">"/etc/resolv.conf"</span>,</span><br><span class="line">  <span class="string">"cpuCFSQuota"</span>: <span class="literal">true</span>,</span><br><span class="line">  <span class="string">"cpuCFSQuotaPeriod"</span>: <span class="string">"100ms"</span>,</span><br><span class="line">  <span class="string">"maxOpenFiles"</span>: 1000000,</span><br><span class="line">  <span class="string">"contentType"</span>: <span class="string">"application/vnd.kubernetes.protobuf"</span>,</span><br><span class="line">  <span class="string">"kubeAPIQPS"</span>: 1000,</span><br><span class="line">  <span class="string">"kubeAPIBurst"</span>: 2000,</span><br><span class="line">  <span class="string">"serializeImagePulls"</span>: <span class="literal">false</span>,</span><br><span class="line">  <span class="string">"evictionHard"</span>: &#123;</span><br><span class="line">    <span class="string">"memory.available"</span>: <span class="string">"100Mi"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"evictionPressureTransitionPeriod"</span>: <span class="string">"5m0s"</span>,</span><br><span class="line">  <span class="string">"enableControllerAttachDetach"</span>: <span class="literal">true</span>,</span><br><span class="line">  <span class="string">"makeIPTablesUtilChains"</span>: <span class="literal">true</span>,</span><br><span class="line">  <span class="string">"iptablesMasqueradeBit"</span>: 14,</span><br><span class="line">  <span class="string">"iptablesDropBit"</span>: 15,</span><br><span class="line">  <span class="string">"failSwapOn"</span>: <span class="literal">true</span>,</span><br><span class="line">  <span class="string">"containerLogMaxSize"</span>: <span class="string">"20Mi"</span>,</span><br><span class="line">  <span class="string">"containerLogMaxFiles"</span>: 10,</span><br><span class="line">  <span class="string">"configMapAndSecretChangeDetectionStrategy"</span>: <span class="string">"Watch"</span>,</span><br><span class="line">  <span class="string">"enforceNodeAllocatable"</span>: [</span><br><span class="line">    <span class="string">"pods"</span></span><br><span class="line">  ],</span><br><span class="line">  <span class="string">"kind"</span>: <span class="string">"KubeletConfiguration"</span>,</span><br><span class="line">  <span class="string">"apiVersion"</span>: <span class="string">"kubelet.config.k8s.io/v1beta1"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="4、部署-kube-proxy-组件"><a href="#4、部署-kube-proxy-组件" class="headerlink" title="4、部署 kube-proxy 组件"></a>4、部署 kube-proxy 组件</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kube-proxy 运行在所有 worker 节点上，它监听 apiserver 中 service 和 endpoint 的变化情况，创建路由规则以提供服务 IP 和负载均衡功能。</p><h4 id="4-1、创建-kube-proxy-证书"><a href="#4-1、创建-kube-proxy-证书" class="headerlink" title="4.1、创建 kube-proxy 证书"></a>4.1、创建 kube-proxy 证书</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; kube-proxy-csr.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"CN"</span>: <span class="string">"system:kube-proxy"</span>,</span><br><span class="line">  <span class="string">"key"</span>: &#123;</span><br><span class="line">    <span class="string">"algo"</span>: <span class="string">"rsa"</span>,</span><br><span class="line">    <span class="string">"size"</span>: 2048</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"names"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"C"</span>: <span class="string">"CN"</span>,</span><br><span class="line">      <span class="string">"ST"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">      <span class="string">"L"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">      <span class="string">"O"</span>: <span class="string">"k8s"</span>,</span><br><span class="line">      <span class="string">"OU"</span>: <span class="string">"4Paradigm"</span></span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li><p>CN：指定该证书的 User 为 system:kube-proxy；</p></li><li><p>预定义的 RoleBinding system:node-proxier 将User system:kube-proxy 与 Role system:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限；</p></li><li><p>该证书只会被 kube-proxy 当做 client 证书使用，所以 hosts 字段为空；</p></li><li><p>生成证书和私钥</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes  kube-proxy-csr.json | cfssljson -bare kube-proxy</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ls kube-proxy*.pem</span></span><br><span class="line">kube-proxy-key.pem  kube-proxy.pem</span><br></pre></td></tr></table></figure></li></ul><h4 id="4-2、创建和分发-kubeconfig-文件"><a href="#4-2、创建和分发-kubeconfig-文件" class="headerlink" title="4.2、创建和分发 kubeconfig 文件"></a>4.2、创建和分发 kubeconfig 文件</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl config set-cluster kubernetes \</span></span><br><span class="line">  --certificate-authority=ca.pem \</span><br><span class="line">  --embed-certs=<span class="literal">true</span> \</span><br><span class="line">  --server=<span class="variable">$&#123;KUBE_APISERVER&#125;</span> \</span><br><span class="line">  --kubeconfig=kube-proxy.kubeconfig</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl config set-credentials kube-proxy \</span></span><br><span class="line">  --client-certificate=kube-proxy.pem \</span><br><span class="line">  --client-key=kube-proxy-key.pem \</span><br><span class="line">  --embed-certs=<span class="literal">true</span> \</span><br><span class="line">  --kubeconfig=kube-proxy.kubeconfig</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl config set-context default \</span></span><br><span class="line">  --cluster=kubernetes \</span><br><span class="line">  --user=kube-proxy \</span><br><span class="line">  --kubeconfig=kube-proxy.kubeconfig</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig</span></span><br></pre></td></tr></table></figure><ul><li>–embed-certs=true：将 ca.pem 和 admin.pem 证书内容嵌入到生成的 kubectl-proxy.kubeconfig 文件中(不加时，写入的是证书文件路径)</li></ul><h4 id="4-3、创建-kube-proxy-配置文件"><a href="#4-3、创建-kube-proxy-配置文件" class="headerlink" title="4.3、创建 kube-proxy 配置文件"></a>4.3、创建 kube-proxy 配置文件</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat /etc/kubernetes/kube-proxy-config.yaml</span></span><br><span class="line">kind: KubeProxyConfiguration</span><br><span class="line">apiVersion: kubeproxy.config.k8s.io/v1alpha1</span><br><span class="line">clientConnection:</span><br><span class="line">  burst: 200</span><br><span class="line">  kubeconfig: <span class="string">"/etc/kubernetes/kube-proxy.kubeconfig"</span></span><br><span class="line">  qps: 100</span><br><span class="line">bindAddress: 0.0.0.0</span><br><span class="line">healthzBindAddress: 172.21.16.87:10256 <span class="comment">#node</span></span><br><span class="line">metricsBindAddress: 172.21.16.87:10249 <span class="comment">#node</span></span><br><span class="line">enableProfiling: <span class="literal">true</span></span><br><span class="line">clusterCIDR: 172.30.0.0/16</span><br><span class="line">hostnameOverride: 172.21.16.87 <span class="comment">#node </span></span><br><span class="line">mode: <span class="string">"ipvs"</span></span><br><span class="line">portRange: <span class="string">""</span></span><br><span class="line">kubeProxyIPTablesConfiguration:</span><br><span class="line">  masqueradeAll: <span class="literal">false</span></span><br><span class="line">kubeProxyIPVSConfiguration:</span><br><span class="line">  scheduler: rr</span><br><span class="line">  excludeCIDRs: []</span><br></pre></td></tr></table></figure><ul><li>bindAddress: 监听地址；</li><li>clientConnection.kubeconfig: 连接 apiserver 的 kubeconfig 文件；</li><li>clusterCIDR: kube-proxy 根据 –cluster-cidr 判断集群内部和外部流量，指定 –cluster-cidr 或 –masquerade-all 选项后 kube-proxy 才会对访问 Service IP 的请求做 SNAT；</li><li>hostnameOverride: 参数值必须与 kubelet 的值一致，否则 kube-proxy 启动后会找不到该 Node，从而不会创建任何 ipvs 规则；</li><li>mode: 使用 ipvs 模式；</li></ul><h4 id="4-4、创建kube-proxy-systemd-unit-文件"><a href="#4-4、创建kube-proxy-systemd-unit-文件" class="headerlink" title="4.4、创建kube-proxy systemd unit 文件"></a>4.4、创建kube-proxy systemd unit 文件</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat /usr/lib/systemd/system/kube-proxy.service </span></span><br><span class="line">[Service]</span><br><span class="line">WorkingDirectory=/var/<span class="built_in">log</span>/k8s/kube-proxy</span><br><span class="line">ExecStart=/usr/bin/kube-proxy \</span><br><span class="line">  --config=/etc/kubernetes/kube-proxy-config.yaml \</span><br><span class="line">  --logtostderr=<span class="literal">true</span> \</span><br><span class="line">  --v=2</span><br><span class="line">Restart=on-failure</span><br><span class="line">RestartSec=5</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><h4 id="4-5、启动-kube-proxy-服务"><a href="#4-5、启动-kube-proxy-服务" class="headerlink" title="4.5、启动 kube-proxy 服务"></a>4.5、启动 kube-proxy 服务</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># mkdir -p /var/log/k8s/kube-proxy</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># systemctl daemon-reload &amp;&amp; systemctl enable kube-proxy &amp;&amp; systemctl restart kube-proxy &amp;&amp; systemctl status kube-proxy</span></span><br></pre></td></tr></table></figure><h4 id="4-5、检查"><a href="#4-5、检查" class="headerlink" title="4.5、检查"></a>4.5、检查</h4><ul><li><p>查看监听端口</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># netstat -lnpt|grep kube-prox</span></span><br><span class="line">tcp        0      0 172.21.16.87:10256      0.0.0.0:*               LISTEN      27423/kube-proxy    </span><br><span class="line">tcp        0      0 172.21.16.87:10249      0.0.0.0:*               LISTEN      27423/kube-proxy</span><br></pre></td></tr></table></figure></li><li><p>查看 ipvs 路由规则</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ipvsadm -ln</span></span><br><span class="line">IP Virtual Server version 1.2.1 (size=4096)</span><br><span class="line">Prot LocalAddress:Port Scheduler Flags</span><br><span class="line">  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</span><br><span class="line">TCP  10.254.0.1:443 rr</span><br><span class="line">  -&gt; 172.21.17.30:6443            Masq    1      0          0         </span><br><span class="line">  -&gt; 172.21.17.31:6443            Masq    1      0          0 </span><br><span class="line">  -&gt; 172.21.16.110:6443           Masq    1      0          0</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;1、安装docker&quot;&gt;&lt;a href=&quot;#1、安装docker&quot; class=&quot;headerlink&quot; title=&quot;1、安装docker&quot;&gt;&lt;/a&gt;1、安装docker&lt;/h3&gt;&lt;h4 id=&quot;1-1、增加docker-源&quot;&gt;&lt;a href=&quot;#1-1、增加docker-源&quot; class=&quot;headerlink&quot; title=&quot;1.1、增加docker 源&quot;&gt;&lt;/a&gt;1.1、增加docker 源&lt;/h4&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;yum-config-manager \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  --add-repo \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  https://download.docker.com/linux/centos/docker-ce.repo&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;h4 id=&quot;1-2、安装docker&quot;&gt;&lt;a href=&quot;#1-2、安装docker&quot; class=&quot;headerlink&quot; title=&quot;1.2、安装docker&quot;&gt;&lt;/a&gt;1.2、安装docker&lt;/h4&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;#  yum -y install docker-ce&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="kubernetes" scheme="https://xxlaila.github.io/categories/kubernetes/"/>
    
    
      <category term="v1.14 node安装" scheme="https://xxlaila.github.io/tags/v1-14-node%E5%AE%89%E8%A3%85/"/>
    
  </entry>
  
  <entry>
    <title>kubernetes-v1.14安装</title>
    <link href="https://xxlaila.github.io/2019/09/11/kubernetes-v1-14%E5%AE%89%E8%A3%85/"/>
    <id>https://xxlaila.github.io/2019/09/11/kubernetes-v1-14安装/</id>
    <published>2019-09-11T08:20:26.000Z</published>
    <updated>2019-09-20T05:47:26.020Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1、环境准备"><a href="#1、环境准备" class="headerlink" title="1、环境准备"></a>1、环境准备</h3><table><thead><tr><th>ip</th><th>type</th><th>docker</th><th>os</th><th>k8s version</th></tr></thead><tbody><tr><td>172.21.17.30</td><td>master,etcd</td><td></td><td>CentOS Linux release 7.4.1708</td><td>v1.14.6</td></tr><tr><td>172.21.17.31</td><td>master,etcd</td><td></td><td>CentOS Linux release 7.4.1708</td><td></td></tr><tr><td>172.21.16.110</td><td>master,etcd</td><td></td><td>CentOS Linux release 7.4.1708</td><td></td></tr><tr><td>172.21.16.87</td><td>node,flanneld</td><td>18.06.2-ce</td><td>CentOS Linux release 7.4.1708</td><td></td></tr><tr><td>172.21.16.240</td><td>node,flanneld,ha+kee</td><td>18.06.2-ce</td><td>CentOS Linux release 7.4.1708</td><td></td></tr><tr><td>172.21.16.204</td><td>node,flanneld,ha+kee</td><td>18.06.2-ce</td><td>CentOS Linux release 7.4.1708</td><td></td></tr><tr><td>172.21.16.45</td><td>vip</td><td></td><td>CentOS Linux release 7.4.1708</td><td></td></tr></tbody></table><h3 id="2、初始化系统"><a href="#2、初始化系统" class="headerlink" title="2、初始化系统"></a>2、初始化系统</h3><h4 id="2-1、安装依赖包"><a href="#2-1、安装依赖包" class="headerlink" title="2.1、安装依赖包"></a>2.1、安装依赖包</h4><a id="more"></a><p>每台服务器均操作,关闭防火墙,关闭selinux</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># yum install -y epel-release</span></span><br><span class="line"><span class="comment"># yum install -y conntrack ntpdate ntp ipvsadm ipset jq iptables curl sysstat libseccomp wget</span></span><br></pre></td></tr></table></figure><h4 id="2-2、关闭-swap-分区"><a href="#2-2、关闭-swap-分区" class="headerlink" title="2.2、关闭 swap 分区"></a>2.2、关闭 swap 分区</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果开启了 swap 分区，kubelet 会启动失败(可以通过将参数 –fail-swap-on 设置为 false 来忽略 swap on)，故需要在每台机器上关闭 swap 分区。同时注释 /etc/fstab 中相应的条目，防止开机自动挂载 swap 分区。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># swapoff -a</span></span><br><span class="line"><span class="comment"># sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab</span></span><br></pre></td></tr></table></figure><h4 id="2-3、加载内核模块"><a href="#2-3、加载内核模块" class="headerlink" title="2.3、加载内核模块"></a>2.3、加载内核模块</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># modprobe ip_vs_rr</span></span><br><span class="line"><span class="comment"># modprobe br_netfilter</span></span><br></pre></td></tr></table></figure><h4 id="2-4、优化内核参数"><a href="#2-4、优化内核参数" class="headerlink" title="2.4、优化内核参数"></a>2.4、优化内核参数</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat /etc/sysctl.d/kubernetes.conf </span></span><br><span class="line">net.bridge.bridge-nf-call-iptables=1</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables=1</span><br><span class="line">net.ipv4.ip_forward=1</span><br><span class="line">net.ipv4.tcp_tw_recycle=0</span><br><span class="line">vm.swappiness=0</span><br><span class="line">vm.overcommit_memory=1</span><br><span class="line">vm.panic_on_oom=0</span><br><span class="line">fs.inotify.max_user_instances=8192</span><br><span class="line">fs.inotify.max_user_watches=1048576</span><br><span class="line">fs.file-max=52706963</span><br><span class="line">fs.nr_open=52706963</span><br><span class="line">net.ipv6.conf.all.disable_ipv6=1</span><br><span class="line">net.netfilter.nf_conntrack_max=2310720</span><br><span class="line"></span><br><span class="line"><span class="comment"># sysctl -p /etc/sysctl.d/kubernetes.conf</span></span><br></pre></td></tr></table></figure><ul><li>必须关闭 tcp_tw_recycle，否则和 NAT 冲突，会导致服务不通；</li><li>关闭 IPV6，防止触发 docker BUG；</li></ul><h4 id="2-5、设置系统时区"><a href="#2-5、设置系统时区" class="headerlink" title="2.5、设置系统时区"></a>2.5、设置系统时区</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># timedatectl set-timezone Asia/Shanghai</span></span><br><span class="line"><span class="comment"># timedatectl set-local-rtc 0</span></span><br><span class="line"><span class="comment"># systemctl restart rsyslog </span></span><br><span class="line"><span class="comment"># systemctl restart crond</span></span><br></pre></td></tr></table></figure><h4 id="2-6、关闭无关的服务"><a href="#2-6、关闭无关的服务" class="headerlink" title="2.6、关闭无关的服务"></a>2.6、关闭无关的服务</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># systemctl stop postfix &amp;&amp; systemctl disable postfix</span></span><br></pre></td></tr></table></figure><h3 id="3、升级内核"><a href="#3、升级内核" class="headerlink" title="3、升级内核"></a>3、升级内核</h3><p>以下在master节点操作<br>CentOS 7.x 系统自带的 3.10.x 内核存在一些 Bugs，导致运行的 Docker、Kubernetes 不稳定，例如:</p><ul><li>1.高版本的 docker(1.13 以后) 启用了 3.10 kernel 实验支持的 kernel memory account 功能(无法关闭)，当节点压力大如频繁启动和停止容器时会导致 cgroup memory leak；</li><li>2.网络设备引用计数泄漏，会导致类似于报错：”kernel:unregister_netdevice: waiting for eth0 to become free. Usage count = 1”;<br>解决方案如下:</li><li>1.升级内核到 4.4.X 以上</li><li>2.或者，手动编译内核，disable CONFIG_MEMCG_KMEM 特性</li><li>或者，安装修复了该问题的 Docker 18.09.1 及以上的版本。但由于 kubelet 也会设置 kmem（它 vendor 了 runc），所以需要重新编译 kubelet 并指定 GOFLAGS=”-tags=nokmem”</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> --branch v1.14.1 --single-branch --depth 1 https://github.com/kubernetes/kubernetes</span><br><span class="line"><span class="built_in">cd</span> kubernetes</span><br><span class="line">KUBE_GIT_VERSION=v1.14.1 ./build/run.sh make kubelet GOFLAGS=<span class="string">"-tags=nokmem"</span></span><br></pre></td></tr></table></figure><h4 id="3-1、内核升级方法"><a href="#3-1、内核升级方法" class="headerlink" title="3.1、内核升级方法"></a>3.1、内核升级方法</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm</span></span><br><span class="line"><span class="comment"># 安装完成后检查 /boot/grub2/grub.cfg 中对应内核 menuentry 中是否包含 initrd16 配置，如果没有，再安装一次！</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># yum --enablerepo=elrepo-kernel install -y kernel-lt</span></span><br><span class="line"><span class="comment"># 设置开机从新内核启动</span></span><br><span class="line"><span class="comment"># grub2-set-default 0</span></span><br></pre></td></tr></table></figure><h4 id="3-2、安装内核源文件"><a href="#3-2、安装内核源文件" class="headerlink" title="3.2、安装内核源文件"></a>3.2、安装内核源文件</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># yum --enablerepo=elrepo-kernel install kernel-lt-devel-$(uname -r) kernel-lt-headers-$(uname -r)</span></span><br></pre></td></tr></table></figure><h4 id="3-3、关闭-NUMA"><a href="#3-3、关闭-NUMA" class="headerlink" title="3.3、关闭 NUMA"></a>3.3、关闭 NUMA</h4><p>在其中一台master节点操作</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cp /etc/default/grub&#123;,.bak&#125;</span></span><br><span class="line"><span class="comment"># 在 GRUB_CMDLINE_LINUX 一行添加 `numa=off` 参数，如下所示</span></span><br><span class="line"><span class="comment"># cat /etc/default/grub</span></span><br><span class="line">GRUB_TIMEOUT=1</span><br><span class="line">GRUB_DISTRIBUTOR=<span class="string">"<span class="variable">$(sed 's, release .*$,,g' /etc/system-release)</span>"</span></span><br><span class="line">GRUB_DEFAULT=saved</span><br><span class="line">GRUB_DISABLE_SUBMENU=<span class="literal">true</span></span><br><span class="line">GRUB_TERMINAL=<span class="string">"serial console"</span></span><br><span class="line">GRUB_SERIAL_COMMAND=<span class="string">"serial --speed=115200"</span></span><br><span class="line">GRUB_CMDLINE_LINUX=<span class="string">"console=tty0 crashkernel=auto console=ttyS0,115200"</span></span><br><span class="line">numa=off</span><br><span class="line">GRUB_DISABLE_RECOVERY=<span class="string">"true"</span></span><br></pre></td></tr></table></figure><ul><li>重新生成 grub2 配置文件<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cp /boot/grub2/grub.cfg&#123;,.bak&#125;</span></span><br><span class="line"><span class="comment"># grub2-mkconfig -o /boot/grub2/grub.cfg</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="4、创建CA证书和秘钥"><a href="#4、创建CA证书和秘钥" class="headerlink" title="4、创建CA证书和秘钥"></a>4、创建CA证书和秘钥</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为确保安全，kubernetes 系统各组件需要使用 x509 证书对通信进行加密和认证。CA (Certificate Authority) 是自签名的根证书，用来签名后续创建的其它证书。使用 CloudFlare 的 PKI 工具集 cfssl 创建所有证书，证书均在一台master节点进行操作，然后通过远程分发到其他的服务器上去。</p><ul><li><strong>注意</strong>: 每生成的证书均要进行分发到其他的master节点</li></ul><h4 id="4-1、安装-cfssl-工具集"><a href="#4-1、安装-cfssl-工具集" class="headerlink" title="4.1、安装 cfssl 工具集"></a>4.1、安装 cfssl 工具集</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># curl -o cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64</span></span><br><span class="line"><span class="comment"># curl -o cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64</span></span><br><span class="line"><span class="comment"># curl -o cfssl-certinfo https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64</span></span><br><span class="line"><span class="comment"># chmod +x * &amp;&amp;mv cfssl* /usr/bin/</span></span><br><span class="line"><span class="comment"># scp /usr/bin/cfssl* &#123;master-ip&#125;:/usr/bin</span></span><br></pre></td></tr></table></figure><h4 id="4-2、创建根证书-CA"><a href="#4-2、创建根证书-CA" class="headerlink" title="4.2、创建根证书 (CA)"></a>4.2、创建根证书 (CA)</h4><p>CA 证书是集群所有节点共享的，只需要创建一个 CA 证书，后续创建的所有证书都由它签名。</p><h4 id="4-3、创建配置文件"><a href="#4-3、创建配置文件" class="headerlink" title="4.3、创建配置文件"></a>4.3、创建配置文件</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CA 配置文件用于配置根证书的使用场景 (profile) 和具体参数 (usage，过期时间、服务端认证、客户端认证、加密等)，后续在签名其它证书时需要指定特定场景。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># mkdir k8s &amp;&amp; cd k8s#后面k8s生成所需要的证书均在该目录执行</span></span><br></pre></td></tr></table></figure><ul><li><p>ca-config.json</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; ca-config.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"signing"</span>: &#123;</span><br><span class="line">    <span class="string">"default"</span>: &#123;</span><br><span class="line">      <span class="string">"expiry"</span>: <span class="string">"87600h"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"profiles"</span>: &#123;</span><br><span class="line">      <span class="string">"kubernetes"</span>: &#123;</span><br><span class="line">        <span class="string">"usages"</span>: [</span><br><span class="line">            <span class="string">"signing"</span>,</span><br><span class="line">            <span class="string">"key encipherment"</span>,</span><br><span class="line">            <span class="string">"server auth"</span>,</span><br><span class="line">            <span class="string">"client auth"</span></span><br><span class="line">        ],</span><br><span class="line">        <span class="string">"expiry"</span>: <span class="string">"87600h"</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></li><li><p>signing：表示该证书可用于签名其它证书，生成的 ca.pem 证书中 CA=TRUE；</p></li><li><p>server auth：表示 client 可以用该该证书对 server 提供的证书进行验证；</p></li><li><p>client auth：表示 server 可以用该该证书对 client 提供的证书进行验证；</p></li></ul><h4 id="4-4、创建证书签名请求文件"><a href="#4-4、创建证书签名请求文件" class="headerlink" title="4.4、创建证书签名请求文件"></a>4.4、创建证书签名请求文件</h4><ul><li><p>ca-csr.json</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="string">"CN"</span>: <span class="string">"kubernetes"</span>,</span><br><span class="line">  <span class="string">"key"</span>: &#123;</span><br><span class="line">    <span class="string">"algo"</span>: <span class="string">"rsa"</span>,</span><br><span class="line">    <span class="string">"size"</span>: 2048</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"names"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"C"</span>: <span class="string">"CN"</span>,</span><br><span class="line">      <span class="string">"ST"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">      <span class="string">"L"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">      <span class="string">"O"</span>: <span class="string">"k8s"</span>,</span><br><span class="line">      <span class="string">"OU"</span>: <span class="string">"4Paradigm"</span></span><br><span class="line">    &#125;</span><br><span class="line">  ],</span><br><span class="line">  <span class="string">"ca"</span>: &#123;</span><br><span class="line">    <span class="string">"expiry"</span>: <span class="string">"876000h"</span></span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></li><li><p>CN：Common Name，kube-apiserver 从证书中提取该字段作为请求的用户名 (User Name)，浏览器使用该字段验证网站是否合法；</p></li><li><p>O：Organization，kube-apiserver 从证书中提取该字段作为请求用户所属的组 (Group)；</p></li><li><p>kube-apiserver 将提取的 User、Group 作为 RBAC 授权的用户标识；</p></li><li><p>生成 CA 证书和私钥</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cfssl gencert -initca ca-csr.json | cfssljson -bare ca</span></span><br><span class="line"><span class="comment"># ls ca*</span></span><br><span class="line"><span class="comment"># mkdir -p /etc/kubernetes/ssl &amp;&amp; cp ca*.pem ca-config.json /etc/kubernetes/ssl</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="5-部署-kubectl-命令行工具"><a href="#5-部署-kubectl-命令行工具" class="headerlink" title="5.部署 kubectl 命令行工具"></a>5.部署 kubectl 命令行工具</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kubectl 默认从 ~/.kube/config 文件读取 kube-apiserver 地址和认证信息，如果没有配置，执行 kubectl 命令时可能会出错：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get pods</span></span><br><span class="line">The connection to the server localhost:8080 was refused - did you specify the right host or port?</span><br></pre></td></tr></table></figure><ul><li><strong>注意</strong>:</li></ul><ul><li>1.如果没有特殊指明，本文档的所有操作均在 zhangjun-k8s01 节点上执行，然后远程分发文件和执行命令；</li><li>2.本文档只需要部署一次，生成的 kubeconfig 文件是通用的，可以拷贝到需要执行 kubectl 命令的机器，重命名为 ~/.kube/config；</li></ul><h4 id="5-1、下载和分发-kubectl-二进制文件"><a href="#5-1、下载和分发-kubectl-二进制文件" class="headerlink" title="5.1、下载和分发 kubectl 二进制文件"></a>5.1、下载和分发 kubectl 二进制文件</h4><p>这里吧把node和master所需要的包均给一次性分发</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># wget https://dl.k8s.io/v1.14.6/kubernetes-server-linux-amd64.tar.gz</span></span><br><span class="line"><span class="comment"># tar -xzvf kubernetes-client-linux-amd64.tar.gz</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># master 节点</span></span><br><span class="line"><span class="comment"># scp kubernetes/server/bin/&#123;apiextensions-apiserver,cloud-controller-manager,kube-apiserver,kube-controller-manager,kube-proxy,kube-scheduler,kubeadm,kubectl,kubelet,mounter&#125; &#123;master-ip&#125;:/usr/bin/</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># node 节点</span></span><br><span class="line"><span class="comment"># scp kubernetes/server/bin/&#123;kube-proxy,kubelet&#125; &#123;node-ip&#125;:/usr/bin/</span></span><br></pre></td></tr></table></figure><h4 id="5-2、创建-admin-证书和私钥"><a href="#5-2、创建-admin-证书和私钥" class="headerlink" title="5.2、创建 admin 证书和私钥"></a>5.2、创建 admin 证书和私钥</h4><p>kubectl 与 apiserver https 安全端口通信，apiserver 对提供的证书进行认证和授权。<br>kubectl 作为集群的管理工具，需要被授予最高权限，这里创建具有<strong>最高权限</strong>的 admin 证书。</p><ul><li><p>创建证书签名请求:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat &gt; admin-csr.json &lt;&lt;EOF</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"CN"</span>: <span class="string">"admin"</span>,</span><br><span class="line">  <span class="string">"hosts"</span>: [],</span><br><span class="line">  <span class="string">"key"</span>: &#123;</span><br><span class="line">    <span class="string">"algo"</span>: <span class="string">"rsa"</span>,</span><br><span class="line">    <span class="string">"size"</span>: 2048</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"names"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"C"</span>: <span class="string">"CN"</span>,</span><br><span class="line">      <span class="string">"ST"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">      <span class="string">"L"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">      <span class="string">"O"</span>: <span class="string">"system:masters"</span>,</span><br><span class="line">      <span class="string">"OU"</span>: <span class="string">"4Paradigm"</span></span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></li><li><p>O 为 system:masters，kube-apiserver 收到该证书后将请求的 Group 设置为 system:masters；</p></li><li><p>预定义的 ClusterRoleBinding cluster-admin 将 Group system:masters 与 Role cluster-admin 绑定，该 Role 授予所有 API的权限；</p></li><li><p>该证书只会被 kubectl 当做 client 证书使用，所以 hosts 字段为空；</p></li><li><p>生成证书和私钥</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json  -profile=kubernetes admin-csr.json | cfssljson -bare admin</span></span><br><span class="line"><span class="comment"># ls admin*</span></span><br><span class="line"><span class="comment"># cp admin*.pem /etc/kubernetes/ssl/</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="5-3、创建-kubeconfig-文件"><a href="#5-3、创建-kubeconfig-文件" class="headerlink" title="5.3、创建 kubeconfig 文件"></a>5.3、创建 kubeconfig 文件</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kubeconfig 为 kubectl 的配置文件，包含访问 apiserver 的所有信息，如 apiserver 地址、CA 证书和自身使用的证书；</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置集群API地址</span></span><br><span class="line"><span class="comment"># KUBE_APISERVER="https://172.21.16.45:8443"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置集群参数</span></span><br><span class="line">kubectl config <span class="built_in">set</span>-cluster kubernetes \</span><br><span class="line">  --certificate-authority=ca.pem \</span><br><span class="line">  --embed-certs=<span class="literal">true</span> \</span><br><span class="line">  --server=<span class="variable">$&#123;KUBE_APISERVER&#125;</span> \</span><br><span class="line">  --kubeconfig=kubectl.kubeconfig</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置客户端认证参数</span></span><br><span class="line">kubectl config <span class="built_in">set</span>-credentials admin \</span><br><span class="line">  --client-certificate=admin.pem \</span><br><span class="line">  --client-key=admin-key.pem \</span><br><span class="line">  --embed-certs=<span class="literal">true</span> \</span><br><span class="line">  --kubeconfig=kubectl.kubeconfig</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置上下文参数</span></span><br><span class="line">kubectl config <span class="built_in">set</span>-context kubernetes \</span><br><span class="line">  --cluster=kubernetes \</span><br><span class="line">  --user=admin \</span><br><span class="line">  --kubeconfig=kubectl.kubeconfig</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置默认上下文</span></span><br><span class="line">kubectl config use-context kubernetes --kubeconfig=kubectl.kubeconfig</span><br></pre></td></tr></table></figure><ul><li><strong>提示</strong>: 分发<code>kubectl.kubeconfig</code>文件，吧文件命名<code>~/.kube/config</code>;</li><li>–certificate-authority：验证 kube-apiserver 证书的根证书；</li><li>–client-certificate、–client-key：刚生成的 admin 证书和私钥，连接 kube-apiserver 时使用；</li><li>–embed-certs=true：将 ca.pem 和 admin.pem 证书内容嵌入到生成的 kubectl.kubeconfig 文件中(不加时，写入的是证书文件路径，后续拷贝 kubeconfig 到其它机器时，还需要单独拷贝证书文件，不方便。)；</li></ul><h3 id="6、部署etcd集群"><a href="#6、部署etcd集群" class="headerlink" title="6、部署etcd集群"></a>6、部署etcd集群</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;etcd 是基于 Raft 的分布式 key-value 存储系统，由 CoreOS 开发，常用于服务发现、共享配置以及并发控制（如 leader 选举、分布式锁等）。kubernetes 使用 etcd 存储所有运行数据。</p><p>三节点高可用 etcd 集群的步骤：</p><ul><li>下载和分发 etcd 二进制文件；</li><li>创建 etcd 集群各节点的 x509 证书，用于加密客户端(如 etcdctl) 与 etcd 集群、etcd 集群之间的数据流；</li><li>创建 etcd 的 systemd unit 文件，配置服务参数</li><li>检查集群工作状态;</li></ul><ul><li><strong>注意</strong>: 均在一台master<code>[etcd]</code>节点操作，其他master<code>[etcd]</code>节点通过分发</li></ul><h4 id="6-1、下载和分发-etcd-二进制文件"><a href="#6-1、下载和分发-etcd-二进制文件" class="headerlink" title="6.1、下载和分发 etcd 二进制文件"></a>6.1、下载和分发 etcd 二进制文件</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># mkdir etcd &amp;&amp;cd etcd</span></span><br><span class="line"><span class="comment"># https://github.com/coreos/etcd/releases/download/v3.3.13/etcd-v3.3.13-linux-amd64.tar.gz</span></span><br><span class="line"><span class="comment"># tar -xvf etcd-v3.3.13-linux-amd64.tar.gz</span></span><br><span class="line"><span class="comment"># scp etcd* &#123;master-ip&#125;:/usr/bin/</span></span><br></pre></td></tr></table></figure><h4 id="6-2、创建-etcd-证书和私钥"><a href="#6-2、创建-etcd-证书和私钥" class="headerlink" title="6.2、创建 etcd 证书和私钥"></a>6.2、创建 etcd 证书和私钥</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat &gt; etcd-csr.json &lt;&lt;EOF</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"CN"</span>: <span class="string">"etcd"</span>,</span><br><span class="line">  <span class="string">"hosts"</span>: [</span><br><span class="line">    <span class="string">"127.0.0.1"</span>,</span><br><span class="line">    <span class="string">"172.21.17.30"</span>,</span><br><span class="line">    <span class="string">"172.21.17.31"</span>,</span><br><span class="line">    <span class="string">"172.21.16.110"</span></span><br><span class="line">  ],</span><br><span class="line">  <span class="string">"key"</span>: &#123;</span><br><span class="line">    <span class="string">"algo"</span>: <span class="string">"rsa"</span>,</span><br><span class="line">    <span class="string">"size"</span>: 2048</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"names"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"C"</span>: <span class="string">"CN"</span>,</span><br><span class="line">      <span class="string">"ST"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">      <span class="string">"L"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">      <span class="string">"O"</span>: <span class="string">"k8s"</span>,</span><br><span class="line">      <span class="string">"OU"</span>: <span class="string">"4Paradigm"</span></span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li>hosts 字段指定授权使用该证书的 etcd 节点 IP 或域名列表，需要将 etcd 集群的三个节点 IP 都列在其中；</li><li>生成证书和私钥<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cfssl gencert -ca=ca.pem cfssl gencert -ca=ca-key.pem -config=ca-config.json -profile=kubernetes etcd-csr.json | cfssljson -bare etcd</span></span><br><span class="line"><span class="comment"># ls etcd*pem</span></span><br><span class="line"><span class="comment"># mkdir -p /etc/etcd/ssl &amp;&amp; cp etcd*pem /etc/etcd/ssl/</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="6-3、创建-etcd-的-systemd-unit-模板文件"><a href="#6-3、创建-etcd-的-systemd-unit-模板文件" class="headerlink" title="6.3、创建 etcd 的 systemd unit 模板文件"></a>6.3、创建 etcd 的 systemd unit 模板文件</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat /etc/systemd/system/etcd.service</span></span><br><span class="line">[Unit]</span><br><span class="line">Description=Etcd Server</span><br><span class="line">After=network.target</span><br><span class="line">After=network-online.target</span><br><span class="line">Wants=network-online.target</span><br><span class="line">Documentation=https://github.com/coreos</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=notify</span><br><span class="line">WorkingDirectory=/var/lib/etcd/data</span><br><span class="line">ExecStart=/usr/bin/etcd \</span><br><span class="line">  --data-dir=/var/lib/etcd/data \</span><br><span class="line">  --wal-dir=/var/lib/etcd/wal \</span><br><span class="line">  --name=etcd1 \<span class="comment">#根据节点名称进行变化</span></span><br><span class="line">  --cert-file=/etc/etcd/ssl/etcd.pem \</span><br><span class="line">  --key-file=/etc/etcd/ssl/etcd-key.pem \</span><br><span class="line">  --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --peer-cert-file=/etc/etcd/ssl/etcd.pem \</span><br><span class="line">  --peer-key-file=/etc/etcd/ssl/etcd-key.pem \</span><br><span class="line">  --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --peer-client-cert-auth \</span><br><span class="line">  --client-cert-auth \</span><br><span class="line">  --listen-peer-urls=https://172.21.17.30:2380 \</span><br><span class="line">  --initial-advertise-peer-urls=https://172.21.17.30:2380 \</span><br><span class="line">  --listen-client-urls=https://172.21.17.30:2379,http://127.0.0.1:2379 \</span><br><span class="line">  --advertise-client-urls=https://172.21.17.30:2379 \</span><br><span class="line">  --initial-cluster-token=etcd-cluster-0 \</span><br><span class="line">  --initial-cluster=etcd1=https://172.21.17.30:2380,etcd2=https://172.21.17.31:2380,etcd3=https://172.21.16.110:2380 \</span><br><span class="line">  --initial-cluster-state=new \</span><br><span class="line">  --auto-compaction-mode=periodic \</span><br><span class="line">  --auto-compaction-retention=1 \</span><br><span class="line">  --max-request-bytes=33554432 \</span><br><span class="line">  --quota-backend-bytes=6442450944 \</span><br><span class="line">  --heartbeat-interval=250 \</span><br><span class="line">  --election-timeout=2000</span><br><span class="line">Restart=on-failure</span><br><span class="line">RestartSec=5</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># mkdir -p /var/lib/etcd/&#123;data,wal&#125;</span></span><br></pre></td></tr></table></figure><ul><li>WorkingDirectory、–data-dir：指定工作目录和数据目录为 ${ETCD_DATA_DIR}，需在启动服务前创建这个目录；</li><li>–wal-dir：指定 wal 目录，为了提高性能，一般使用 SSD 或者和 –data-dir 不同的磁盘；</li><li>–name：指定节点名称，当 –initial-cluster-state 值为 new 时，–name 的参数值必须位于 –initial-cluster 列表中；</li><li>–cert-file、–key-file：etcd server 与 client 通信时使用的证书和私钥；</li><li>–trusted-ca-file：签名 client 证书的 CA 证书，用于验证 client 证书；</li><li>–peer-cert-file、–peer-key-file：etcd 与 peer 通信使用的证书和私钥；</li><li>–peer-trusted-ca-file：签名 peer 证书的 CA 证书，用于验证 peer 证书；</li></ul><h4 id="6-4、启动-etcd-服务"><a href="#6-4、启动-etcd-服务" class="headerlink" title="6.4、启动 etcd 服务"></a>6.4、启动 etcd 服务</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># systemctl daemon-reload &amp;&amp; systemctl enable etcd &amp;&amp; systemctl restart etcd &amp;&amp; systemctl status etcd</span></span><br></pre></td></tr></table></figure><h4 id="6-5、检查启动结果"><a href="#6-5、检查启动结果" class="headerlink" title="6.5、检查启动结果"></a>6.5、检查启动结果</h4><ul><li>确保状态为 active (running)，否则查看日志，确认原因：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># journalctl -u etcd</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="6-6、验证服务状态"><a href="#6-6、验证服务状态" class="headerlink" title="6.6、验证服务状态"></a>6.6、验证服务状态</h4><p>部署完 etcd 集群后，在任一 etcd 节点上执行如下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ETCDCTL_API=3 etcdctl \</span></span><br><span class="line">    --endpoints=https://172.21.17.31:2379 \</span><br><span class="line">    --cacert=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">    --cert=/etc/etcd/ssl/etcd.pem \</span><br><span class="line">    --key=/etc/etcd/ssl/etcd-key.pem endpoint health</span><br></pre></td></tr></table></figure><p>检查输出均为 healthy 时表示集群服务正常</p><h4 id="6-7、查看当前的-leader"><a href="#6-7、查看当前的-leader" class="headerlink" title="6.7、查看当前的 leader"></a>6.7、查看当前的 leader</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ETCD_ENDPOINTS="https://172.21.17.30:2379,https://172.21.17.31:2379,https://172.21.16.110:2379"</span></span><br><span class="line"><span class="comment"># ETCDCTL_API=3 etcdctl \</span></span><br><span class="line">  -w table --cacert=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --cert=/etc/etcd/ssl/etcd.pem \</span><br><span class="line">  --key=/etc/etcd/ssl/etcd-key.pem \</span><br><span class="line">  --endpoints=<span class="variable">$&#123;ETCD_ENDPOINTS&#125;</span> endpoint status </span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+-----------+------------+</span><br><span class="line">|          ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX |</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+-----------+------------+</span><br><span class="line">|  https://172.21.17.30:2379 | 5d23ebc4382fa16f |  3.3.13 |  1.2 MB |     <span class="literal">false</span> |        83 |      58127 |</span><br><span class="line">|  https://172.21.17.31:2379 |  ceaae5134701946 |  3.3.13 |  1.2 MB |     <span class="literal">false</span> |        83 |      58127 |</span><br><span class="line">| https://172.21.16.110:2379 | 575020c8e15d3a06 |  3.3.13 |  1.2 MB |      <span class="literal">true</span> |        83 |      58128 |</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+-----------+------------+</span><br></pre></td></tr></table></figure><ul><li>当前的 leader 为 172.21.16.110</li></ul><h3 id="7、部署-flannel-网络"><a href="#7、部署-flannel-网络" class="headerlink" title="7、部署 flannel 网络"></a>7、部署 flannel 网络</h3><p>flannel 网络部署在node节点，证书在master节点生成分发</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kubernetes 要求集群内各节点(包括 master 节点)能通过 Pod 网段互联互通。flannel 使用 vxlan 技术为各节点创建一个可以互通的 Pod 网络，使用的端口为 UDP 8472（需要开放该端口）<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;flanneld 第一次启动时，从 etcd 获取配置的 Pod 网段信息，为本节点分配一个未使用的地址段，然后创建 flannedl.1 网络接口（也可能是其它名称，如 flannel1 等）<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;flannel 将分配给自己的 Pod 网段信息写入 /run/flannel/docker 文件，docker 后续使用这个文件中的环境变量设置 docker0 网桥，从而从这个地址段为本节点的所有 Pod 容器分配 IP。</p><h4 id="7-1、下载和分发-flanneld-二进制文件"><a href="#7-1、下载和分发-flanneld-二进制文件" class="headerlink" title="7.1、下载和分发 flanneld 二进制文件"></a>7.1、下载和分发 flanneld 二进制文件</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># mkdir flannel &amp;&amp;cd flannel</span></span><br><span class="line"><span class="comment"># wget https://github.com/coreos/flannel/releases/download/v0.11.0/flannel-v0.11.0-linux-amd64.tar.gz</span></span><br><span class="line"><span class="comment"># tar -xzvf flannel-v0.11.0-linux-amd64.tar.gz -C flannel</span></span><br></pre></td></tr></table></figure><ul><li>分发flanneld 可执行文件到node节点</li></ul><h4 id="7-2、创建-flannel-证书和私钥"><a href="#7-2、创建-flannel-证书和私钥" class="headerlink" title="7.2、创建 flannel 证书和私钥"></a>7.2、创建 flannel 证书和私钥</h4><ul><li><p>flanneld-csr.json</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; flanneld-csr.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"CN"</span>: <span class="string">"flanneld"</span>,</span><br><span class="line">  <span class="string">"hosts"</span>: [],</span><br><span class="line">  <span class="string">"key"</span>: &#123;</span><br><span class="line">    <span class="string">"algo"</span>: <span class="string">"rsa"</span>,</span><br><span class="line">    <span class="string">"size"</span>: 2048</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"names"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"C"</span>: <span class="string">"CN"</span>,</span><br><span class="line">      <span class="string">"ST"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">      <span class="string">"L"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">      <span class="string">"O"</span>: <span class="string">"k8s"</span>,</span><br><span class="line">      <span class="string">"OU"</span>: <span class="string">"4Paradigm"</span></span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></li><li><p>该证书只会被 kubectl 当做 client 证书使用，所以 hosts 字段为空；</p></li><li><p>生成证书和私钥:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes flanneld-csr.json | cfssljson -bare flanneld </span></span><br><span class="line"><span class="comment"># ls flanneld*pem</span></span><br><span class="line"><span class="comment"># scp flanneld*pem &#123;node-ip&#125;:/etc/flanneld/ssl/</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="7-3、向-etcd-写入集群-Pod-网段信息"><a href="#7-3、向-etcd-写入集群-Pod-网段信息" class="headerlink" title="7.3、向 etcd 写入集群 Pod 网段信息"></a>7.3、向 etcd 写入集群 Pod 网段信息</h4><p><strong>注意</strong>：本步骤只需执行一次。在etcd集群上执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">etcdctl \</span><br><span class="line">  --endpoints=<span class="variable">$&#123;ETCD_ENDPOINTS&#125;</span> \</span><br><span class="line">  --ca-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --cert-file=flanneld.pem \</span><br><span class="line">  --key-file=flanneld-key.pem \</span><br><span class="line">  mk /kubernetes/network/config <span class="string">'&#123;"Network":"172.30.0.0/16", "SubnetLen": 21, "Backend": &#123;"Type": "vxlan"&#125;&#125;'</span></span><br></pre></td></tr></table></figure><ul><li>flanneld 当前版本 (v0.11.0) 不支持 etcd v3，故使用 etcd v2 API 写入配置 key 和网段数据；</li><li>写入的 Pod 网段 ${CLUSTER_CIDR} 地址段（如 /16）必须小于 SubnetLen，必须与 kube-controller-manager 的 –cluster-cidr 参数值一致；</li></ul><h4 id="7-4、创建-flanneld-的-systemd-unit-文件"><a href="#7-4、创建-flanneld-的-systemd-unit-文件" class="headerlink" title="7.4、创建 flanneld 的 systemd unit 文件"></a>7.4、创建 flanneld 的 systemd unit 文件</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat /usr/lib/systemd/system/flanneld.service </span></span><br><span class="line">[Unit]</span><br><span class="line">Description=Flanneld overlay address etcd agent</span><br><span class="line">After=network.target</span><br><span class="line">After=network-online.target</span><br><span class="line">Wants=network-online.target</span><br><span class="line">After=etcd.service</span><br><span class="line">Before=docker.service</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=notify</span><br><span class="line">ExecStart=/usr/bin/flanneld \</span><br><span class="line">  -etcd-cafile=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  -etcd-certfile=/etc/flanneld/ssl/flanneld.pem \</span><br><span class="line">  -etcd-keyfile=/etc/flanneld/ssl/flanneld-key.pem \</span><br><span class="line">  -etcd-endpoints=https://172.21.17.30:2379,https://172.21.17.31:2379,https://172.21.16.110:2379 \</span><br><span class="line">  -etcd-prefix=/kubernetes/network \</span><br><span class="line">  -iface=eth0 \</span><br><span class="line">  -ip-masq</span><br><span class="line">ExecStartPost=/usr/bin/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker</span><br><span class="line">Restart=always</span><br><span class="line">RestartSec=5</span><br><span class="line">StartLimitInterval=0</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">RequiredBy=docker.service</span><br></pre></td></tr></table></figure><ul><li>mk-docker-opts.sh 脚本将分配给 flanneld 的 Pod 子网段信息写入 /run/flannel/docker 文件，后续 docker 启动时使用这个文件中的环境变量配置 docker0 网桥；</li><li>flanneld 使用系统缺省路由所在的接口与其它节点通信，对于有多个网络接口（如内网和公网）的节点，可以用 -iface 参数指定通信接口;</li><li>flanneld 运行时需要 root 权限；</li><li>-ip-masq: flanneld 为访问 Pod 网络外的流量设置 SNAT 规则，同时将传递给 Docker 的变量 –ip-masq（/run/flannel/docker 文件中）设置为 false，这样 Docker 将不再创建 SNAT 规则； Docker 的 –ip-masq 为 true 时，创建的 SNAT 规则比较“暴力”：将所有本节点 Pod 发起的、访问非 docker0 接口的请求做 SNAT，这样访问其他节点 Pod 的请求来源 IP 会被设置为 flannel.1 接口的 IP，导致目的 Pod 看不到真实的来源 Pod IP。 flanneld 创建的 SNAT 规则比较温和，只对访问非 Pod 网段的请求做 SNAT。</li></ul><h4 id="7-5、启动-flanneld-服务"><a href="#7-5、启动-flanneld-服务" class="headerlink" title="7.5、启动 flanneld 服务"></a>7.5、启动 flanneld 服务</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># systemctl daemon-reload &amp;&amp; systemctl enable flanneld &amp;&amp; systemctl restart flanneld &amp;&amp; systemctl status flanneld</span></span><br></pre></td></tr></table></figure><h4 id="7-6、检查分配给各-flanneld-的-Pod-网段信息"><a href="#7-6、检查分配给各-flanneld-的-Pod-网段信息" class="headerlink" title="7.6、检查分配给各 flanneld 的 Pod 网段信息"></a>7.6、检查分配给各 flanneld 的 Pod 网段信息</h4><ul><li><p>查看集群 Pod 网段(/16)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># etcdctl \</span></span><br><span class="line">  --endpoints=<span class="variable">$&#123;ETCD_ENDPOINTS&#125;</span> \</span><br><span class="line">  --ca-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --cert-file=/etc/flanneld/ssl/flanneld.pem \</span><br><span class="line">  --key-file=/etc/flanneld/ssl/flanneld-key.pem \</span><br><span class="line">  get /kubernetes/network/config</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line">&#123;<span class="string">"Network"</span>:<span class="string">"172.30.0.0/16"</span>, <span class="string">"SubnetLen"</span>: 21, <span class="string">"Backend"</span>: &#123;<span class="string">"Type"</span>: <span class="string">"vxlan"</span>&#125;&#125;</span><br></pre></td></tr></table></figure></li><li><p>查看已分配的 Pod 子网段列表(/24):</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># etcdctl \</span></span><br><span class="line">  --endpoints=<span class="variable">$&#123;ETCD_ENDPOINTS&#125;</span> \</span><br><span class="line">  --ca-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --cert-file=/etc/flanneld/ssl/flanneld.pem \</span><br><span class="line">  --key-file=/etc/flanneld/ssl/flanneld-key.pem \</span><br><span class="line">  ls /kubernetes/network/subnets</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">/kubernetes/network/subnets/172.30.232.0-21</span><br><span class="line">/kubernetes/network/subnets/172.30.128.0-21</span><br><span class="line">/kubernetes/network/subnets/172.30.176.0-21</span><br></pre></td></tr></table></figure></li><li><p>查看某一 Pod 网段对应的节点 IP 和 flannel 接口地址</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># etcdctl \</span></span><br><span class="line">  --endpoints=<span class="variable">$&#123;ETCD_ENDPOINTS&#125;</span> \</span><br><span class="line">  --ca-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --cert-file=/etc/flanneld/ssl/flanneld.pem \</span><br><span class="line">  --key-file=/etc/flanneld/ssl/flanneld-key.pem \</span><br><span class="line">  get /kubernetes/network/subnets/172.30.232.0-21</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">&#123;<span class="string">"PublicIP"</span>:<span class="string">"172.21.16.204"</span>,<span class="string">"BackendType"</span>:<span class="string">"vxlan"</span>,<span class="string">"BackendData"</span>:&#123;<span class="string">"VtepMAC"</span>:<span class="string">"f6:50:05:5c:9a:20"</span>&#125;&#125;</span><br></pre></td></tr></table></figure></li><li><p>172.30.232.0/21 被分配给节点172.21.16.204）；</p></li><li><p>VtepMAC 为172.21.16.204节点的 flannel.1 网卡 MAC 地址；</p></li></ul><h4 id="7-7、检查节点-flannel-网络信息"><a href="#7-7、检查节点-flannel-网络信息" class="headerlink" title="7.7、检查节点 flannel 网络信息"></a>7.7、检查节点 flannel 网络信息</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ip addr show</span></span><br></pre></td></tr></table></figure><ul><li><p>flannel.1 网卡的地址为分配的 Pod 子网段的第一个 IP（.0），且是 /32 的地址；</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ip route show |grep flannel.1</span></span><br><span class="line">172.30.128.0/21 via 172.30.128.0 dev flannel.1 onlink </span><br><span class="line">172.30.176.0/21 via 172.30.176.0 dev flannel.1 onlink</span><br></pre></td></tr></table></figure></li><li><p>到其它节点 Pod 网段请求都被转发到 flannel.1 网卡；</p></li><li><p>flanneld 根据 etcd 中子网段的信息，如/kubernetes/network/subnets/172.30.232.0-21 ，来决定进请求发送给哪个节点的互联 IP；</p></li><li><p>验证各节点能通过 Pod 网段互通</p></li></ul><h3 id="8、master节点部署"><a href="#8、master节点部署" class="headerlink" title="8、master节点部署"></a>8、master节点部署</h3><p>kubernetes master 节点运行如下组件：</p><ul><li>kube-apiserver</li><li>kube-scheduler</li><li>kube-controller-manager<br>kube-apiserver、kube-scheduler 和 kube-controller-manager 均以多实例模式运行：<br>1、kube-scheduler 和 kube-controller-manager 会自动选举产生一个 leader 实例，其它实例处于阻塞模式，当 leader 挂了后，重新选举产生新的 leader，从而保证服务可用性；<br>2、kube-apiserver 是无状态的，需要通过<a href="https://xxlaila.github.io/2019/08/10/haproxy-keepalived/">haproxy+keepalived</a>进行代理访问，从而保证服务可用性；</li></ul><h4 id="8-1、创建-kubernetes-证书和私钥"><a href="#8-1、创建-kubernetes-证书和私钥" class="headerlink" title="8.1、创建 kubernetes 证书和私钥"></a>8.1、创建 kubernetes 证书和私钥</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat &gt; kubernetes-csr.json &lt;&lt;EOF</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"CN"</span>: <span class="string">"kubernetes"</span>,</span><br><span class="line">  <span class="string">"hosts"</span>: [</span><br><span class="line">    <span class="string">"127.0.0.1"</span>,</span><br><span class="line">    <span class="string">"172.21.17.30"</span>,</span><br><span class="line">    <span class="string">"172.21.17.31"</span>,</span><br><span class="line">    <span class="string">"172.21.16.110"</span>,</span><br><span class="line">    <span class="string">"172.21.16.45"</span>,</span><br><span class="line">    <span class="string">"10.254.0.1"</span>,</span><br><span class="line">    <span class="string">"kubernetes"</span>,</span><br><span class="line">    <span class="string">"kubernetes.default"</span>,</span><br><span class="line">    <span class="string">"kubernetes.default.svc"</span>,</span><br><span class="line">    <span class="string">"kubernetes.default.svc.cluster"</span>,</span><br><span class="line">    <span class="string">"kubernetes.default.svc.cluster.local."</span></span><br><span class="line">  ],</span><br><span class="line">  <span class="string">"key"</span>: &#123;</span><br><span class="line">    <span class="string">"algo"</span>: <span class="string">"rsa"</span>,</span><br><span class="line">    <span class="string">"size"</span>: 2048</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"names"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"C"</span>: <span class="string">"CN"</span>,</span><br><span class="line">      <span class="string">"ST"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">      <span class="string">"L"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">      <span class="string">"O"</span>: <span class="string">"k8s"</span>,</span><br><span class="line">      <span class="string">"OU"</span>: <span class="string">"4Paradigm"</span></span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li><p>hosts 字段指定授权使用该证书的 IP 和域名列表，这里列出了 master 节点 IP、kubernetes 服务的 IP 和域名,以及VIP地址；</p></li><li><p>kubernetes 服务 IP 是 apiserver 自动创建的，一般是 –service-cluster-ip-range 参数指定的网段的第一个IP,后续可以通过下面命令获取：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get svc kubernetes</span></span><br><span class="line">NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">kubernetes   ClusterIP   10.254.0.1   &lt;none&gt;        443/TCP   4h13m</span><br></pre></td></tr></table></figure></li><li><p>生成证书和私钥</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes</span></span><br><span class="line"><span class="comment"># ls kubernetes*pem</span></span><br><span class="line">kubernetes-key.pem  kubernetes.pem</span><br><span class="line"><span class="comment"># cp kubernetes*pem /etc/kubernetes/ssl/</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="8-2、创建加密配置文件"><a href="#8-2、创建加密配置文件" class="headerlink" title="8.2、创建加密配置文件"></a>8.2、创建加密配置文件</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64)</span></span><br><span class="line"><span class="comment"># cat &gt; encryption-config.yaml &lt;&lt;EOF</span></span><br><span class="line">kind: EncryptionConfig</span><br><span class="line">apiVersion: v1</span><br><span class="line">resources:</span><br><span class="line">  - resources:</span><br><span class="line">      - secrets</span><br><span class="line">    providers:</span><br><span class="line">      - aescbc:</span><br><span class="line">          keys:</span><br><span class="line">            - name: key1</span><br><span class="line">              secret: <span class="variable">$&#123;ENCRYPTION_KEY&#125;</span></span><br><span class="line">      - identity: &#123;&#125;</span><br><span class="line">EOF</span><br><span class="line"><span class="comment"># cp encryption-config.yaml /etc/kubernetes/</span></span><br></pre></td></tr></table></figure><h4 id="8-3、创建审计策略文件"><a href="#8-3、创建审计策略文件" class="headerlink" title="8.3、创建审计策略文件"></a>8.3、创建审计策略文件</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat &gt; audit-policy.yaml &lt;&lt;EOF</span></span><br><span class="line">apiVersion: audit.k8s.io/v1beta1</span><br><span class="line">kind: Policy</span><br><span class="line">rules:</span><br><span class="line">  <span class="comment"># The following requests were manually identified as high-volume and low-risk, so drop them.</span></span><br><span class="line">  - level: None</span><br><span class="line">    resources:</span><br><span class="line">      - group: <span class="string">""</span></span><br><span class="line">        resources:</span><br><span class="line">          - endpoints</span><br><span class="line">          - services</span><br><span class="line">          - services/status</span><br><span class="line">    users:</span><br><span class="line">      - <span class="string">'system:kube-proxy'</span></span><br><span class="line">    verbs:</span><br><span class="line">      - watch</span><br><span class="line"></span><br><span class="line">  - level: None</span><br><span class="line">    resources:</span><br><span class="line">      - group: <span class="string">""</span></span><br><span class="line">        resources:</span><br><span class="line">          - nodes</span><br><span class="line">          - nodes/status</span><br><span class="line">    userGroups:</span><br><span class="line">      - <span class="string">'system:nodes'</span></span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line"></span><br><span class="line">  - level: None</span><br><span class="line">    namespaces:</span><br><span class="line">      - kube-system</span><br><span class="line">    resources:</span><br><span class="line">      - group: <span class="string">""</span></span><br><span class="line">        resources:</span><br><span class="line">          - endpoints</span><br><span class="line">    users:</span><br><span class="line">      - <span class="string">'system:kube-controller-manager'</span></span><br><span class="line">      - <span class="string">'system:kube-scheduler'</span></span><br><span class="line">      - <span class="string">'system:serviceaccount:kube-system:endpoint-controller'</span></span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - update</span><br><span class="line"></span><br><span class="line">  - level: None</span><br><span class="line">    resources:</span><br><span class="line">      - group: <span class="string">""</span></span><br><span class="line">        resources:</span><br><span class="line">          - namespaces</span><br><span class="line">          - namespaces/status</span><br><span class="line">          - namespaces/finalize</span><br><span class="line">    users:</span><br><span class="line">      - <span class="string">'system:apiserver'</span></span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Don't log HPA fetching metrics.</span></span><br><span class="line">  - level: None</span><br><span class="line">    resources:</span><br><span class="line">      - group: metrics.k8s.io</span><br><span class="line">    users:</span><br><span class="line">      - <span class="string">'system:kube-controller-manager'</span></span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Don't log these read-only URLs.</span></span><br><span class="line">  - level: None</span><br><span class="line">    nonResourceURLs:</span><br><span class="line">      - <span class="string">'/healthz*'</span></span><br><span class="line">      - /version</span><br><span class="line">      - <span class="string">'/swagger*'</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Don't log events requests.</span></span><br><span class="line">  - level: None</span><br><span class="line">    resources:</span><br><span class="line">      - group: <span class="string">""</span></span><br><span class="line">        resources:</span><br><span class="line">          - events</span><br><span class="line"></span><br><span class="line">  <span class="comment"># node and pod status calls from nodes are high-volume and can be large, don't log responses for expected updates from nodes</span></span><br><span class="line">  - level: Request</span><br><span class="line">    omitStages:</span><br><span class="line">      - RequestReceived</span><br><span class="line">    resources:</span><br><span class="line">      - group: <span class="string">""</span></span><br><span class="line">        resources:</span><br><span class="line">          - nodes/status</span><br><span class="line">          - pods/status</span><br><span class="line">    users:</span><br><span class="line">      - kubelet</span><br><span class="line">      - <span class="string">'system:node-problem-detector'</span></span><br><span class="line">      - <span class="string">'system:serviceaccount:kube-system:node-problem-detector'</span></span><br><span class="line">    verbs:</span><br><span class="line">      - update</span><br><span class="line">      - patch</span><br><span class="line"></span><br><span class="line">  - level: Request</span><br><span class="line">    omitStages:</span><br><span class="line">      - RequestReceived</span><br><span class="line">    resources:</span><br><span class="line">      - group: <span class="string">""</span></span><br><span class="line">        resources:</span><br><span class="line">          - nodes/status</span><br><span class="line">          - pods/status</span><br><span class="line">    userGroups:</span><br><span class="line">      - <span class="string">'system:nodes'</span></span><br><span class="line">    verbs:</span><br><span class="line">      - update</span><br><span class="line">      - patch</span><br><span class="line"></span><br><span class="line">  <span class="comment"># deletecollection calls can be large, don't log responses for expected namespace deletions</span></span><br><span class="line">  - level: Request</span><br><span class="line">    omitStages:</span><br><span class="line">      - RequestReceived</span><br><span class="line">    users:</span><br><span class="line">      - <span class="string">'system:serviceaccount:kube-system:namespace-controller'</span></span><br><span class="line">    verbs:</span><br><span class="line">      - deletecollection</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Secrets, ConfigMaps, and TokenReviews can contain sensitive &amp; binary data,</span></span><br><span class="line">  <span class="comment"># so only log at the Metadata level.</span></span><br><span class="line">  - level: Metadata</span><br><span class="line">    omitStages:</span><br><span class="line">      - RequestReceived</span><br><span class="line">    resources:</span><br><span class="line">      - group: <span class="string">""</span></span><br><span class="line">        resources:</span><br><span class="line">          - secrets</span><br><span class="line">          - configmaps</span><br><span class="line">      - group: authentication.k8s.io</span><br><span class="line">        resources:</span><br><span class="line">          - tokenreviews</span><br><span class="line">  <span class="comment"># Get repsonses can be large; skip them.</span></span><br><span class="line">  - level: Request</span><br><span class="line">    omitStages:</span><br><span class="line">      - RequestReceived</span><br><span class="line">    resources:</span><br><span class="line">      - group: <span class="string">""</span></span><br><span class="line">      - group: admissionregistration.k8s.io</span><br><span class="line">      - group: apiextensions.k8s.io</span><br><span class="line">      - group: apiregistration.k8s.io</span><br><span class="line">      - group: apps</span><br><span class="line">      - group: authentication.k8s.io</span><br><span class="line">      - group: authorization.k8s.io</span><br><span class="line">      - group: autoscaling</span><br><span class="line">      - group: batch</span><br><span class="line">      - group: certificates.k8s.io</span><br><span class="line">      - group: extensions</span><br><span class="line">      - group: metrics.k8s.io</span><br><span class="line">      - group: networking.k8s.io</span><br><span class="line">      - group: policy</span><br><span class="line">      - group: rbac.authorization.k8s.io</span><br><span class="line">      - group: scheduling.k8s.io</span><br><span class="line">      - group: settings.k8s.io</span><br><span class="line">      - group: storage.k8s.io</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Default level for known APIs</span></span><br><span class="line">  - level: RequestResponse</span><br><span class="line">    omitStages:</span><br><span class="line">      - RequestReceived</span><br><span class="line">    resources:</span><br><span class="line">      - group: <span class="string">""</span></span><br><span class="line">      - group: admissionregistration.k8s.io</span><br><span class="line">      - group: apiextensions.k8s.io</span><br><span class="line">      - group: apiregistration.k8s.io</span><br><span class="line">      - group: apps</span><br><span class="line">      - group: authentication.k8s.io</span><br><span class="line">      - group: authorization.k8s.io</span><br><span class="line">      - group: autoscaling</span><br><span class="line">      - group: batch</span><br><span class="line">      - group: certificates.k8s.io</span><br><span class="line">      - group: extensions</span><br><span class="line">      - group: metrics.k8s.io</span><br><span class="line">      - group: networking.k8s.io</span><br><span class="line">      - group: policy</span><br><span class="line">      - group: rbac.authorization.k8s.io</span><br><span class="line">      - group: scheduling.k8s.io</span><br><span class="line">      - group: settings.k8s.io</span><br><span class="line">      - group: storage.k8s.io</span><br><span class="line">      </span><br><span class="line">  <span class="comment"># Default level for all other requests.</span></span><br><span class="line">  - level: Metadata</span><br><span class="line">    omitStages:</span><br><span class="line">      - RequestReceived</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># cp audit-policy.yaml /etc/kubernetes/</span></span><br></pre></td></tr></table></figure><h4 id="8-4、创建后续访问-metrics-server-使用的证书"><a href="#8-4、创建后续访问-metrics-server-使用的证书" class="headerlink" title="8.4、创建后续访问 metrics-server 使用的证书"></a>8.4、创建后续访问 metrics-server 使用的证书</h4><ul><li><p>创建证书签名请求:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat &gt; proxy-client-csr.json &lt;&lt;EOF</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"CN"</span>: <span class="string">"aggregator"</span>,</span><br><span class="line">  <span class="string">"hosts"</span>: [],</span><br><span class="line">  <span class="string">"key"</span>: &#123;</span><br><span class="line">    <span class="string">"algo"</span>: <span class="string">"rsa"</span>,</span><br><span class="line">    <span class="string">"size"</span>: 2048</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"names"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"C"</span>: <span class="string">"CN"</span>,</span><br><span class="line">      <span class="string">"ST"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">      <span class="string">"L"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">      <span class="string">"O"</span>: <span class="string">"k8s"</span>,</span><br><span class="line">      <span class="string">"OU"</span>: <span class="string">"4Paradigm"</span></span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></li><li><p>CN 名称需要位于 kube-apiserver 的 –requestheader-allowed-names 参数中，否则后续访问 metrics 时会提示权限不足。</p></li><li><p>生成证书和私钥</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cfssl gencert -ca=ca.pem  -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes proxy-client-csr.json | cfssljson -bare proxy-client</span></span><br><span class="line"><span class="comment"># ls proxy-client*.pem</span></span><br><span class="line">proxy-client-key.pem  proxy-client.pem</span><br><span class="line"><span class="comment"># cp proxy-client*.pem /etc/kubernetes/ssl/</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="8-5、创建-kube-apiserver-systemd-unit-模板文件"><a href="#8-5、创建-kube-apiserver-systemd-unit-模板文件" class="headerlink" title="8.5、创建 kube-apiserver systemd unit 模板文件"></a>8.5、创建 kube-apiserver systemd unit 模板文件</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat /usr/lib/systemd/system/kube-apiserver.service </span></span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes API Server</span><br><span class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</span><br><span class="line">After=network.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">WorkingDirectory=/var/<span class="built_in">log</span>/k8s/kube-apiserver</span><br><span class="line">ExecStart=/usr/bin/kube-apiserver \</span><br><span class="line">  --advertise-address=172.21.17.30 \<span class="comment">#master 节点的ip</span></span><br><span class="line">  --default-not-ready-toleration-seconds=360 \</span><br><span class="line">  --default-unreachable-toleration-seconds=360 \</span><br><span class="line">  --feature-gates=DynamicAuditing=<span class="literal">true</span> \</span><br><span class="line">  --max-mutating-requests-inflight=2000 \</span><br><span class="line">  --max-requests-inflight=4000 \</span><br><span class="line">  --default-watch-cache-size=200 \</span><br><span class="line">  --delete-collection-workers=2 \</span><br><span class="line">  --encryption-provider-config=/etc/kubernetes/encryption-config.yaml \</span><br><span class="line">  --etcd-cafile=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem \</span><br><span class="line">  --etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem \</span><br><span class="line">  --etcd-servers=https://172.21.17.30:2379,https://172.21.17.31:2379,https://172.21.16.110:2379 \</span><br><span class="line">  --<span class="built_in">bind</span>-address=0.0.0.0 \</span><br><span class="line">  --secure-port=6443 \</span><br><span class="line">  --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \</span><br><span class="line">  --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \</span><br><span class="line">  --insecure-port=0 \</span><br><span class="line">  --audit-dynamic-configuration \</span><br><span class="line">  --audit-log-maxage=15 \</span><br><span class="line">  --audit-log-maxbackup=3 \</span><br><span class="line">  --audit-log-maxsize=100 \</span><br><span class="line">  --audit-log-truncate-enabled \</span><br><span class="line">  --audit-log-path=/var/<span class="built_in">log</span>/k8s/kube-apiserver/audit.log \</span><br><span class="line">  --audit-policy-file=/etc/kubernetes/audit-policy.yaml \</span><br><span class="line">  --profiling \</span><br><span class="line">  --anonymous-auth=<span class="literal">false</span> \</span><br><span class="line">  --client-ca-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --<span class="built_in">enable</span>-bootstrap-token-auth \</span><br><span class="line">  --requestheader-allowed-names=<span class="string">"aggregator"</span> \</span><br><span class="line">  --requestheader-client-ca-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --requestheader-extra-headers-prefix=<span class="string">"X-Remote-Extra-"</span> \</span><br><span class="line">  --requestheader-group-headers=X-Remote-Group \</span><br><span class="line">  --requestheader-username-headers=X-Remote-User \</span><br><span class="line">  --service-account-key-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --authorization-mode=Node,RBAC \</span><br><span class="line">  --runtime-config=api/all=<span class="literal">true</span> \</span><br><span class="line">  --<span class="built_in">enable</span>-admission-plugins=NodeRestriction \</span><br><span class="line">  --allow-privileged=<span class="literal">true</span> \</span><br><span class="line">  --apiserver-count=3 \</span><br><span class="line">  --event-ttl=168h \</span><br><span class="line">  --kubelet-certificate-authority=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --kubelet-client-certificate=/etc/kubernetes/ssl/kubernetes.pem \</span><br><span class="line">  --kubelet-client-key=/etc/kubernetes/ssl/kubernetes-key.pem \</span><br><span class="line">  --kubelet-https=<span class="literal">true</span> \</span><br><span class="line">  --kubelet-timeout=10s \</span><br><span class="line">  --proxy-client-cert-file=/etc/kubernetes/ssl/proxy-client.pem \</span><br><span class="line">  --proxy-client-key-file=/etc/kubernetes/ssl/proxy-client-key.pem \</span><br><span class="line">  --service-cluster-ip-range=10.254.0.0/16 \</span><br><span class="line">  --service-node-port-range=30000-32767 \</span><br><span class="line">  --logtostderr=<span class="literal">true</span> \</span><br><span class="line">  --<span class="built_in">enable</span>-aggregator-routing=<span class="literal">true</span> \</span><br><span class="line">  --v=2</span><br><span class="line">Restart=on-failure</span><br><span class="line">RestartSec=10</span><br><span class="line">Type=notify</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># mkdir -p /var/log/k8s/kube-apiserver</span></span><br></pre></td></tr></table></figure><ul><li>–advertise-address：apiserver 对外通告的 IP（kubernetes 服务后端节点 IP）；</li><li>–default-*-toleration-seconds：设置节点异常相关的阈值；</li><li>–max-*-requests-inflight：请求相关的最大阈值；</li><li>–etcd-*：访问 etcd 的证书和 etcd 服务器地址；</li><li>–experimental-encryption-provider-config：指定用于加密 etcd 中 secret 的配置；</li><li>–bind-address： https 监听的 IP，不能为 127.0.0.1，否则外界不能访问它的安全端口 6443；</li><li>–secret-port：https 监听端口；</li><li>–insecure-port=0：关闭监听 http 非安全端口(8080)；</li><li>–tls-*-file：指定 apiserver 使用的证书、私钥和 CA 文件；</li><li>–audit-*：配置审计策略和审计日志文件相关的参数；</li><li>–client-ca-file：验证 client (kue-controller-manager、kube-scheduler、kubelet、kube-proxy 等)请求所带的证书；</li><li>–enable-bootstrap-token-auth：启用 kubelet bootstrap 的 token 认证；</li><li>–requestheader-*：kube-apiserver 的 aggregator layer 相关的配置参数，proxy-client &amp; HPA 需要使用；</li><li>–requestheader-client-ca-file：用于签名 –proxy-client-cert-file 和 –proxy-client-key-file 指定的证书；在启用了 metric aggregator 时使用；</li><li>–requestheader-allowed-names：不能为空，值为逗号分割的 –proxy-client-cert-file 证书的 CN 名称，这里设置为 “aggregator”；</li><li>–service-account-key-file：签名 ServiceAccount Token 的公钥文件，kube-controller-manager 的 –service-account-private-key-file 指定私钥文件，两者配对使用；</li><li>–runtime-config=api/all=true： 启用所有版本的 APIs，如 autoscaling/v2alpha1；</li><li>–authorization-mode=Node,RBAC、–anonymous-auth=false： 开启 Node 和 RBAC 授权模式，拒绝未授权的请求；</li><li>–enable-admission-plugins：启用一些默认关闭的 plugins；</li><li>–allow-privileged：运行执行 privileged 权限的容器；</li><li>–apiserver-count=3：指定 apiserver 实例的数量；</li><li>–event-ttl：指定 events 的保存时间；</li><li>–kubelet-<em>：如果指定，则使用 https 访问 kubelet APIs；需要为证书对应的用户(上面 kubernetes</em>.pem 证书的用户为 kubernetes) 用户定义 RBAC 规则，否则访问 kubelet API 时提示未授权；</li><li>–proxy-client-*：apiserver 访问 metrics-server 使用的证书；</li><li>–service-cluster-ip-range： 指定 Service Cluster IP 地址段；</li><li>–service-node-port-range： 指定 NodePort 的端口范围；</li><li>如果 kube-apiserver 机器没有运行 kube-proxy，则还需要添加 –enable-aggregator-routing=true 参数</li></ul><p><strong>注意</strong>:<br>  1.requestheader-client-ca-file 指定的 CA 证书，必须具有 client auth and server auth；<br>  2.如果 –requestheader-allowed-names 为空，或者 –proxy-client-cert-file 证书的 CN 名称不在 allowed-names 中，则后续查看 node 或 pods 的 metrics 失败，提示：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl top nodes</span></span><br><span class="line">Error from server (Forbidden): nodes.metrics.k8s.io is forbidden: User <span class="string">"aggregator"</span> cannot list resource <span class="string">"nodes"</span> <span class="keyword">in</span> API group <span class="string">"metrics.k8s.io"</span> at the cluster scope</span><br></pre></td></tr></table></figure><h4 id="8-6、启动-kube-apiserver-服务"><a href="#8-6、启动-kube-apiserver-服务" class="headerlink" title="8.6、启动 kube-apiserver 服务"></a>8.6、启动 kube-apiserver 服务</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># systemctl daemon-reload &amp;&amp; systemctl enable kube-apiserver &amp;&amp; systemctl restart kube-apiserver &amp;&amp;systemctl status kube-apiserver</span></span><br><span class="line"><span class="comment"># systemctl status kube-apiserver |grep 'Active:'</span></span><br><span class="line">   Active: active (running) since Mon 2019-09-16 14:38:31 CST; 1min 41s ago</span><br></pre></td></tr></table></figure><h4 id="8-6、打印-kube-apiserver-写入-etcd-的数据"><a href="#8-6、打印-kube-apiserver-写入-etcd-的数据" class="headerlink" title="8.6、打印 kube-apiserver 写入 etcd 的数据"></a>8.6、打印 kube-apiserver 写入 etcd 的数据</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ETCDCTL_API=3 etcdctl \</span></span><br><span class="line">    --endpoints=<span class="variable">$&#123;ETCD_ENDPOINTS&#125;</span> \</span><br><span class="line">    --cacert=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">    --cert=/etc/etcd/ssl/etcd.pem \</span><br><span class="line">    --key=/etc/etcd/ssl/etcd-key.pem \</span><br><span class="line">    get /registry/ --prefix --keys-only</span><br></pre></td></tr></table></figure><h4 id="8-9、检查集群信息"><a href="#8-9、检查集群信息" class="headerlink" title="8.9、检查集群信息"></a>8.9、检查集群信息</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl cluster-info</span></span><br><span class="line">Kubernetes master is running at https://172.21.16.45:8443</span><br><span class="line"></span><br><span class="line">To further debug and diagnose cluster problems, use <span class="string">'kubectl cluster-info dump'</span>.</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl get all --all-namespaces</span></span><br><span class="line">NAMESPACE   NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">default     service/kubernetes   ClusterIP   10.254.0.1   &lt;none&gt;        443/TCP   12m</span><br><span class="line"></span><br><span class="line"><span class="comment">#  kubectl get componentstatuses</span></span><br><span class="line">NAME                 STATUS      MESSAGE                                                                                     ERROR</span><br><span class="line">controller-manager   Unhealthy   Get http://127.0.0.1:10252/healthz: dial tcp 127.0.0.1:10252: connect: connection refused</span><br><span class="line">scheduler            Unhealthy   Get http://127.0.0.1:10251/healthz: dial tcp 127.0.0.1:10251: connect: connection refused</span><br><span class="line">etcd-0               Healthy     &#123;<span class="string">"health"</span>:<span class="string">"true"</span>&#125;</span><br><span class="line">etcd-2               Healthy     &#123;<span class="string">"health"</span>:<span class="string">"true"</span>&#125;</span><br><span class="line">etcd-1               Healthy     &#123;<span class="string">"health"</span>:<span class="string">"true"</span>&#125;</span><br></pre></td></tr></table></figure><ul><li>执行 kubectl get componentstatuses 命令时，apiserver 默认向 127.0.0.1 发送请求。当 controller-manager、scheduler 以集群模式运行时，有可能和 kube-apiserver 不在一台机器上，这时 controller-manager 或 scheduler 的状态为 Unhealthy，但实际上它们工作正常。</li></ul><h4 id="8-10、检查-kube-apiserver-监听的端口"><a href="#8-10、检查-kube-apiserver-监听的端口" class="headerlink" title="8.10、检查 kube-apiserver 监听的端口"></a>8.10、检查 kube-apiserver 监听的端口</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># netstat -lnpt|grep kube</span></span><br><span class="line">tcp6       0      0 :::6443                 :::*                    LISTEN      10845/kube-apiserve</span><br></pre></td></tr></table></figure><ul><li>6443: 接收 https 请求的安全端口，对所有请求做认证和授权；</li><li>由于关闭了非安全端口，故没有监听 8080；</li></ul><h4 id="8-11、授予-kube-apiserver-访问-kubelet-API-的权限"><a href="#8-11、授予-kube-apiserver-访问-kubelet-API-的权限" class="headerlink" title="8.11、授予 kube-apiserver 访问 kubelet API 的权限"></a>8.11、授予 kube-apiserver 访问 kubelet API 的权限</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在执行 kubectl exec、run、logs 等命令时，apiserver 会将请求转发到 kubelet 的 https 端口。这里定义 RBAC 规则，授权 apiserver 使用的证书（kubernetes.pem）用户名（CN：kuberntes）访问 kubelet API 的权限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create clusterrolebinding kube-apiserver:kubelet-apis --clusterrole=system:kubelet-api-admin --user kubernetes</span><br></pre></td></tr></table></figure><h3 id="9、部署高可用-kube-controller-manager"><a href="#9、部署高可用-kube-controller-manager" class="headerlink" title="9、部署高可用 kube-controller-manager"></a>9、部署高可用 kube-controller-manager</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;该集群包含 3 个节点，启动后将通过竞争选举机制产生一个 leader 节点，其它节点为阻塞状态。当 leader 节点不可用时，阻塞的节点将再次进行选举产生新的 leader 节点，从而保证服务的可用性。<br>为保证通信安全，本文档先生成 x509 证书和私钥，kube-controller-manager 在如下两种情况下使用该证书：<br>  1、与 kube-apiserver 的安全端口通信;<br>  2、在安全端口(https，10252) 输出 prometheus 格式的 metrics；</p><h4 id="9-1、创建-kube-controller-manager-证书和私钥"><a href="#9-1、创建-kube-controller-manager-证书和私钥" class="headerlink" title="9.1、创建 kube-controller-manager 证书和私钥"></a>9.1、创建 kube-controller-manager 证书和私钥</h4><ul><li><p>创建证书签名请求</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat &gt; kube-controller-manager-csr.json &lt;&lt;EOF</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"CN"</span>: <span class="string">"system:kube-controller-manager"</span>,</span><br><span class="line">    <span class="string">"key"</span>: &#123;</span><br><span class="line">        <span class="string">"algo"</span>: <span class="string">"rsa"</span>,</span><br><span class="line">        <span class="string">"size"</span>: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"hosts"</span>: [</span><br><span class="line">      <span class="string">"127.0.0.1"</span>,</span><br><span class="line">      <span class="string">"172.21.17.30"</span>,</span><br><span class="line">      <span class="string">"172.21.17.31"</span>,</span><br><span class="line">      <span class="string">"172.21.16.110"</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"names"</span>: [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="string">"C"</span>: <span class="string">"CN"</span>,</span><br><span class="line">        <span class="string">"ST"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">        <span class="string">"L"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">        <span class="string">"O"</span>: <span class="string">"system:kube-controller-manager"</span>,</span><br><span class="line">        <span class="string">"OU"</span>: <span class="string">"4Paradigm"</span></span><br><span class="line">      &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></li><li><p>hosts 列表包含所有 kube-controller-manager 节点 IP；</p></li><li><p>CN 和 O 均为 system:kube-controller-manager，kubernetes 内置的 ClusterRoleBindings system:kube-controller-manager 赋予 kube-controller-manager 工作所需的权限。</p></li><li><p>生成证书和私钥</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json   -profile=kubernetes kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager</span></span><br><span class="line"><span class="comment"># ls kube-controller-manager*pem</span></span><br><span class="line">kube-controller-manager-key.pem  kube-controller-manager.pem</span><br><span class="line"></span><br><span class="line"><span class="comment"># cp kube-controller-manager*pem /etc/kubernetes/ssl/</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="9-2、创建和分发-kubeconfig-文件"><a href="#9-2、创建和分发-kubeconfig-文件" class="headerlink" title="9.2、创建和分发 kubeconfig 文件"></a>9.2、创建和分发 kubeconfig 文件</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kube-controller-manager 使用 kubeconfig 文件访问 apiserver，该文件提供了 apiserver 地址、嵌入的 CA 证书和 kube-controller-manager 证书：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl config set-cluster kubernetes \</span></span><br><span class="line">  --certificate-authority=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --embed-certs=<span class="literal">true</span> \</span><br><span class="line">  --server=<span class="variable">$&#123;KUBE_APISERVER&#125;</span> \</span><br><span class="line">  --kubeconfig=kube-controller-manager.kubeconfig</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl config set-credentials system:kube-controller-manager \</span></span><br><span class="line">  --client-certificate=kube-controller-manager.pem \</span><br><span class="line">  --client-key=kube-controller-manager-key.pem \</span><br><span class="line">  --embed-certs=<span class="literal">true</span> \</span><br><span class="line">  --kubeconfig=kube-controller-manager.kubeconfig</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl config set-context system:kube-controller-manager \</span></span><br><span class="line">  --cluster=kubernetes \</span><br><span class="line">  --user=system:kube-controller-manager \</span><br><span class="line">  --kubeconfig=kube-controller-manager.kubeconfig</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl config use-context system:kube-controller-manager --kubeconfig=kube-controller-manager.kubeconfig</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># cp kube-controller-manager.kubeconfig /etc/kubernetes/</span></span><br></pre></td></tr></table></figure><h4 id="9-3、创建-kube-controller-manager-systemd-unit文件"><a href="#9-3、创建-kube-controller-manager-systemd-unit文件" class="headerlink" title="9.3、创建 kube-controller-manager systemd unit文件"></a>9.3、创建 kube-controller-manager systemd unit文件</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat /usr/lib/systemd/system/kube-controller-manager.service </span></span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes Controller Manager</span><br><span class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">WorkingDirectory=/var/<span class="built_in">log</span>/k8s/kube-controller-manager</span><br><span class="line">ExecStart=/usr/bin/kube-controller-manager \</span><br><span class="line">  --profiling \</span><br><span class="line">  --cluster-name=kubernetes \</span><br><span class="line">  --controllers=*,bootstrapsigner,tokencleaner \</span><br><span class="line">  --kube-api-qps=1000 \</span><br><span class="line">  --kube-api-burst=2000 \</span><br><span class="line">  --leader-elect \</span><br><span class="line">  --use-service-account-credentials\</span><br><span class="line">  --concurrent-service-syncs=2 \</span><br><span class="line">  --<span class="built_in">bind</span>-address=0.0.0.0 \</span><br><span class="line">  --secure-port=10252 \</span><br><span class="line">  --tls-cert-file=/etc/kubernetes/ssl/kube-controller-manager.pem \</span><br><span class="line">  --tls-private-key-file=/etc/kubernetes/ssl/kube-controller-manager-key.pem \</span><br><span class="line">  --port=0 \</span><br><span class="line">  --authentication-kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \</span><br><span class="line">  --client-ca-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --requestheader-allowed-names=<span class="string">""</span> \</span><br><span class="line">  --requestheader-client-ca-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --requestheader-extra-headers-prefix=<span class="string">"X-Remote-Extra-"</span> \</span><br><span class="line">  --requestheader-group-headers=X-Remote-Group \</span><br><span class="line">  --requestheader-username-headers=X-Remote-User \</span><br><span class="line">  --authorization-kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \</span><br><span class="line">  --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem \</span><br><span class="line">  --experimental-cluster-signing-duration=876000h \</span><br><span class="line">  --horizontal-pod-autoscaler-sync-period=10s \</span><br><span class="line">  --concurrent-deployment-syncs=10 \</span><br><span class="line">  --concurrent-gc-syncs=30 \</span><br><span class="line">  --node-cidr-mask-size=24 \</span><br><span class="line">  --service-cluster-ip-range=10.254.0.0/16 \</span><br><span class="line">  --pod-eviction-timeout=6m \</span><br><span class="line">  --terminated-pod-gc-threshold=10000 \</span><br><span class="line">  --root-ca-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem \</span><br><span class="line">  --kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \</span><br><span class="line">  --logtostderr=<span class="literal">true</span> \</span><br><span class="line">  --v=2</span><br><span class="line">Restart=on-failure</span><br><span class="line">RestartSec=5</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><ul><li>–port=0：关闭监听非安全端口（http），同时 –address 参数无效，–bind-address 参数有效；</li><li>–secure-port=10252、–bind-address=0.0.0.0: 在所有网络接口监听 10252 端口的 https /metrics 请求；</li><li>–kubeconfig：指定 kubeconfig 文件路径，kube-controller-manager 使用它连接和验证 kube-apiserver；</li><li>–authentication-kubeconfig 和 –authorization-kubeconfig：kube-controller-manager 使用它连接 apiserver，对 client 的请求进行认证和授权。kube-controller-manager 不再使用 –tls-ca-file 对请求 https metrics 的 Client 证书进行校验。如果没有配置这两个 kubeconfig 参数，则 client 连接 kube-controller-manager https 端口的请求会被拒绝(提示权限不足)。</li><li>–cluster-signing-*-file：签名 TLS Bootstrap 创建的证书；</li><li>–experimental-cluster-signing-duration：指定 TLS Bootstrap 证书的有效期；</li><li>–root-ca-file：放置到容器 ServiceAccount 中的 CA 证书，用来对 kube-apiserver 的证书进行校验；</li><li>–service-account-private-key-file：签名 ServiceAccount 中 Token 的私钥文件，必须和 kube-apiserver 的 –service-account-key-file 指定的公钥文件配对使用；</li><li>–service-cluster-ip-range ：指定 Service Cluster IP 网段，必须和 kube-apiserver 中的同名参数一致；</li><li>–leader-elect=true：集群运行模式，启用选举功能；被选为 leader 的节点负责处理工作，其它节点为阻塞状态；</li><li>–controllers=*,bootstrapsigner,tokencleaner：启用的控制器列表，tokencleaner 用于自动清理过期的 Bootstrap token；</li><li>–horizontal-pod-autoscaler-*：custom metrics 相关参数，支持 autoscaling/v2alpha1；</li><li>–tls-cert-file、–tls-private-key-file：使用 https 输出 metrics 时使用的 Server 证书和秘钥；</li><li>–use-service-account-credentials=true: kube-controller-manager 中各 controller 使用 serviceaccount 访问 kube-apiserver；</li></ul><h4 id="9-4、启动-kube-controller-manager-服务"><a href="#9-4、启动-kube-controller-manager-服务" class="headerlink" title="9.4、启动 kube-controller-manager 服务"></a>9.4、启动 kube-controller-manager 服务</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># mkdir -p /var/log/k8s/kube-controller-manager</span></span><br><span class="line"><span class="comment"># systemctl daemon-reload &amp;&amp; systemctl enable kube-controller-manager &amp;&amp; systemctl restart kube-controller-manager &amp;&amp; systemctl status kube-controller-manager</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # netstat -lnpt | grep kube-cont</span></span><br><span class="line">tcp6       0      0 :::10252                :::*                    LISTEN      8335/kube-controlle</span><br></pre></td></tr></table></figure><h4 id="9-5、查看输出的-metrics"><a href="#9-5、查看输出的-metrics" class="headerlink" title="9.5、查看输出的 metrics"></a>9.5、查看输出的 metrics</h4><p><strong>注意:</strong> 以下命令在 kube-controller-manager 节点上执行。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># curl -s --cacert /etc/kubernetes/ssl/ca.pem --cert /etc/kubernetes/ssl/admin.pem --key /etc/kubernetes/ssl/admin-key.pem https://172.21.17.30:10252/metrics |head</span></span><br><span class="line"><span class="comment"># HELP apiserver_audit_event_total Counter of audit events generated and sent to the audit backend.</span></span><br><span class="line"><span class="comment"># TYPE apiserver_audit_event_total counter</span></span><br><span class="line">apiserver_audit_event_total 0</span><br><span class="line"><span class="comment"># HELP apiserver_audit_requests_rejected_total Counter of apiserver requests rejected due to an error in audit logging backend.</span></span><br><span class="line"><span class="comment"># TYPE apiserver_audit_requests_rejected_total counter</span></span><br><span class="line">apiserver_audit_requests_rejected_total 0</span><br><span class="line"><span class="comment"># HELP apiserver_client_certificate_expiration_seconds Distribution of the remaining lifetime on the certificate used to authenticate a request.</span></span><br><span class="line"><span class="comment"># TYPE apiserver_client_certificate_expiration_seconds histogram</span></span><br><span class="line">apiserver_client_certificate_expiration_seconds_bucket&#123;le=<span class="string">"0"</span>&#125; 0</span><br><span class="line">apiserver_client_certificate_expiration_seconds_bucket&#123;le=<span class="string">"1800"</span>&#125; 0</span><br></pre></td></tr></table></figure><h4 id="9-6-kube-controller-manager-的权限"><a href="#9-6-kube-controller-manager-的权限" class="headerlink" title="9.6 kube-controller-manager 的权限"></a>9.6 kube-controller-manager 的权限</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ClusteRole system:kube-controller-manager 的权限很小，只能创建 secret、serviceaccount 等资源对象，各 controller 的权限分散到 ClusterRole system:controller:XXX 中：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl describe clusterrole system:kube-controller-manager</span></span><br><span class="line">Name:         system:kube-controller-manager</span><br><span class="line">Labels:       kubernetes.io/bootstrapping=rbac-defaults</span><br><span class="line">Annotations:  rbac.authorization.kubernetes.io/autoupdate: <span class="literal">true</span></span><br><span class="line">PolicyRule:</span><br><span class="line">  Resources                                  Non-Resource URLs  Resource Names  Verbs</span><br><span class="line">  ---------                                  -----------------  --------------  -----</span><br><span class="line">  secrets                                    []                 []              [create delete get update]</span><br><span class="line">  endpoints                                  []                 []              [create get update]</span><br><span class="line">  serviceaccounts                            []                 []              [create get update]</span><br><span class="line">  events                                     []                 []              [create patch update]</span><br><span class="line">  tokenreviews.authentication.k8s.io         []                 []              [create]</span><br><span class="line">  subjectaccessreviews.authorization.k8s.io  []                 []              [create]</span><br><span class="line">  configmaps                                 []                 []              [get]</span><br><span class="line">  namespaces                                 []                 []              [get]</span><br><span class="line">  *.*                                        []                 []              [list watch]</span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;需要在 kube-controller-manager 的启动参数中添加 –use-service-account-credentials=true 参数，这样 main controller 会为各 controller 创建对应的 ServiceAccount XXX-controller。内置的 ClusterRoleBinding system:controller:XXX 将赋予各 XXX-controller ServiceAccount 对应的 ClusterRole system:controller:XXX 权限</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get clusterrole|grep controller</span></span><br><span class="line">system:controller:attachdetach-controller                              4h52m</span><br><span class="line">system:controller:certificate-controller                               4h52m</span><br><span class="line">system:controller:clusterrole-aggregation-controller                   4h52m</span><br><span class="line">system:controller:cronjob-controller                                   4h52m</span><br><span class="line">system:controller:daemon-set-controller                                4h52m</span><br><span class="line">system:controller:deployment-controller                                4h52m</span><br><span class="line">system:controller:disruption-controller                                4h52m</span><br><span class="line">system:controller:endpoint-controller                                  4h52m</span><br><span class="line">system:controller:expand-controller                                    4h52m</span><br><span class="line">system:controller:generic-garbage-collector                            4h52m</span><br><span class="line">system:controller:horizontal-pod-autoscaler                            4h52m</span><br><span class="line">system:controller:job-controller                                       4h52m</span><br><span class="line">system:controller:namespace-controller                                 4h52m</span><br><span class="line">system:controller:node-controller                                      4h52m</span><br><span class="line">system:controller:persistent-volume-binder                             4h52m</span><br><span class="line">system:controller:pod-garbage-collector                                4h52m</span><br><span class="line">system:controller:pv-protection-controller                             4h52m</span><br><span class="line">system:controller:pvc-protection-controller                            4h52m</span><br><span class="line">system:controller:replicaset-controller                                4h52m</span><br><span class="line">system:controller:replication-controller                               4h52m</span><br><span class="line">system:controller:resourcequota-controller                             4h52m</span><br><span class="line">system:controller:route-controller                                     4h52m</span><br><span class="line">system:controller:service-account-controller                           4h52m</span><br><span class="line">system:controller:service-controller                                   4h52m</span><br><span class="line">system:controller:statefulset-controller                               4h52m</span><br><span class="line">system:controller:ttl-controller                                       4h52m</span><br><span class="line">system:kube-controller-manager                                         4h52m</span><br></pre></td></tr></table></figure><ul><li>以 deployment controller 为例：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl describe clusterrole system:controller:deployment-controller</span></span><br><span class="line">Name:         system:controller:deployment-controller</span><br><span class="line">Labels:       kubernetes.io/bootstrapping=rbac-defaults</span><br><span class="line">Annotations:  rbac.authorization.kubernetes.io/autoupdate: <span class="literal">true</span></span><br><span class="line">PolicyRule:</span><br><span class="line">  Resources                          Non-Resource URLs  Resource Names  Verbs</span><br><span class="line">  ---------                          -----------------  --------------  -----</span><br><span class="line">  replicasets.apps                   []                 []              [create delete get list patch update watch]</span><br><span class="line">  replicasets.extensions             []                 []              [create delete get list patch update watch]</span><br><span class="line">  events                             []                 []              [create patch update]</span><br><span class="line">  pods                               []                 []              [get list update watch]</span><br><span class="line">  deployments.apps                   []                 []              [get list update watch]</span><br><span class="line">  deployments.extensions             []                 []              [get list update watch]</span><br><span class="line">  deployments.apps/finalizers        []                 []              [update]</span><br><span class="line">  deployments.apps/status            []                 []              [update]</span><br><span class="line">  deployments.extensions/finalizers  []                 []              [update]</span><br><span class="line">  deployments.extensions/status      []                 []              [update]</span><br></pre></td></tr></table></figure></li></ul><h4 id="9-7、查看当前的-leader"><a href="#9-7、查看当前的-leader" class="headerlink" title="9.7、查看当前的 leader"></a>9.7、查看当前的 leader</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get endpoints kube-controller-manager --namespace=kube-system  -o yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Endpoints</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    control-plane.alpha.kubernetes.io/leader: <span class="string">'&#123;"holderIdentity":"k8s-master-01-2.kxl_8890f530-d829-11e9-873f-fa163e5af833","leaseDurationSeconds":15,"acquireTime":"2019-09-16T06:00:15Z","renewTime":"2019-09-16T07:05:38Z","leaderTransitions":1&#125;'</span></span><br><span class="line">  creationTimestamp: <span class="string">"2019-09-16T02:27:06Z"</span></span><br><span class="line">  name: kube-controller-manager</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  resourceVersion: <span class="string">"25734"</span></span><br><span class="line">  selfLink: /api/v1/namespaces/kube-system/endpoints/kube-controller-manager</span><br><span class="line">  uid: 7a5b872e-d829-11e9-9b67-fa163effd55b</span><br></pre></td></tr></table></figure><p>当前的 leader 为k8s-master-01-2节点。</p><p>测试 kube-controller-manager 集群的高可用,停掉一个或两个节点的 kube-controller-manager 服务，观察其它节点的日志，看是否获取了 leader 权限。</p><h3 id="10、scheduler集群"><a href="#10、scheduler集群" class="headerlink" title="10、scheduler集群"></a>10、scheduler集群</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3 个节点，启动后将通过竞争选举机制产生一个 leader 节点，其它节点为阻塞状态。当 leader 节点不可用后，剩余节点将再次进行选举产生新的 leader 节点，从而保证服务的可用性。</p><p>为保证通信安全，本文档先生成 x509 证书和私钥，kube-scheduler 在如下两种情况下使用该证书：<br>  1.与 kube-apiserver 的安全端口通信;<br>  2.在安全端口(https，10251) 输出 prometheus 格式的 metrics；</p><h4 id="10-1、创建-kube-scheduler-证书和私钥"><a href="#10-1、创建-kube-scheduler-证书和私钥" class="headerlink" title="10.1、创建 kube-scheduler 证书和私钥"></a>10.1、创建 kube-scheduler 证书和私钥</h4><ul><li><p>创建证书签名请求</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat &gt; kube-scheduler-csr.json &lt;&lt;EOF</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"CN"</span>: <span class="string">"system:kube-scheduler"</span>,</span><br><span class="line">    <span class="string">"hosts"</span>: [</span><br><span class="line">      <span class="string">"127.0.0.1"</span>,</span><br><span class="line">      <span class="string">"172.21.17.30"</span>,</span><br><span class="line">      <span class="string">"172.21.17.31"</span>,</span><br><span class="line">      <span class="string">"172.21.16.110"</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"key"</span>: &#123;</span><br><span class="line">        <span class="string">"algo"</span>: <span class="string">"rsa"</span>,</span><br><span class="line">        <span class="string">"size"</span>: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"names"</span>: [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="string">"C"</span>: <span class="string">"CN"</span>,</span><br><span class="line">        <span class="string">"ST"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">        <span class="string">"L"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">        <span class="string">"O"</span>: <span class="string">"system:kube-scheduler"</span>,</span><br><span class="line">        <span class="string">"OU"</span>: <span class="string">"4Paradigm"</span></span><br><span class="line">      &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></li><li><p>hosts 列表包含所有 kube-scheduler 节点 IP；</p></li><li><p>CN 和 O 均为 system:kube-scheduler，kubernetes 内置的 ClusterRoleBindings system:kube-scheduler 将赋予 kube-scheduler 工作所需的权限；</p></li><li><p>生成证书和私钥:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-scheduler-csr.json | cfssljson -bare kube-scheduler</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ls kube-scheduler*pem</span></span><br><span class="line">kube-scheduler-key.pem  kube-scheduler.pem</span><br><span class="line"></span><br><span class="line"><span class="comment"># cp kube-scheduler*pem  /etc/kubernetes/ssl/</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="10-2、创建和分发-kubeconfig-文件"><a href="#10-2、创建和分发-kubeconfig-文件" class="headerlink" title="10.2、创建和分发 kubeconfig 文件"></a>10.2、创建和分发 kubeconfig 文件</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kube-scheduler 使用 kubeconfig 文件访问 apiserver，该文件提供了 apiserver 地址、嵌入的 CA 证书和 kube-scheduler 证书：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl config set-cluster kubernetes \</span></span><br><span class="line">  --certificate-authority=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --embed-certs=<span class="literal">true</span> \</span><br><span class="line">  --server=<span class="variable">$&#123;KUBE_APISERVER&#125;</span> \</span><br><span class="line">  --kubeconfig=kube-scheduler.kubeconfig</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl config set-credentials system:kube-scheduler \</span></span><br><span class="line">  --client-certificate=kube-scheduler.pem \</span><br><span class="line">  --client-key=kube-scheduler-key.pem \</span><br><span class="line">  --embed-certs=<span class="literal">true</span> \</span><br><span class="line">  --kubeconfig=kube-scheduler.kubeconfig</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl config set-context system:kube-scheduler \</span></span><br><span class="line">  --cluster=kubernetes \</span><br><span class="line">  --user=system:kube-scheduler \</span><br><span class="line">  --kubeconfig=kube-scheduler.kubeconfig</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl config use-context system:kube-scheduler --kubeconfig=kube-scheduler.kubeconfig</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># cp kube-scheduler.kubeconfig /etc/kubernetes/</span></span><br></pre></td></tr></table></figure><h4 id="10-3、创建-kube-scheduler-配置文件"><a href="#10-3、创建-kube-scheduler-配置文件" class="headerlink" title="10.3、创建 kube-scheduler 配置文件"></a>10.3、创建 kube-scheduler 配置文件</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat &gt;kube-scheduler.yaml &lt;&lt;EOF</span></span><br><span class="line">apiVersion: kubescheduler.config.k8s.io/v1alpha1</span><br><span class="line">kind: KubeSchedulerConfiguration</span><br><span class="line">bindTimeoutSeconds: 600</span><br><span class="line">clientConnection:</span><br><span class="line">  burst: 200</span><br><span class="line">  kubeconfig: <span class="string">"/etc/kubernetes/kube-scheduler.kubeconfig"</span></span><br><span class="line">  qps: 100</span><br><span class="line">enableContentionProfiling: <span class="literal">false</span></span><br><span class="line">enableProfiling: <span class="literal">true</span></span><br><span class="line">hardPodAffinitySymmetricWeight: 1</span><br><span class="line">healthzBindAddress: <span class="comment">##NODE_IP##:10251</span></span><br><span class="line">leaderElection:</span><br><span class="line">  leaderElect: <span class="literal">true</span></span><br><span class="line">metricsBindAddress: <span class="comment">##NODE_IP##:10251</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># cp kube-scheduler.yaml /etc/kubernetes/</span></span><br></pre></td></tr></table></figure><ul><li>–kubeconfig：指定 kubeconfig 文件路径，kube-scheduler 使用它连接和验证 kube-apiserver；</li><li>–leader-elect=true：集群运行模式，启用选举功能；被选为 leader 的节点负责处理工作，其它节点为阻塞状态；</li></ul><h4 id="10-4、创建-kube-scheduler-systemd-unit-模板文件"><a href="#10-4、创建-kube-scheduler-systemd-unit-模板文件" class="headerlink" title="10.4、创建 kube-scheduler systemd unit 模板文件"></a>10.4、创建 kube-scheduler systemd unit 模板文件</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat /usr/lib/systemd/system/kube-scheduler.service </span></span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes Scheduler</span><br><span class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">WorkingDirectory=/var/<span class="built_in">log</span>/k8s/kube-scheduler</span><br><span class="line">ExecStart=/usr/bin/kube-scheduler \</span><br><span class="line">  --config=/etc/kubernetes/kube-scheduler.yaml \</span><br><span class="line">  --<span class="built_in">bind</span>-address=0.0.0.0 \</span><br><span class="line">  --secure-port=10259 \</span><br><span class="line">  --port=0 \</span><br><span class="line">  --tls-cert-file=/etc/kubernetes/ssl/kube-scheduler.pem \</span><br><span class="line">  --tls-private-key-file=/etc/kubernetes/ssl/kube-scheduler-key.pem \</span><br><span class="line">  --authentication-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \</span><br><span class="line">  --client-ca-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --requestheader-allowed-names=<span class="string">""</span> \</span><br><span class="line">  --requestheader-client-ca-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --requestheader-extra-headers-prefix=<span class="string">"X-Remote-Extra-"</span> \</span><br><span class="line">  --requestheader-group-headers=X-Remote-Group \</span><br><span class="line">  --requestheader-username-headers=X-Remote-User \</span><br><span class="line">  --authorization-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \</span><br><span class="line">  --logtostderr=<span class="literal">true</span> \</span><br><span class="line">  --v=2</span><br><span class="line">Restart=always</span><br><span class="line">RestartSec=5</span><br><span class="line">StartLimitInterval=0</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><h4 id="10-5、启动-kube-scheduler-服务"><a href="#10-5、启动-kube-scheduler-服务" class="headerlink" title="10.5、启动 kube-scheduler 服务"></a>10.5、启动 kube-scheduler 服务</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># mkdir -p /var/log/k8s/kube-scheduler</span></span><br><span class="line"><span class="comment"># systemctl daemon-reload &amp;&amp; systemctl enable kube-scheduler &amp;&amp; systemctl restart kube-scheduler &amp;&amp; systemctl status kube-scheduler</span></span><br></pre></td></tr></table></figure><h4 id="10-6、查看输出的-metrics"><a href="#10-6、查看输出的-metrics" class="headerlink" title="10.6、查看输出的 metrics"></a>10.6、查看输出的 metrics</h4><p>kube-scheduler 监听 10251 和 10251 端口：</p><ul><li>10251：接收 http 请求，非安全端口，不需要认证授权</li><li>10259：接收 https 请求，安全端口，需要认证授权</li></ul><p>两个接口都对外提供 /metrics 和 /healthz 的访问。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># netstat -lnpt |grep kube-sch</span></span><br><span class="line">tcp        0      0 172.21.17.31:10251      0.0.0.0:*               LISTEN      8441/kube-scheduler </span><br><span class="line">tcp6       0      0 :::10259                :::*                    LISTEN      8441/kube-scheduler</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># curl -s http://172.21.17.30:10251/metrics |head</span></span><br><span class="line"><span class="comment"># HELP apiserver_audit_event_total Counter of audit events generated and sent to the audit backend.</span></span><br><span class="line"><span class="comment"># TYPE apiserver_audit_event_total counter</span></span><br><span class="line">apiserver_audit_event_total 0</span><br><span class="line"><span class="comment"># HELP apiserver_audit_requests_rejected_total Counter of apiserver requests rejected due to an error in audit logging backend.</span></span><br><span class="line"><span class="comment"># TYPE apiserver_audit_requests_rejected_total counter</span></span><br><span class="line">apiserver_audit_requests_rejected_total 0</span><br><span class="line"><span class="comment"># HELP apiserver_client_certificate_expiration_seconds Distribution of the remaining lifetime on the certificate used to authenticate a request.</span></span><br><span class="line"><span class="comment"># TYPE apiserver_client_certificate_expiration_seconds histogram</span></span><br><span class="line">apiserver_client_certificate_expiration_seconds_bucket&#123;le=<span class="string">"0"</span>&#125; 0</span><br><span class="line">apiserver_client_certificate_expiration_seconds_bucket&#123;le=<span class="string">"1800"</span>&#125; 0</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># curl -s --cacert /etc/kubernetes/ssl/ca.pem --cert /etc/kubernetes/ssl/admin.pem --key /etc/kubernetes/ssl/admin-key.pem https://172.21.17.30:10259/metrics |head</span></span><br><span class="line"><span class="comment"># HELP apiserver_audit_event_total Counter of audit events generated and sent to the audit backend.</span></span><br><span class="line"><span class="comment"># TYPE apiserver_audit_event_total counter</span></span><br><span class="line">apiserver_audit_event_total 0</span><br><span class="line"><span class="comment"># HELP apiserver_audit_requests_rejected_total Counter of apiserver requests rejected due to an error in audit logging backend.</span></span><br><span class="line"><span class="comment"># TYPE apiserver_audit_requests_rejected_total counter</span></span><br><span class="line">apiserver_audit_requests_rejected_total 0</span><br><span class="line"><span class="comment"># HELP apiserver_client_certificate_expiration_seconds Distribution of the remaining lifetime on the certificate used to authenticate a request.</span></span><br><span class="line"><span class="comment"># TYPE apiserver_client_certificate_expiration_seconds histogram</span></span><br><span class="line">apiserver_client_certificate_expiration_seconds_bucket&#123;le=<span class="string">"0"</span>&#125; 0</span><br><span class="line">apiserver_client_certificate_expiration_seconds_bucket&#123;le=<span class="string">"1800"</span>&#125; 0</span><br></pre></td></tr></table></figure><h4 id="10-7、查看当前的-leader"><a href="#10-7、查看当前的-leader" class="headerlink" title="10.7、查看当前的 leader"></a>10.7、查看当前的 leader</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  kubectl get endpoints kube-scheduler --namespace=kube-system  -o yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Endpoints</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    control-plane.alpha.kubernetes.io/leader: <span class="string">'&#123;"holderIdentity":"k8s-master-01.kxl_2a6e0bf9-d82b-11e9-b946-fa163effd55b","leaseDurationSeconds":15,"acquireTime":"2019-09-16T06:00:28Z","renewTime":"2019-09-16T07:41:57Z","leaderTransitions":1&#125;'</span></span><br><span class="line">  creationTimestamp: <span class="string">"2019-09-16T02:38:55Z"</span></span><br><span class="line">  name: kube-scheduler</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  resourceVersion: <span class="string">"29215"</span></span><br><span class="line">  selfLink: /api/v1/namespaces/kube-system/endpoints/kube-scheduler</span><br><span class="line">  uid: 20a04151-d82b-11e9-baf3-fa163e53d4c8</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;1、环境准备&quot;&gt;&lt;a href=&quot;#1、环境准备&quot; class=&quot;headerlink&quot; title=&quot;1、环境准备&quot;&gt;&lt;/a&gt;1、环境准备&lt;/h3&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;ip&lt;/th&gt;
&lt;th&gt;type&lt;/th&gt;
&lt;th&gt;docker&lt;/th&gt;
&lt;th&gt;os&lt;/th&gt;
&lt;th&gt;k8s version&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;172.21.17.30&lt;/td&gt;
&lt;td&gt;master,etcd&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;CentOS Linux release 7.4.1708&lt;/td&gt;
&lt;td&gt;v1.14.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;172.21.17.31&lt;/td&gt;
&lt;td&gt;master,etcd&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;CentOS Linux release 7.4.1708&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;172.21.16.110&lt;/td&gt;
&lt;td&gt;master,etcd&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;CentOS Linux release 7.4.1708&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;172.21.16.87&lt;/td&gt;
&lt;td&gt;node,flanneld&lt;/td&gt;
&lt;td&gt;18.06.2-ce&lt;/td&gt;
&lt;td&gt;CentOS Linux release 7.4.1708&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;172.21.16.240&lt;/td&gt;
&lt;td&gt;node,flanneld,ha+kee&lt;/td&gt;
&lt;td&gt;18.06.2-ce&lt;/td&gt;
&lt;td&gt;CentOS Linux release 7.4.1708&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;172.21.16.204&lt;/td&gt;
&lt;td&gt;node,flanneld,ha+kee&lt;/td&gt;
&lt;td&gt;18.06.2-ce&lt;/td&gt;
&lt;td&gt;CentOS Linux release 7.4.1708&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;172.21.16.45&lt;/td&gt;
&lt;td&gt;vip&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;CentOS Linux release 7.4.1708&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;h3 id=&quot;2、初始化系统&quot;&gt;&lt;a href=&quot;#2、初始化系统&quot; class=&quot;headerlink&quot; title=&quot;2、初始化系统&quot;&gt;&lt;/a&gt;2、初始化系统&lt;/h3&gt;&lt;h4 id=&quot;2-1、安装依赖包&quot;&gt;&lt;a href=&quot;#2-1、安装依赖包&quot; class=&quot;headerlink&quot; title=&quot;2.1、安装依赖包&quot;&gt;&lt;/a&gt;2.1、安装依赖包&lt;/h4&gt;
    
    </summary>
    
      <category term="kubernetes" scheme="https://xxlaila.github.io/categories/kubernetes/"/>
    
    
      <category term="kuberntes v1.14" scheme="https://xxlaila.github.io/tags/kuberntes-v1-14/"/>
    
  </entry>
  
  <entry>
    <title>路由器端口映射</title>
    <link href="https://xxlaila.github.io/2019/09/10/%E8%B7%AF%E7%94%B1%E5%99%A8%E7%AB%AF%E5%8F%A3%E6%98%A0%E5%B0%84/"/>
    <id>https://xxlaila.github.io/2019/09/10/路由器端口映射/</id>
    <published>2019-09-10T00:56:24.000Z</published>
    <updated>2019-09-20T06:30:30.956Z</updated>
    
    <content type="html"><![CDATA[<p>好记性不如烂笔头，h3c MSR3620路由器做端口映射到后端服务器,包含单个端口和端口段的映射</p><a id="more"></a><h3 id="单个端口的映射"><a href="#单个端口的映射" class="headerlink" title="单个端口的映射"></a>单个端口的映射</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[router] interface GigabitEthernet0/2 公网ip接口</span><br><span class="line">[router] nat server protocol tcp global 公网ip 80 inside 内网ip 80</span><br></pre></td></tr></table></figure><h3 id="多端口"><a href="#多端口" class="headerlink" title="多端口"></a>多端口</h3><p>vsftp可以使用这个</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[router] interface GigabitEthernet0/2 公网ip接口</span><br><span class="line">[router] nat server protocol tcp global current-interface 9000 9045 inside 内网ip 9000 9045</span><br></pre></td></tr></table></figure><p><strong>备注:</strong> 可以使用vsftp场景，<a href="https://xxlaila.github.io/2019/08/09/vsftpd%E5%AE%89%E8%A3%85/">vsftp安装</a> </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;好记性不如烂笔头，h3c MSR3620路由器做端口映射到后端服务器,包含单个端口和端口段的映射&lt;/p&gt;
    
    </summary>
    
      <category term="网络设备" scheme="https://xxlaila.github.io/categories/%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87/"/>
    
    
      <category term="MSR3620" scheme="https://xxlaila.github.io/tags/MSR3620/"/>
    
  </entry>
  
  <entry>
    <title>traefik https应用</title>
    <link href="https://xxlaila.github.io/2019/09/06/traefik-https%E5%BA%94%E7%94%A8/"/>
    <id>https://xxlaila.github.io/2019/09/06/traefik-https应用/</id>
    <published>2019-09-06T03:31:09.000Z</published>
    <updated>2019-09-20T05:47:26.068Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;之前已经使用traefik服务作为入口，测试并访问了tomcat应用，之前是通过http来访问的，而我们在yaml文件里面也添加8443端口用于https访问，在实际环境中我们也是需要https来进行访问应用，通过traefik实现https，<a href="https://xxlaila.github.io/2019/09/05/traefik-ingress%E4%BD%BF%E7%94%A8/">traefik http应用</a></p><h3 id="操作实践"><a href="#操作实践" class="headerlink" title="操作实践"></a>操作实践</h3><ul><li>这里我用了公司的证书，就是为了贴近真实，也满足测试需求，</li><li>创建一个secret，保存https证书</li></ul><a id="more"></a><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ls</span></span><br><span class="line">1_test.kingxunlian.com_bundle.crt  2_test.kingxunlian.com.key</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl create secret generic traefik-cert --from-file=1_test.kingxunlian.com_bundle.crt --from-file=2_test.kingxunlian.com.key -n kube-system</span></span><br><span class="line">secret/traefik-cert created</span><br></pre></td></tr></table></figure><p>把证书拷贝到k8s node节点，存放路径为/etc/kubernetes/certs。</p><h3 id="创建一个configmap，保存traefix的配置"><a href="#创建一个configmap，保存traefix的配置" class="headerlink" title="创建一个configmap，保存traefix的配置"></a>创建一个configmap，保存traefix的配置</h3><p>traefix中配置了把所有http请求全部rewrite为https的规则，并配置相应的证书位置</p><ul><li>traefik.toml<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">defaultEntryPoints = [<span class="string">"http"</span>,<span class="string">"https"</span>]</span><br><span class="line">[entryPoints]</span><br><span class="line">  [entryPoints.http]</span><br><span class="line">  address = <span class="string">":80"</span></span><br><span class="line">    [entryPoints.http.redirect]</span><br><span class="line">    entryPoint = <span class="string">"https"</span></span><br><span class="line">  [entryPoints.https]</span><br><span class="line">  address = <span class="string">":443"</span></span><br><span class="line">    [entryPoints.https.tls]</span><br><span class="line">      [[entryPoints.https.tls.certificates]]</span><br><span class="line">      certFile = <span class="string">"/etc/kubernetes/certs/1_test.kingxunlian.com_bundle.crt"</span></span><br><span class="line">      keyFile = <span class="string">"/etc/kubernetes/certs/2_test.kingxunlian.com.key"</span></span><br><span class="line"></span><br><span class="line">$ kubectl create configmap traefik-conf --from-file=traefik.toml -n kube-system</span><br><span class="line">configmap/traefik-conf created</span><br></pre></td></tr></table></figure></li></ul><p>把traefik.toml文件拷贝到k8s node节点,存放路径为/etc/kubernetes/conf。</p><h3 id="重新部署Traefix"><a href="#重新部署Traefix" class="headerlink" title="重新部署Traefix"></a>重新部署Traefix</h3><p>主要是要关联创建的secret和configMap，并挂载相对应的主机目录。</p><ul><li>deployment.yaml</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">$ cat deployment.yaml</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: traefik-ingress-lb</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: traefik-ingress-lb</span><br><span class="line">spec:</span><br><span class="line">  replicas: 2 <span class="comment"># 增加行</span></span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        k8s-app: traefik-ingress-lb</span><br><span class="line">        name: traefik-ingress-lb</span><br><span class="line">    spec:</span><br><span class="line">      terminationGracePeriodSeconds: 60</span><br><span class="line">      hostNetwork: <span class="literal">true</span></span><br><span class="line">      restartPolicy: Always</span><br><span class="line">      serviceAccountName: ingress</span><br><span class="line">      containers:</span><br><span class="line">      - image: traefik</span><br><span class="line">        name: traefik-ingress-lb</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - mountPath: <span class="string">"/etc/kubernetes/certs"</span></span><br><span class="line">          name: <span class="string">"ssl"</span></span><br><span class="line">        - mountPath: <span class="string">"/etc/kubernetes/conf"</span></span><br><span class="line">          name: <span class="string">"config"</span></span><br><span class="line">        resources:</span><br><span class="line">          limits:</span><br><span class="line">            cpu: 1000m</span><br><span class="line">            memory: 3000Mi</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 500m</span><br><span class="line">            memory: 2000Mi</span><br><span class="line">        ports:</span><br><span class="line">        - name: http</span><br><span class="line">          containerPort: 80</span><br><span class="line">          hostPort: 80</span><br><span class="line">        - name: admin</span><br><span class="line">          containerPort: 8580</span><br><span class="line">          hostPort: 8580</span><br><span class="line">        args:</span><br><span class="line">        - --configFile=/etc/kubernetes/conf/traefik.toml</span><br><span class="line">        - --web</span><br><span class="line">        - --web.address=:8580</span><br><span class="line">        - --kubernetes</span><br><span class="line"></span><br><span class="line">$ kubectl apply -f deployment.yaml </span><br><span class="line">deployment.extensions/traefik-ingress-lb configured</span><br></pre></td></tr></table></figure><h3 id="测试效果"><a href="#测试效果" class="headerlink" title="测试效果"></a>测试效果</h3><p>登陆traefik-ui界面,用原本http的访问，traefik会直接给我们重定向至https。<br><img src="http://zxc.kingxunlian.com/1567748749337.jpg" alt="img"></p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于traefik-ui使用的域名不是我们证书所支持的域名，所以这里提示不安全，修改之前创建的ingress，修改其中的域名为支持证书的域名</p><ul><li>traefik-ui.yaml<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: traefik-web-ui</span><br><span class="line">  namespace: kube-system </span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    k8s-app: traefik-ingress-lb</span><br><span class="line">  ports:</span><br><span class="line">  - name: web</span><br><span class="line">    port: 80</span><br><span class="line">    targetPort: 8580</span><br><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: traefik-web-ui</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: traefik.test.kingxunlian.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: /</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: traefik-web-ui</span><br><span class="line">          servicePort: web</span><br><span class="line"></span><br><span class="line">$ kubectl apply -f traefik-ui.yaml </span><br><span class="line">service/traefik-web-ui unchanged</span><br><span class="line">ingress.extensions/traefik-web-ui configured</span><br></pre></td></tr></table></figure></li></ul><p>修改hosts版定的域名进行访问<br><img src="http://zxc.kingxunlian.com/1567749086159.jpg" alt="img"></p><ul><li><p>修改之前部署的tomcat程序</p></li><li><p>ingress-tomcat.yaml </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: tomcat-test-web</span><br><span class="line">  namespace: default</span><br><span class="line">  annotations:</span><br><span class="line">    kubernetes.io/ingress.class: traefik</span><br><span class="line">    traefik.frontend.rule.type: PathPrefixStrip</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: tomcat.test.kingxunlian.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: /test1/</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: tomcat-test1</span><br><span class="line">          servicePort: 8080</span><br><span class="line">      - path: /test2/</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: tomcat-test2</span><br><span class="line">          servicePort: 8080</span><br><span class="line"></span><br><span class="line">$ kubectl apply -f ingress-tomcat.yaml </span><br><span class="line">ingress.extensions/tomcat-test-web configured</span><br></pre></td></tr></table></figure></li></ul><p>访问链接测试<br><img src="http://zxc.kingxunlian.com/1567749278980.jpg" alt="img"><br><img src="http://zxc.kingxunlian.com/1567749309772.jpg" alt="img"></p><h3 id="其他需求"><a href="#其他需求" class="headerlink" title="其他需求"></a>其他需求</h3><p>在我们真实的应用场景中，需求肯定有不同的，比如我所在的公司开发环境就要只是http和https，测试环境以上的就全部强制https。这就得分开进行配置</p><h4 id="同时支持http和https"><a href="#同时支持http和https" class="headerlink" title="同时支持http和https"></a>同时支持http和https</h4><p>把http中的rewrite代码改掉</p><ul><li>traefik.toml</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">defaultEntryPoints = [<span class="string">"http"</span>,<span class="string">"https"</span>]</span><br><span class="line">[entryPoints]</span><br><span class="line">  [entryPoints.http]</span><br><span class="line">  address = <span class="string">":80"</span></span><br><span class="line">    entryPoint = <span class="string">"https"</span></span><br><span class="line">  [entryPoints.https]</span><br><span class="line">  address = <span class="string">":443"</span></span><br><span class="line">    [entryPoints.https.tls]</span><br><span class="line">      [[entryPoints.https.tls.certificates]]</span><br><span class="line">      certFile = <span class="string">"/etc/kubernetes/certs/1_test.kingxunlian.com_bundle.crt"</span></span><br><span class="line">      keyFile = <span class="string">"/etc/kubernetes/certs/2_test.kingxunlian.com.key"</span></span><br><span class="line">      [[entryPoints.https.tls.certificates]]</span><br><span class="line">      certFile = <span class="string">"/etc/kubernetes/certs/1_dev.kingxunlian.com_bundle.crt"</span></span><br><span class="line">      keyFile = <span class="string">"/etc/kubernetes/certs/2_dev.kingxunlian.com.key"</span></span><br><span class="line"></span><br><span class="line">[file]</span><br><span class="line"></span><br><span class="line"><span class="comment"># rules</span></span><br><span class="line">[entryPoints]</span><br><span class="line">  [entryPoints.http]</span><br><span class="line">  address = <span class="string">":80"</span></span><br><span class="line">    [entryPoints.http.redirect]</span><br><span class="line">      regex = <span class="string">"^http://traefix.test.kingxunlian.com/(.*)"</span></span><br><span class="line">      replacement = <span class="string">"https://traefix.test.kingxunlian.com/<span class="variable">$1</span>"</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;之前已经使用traefik服务作为入口，测试并访问了tomcat应用，之前是通过http来访问的，而我们在yaml文件里面也添加8443端口用于https访问，在实际环境中我们也是需要https来进行访问应用，通过traefik实现https，&lt;a href=&quot;https://xxlaila.github.io/2019/09/05/traefik-ingress%E4%BD%BF%E7%94%A8/&quot;&gt;traefik http应用&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;操作实践&quot;&gt;&lt;a href=&quot;#操作实践&quot; class=&quot;headerlink&quot; title=&quot;操作实践&quot;&gt;&lt;/a&gt;操作实践&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;这里我用了公司的证书，就是为了贴近真实，也满足测试需求，&lt;/li&gt;
&lt;li&gt;创建一个secret，保存https证书&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="kubernetes" scheme="https://xxlaila.github.io/categories/kubernetes/"/>
    
    
      <category term="traefik" scheme="https://xxlaila.github.io/tags/traefik/"/>
    
  </entry>
  
  <entry>
    <title>traefik ingress使用</title>
    <link href="https://xxlaila.github.io/2019/09/05/traefik-ingress%E4%BD%BF%E7%94%A8/"/>
    <id>https://xxlaila.github.io/2019/09/05/traefik-ingress使用/</id>
    <published>2019-09-05T09:16:25.000Z</published>
    <updated>2019-09-20T05:47:26.073Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Traefik介绍"><a href="#Traefik介绍" class="headerlink" title="Traefik介绍"></a>Traefik介绍</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;简单的说，ingress就是从kubernetes集群外访问集群的入口，将用户的URL请求转发到不同的service上。Ingress相当于nginx、apache等负载均衡反向代理服务器，其中还包括规则定义，即URL的路由信息。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Traefik是一款开源的反向代理与负载均衡工具。它最大的优点是能够与常见的微服务系统直接整合，实现自动化动态配置。Traefik通过不断地跟 kubernetes API 打交道，实时的感知后端 service、pod 等变化，比如pod，service 增加与减少等；当得到这些变化信息后，Ingress自动更新配置并热重载 ，达到服务发现的作用。</p><a id="more"></a><p>traefix 整体架构图如下:<br><img src="http://zxc.kingxunlian.com/34238937snkhfskdy8923.png" alt="img"></p><h3 id="Traefik主要特性详解"><a href="#Traefik主要特性详解" class="headerlink" title="Traefik主要特性详解"></a>Traefik主要特性详解</h3><ul><li><p>自动熔断</p><ul><li>在集群中，当某一个服务大量出现请求错误，或者请求响应时间过久，或者返回500+错误状态码时，我们希望可以主动剔除该服务，也就是不在将请求转发到该服务上，而这一个过程是自动完成，不需要人工执行。Traefik 通过配置很容易就能帮我们实现，Traefik 可以通过定义策略来主动熔断服务。</li><li>NetworkErrorRatio() &gt; 0.5：监测服务错误率达到50%时，熔断</li><li>LatencyAtQuantileMS(50.0) &gt; 50：监测延时大于50ms时，熔断</li><li>ResponseCodeRatio(500, 600, 0, 600) &gt; 0.5：监测返回状态码为[500-600]在[0-600]区间占比超过50%时，熔断</li></ul></li><li><p>负载均衡策略</p><ul><li>Traefik 提供两种负载均衡策略支持。一种是 wrr（加权轮训调度算法），一种是 drr（动态加权循环调度算法）</li><li>wrr是默认的负载均衡策略，新创建的 service 权重都是一样为1，这样的话，请求会平均分给每个服务，但是这样很多时候会出现资源分配不均衡的问题，比如由于集群中每个机器配置不一样，而且服务消耗不一样，假设 A 资源使用率已经很高，而 B 属于空闲状态，如果还是均摊到每个服务的话，会加重 A 的负荷，这时候因该有一种策略能够主动识别并分担更多流量到 B 才对</li><li>drr 就更加智能，它是一种动态加权轮训调度方式，它会记录一段时间内转发到 A 的请求数，跟转发到 B 的请求数对比，转发数量多，说明处理速度快，响应时间快。如果 A 处理请求速度比 B 快，那么就会调整 A 的权重，接下来的一段时间，就会转发更多请求给 A，相应的 B 的转发就少一些。整个过程都在不断的调整权重，实现请求的合理分配，从而达到资源使用最大化</li></ul></li></ul><h3 id="部署Traefik-ingress"><a href="#部署Traefik-ingress" class="headerlink" title="部署Traefik ingress"></a>部署Traefik ingress</h3><ul><li><p>创建ingress-rbac.yaml，将用于service account验证。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat ingress-rbac.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: ingress</span><br><span class="line">  namespace: kube-system</span><br><span class="line">---</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: ingress</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: ingress</span><br><span class="line">    namespace: kube-system</span><br><span class="line">roleRef:</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: cluster-admin</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br></pre></td></tr></table></figure></li><li><p>创建Depeloyment部署traefik，如文件名为deployment.yaml</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: traefik-ingress-lb</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: traefik-ingress-lb</span><br><span class="line">spec:</span><br><span class="line">  replicas: 2 <span class="comment"># 增加行</span></span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        k8s-app: traefik-ingress-lb</span><br><span class="line">        name: traefik-ingress-lb</span><br><span class="line">    spec:</span><br><span class="line">      terminationGracePeriodSeconds: 60</span><br><span class="line">      hostNetwork: <span class="literal">true</span></span><br><span class="line">      restartPolicy: Always</span><br><span class="line">      serviceAccountName: ingress</span><br><span class="line">      containers:</span><br><span class="line">      - image: traefik</span><br><span class="line">        name: traefik-ingress-lb</span><br><span class="line">        resources:</span><br><span class="line">          limits:</span><br><span class="line">            cpu: 1000m</span><br><span class="line">            memory: 3000Mi</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 500m</span><br><span class="line">            memory: 2000Mi</span><br><span class="line">        ports:</span><br><span class="line">        - name: http</span><br><span class="line">          containerPort: 80</span><br><span class="line">          hostPort: 80</span><br><span class="line">        - name: admin</span><br><span class="line">          containerPort: 8580</span><br><span class="line">          hostPort: 8580</span><br><span class="line">        args:</span><br><span class="line">        - --web</span><br><span class="line">        - --web.address=:8580</span><br><span class="line">        - --kubernetes</span><br></pre></td></tr></table></figure></li></ul><ul><li><strong>注意</strong>: 我们这里用的是Deploy类型，没有限定该pod运行在哪个主机上。Traefik的端口是8580。</li></ul><ul><li>编写Traefik UI的ingress部署文件，如文件名为traefik-ui.yaml<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: traefik-web-ui</span><br><span class="line">  namespace: kube-system <span class="comment">#增加行</span></span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    k8s-app: traefik-ingress-lb</span><br><span class="line">  ports:</span><br><span class="line">  - name: web</span><br><span class="line">    port: 80</span><br><span class="line">    targetPort: 8580</span><br><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: traefik-web-ui</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: traefik.xxlaila.io</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: /</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: traefik-web-ui</span><br><span class="line">          servicePort: web</span><br></pre></td></tr></table></figure></li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p><ul><li><code>backend</code>中要配置default namespace中启动的service名字。</li><li><code>path</code>就是URL地址后的路径，如<code>traefik.frontend.io/path</code>，service将会接受path这个路径</li><li><code>host</code>最好使用service-name.filed1.filed2.domain-name这种类似主机名称的命名方式，方便区分服务。</li></ul><ul><li><strong>逼逼一下</strong>: 目前我所在的公司后端微服务100+，前端60+，如果用传统nginx的local来匹配，估计要写死人，而且对于运维自动化来也不是很好做，再则是出了问题也还要去看一下是哪个应用；我们目前是通过每个服务每一个域名，域名是根据服务名来自动生成，除了几个特定对外公开的是特制的域名，其他的均采用这种机制，当有问题的时候，一下就能判断出那里出问题，很好定位，有域名有特殊配置的时候，也可以单独的进行设置，但是截止目前两年多来，运维拒绝这种特殊需求(有还有，很少，只有那么两三个)</li></ul><ul><li><p>配置完成后就可以启动treafik ingress了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl apply -f ./</span></span><br><span class="line">deployment.extensions/traefik-ingress-lb created</span><br><span class="line">serviceaccount/ingress created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/ingress created</span><br><span class="line">service/traefik-web-ui created</span><br><span class="line">ingress.extensions/traefik-web-ui created</span><br></pre></td></tr></table></figure></li><li><p>查看是否部署成功</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get pods -n kube-system | grep traefik</span></span><br><span class="line"></span><br><span class="line">traefik-ingress-lb-5d7f658cfd-4vkjc     1/1     Running   0          29m</span><br><span class="line">traefik-ingress-lb-5d7f658cfd-7sszp     1/1     Running   0          19m</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl get ingress -o wide --all-namespaces </span></span><br><span class="line">NAMESPACE     NAME                HOSTS                ADDRESS   PORTS   AGE</span><br><span class="line">kube-system   traefik-web-ui      traefik.xxlaila.io             80      29m</span><br></pre></td></tr></table></figure></li></ul><p>在浏览器绑定hosts域名解析，node的ip地址，在浏览器输入traefik.xxlaila.io即可访问了<br><img src="http://zxc.kingxunlian.com/1567676343800.jpg" alt="img"></p><p>左侧蓝色部分列出的是所有的前端(frontends)，右侧绿色部分是所有的后端(backend)。</p><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>下面模拟部署一个程序，以Nginx 为例，并使用drr动态轮训加权策略。</p><ul><li><p>nginx-deployment.yaml </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-pod</span><br><span class="line">spec:</span><br><span class="line">  replicas: 2</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx-pod</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.15.5</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-service</span><br><span class="line">  annotations:</span><br><span class="line">    traefik.ingress.kubernetes.io/load-balancer-method: drr</span><br><span class="line">spec:</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        name: nginx-service</span><br><span class="line">        namespace: default</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: nginx-pod</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80</span><br><span class="line">    targetPort: 80</span><br><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ingress</span><br><span class="line">  annotations:</span><br><span class="line">    kubernetes.io/ingress.class: traefik</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: k8s.nginx.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - backend:</span><br><span class="line">          serviceName: nginx-service</span><br><span class="line">          servicePort: 80</span><br></pre></td></tr></table></figure></li><li><p>创建nginx</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl apply -f nginx-deployment.yaml </span></span><br><span class="line">deployment.apps/nginx-pod created</span><br><span class="line">service/nginx-service created</span><br><span class="line">ingress.extensions/nginx-ingress created</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl get pods</span></span><br></pre></td></tr></table></figure></li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;所有访问这些地址的流量都会发送给172.16.0.180这台主机，就是我们启动traefik的主机。Traefik会解析http请求header里的Host参数将流量转发给Ingress配置里的相应service。<br><img src="http://zxc.kingxunlian.com/1567676984150.jpg" alt="img"></p><p>客户端绑定host，浏览器进行访问: <a href="http://k8s.nginx.com" target="_blank" rel="noopener">http://k8s.nginx.com</a><br><img src="http://zxc.kingxunlian.com/1567677018297.jpg" alt="img"></p><p>在K8s集群节点上访问测试</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># curl -x 172.21.16.204 http://k8s.nginx.com</span></span><br><span class="line">&lt;html&gt;&lt;body&gt;&lt;h1&gt;503 Service Unavailable&lt;/h1&gt;</span><br><span class="line">No server is available to handle this request.</span><br><span class="line">&lt;/body&gt;&lt;/html&gt;</span><br><span class="line">[root@k8s ~]<span class="comment"># curl -x 172.21.16.204:80 http://k8s.nginx.com</span></span><br><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;</span><br><span class="line">&lt;title&gt;Welcome to nginx!&lt;/title&gt;</span><br><span class="line">&lt;style&gt;</span><br><span class="line">    body &#123;</span><br><span class="line">        width: 35em;</span><br><span class="line">        margin: 0 auto;</span><br><span class="line">        font-family: Tahoma, Verdana, Arial, sans-serif;</span><br><span class="line">    &#125;</span><br><span class="line">&lt;/style&gt;</span><br><span class="line">&lt;/head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;</span><br><span class="line">&lt;p&gt;If you see this page, the nginx web server is successfully installed and</span><br><span class="line">working. Further configuration is required.&lt;/p&gt;</span><br><span class="line"></span><br><span class="line">&lt;p&gt;For online documentation and support please refer to</span><br><span class="line">&lt;a href=<span class="string">"http://nginx.org/"</span>&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;</span><br><span class="line">Commercial support is available at</span><br><span class="line">&lt;a href=<span class="string">"http://nginx.com/"</span>&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;</span><br><span class="line"></span><br><span class="line">&lt;p&gt;&lt;em&gt;Thank you <span class="keyword">for</span> using nginx.&lt;/em&gt;&lt;/p&gt;</span><br><span class="line">&lt;/body&gt;</span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></table></figure><h4 id="ingress配置同域名不同路径代理web应用"><a href="#ingress配置同域名不同路径代理web应用" class="headerlink" title="ingress配置同域名不同路径代理web应用"></a>ingress配置同域名不同路径代理web应用</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;很多时候我们不想配置太多的域名来区别应用，使用同域名分路径的方式来区别应用就简洁方便很多。ingress也提供了相关的配置。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;假设两个应用tomcat-test1和tomcat-test2。这里可配置域名tomcat.xxlaila.io，通过路径test1、test2来分别代理两个tomcat应用。其中，分路径配置需添加配置：traefik.frontend.rule.type: PathPrefixStrip,首先，我先创建tomcat-test1和tomcat-test2的pod和service，其中8080为tomcat的http端口，8443为tomcat的https端口，本例中仅使用http端口测试。</p><ul><li><p>tomcat-test1.yaml</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: tomcat-test1</span><br><span class="line">  labels: </span><br><span class="line">    app: tomcat-test1</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1 </span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: tomcat-test1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: tomcat-test1</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: tomcat-test1</span><br><span class="line">        image: tomcat</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 8443</span><br><span class="line">        - containerPort: 8080</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: tomcat-test1</span><br><span class="line">  labels:</span><br><span class="line">    name: tomcat-test1</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - port: 8443</span><br><span class="line">    targetPort: 8443</span><br><span class="line">  selector:</span><br><span class="line">    app: tomcat-test1</span><br><span class="line">  ports:</span><br><span class="line">  - port: 8080 </span><br><span class="line">    targetPort: 8080</span><br><span class="line">  selector:</span><br><span class="line">    app: tomcat-test1</span><br></pre></td></tr></table></figure></li><li><p>tomcat-test2.yaml </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: tomcat-test2</span><br><span class="line">  labels: </span><br><span class="line">    app: tomcat-test2</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1 </span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: tomcat-test2</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: tomcat-test2</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: tomcat-test2</span><br><span class="line">        image: manjeetchauhan211/tomcat_test2</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 8443</span><br><span class="line">        - containerPort: 8080</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: tomcat-test2</span><br><span class="line">  labels:</span><br><span class="line">    name: tomcat-test2</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - port: 8443</span><br><span class="line">    targetPort: 8443</span><br><span class="line">  ports:</span><br><span class="line">  - port: 8080 </span><br><span class="line">    targetPort: 8080</span><br><span class="line">  selector:</span><br><span class="line">    app: tomcat-test2</span><br></pre></td></tr></table></figure></li><li><p>执行创建</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl apply -f ./</span></span><br><span class="line"></span><br><span class="line">$ kubectl get deployment</span><br><span class="line">NAME           READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">nginx-pod      2/2     2            2           17h</span><br><span class="line">tomcat-test1   1/1     1            1           42m</span><br><span class="line">tomcat-test2   1/1     1            1           42m</span><br><span class="line"></span><br><span class="line">$ kubectl get svc</span><br><span class="line">NAME             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE</span><br><span class="line">kubernetes       ClusterIP   10.254.0.1       &lt;none&gt;        443/TCP    5d23h</span><br><span class="line">nginx-service    ClusterIP   10.254.149.69    &lt;none&gt;        80/TCP     17h</span><br><span class="line">tomcat-test1     ClusterIP   10.254.195.108   &lt;none&gt;        8080/TCP   42m</span><br><span class="line">tomcat-test2     ClusterIP   10.254.6.88      &lt;none&gt;        8080/TCP   42m</span><br><span class="line">traefik-web-ui   ClusterIP   10.254.22.102    &lt;none&gt;        80/TCP     17h</span><br></pre></td></tr></table></figure></li></ul><p>创建test1的ingress，来发布tomcat-test1服务</p><ul><li>ingress-tomcat1.yam<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">$ cat ingress-tomcat1.yaml </span><br><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: tomcat-test1-web</span><br><span class="line">  namespace: default</span><br><span class="line">  annotations:</span><br><span class="line">    kubernetes.io/ingress.class: traefik</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: tomcat.xxlaila.io</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: /</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: tomcat-test1</span><br><span class="line">          servicePort: 8080</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl create -f ingress-tomcat.yaml</span></span><br></pre></td></tr></table></figure></li></ul><p>在traefix-ui界面上，可以看到已经有了一个<code>tomcat.xxlaila.io</code>的域名规则.<br><img src="http://zxc.kingxunlian.com/1567739051461.jpg" alt="img"></p><p>在hosts文件添加tomcat.xxlaila.io绑定来进行访问<br><img src="http://zxc.kingxunlian.com/1567739162707.jpg" alt="img"></p><h5 id="ingress配置同域名对应location"><a href="#ingress配置同域名对应location" class="headerlink" title="ingress配置同域名对应location"></a>ingress配置同域名对应location</h5><ul><li><p>ingress-tomcat.yaml </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: tomcat-test-web</span><br><span class="line">  namespace: default</span><br><span class="line">  annotations:</span><br><span class="line">    kubernetes.io/ingress.class: traefik</span><br><span class="line">    traefik.frontend.rule.type: PathPrefixStrip</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: tomcat.xxlaila.io</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: /test1/</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: tomcat-test1</span><br><span class="line">          servicePort: 8080</span><br><span class="line">      - path: /test2/</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: tomcat-test2</span><br><span class="line">          servicePort: 8080</span><br></pre></td></tr></table></figure></li><li><p>创建并查看</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f ingress-tomcat.yaml </span><br><span class="line"></span><br><span class="line">$ kubectl describe ingress tomcat-test-web</span><br><span class="line">Name:             tomcat-test-web</span><br><span class="line">Namespace:        default</span><br><span class="line">Address:          </span><br><span class="line">Default backend:  default-http-backend:80 (&lt;none&gt;)</span><br><span class="line">Rules:</span><br><span class="line">  Host               Path  Backends</span><br><span class="line">  ----               ----  --------</span><br><span class="line">  tomcat.xxlaila.io  </span><br><span class="line">                     /test1/   tomcat-test1:8080 (&lt;none&gt;)</span><br><span class="line">                     /test2/   tomcat-test2:8080 (&lt;none&gt;)</span><br><span class="line">Annotations:</span><br><span class="line">  traefik.frontend.rule.type:                        PathPrefixStrip</span><br><span class="line">  kubectl.kubernetes.io/last-applied-configuration:  &#123;<span class="string">"apiVersion"</span>:<span class="string">"extensions/v1beta1"</span>,<span class="string">"kind"</span>:<span class="string">"Ingress"</span>,<span class="string">"metadata"</span>:&#123;<span class="string">"annotations"</span>:&#123;<span class="string">"kubernetes.io/ingress.class"</span>:<span class="string">"traefik"</span>,<span class="string">"traefik.frontend.rule.type"</span>:<span class="string">"PathPrefixStrip"</span>&#125;,<span class="string">"name"</span>:<span class="string">"tomcat-test-web"</span>,<span class="string">"namespace"</span>:<span class="string">"default"</span>&#125;,<span class="string">"spec"</span>:&#123;<span class="string">"rules"</span>:[&#123;<span class="string">"host"</span>:<span class="string">"tomcat.xxlaila.io"</span>,<span class="string">"http"</span>:&#123;<span class="string">"paths"</span>:[&#123;<span class="string">"backend"</span>:&#123;<span class="string">"serviceName"</span>:<span class="string">"tomcat-test1"</span>,<span class="string">"servicePort"</span>:8080&#125;,<span class="string">"path"</span>:<span class="string">"/test1/"</span>&#125;,&#123;<span class="string">"backend"</span>:&#123;<span class="string">"serviceName"</span>:<span class="string">"tomcat-test2"</span>,<span class="string">"servicePort"</span>:8080&#125;,<span class="string">"path"</span>:<span class="string">"/test2/"</span>&#125;]&#125;&#125;]&#125;&#125;</span><br><span class="line"></span><br><span class="line">  kubernetes.io/ingress.class:  traefik</span><br><span class="line">Events:                         &lt;none&gt;</span><br></pre></td></tr></table></figure></li></ul><h3 id="给节点设置label"><a href="#给节点设置label" class="headerlink" title="给节点设置label"></a>给节点设置label</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于是 Kubernetes DeamonSet 这种方式部署 Traefik，所以需要提前给节点设置 Label，这样当程序部署时 Pod 会自动调度到设置 Label 的点上。</p><h4 id="节点设置-Label-标签"><a href="#节点设置-Label-标签" class="headerlink" title="节点设置 Label 标签"></a>节点设置 Label 标签</h4><ul><li>格式：kubectl label nodes [节点名] [key=value]<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"> kubectl get nodes</span><br><span class="line">NAME            STATUS   ROLES    AGE    VERSION</span><br><span class="line">172.21.16.204   Ready    &lt;none&gt;   7d5h   v1.13.3</span><br><span class="line">172.21.16.240   Ready    &lt;none&gt;   7d2h   v1.13.3</span><br><span class="line">172.21.16.87    Ready    &lt;none&gt;   7d2h   v1.13.3</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl label nodes 172.21.16.204 IngressProxy=true</span></span><br><span class="line">node/172.21.16.204 labeled</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看节点label设置是否成功</span></span><br><span class="line"><span class="comment"># kubectl get nodes --show-labels</span></span><br><span class="line">NAME            STATUS   ROLES    AGE    VERSION   LABELS</span><br><span class="line">172.21.16.204   Ready    &lt;none&gt;   7d5h   v1.13.3   IngressProxy=<span class="literal">true</span>,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=172.21.16.204,node.kubernetes.io/k8s-node=<span class="literal">true</span></span><br><span class="line">172.21.16.240   Ready    &lt;none&gt;   7d2h   v1.13.3   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=172.21.16.240,node.kubernetes.io/k8s-node=<span class="literal">true</span></span><br><span class="line">172.21.16.87    Ready    &lt;none&gt;   7d2h   v1.13.3   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=172.21.16.87,node.kubernetes.io/k8s-node=<span class="literal">true</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="修改Traefix部署文件"><a href="#修改Traefix部署文件" class="headerlink" title="修改Traefix部署文件"></a>修改Traefix部署文件</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat deployment.yaml</span></span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: traefik-ingress-lb</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: traefik-ingress-lb</span><br><span class="line">spec:</span><br><span class="line">  replicas: 2 <span class="comment"># 增加行</span></span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        k8s-app: traefik-ingress-lb</span><br><span class="line">        name: traefik-ingress-lb</span><br><span class="line">    spec:</span><br><span class="line">      terminationGracePeriodSeconds: 60</span><br><span class="line">      volumes:</span><br><span class="line">      - name: ssl</span><br><span class="line">        secret:</span><br><span class="line">          secretName: traefik-cert</span><br><span class="line">      - name: config</span><br><span class="line">        configMap:</span><br><span class="line">          name: traefik-conf</span><br><span class="line">      hostNetwork: <span class="literal">true</span></span><br><span class="line">      restartPolicy: Always</span><br><span class="line">      serviceAccountName: ingress</span><br><span class="line">      containers:</span><br><span class="line">      - image: traefik</span><br><span class="line">        name: traefik-ingress-lb</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - mountPath: <span class="string">"/etc/kubernetes/certs"</span></span><br><span class="line">          name: <span class="string">"ssl"</span></span><br><span class="line">        - mountPath: <span class="string">"/etc/kubernetes/conf"</span></span><br><span class="line">          name: <span class="string">"config"</span></span><br><span class="line">        resources:</span><br><span class="line">          limits:</span><br><span class="line">            cpu: 1000m</span><br><span class="line">            memory: 3000Mi</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 500m</span><br><span class="line">            memory: 2000Mi</span><br><span class="line">        ports:</span><br><span class="line">        - name: http</span><br><span class="line">          containerPort: 80</span><br><span class="line">          hostPort: 80</span><br><span class="line">        - name: admin</span><br><span class="line">          containerPort: 8580</span><br><span class="line">          hostPort: 8580</span><br><span class="line">        args:</span><br><span class="line">        - --configFile=/etc/kubernetes/conf/traefik.toml</span><br><span class="line">        - --web</span><br><span class="line">        - --web.address=:8580</span><br><span class="line">        - --kubernetes</span><br><span class="line">      nodeSelector:</span><br><span class="line">        IngressProxy: <span class="string">"true"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行部署即可</span></span><br></pre></td></tr></table></figure><ul><li>traefix-ui界面上可以看到<br><img src="http://zxc.kingxunlian.com/1567739737385.jpg" alt="img"></li></ul><p>从describe信息和ui界面上可以看到，tomcat.test.k8s分别有了/test1/和/test2/的域名代理以及相对应的后端<br><img src="http://zxc.kingxunlian.com/1567739822127.jpg" alt="img"><br><img src="http://zxc.kingxunlian.com/1567739856570.jpg" alt="img"></p><p><a href="https://xuchao918.github.io/2019/03/01/Kubernetes-traefik-ingress%E4%BD%BF%E7%94%A8/" target="_blank" rel="noopener">参考文献2</a><br><a href="https://blog.51cto.com/icenycmh/2124502" target="_blank" rel="noopener">参考文献1</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Traefik介绍&quot;&gt;&lt;a href=&quot;#Traefik介绍&quot; class=&quot;headerlink&quot; title=&quot;Traefik介绍&quot;&gt;&lt;/a&gt;Traefik介绍&lt;/h3&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;简单的说，ingress就是从kubernetes集群外访问集群的入口，将用户的URL请求转发到不同的service上。Ingress相当于nginx、apache等负载均衡反向代理服务器，其中还包括规则定义，即URL的路由信息。&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Traefik是一款开源的反向代理与负载均衡工具。它最大的优点是能够与常见的微服务系统直接整合，实现自动化动态配置。Traefik通过不断地跟 kubernetes API 打交道，实时的感知后端 service、pod 等变化，比如pod，service 增加与减少等；当得到这些变化信息后，Ingress自动更新配置并热重载 ，达到服务发现的作用。&lt;/p&gt;
    
    </summary>
    
      <category term="kubernetes" scheme="https://xxlaila.github.io/categories/kubernetes/"/>
    
    
      <category term="traefik" scheme="https://xxlaila.github.io/tags/traefik/"/>
    
  </entry>
  
  <entry>
    <title>k8s-helm</title>
    <link href="https://xxlaila.github.io/2019/09/04/k8s-helm/"/>
    <id>https://xxlaila.github.io/2019/09/04/k8s-helm/</id>
    <published>2019-09-04T10:52:26.000Z</published>
    <updated>2019-09-20T05:47:25.865Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Helm类似与linux下面的yum，Helm是一个用于kubernetes的包管理器，每一个包为一个chart，一个chart是一个目录，常常会对目录进行打包压缩，形成一个${name}-version.tgz的格式进行传输和存储。</p><ul><li>对于应用发布者而言，可以通过Helm打包应用，管理应用依赖关系，管理应用版本并发布应用到软件仓库。</li><li>对于使用者而言，使用Helm后不用需要了解Kubernetes的Yaml语法并编写应用部署文件，可以通过Helm下载并在kubernetes上安装需要的应用。</li></ul><p>Helm还提供了kubernetes上的软件部署，删除，升级，回滚应用的强大功能</p><a id="more"></a><h3 id="1、helm组件"><a href="#1、helm组件" class="headerlink" title="1、helm组件"></a>1、helm组件</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Helm 是一个命令行下的客户端工具。主要用于 Kubernetes 应用程序 Chart 的创建、打包、发布以及创建和管理本地和远程的 Chart 仓库。</p><h4 id="1-1、Tiller"><a href="#1-1、Tiller" class="headerlink" title="1.1、Tiller"></a>1.1、Tiller</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Tiller 是 Helm 的服务端，部署在 Kubernetes 集群中。Tiller 用于接收 Helm 的请求，并根据 Chart 生成 Kubernetes 的部署文件（ Helm 称为 Release ），然后提交给 Kubernetes 创建应用。Tiller 还提供了 Release 的升级、删除、回滚等一系列功能。</p><h4 id="1-2、Chart"><a href="#1-2、Chart" class="headerlink" title="1.2、Chart"></a>1.2、Chart</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Helm 的软件包，采用 TAR 格式。类似于 APT 的 DEB 包或者 YUM 的 RPM 包，其包含了一组定义 Kubernetes 资源相关的 YAML 文件</p><h4 id="1-3、Repoistory"><a href="#1-3、Repoistory" class="headerlink" title="1.3、Repoistory"></a>1.3、Repoistory</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Helm 的软件仓库，Repository 本质上是一个 Web 服务器，该服务器保存了一系列的 Chart 软件包以供用户下载，并且提供了一个该 Repository 的 Chart 包的清单文件以供查询。Helm 可以同时管理多个不同的 Repository</p><h4 id="1-4、Release"><a href="#1-4、Release" class="headerlink" title="1.4、Release"></a>1.4、Release</h4><p>使用 helm install 命令在 Kubernetes 集群中部署的 Chart 称为 Release</p><h3 id="2、helm安装"><a href="#2、helm安装" class="headerlink" title="2、helm安装"></a>2、helm安装</h3><ul><li>下载helm<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># wget https://storage.googleapis.com/kubernetes-helm/helm-v2.14.3-linux-amd64.tar.gz</span></span><br><span class="line"><span class="comment"># tar zxf helm-v2.14.3-linux-amd64.tar.gz &amp;&amp; mv linux-amd64/&#123;helm,tiller&#125; /usr/bin</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="2-1、创建分蘖服务帐户"><a href="#2-1、创建分蘖服务帐户" class="headerlink" title="2.1、创建分蘖服务帐户"></a>2.1、创建分蘖服务帐户</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl create serviceaccount tiller --namespace kube-system</span></span><br><span class="line">serviceaccount/tiller created</span><br></pre></td></tr></table></figure><h4 id="2-2、授予分蘖集群管理员角色"><a href="#2-2、授予分蘖集群管理员角色" class="headerlink" title="2.2、授予分蘖集群管理员角色"></a>2.2、授予分蘖集群管理员角色</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl create clusterrolebinding tiller-admin-binding --clusterrole=cluster-admin --serviceaccount=kube-system:tiller</span></span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/tiller-admin-binding created</span><br></pre></td></tr></table></figure><h4 id="2-2、安装tiller"><a href="#2-2、安装tiller" class="headerlink" title="2.2、安装tiller"></a>2.2、安装tiller</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># helm init --service-account tiller --upgrade -i docker.io/sapcc/tiller:v2.14.3 --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts</span></span><br></pre></td></tr></table></figure><h5 id="2-2-1、检查是否安装成功"><a href="#2-2-1、检查是否安装成功" class="headerlink" title="2.2.1、检查是否安装成功"></a>2.2.1、检查是否安装成功</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl -n kube-system get pods|grep tiller</span></span><br><span class="line">tiller-deploy-75b8f8575d-fplck          1/1     Running   0          17h</span><br><span class="line"></span><br><span class="line"><span class="comment"># helm version</span></span><br><span class="line">Client: &amp;version.Version&#123;SemVer:<span class="string">"v2.14.3"</span>, GitCommit:<span class="string">"0e7f3b6637f7af8fcfddb3d2941fcc7cbebb0085"</span>, GitTreeState:<span class="string">"clean"</span>&#125;</span><br><span class="line">Server: &amp;version.Version&#123;SemVer:<span class="string">"v2.14.3"</span>, GitCommit:<span class="string">"0e7f3b6637f7af8fcfddb3d2941fcc7cbebb0085"</span>, GitTreeState:<span class="string">"clean"</span>&#125;</span><br></pre></td></tr></table></figure><ul><li><strong>错误</strong>: 这里安装完成后执行<code>helm version</code>提示错误，内容如下:<br><code>E0904 18:51:07.730671   22845 portforward.go:391] an error occurred forwarding 38767 -&gt; 44134: error forwarding port 44134 to pod b52064300cfa79e6d83795535584f89c97c33dc91ea39c024492b7b40e3fb68e, uid : unable to do port forwarding: socat not found.</code>这个错误需要在客户端安装一个<a href="https://github.com/helm/helm/issues/1371" target="_blank" rel="noopener">socat插件</a></li></ul><ul><li><p>在node安装socat</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># yum install -y socat</span></span><br></pre></td></tr></table></figure></li><li><p>修改helm第三方存储库(可选)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># helm repo add stable https://burdenbear.github.io/kube-charts-mirror/</span></span><br><span class="line"><span class="comment"># helm repo list</span></span><br><span class="line">NAME     URL                                             </span><br><span class="line"><span class="built_in">local</span>    http://127.0.0.1:8879/charts                    </span><br><span class="line">monocular https://helm.github.io/monocular                </span><br><span class="line">stable   https://burdenbear.github.io/kube-charts-mirror/</span><br></pre></td></tr></table></figure></li></ul><h3 id="3、测试和启动本地helm-web"><a href="#3、测试和启动本地helm-web" class="headerlink" title="3、测试和启动本地helm web"></a>3、测试和启动本地helm web</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># helm list</span></span><br><span class="line"><span class="comment"># helm search</span></span><br><span class="line"><span class="comment"># helm search mysql --versions</span></span><br><span class="line"><span class="comment"># helm repo list</span></span><br><span class="line"><span class="comment"># helm serve --address 0.0.0.0:8879 &amp;</span></span><br></pre></td></tr></table></figure><p><img src="http://zxc.kingxunlian.com/1567651293339.jpg" alt="img"></p><h3 id="4、helm-web-ui"><a href="#4、helm-web-ui" class="headerlink" title="4、helm web ui"></a>4、helm web ui</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;helm安装以后，经常使用helm cli命来进行部署还是比较吃力的，而且对于有些人不喜欢cli的来说，是一个非常痛苦的事情，这里介绍一款kubeapps，Kubeapps是一个基于Web的UI，用于在Kubernetes集群中部署和管理应用程序。 Kubeapps允许您：</p><ul><li>从图表存储库中浏览和部署Helm图表</li><li>检查，升级和删除群集中安装的基于Helm的应用程序</li><li>添加自定义和私有图表存储库（支持ChartMuseum和JFrog Artifactory)</li><li>从服务目录和可用的Service Brokers浏览和配置外部服务</li><li>使用服务目录绑定将基于Helm的应用程序连接到外部服务</li><li>基于Kubernetes基于角色的访问控制的安全身份验证和授权</li></ul><h4 id="4-1、安装kubeapps"><a href="#4-1、安装kubeapps" class="headerlink" title="4.1、安装kubeapps"></a>4.1、安装kubeapps</h4><p>使用Helm图表安装<a href="https://github.com/kubeapps/kubeapps" target="_blank" rel="noopener">最新版本的Kubeapps</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># helm repo add bitnami https://charts.bitnami.com/bitnami</span></span><br><span class="line"><span class="comment"># helm install --name kubeapps --namespace kubeapps bitnami/kubeapps</span></span><br></pre></td></tr></table></figure><h4 id="4-2、启动kubeappsDashboard"><a href="#4-2、启动kubeappsDashboard" class="headerlink" title="4.2、启动kubeappsDashboard"></a>4.2、启动kubeappsDashboard</h4><p>安装Kubeapps后，运行以下命令从系统安全访问Kubeapps Dashboard</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># export POD_NAME=$(kubectl get pods --namespace kubeapps -l "app=kubeapps" -o jsonpath="&#123;.items[0].metadata.name&#125;")</span></span><br><span class="line"><span class="comment"># kubectl port-forward -n kubeapps $POD_NAME --address 0.0.0.0 8081:8080 &amp;</span></span><br><span class="line"><span class="comment"># 把容器的8080 映射到本地的8081端口，用于浏览器访问</span></span><br></pre></td></tr></table></figure><p><img src="http://zxc.kingxunlian.com/1567650844980.jpg" alt="img"></p><h4 id="4-3、创建token"><a href="#4-3、创建token" class="headerlink" title="4.3、创建token"></a>4.3、创建token</h4><p>访问仪表板需要Kubernetes API令牌才能通过Kubernetes API服务器进行身份验证</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl create serviceaccount kubeapps-operator</span></span><br><span class="line">serviceaccount/kubeapps-operator created</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl create clusterrolebinding kubeapps-operator --clusterrole=cluster-admin --serviceaccount=default:kubeapps-operator</span></span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/kubeapps-operator created</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取token</span></span><br><span class="line"><span class="comment"># kubectl get secret $(kubectl get serviceaccount kubeapps-operator -o jsonpath='&#123;.secrets[].name&#125;') -o jsonpath='&#123;.data.token&#125;' | base64 --decode</span></span><br></pre></td></tr></table></figure><p><img src="http://zxc.kingxunlian.com/1567650971425.jpg" alt="img"></p><h5 id="4-3-1、创建token访问脚本"><a href="#4-3-1、创建token访问脚本" class="headerlink" title="4.3.1、创建token访问脚本"></a>4.3.1、创建token访问脚本</h5><p>每次访问kubeapps的token 都要输入一长串，这里我们写一个shell脚本，放在<code>/usr/bin</code>目录，需要的时候执行命令即可，这样方便用于记</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vim /usr/bin/kubeapps</span></span><br><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">kubectl get secret $(kubectl get serviceaccount kubeapps-operator -o jsonpath=<span class="string">'&#123;.secrets[].name&#125;'</span>) -o jsonpath=<span class="string">'&#123;.data.token&#125;'</span> | base64 --decode</span><br><span class="line"><span class="comment"># chmod +x /usr/bin/kubeapps</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Helm类似与linux下面的yum，Helm是一个用于kubernetes的包管理器，每一个包为一个chart，一个chart是一个目录，常常会对目录进行打包压缩，形成一个${name}-version.tgz的格式进行传输和存储。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于应用发布者而言，可以通过Helm打包应用，管理应用依赖关系，管理应用版本并发布应用到软件仓库。&lt;/li&gt;
&lt;li&gt;对于使用者而言，使用Helm后不用需要了解Kubernetes的Yaml语法并编写应用部署文件，可以通过Helm下载并在kubernetes上安装需要的应用。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Helm还提供了kubernetes上的软件部署，删除，升级，回滚应用的强大功能&lt;/p&gt;
    
    </summary>
    
      <category term="kubernetes" scheme="https://xxlaila.github.io/categories/kubernetes/"/>
    
    
      <category term="helm" scheme="https://xxlaila.github.io/tags/helm/"/>
    
  </entry>
  
  <entry>
    <title>metrics-server安装季</title>
    <link href="https://xxlaila.github.io/2019/09/04/metrics-server%E5%AE%89%E8%A3%85%E5%AD%A3/"/>
    <id>https://xxlaila.github.io/2019/09/04/metrics-server安装季/</id>
    <published>2019-09-04T05:57:07.000Z</published>
    <updated>2019-09-20T05:47:26.030Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;metrics-server 通过 kube-apiserver 发现所有节点，然后调用 kubelet APIs（通过 https 接口）获得各节点（Node）和 Pod 的 CPU、Memory 等资源使用情况。Kubernetes 1.12 开始，kubernetes 的安装脚本移除了 Heapster，从 1.13 开始完全移除了对 Heapster 的支持，Heapster 不再被维护。</p><ul><li>替代方案如下:<ul><li>用于支持自动扩缩容的 CPU/memory HPA metrics：metrics-server</li><li>通用的监控方案：使用第三方可以获取 Prometheus 格式监控指标的监控系统，如 Prometheus Operator</li><li>事件传输：使用第三方工具来传输、归档 kubernetes events</li></ul></li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;使用 metrics-server 替代 Heapster，将无法在 dashboard 中以图形展示 Pod 的内存和 CPU 情况，需要通过 Prometheus、Grafana 等监控方案来弥补。</p><a id="more"></a><h4 id="1、监控架构"><a href="#1、监控架构" class="headerlink" title="1、监控架构"></a>1、监控架构</h4><p><img src="http://zxc.kingxunlian.com/2748678bdjsg848sd.png" alt="img"></p><h4 id="2、安装-metrics-server"><a href="#2、安装-metrics-server" class="headerlink" title="2、安装 metrics-server"></a>2、安装 metrics-server</h4><ul><li>从 github clone 源码<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># git clone https://github.com/xxlaila/kubernetes-yaml.git</span></span><br><span class="line"><span class="comment"># cd kubernetes-yaml/metrics-server</span></span><br><span class="line"><span class="comment"># ls</span></span><br><span class="line">aggregated-metrics-reader.yaml  auth-delegator.yaml  auth-reader.yaml  metrics-apiservice.yaml  metrics-server-deployment.yaml  metrics-server-service.yaml  resource-reader.yaml</span><br></pre></td></tr></table></figure></li></ul><ul><li><strong>注意</strong>: 之前在安装的时候遇到很多坑，而且网上看了教程基本上不能用，很坑，自己看网上教程，然后根据每一个错误来进行解决，终于，功夫不负有心人，花了一天半终于搞定啦。</li></ul><h4 id="3、metrics-server-文件修改"><a href="#3、metrics-server-文件修改" class="headerlink" title="3、metrics-server 文件修改"></a>3、metrics-server 文件修改</h4><p>metrics-server yaml文件这里文件已经修改好了，可以直接拿来用，<a href="https://github.com/kubernetes-incubator/metrics-server/issues/247" target="_blank" rel="noopener">参考文献</a>，</p><h5 id="3-1、metrics-server-deployment-yaml"><a href="#3-1、metrics-server-deployment-yaml" class="headerlink" title="3.1、metrics-server-deployment.yaml"></a>3.1、metrics-server-deployment.yaml</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat metrics-server-deployment.yaml</span></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: metrics-server</span><br><span class="line">  namespace: kube-system</span><br><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: metrics-server</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: metrics-server</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      k8s-app: metrics-server</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: metrics-server</span><br><span class="line">      labels:</span><br><span class="line">        k8s-app: metrics-server</span><br><span class="line">    spec:</span><br><span class="line">      serviceAccountName: metrics-server</span><br><span class="line">      hostNetwork: <span class="literal">true</span>   <span class="comment">#增加行</span></span><br><span class="line">      volumes:</span><br><span class="line">      <span class="comment"># mount in tmp so we can safely use from-scratch images and/or read-only containers</span></span><br><span class="line">      - name: tmp-dir</span><br><span class="line">        emptyDir: &#123;&#125;</span><br><span class="line">      containers:</span><br><span class="line">      - name: metrics-server</span><br><span class="line">        image: mirrorgooglecontainers/metrics-server-amd64:v0.3.3</span><br><span class="line">        imagePullPolicy: Always</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - mountPath: /etc/ssl/kubernetes/</span><br><span class="line">          name: ca-ssl</span><br><span class="line">        <span class="built_in">command</span>:   <span class="comment"># command内容均为增加</span></span><br><span class="line">        - /metrics-server</span><br><span class="line">        - --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP</span><br><span class="line">        - --requestheader-client-ca-file=/etc/ssl/kubernetes/front-proxy-ca.pem</span><br><span class="line">        - --kubelet-insecure-tls=<span class="literal">true</span></span><br><span class="line">      volumes:</span><br><span class="line">       - name: ca-ssl</span><br><span class="line">         hostPath:</span><br><span class="line">          path: /etc/kubernetes/ssl</span><br></pre></td></tr></table></figure><h5 id="3-2、"><a href="#3-2、" class="headerlink" title="3.2、"></a>3.2、</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat resource-reader.yaml</span></span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRole</span><br><span class="line">metadata:</span><br><span class="line">  name: system:metrics-server</span><br><span class="line">rules:</span><br><span class="line">- apiGroups:</span><br><span class="line">  - <span class="string">""</span></span><br><span class="line">  resources:</span><br><span class="line">  - pods</span><br><span class="line">  - nodes</span><br><span class="line">  - nodes/stats</span><br><span class="line">  - namespaces</span><br><span class="line">  verbs:</span><br><span class="line">  - get</span><br><span class="line">  - list</span><br><span class="line">  - watch</span><br><span class="line">- apiGroups: <span class="comment"># 增加</span></span><br><span class="line">  - <span class="string">"extensions"</span></span><br><span class="line">  resources:</span><br><span class="line">  - deployments</span><br><span class="line">  verbs:</span><br><span class="line">  - get</span><br><span class="line">  - list</span><br><span class="line">  - update</span><br><span class="line">  - watch</span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: system:metrics-server</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: system:metrics-server</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: metrics-server</span><br><span class="line">  namespace: kube-system</span><br></pre></td></tr></table></figure><h4 id="4、准备证书"><a href="#4、准备证书" class="headerlink" title="4、准备证书"></a>4、准备证书</h4><p>这些证书文件主要用在Metrics API aggregator 上,<a href="https://blog.51cto.com/ylw6006/2114338" target="_blank" rel="noopener">参考文献</a></p><ul><li><p>front-proxy-ca-csr.json</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat front-proxy-ca-csr.json </span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"CN"</span>: <span class="string">"kubernetes"</span>,</span><br><span class="line">  <span class="string">"key"</span>: &#123;</span><br><span class="line">    <span class="string">"algo"</span>: <span class="string">"rsa"</span>,</span><br><span class="line">    <span class="string">"size"</span>: 2048</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>front-proxy-client-csr.json</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="string">"CN"</span>: <span class="string">"front-proxy-client"</span>,</span><br><span class="line">  <span class="string">"key"</span>: &#123;</span><br><span class="line">    <span class="string">"algo"</span>: <span class="string">"rsa"</span>,</span><br><span class="line">    <span class="string">"size"</span>: 2048</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h5 id="4-1、生成证书"><a href="#4-1、生成证书" class="headerlink" title="4.1、生成证书"></a>4.1、生成证书</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cfssl gencert   -initca front-proxy-ca-csr.json | cfssljson -bare front-proxy-ca</span></span><br><span class="line"><span class="comment"># cfssl gencert \</span></span><br><span class="line"> -ca=front-proxy-ca.pem \</span><br><span class="line"> -ca-key=front-proxy-ca-key.pem \</span><br><span class="line"> -config=/root/ssl/kubernetes-gencert.json \</span><br><span class="line"> -profile=kubernetes \</span><br><span class="line"> front-proxy-client-csr.json | cfssljson -bare front-proxy-client</span><br><span class="line"><span class="comment"># ls *.pem</span></span><br><span class="line">front-proxy-ca-key.pem  front-proxy-ca.pem  front-proxy-client-key.pem  front-proxy-client.pem</span><br></pre></td></tr></table></figure><ul><li>证书生成完成后，吧证书复制到所有的master节点和node节点</li></ul><h4 id="5、master修改配置文件"><a href="#5、master修改配置文件" class="headerlink" title="5、master修改配置文件"></a>5、master修改配置文件</h4><h4 id="5-1、apiserver"><a href="#5-1、apiserver" class="headerlink" title="5.1、apiserver"></a>5.1、apiserver</h4><p>在apiserver配置文件里面增加如下配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">--runtime-config=api/all=<span class="literal">true</span> \</span><br><span class="line">--<span class="built_in">enable</span>-aggregator-routing=<span class="literal">true</span> \</span><br><span class="line">--requestheader-client-ca-file=/etc/kubernetes/ssl/front-proxy-ca.pem \</span><br><span class="line">--requestheader-allowed-names=aggregator \</span><br><span class="line">--requestheader-extra-headers-prefix=X-Remote-Extra- \</span><br><span class="line">--requestheader-group-headers=X-Remote-Group \</span><br><span class="line">--requestheader-username-headers=X-Remote-User \</span><br><span class="line">--proxy-client-cert-file=/etc/kubernetes/ssl/front-proxy-client.pem \</span><br><span class="line">--proxy-client-key-file=/etc/kubernetes/ssl/front-proxy-client-key.pem \</span><br></pre></td></tr></table></figure><ul><li>apiserver配置文件KUBE_API_ARGS内容如下<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">KUBE_API_ARGS=<span class="string">" --allow-privileged=true \</span></span><br><span class="line"><span class="string">                --anonymous-auth=false \</span></span><br><span class="line"><span class="string">                --alsologtostderr \</span></span><br><span class="line"><span class="string">                --apiserver-count=3 \</span></span><br><span class="line"><span class="string">                --audit-log-maxage=30 \</span></span><br><span class="line"><span class="string">                --audit-log-maxbackup=3 \</span></span><br><span class="line"><span class="string">                --audit-log-maxsize=100 \</span></span><br><span class="line"><span class="string">                --enable-aggregator-routing=true \</span></span><br><span class="line"><span class="string">                --audit-log-path=/var/log/kube-audit/audit.log \</span></span><br><span class="line"><span class="string">                --audit-policy-file=/etc/kubernetes/audit-policy.yaml \</span></span><br><span class="line"><span class="string">                --authorization-mode=Node,RBAC \</span></span><br><span class="line"><span class="string">                --client-ca-file=/etc/kubernetes/ssl/kubernetes-ca.pem \</span></span><br><span class="line"><span class="string">                --enable-bootstrap-token-auth \</span></span><br><span class="line"><span class="string">                --enable-garbage-collector \</span></span><br><span class="line"><span class="string">                --enable-logs-handler \</span></span><br><span class="line"><span class="string">                --endpoint-reconciler-type=lease \</span></span><br><span class="line"><span class="string">                --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem \</span></span><br><span class="line"><span class="string">                --etcd-certfile=/etc/etcd/ssl/etcd.pem \</span></span><br><span class="line"><span class="string">                --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem \</span></span><br><span class="line"><span class="string">                --etcd-compaction-interval=0s \</span></span><br><span class="line"><span class="string">                --event-ttl=168h0m0s \</span></span><br><span class="line"><span class="string">                --kubelet-https=true \</span></span><br><span class="line"><span class="string">                --kubelet-certificate-authority=/etc/kubernetes/ssl/kubernetes-ca.pem \</span></span><br><span class="line"><span class="string">                --kubelet-client-certificate=/etc/kubernetes/ssl/kubelet-api-admin.pem \</span></span><br><span class="line"><span class="string">                --kubelet-client-key=/etc/kubernetes/ssl/kubelet-api-admin-key.pem \</span></span><br><span class="line"><span class="string">                --kubelet-timeout=3s \</span></span><br><span class="line"><span class="string">                --runtime-config=api/all=true \</span></span><br><span class="line"><span class="string">                --service-node-port-range=30000-50000 \</span></span><br><span class="line"><span class="string">                --service-account-key-file=/etc/kubernetes/ssl/kubernetes-ca.pem \</span></span><br><span class="line"><span class="string">                --tls-cert-file=/etc/kubernetes/ssl/kube-apiserver.pem \</span></span><br><span class="line"><span class="string">                --tls-private-key-file=/etc/kubernetes/ssl/kube-apiserver-key.pem \</span></span><br><span class="line"><span class="string">                --requestheader-client-ca-file=/etc/kubernetes/ssl/front-proxy-ca.pem \</span></span><br><span class="line"><span class="string">                --requestheader-allowed-names=aggregator \</span></span><br><span class="line"><span class="string">                --requestheader-extra-headers-prefix=X-Remote-Extra- \</span></span><br><span class="line"><span class="string">                --requestheader-group-headers=X-Remote-Group \</span></span><br><span class="line"><span class="string">                --requestheader-username-headers=X-Remote-User \</span></span><br><span class="line"><span class="string">                --proxy-client-cert-file=/etc/kubernetes/ssl/front-proxy-client.pem \</span></span><br><span class="line"><span class="string">                --proxy-client-key-file=/etc/kubernetes/ssl/front-proxy-client-key.pem \</span></span><br><span class="line"><span class="string">                --v=2"</span></span><br></pre></td></tr></table></figure></li></ul><h5 id="5-2、kube-control-manager"><a href="#5-2、kube-control-manager" class="headerlink" title="5.2、kube-control-manager"></a>5.2、kube-control-manager</h5><p>在controller-manager文件增加如下配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--horizontal-pod-autoscaler-use-rest-clients=<span class="literal">true</span> \</span><br></pre></td></tr></table></figure><ul><li>kube-control-manager配置文件KUBE_CONTROLLER_MANAGER_ARGS如下<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">KUBE_CONTROLLER_MANAGER_ARGS=<span class="string">"  --address=0.0.0.0 \</span></span><br><span class="line"><span class="string">                                --authentication-kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \</span></span><br><span class="line"><span class="string">                                --authorization-kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \</span></span><br><span class="line"><span class="string">                                --bind-address=0.0.0.0 \</span></span><br><span class="line"><span class="string">                                --cluster-name=kubernetes \</span></span><br><span class="line"><span class="string">                                --cluster-signing-cert-file=/etc/kubernetes/ssl/kubernetes-ca.pem \</span></span><br><span class="line"><span class="string">                                --cluster-signing-key-file=/etc/kubernetes/ssl/kubernetes-ca-key.pem \</span></span><br><span class="line"><span class="string">                                --client-ca-file=/etc/kubernetes/ssl/kubernetes-ca.pem \</span></span><br><span class="line"><span class="string">                                --controllers=*,bootstrapsigner,tokencleaner \</span></span><br><span class="line"><span class="string">                                --deployment-controller-sync-period=10s \</span></span><br><span class="line"><span class="string">                                --experimental-cluster-signing-duration=87600h0m0s \</span></span><br><span class="line"><span class="string">                                --enable-garbage-collector=true \</span></span><br><span class="line"><span class="string">                                --kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \</span></span><br><span class="line"><span class="string">                                --leader-elect=true \</span></span><br><span class="line"><span class="string">                                --node-monitor-grace-period=20s \</span></span><br><span class="line"><span class="string">                                --node-monitor-period=5s \</span></span><br><span class="line"><span class="string">                                --port=10252 \</span></span><br><span class="line"><span class="string">                                --pod-eviction-timeout=2m0s \</span></span><br><span class="line"><span class="string">                                --requestheader-client-ca-file=/etc/kubernetes/ssl/kubernetes-ca.pem \</span></span><br><span class="line"><span class="string">                                --terminated-pod-gc-threshold=50 \</span></span><br><span class="line"><span class="string">                                --tls-cert-file=/etc/kubernetes/ssl/kube-controller-manager.pem \</span></span><br><span class="line"><span class="string">                                --tls-private-key-file=/etc/kubernetes/ssl/kube-controller-manager-key.pem \</span></span><br><span class="line"><span class="string">                                --root-ca-file=/etc/kubernetes/ssl/kubernetes-ca.pem \</span></span><br><span class="line"><span class="string">                                --secure-port=10257 \</span></span><br><span class="line"><span class="string">                                --service-cluster-ip-range=10.254.0.0/16 \</span></span><br><span class="line"><span class="string">                                --service-account-private-key-file=/etc/kubernetes/ssl/kubernetes-ca-key.pem \</span></span><br><span class="line"><span class="string">                                --use-service-account-credentials=true \</span></span><br><span class="line"><span class="string">                                --horizontal-pod-autoscaler-use-rest-clients=true \</span></span><br><span class="line"><span class="string">                                --v=2"</span></span><br></pre></td></tr></table></figure></li></ul><h5 id="5-3、重启服务"><a href="#5-3、重启服务" class="headerlink" title="5.3、重启服务"></a>5.3、重启服务</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># systemctl  restart kube-apiserver.service &amp;&amp;systemctl  restart kube-controller-manager</span></span><br></pre></td></tr></table></figure><h4 id="6、node节点配置文件修改"><a href="#6、node节点配置文件修改" class="headerlink" title="6、node节点配置文件修改"></a>6、node节点配置文件修改</h4><p>node 节点修改修改kubelet文件</p><ul><li><p>kubelet配置文件完成的KUBELET_ARGS参数</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">KUBELET_ARGS=<span class="string">"  --address=0.0.0.0 \</span></span><br><span class="line"><span class="string">                --allow-privileged \</span></span><br><span class="line"><span class="string">                --anonymous-auth=false \</span></span><br><span class="line"><span class="string">                --authorization-mode=Webhook \</span></span><br><span class="line"><span class="string">                --authentication-token-webhook=true \</span></span><br><span class="line"><span class="string">                --bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \</span></span><br><span class="line"><span class="string">                --client-ca-file=/etc/kubernetes/ssl/kubernetes-ca.pem \</span></span><br><span class="line"><span class="string">                --cgroup-driver=cgroupfs \</span></span><br><span class="line"><span class="string">                --cert-dir=/etc/kubernetes/ssl \</span></span><br><span class="line"><span class="string">                --cluster-dns=10.254.0.2 \</span></span><br><span class="line"><span class="string">                --cluster-domain=cluster.local \</span></span><br><span class="line"><span class="string">                --eviction-soft=imagefs.available&lt;15%,memory.available&lt;512Mi,nodefs.available&lt;15%,nodefs.inodesFree&lt;10% \</span></span><br><span class="line"><span class="string">                --eviction-soft-grace-period=imagefs.available=3m,memory.available=1m,nodefs.available=3m,nodefs.inodesFree=1m \</span></span><br><span class="line"><span class="string">                --eviction-hard=imagefs.available&lt;10%,memory.available&lt;256Mi,nodefs.available&lt;10%,nodefs.inodesFree&lt;5% \</span></span><br><span class="line"><span class="string">                --eviction-max-pod-grace-period=30 \</span></span><br><span class="line"><span class="string">                --image-gc-high-threshold=80 \</span></span><br><span class="line"><span class="string">                --image-gc-low-threshold=70 \</span></span><br><span class="line"><span class="string">                --image-pull-progress-deadline=30s \</span></span><br><span class="line"><span class="string">                --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \</span></span><br><span class="line"><span class="string">                --max-pods=100 \</span></span><br><span class="line"><span class="string">                --minimum-image-ttl-duration=720h0m0s \</span></span><br><span class="line"><span class="string">                --node-labels=node.kubernetes.io/k8s-node=true \</span></span><br><span class="line"><span class="string">                --pod-infra-container-image=docker.io/kubernetes/pause:latest \</span></span><br><span class="line"><span class="string">                --port=10250 \</span></span><br><span class="line"><span class="string">                --read-only-port=0 \</span></span><br><span class="line"><span class="string">                --rotate-certificates \</span></span><br><span class="line"><span class="string">                --rotate-server-certificates \</span></span><br><span class="line"><span class="string">                --fail-swap-on=false \</span></span><br><span class="line"><span class="string">                --v=2"</span></span><br></pre></td></tr></table></figure></li><li><p>重启kubelet</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># systemctl restart kubelet</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="7、创建metrics"><a href="#7、创建metrics" class="headerlink" title="7、创建metrics"></a>7、创建metrics</h4><p>通过yaml文件创建对应的资源</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl create -f ./</span></span><br></pre></td></tr></table></figure><h5 id="7-1、查看运行情况"><a href="#7-1、查看运行情况" class="headerlink" title="7.1、查看运行情况"></a>7.1、查看运行情况</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl -n kube-system get pods -l k8s-app=metrics-server</span></span><br><span class="line">NAME                              READY   STATUS    RESTARTS   AGE</span><br><span class="line">metrics-server-84b786c9bb-7trdr   1/1     Running   0          62m</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl get svc -n kube-system  metrics-server</span></span><br><span class="line">NAME             TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">metrics-server   ClusterIP   10.254.45.238   &lt;none&gt;        443/TCP   3h6m</span><br></pre></td></tr></table></figure><h5 id="7-2、获取v1beta1-metrics-k8s-io并验证"><a href="#7-2、获取v1beta1-metrics-k8s-io并验证" class="headerlink" title="7.2、获取v1beta1.metrics.k8s.io并验证"></a>7.2、获取v1beta1.metrics.k8s.io并验证</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;之前<code>v1beta1.metrics.k8s.io                  kube-system/metrics-server   True        3h</code>参数一直是<code>v1beta1.metrics.k8s.io                  kube-system/metrics-server   False (FailedDiscoveryCheck)   16m</code>,</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  kubectl get apiservice</span></span><br><span class="line">NAME                                    SERVICE                      AVAILABLE   AGE</span><br><span class="line">v1.                                     Local                        True        4d</span><br><span class="line">v1.apps                                 Local                        True        4d</span><br><span class="line">v1.authentication.k8s.io                Local                        True        4d</span><br><span class="line">v1.authorization.k8s.io                 Local                        True        4d</span><br><span class="line">v1.autoscaling                          Local                        True        4d</span><br><span class="line">v1.batch                                Local                        True        4d</span><br><span class="line">v1.networking.k8s.io                    Local                        True        4d</span><br><span class="line">v1.rbac.authorization.k8s.io            Local                        True        4d</span><br><span class="line">v1.storage.k8s.io                       Local                        True        4d</span><br><span class="line">v1alpha1.admissionregistration.k8s.io   Local                        True        4d</span><br><span class="line">v1alpha1.auditregistration.k8s.io       Local                        True        4d</span><br><span class="line">v1alpha1.rbac.authorization.k8s.io      Local                        True        4d</span><br><span class="line">v1alpha1.scheduling.k8s.io              Local                        True        4d</span><br><span class="line">v1alpha1.settings.k8s.io                Local                        True        4d</span><br><span class="line">v1alpha1.storage.k8s.io                 Local                        True        4d</span><br><span class="line">v1beta1.admissionregistration.k8s.io    Local                        True        4d</span><br><span class="line">v1beta1.apiextensions.k8s.io            Local                        True        4d</span><br><span class="line">v1beta1.apps                            Local                        True        4d</span><br><span class="line">v1beta1.authentication.k8s.io           Local                        True        4d</span><br><span class="line">v1beta1.authorization.k8s.io            Local                        True        4d</span><br><span class="line">v1beta1.batch                           Local                        True        4d</span><br><span class="line">v1beta1.certificates.k8s.io             Local                        True        4d</span><br><span class="line">v1beta1.coordination.k8s.io             Local                        True        4d</span><br><span class="line">v1beta1.events.k8s.io                   Local                        True        4d</span><br><span class="line">v1beta1.extensions                      Local                        True        4d</span><br><span class="line">v1beta1.metrics.k8s.io                  kube-system/metrics-server   True        3h</span><br><span class="line">v1beta1.policy                          Local                        True        4d</span><br><span class="line">v1beta1.rbac.authorization.k8s.io       Local                        True        4d</span><br><span class="line">v1beta1.scheduling.k8s.io               Local                        True        4d</span><br><span class="line">v1beta1.storage.k8s.io                  Local                        True        4d</span><br><span class="line">v1beta2.apps                            Local                        True        4d</span><br><span class="line">v2alpha1.batch                          Local                        True        4d</span><br><span class="line">v2beta1.autoscaling                     Local                        True        4d</span><br><span class="line">v2beta2.autoscaling                     Local                        True        4d</span><br></pre></td></tr></table></figure><h4 id="8、查看-metrics-server-输出的-metrics"><a href="#8、查看-metrics-server-输出的-metrics" class="headerlink" title="8、查看 metrics-server 输出的 metrics"></a>8、查看 metrics-server 输出的 metrics</h4><h5 id="8-1、通过-kube-apiserver-或-kubectl-proxy-访问"><a href="#8-1、通过-kube-apiserver-或-kubectl-proxy-访问" class="headerlink" title="8.1、通过 kube-apiserver 或 kubectl proxy 访问"></a>8.1、通过 kube-apiserver 或 kubectl proxy 访问</h5><ul><li><a href="https://172.21.17.31:6443/apis/metrics.k8s.io/v1beta1/nodes" target="_blank" rel="noopener">https://172.21.17.31:6443/apis/metrics.k8s.io/v1beta1/nodes</a></li><li><a href="https://172.21.17.31:6443/apis/metrics.k8s.io/v1beta1/nodes/" target="_blank" rel="noopener">https://172.21.17.31:6443/apis/metrics.k8s.io/v1beta1/nodes/</a></li><li><a href="https://172.21.17.31:6443/apis/metrics.k8s.io/v1beta1/pods" target="_blank" rel="noopener">https://172.21.17.31:6443/apis/metrics.k8s.io/v1beta1/pods</a></li><li><a href="https://172.21.17.31:6443/apis/metrics.k8s.io/v1beta1/namespace//pods/" target="_blank" rel="noopener">https://172.21.17.31:6443/apis/metrics.k8s.io/v1beta1/namespace//pods/</a></li></ul><h5 id="8-2、直接使用-kubectl-命令访问"><a href="#8-2、直接使用-kubectl-命令访问" class="headerlink" title="8.2、直接使用 kubectl 命令访问"></a>8.2、直接使用 kubectl 命令访问</h5><ul><li>kubectl get –raw /apis/metrics.k8s.io/v1beta1/nodes </li><li>kubectl get –raw /apis/metrics.k8s.io/v1beta1/pods </li><li>kubectl get –raw /apis/metrics.k8s.io/v1beta1/nodes/ </li><li>kubectl get –raw /apis/metrics.k8s.io/v1beta1/namespace//pods/</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get --raw "/apis/metrics.k8s.io/v1beta1" | jq .</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"kind"</span>: <span class="string">"APIResourceList"</span>,</span><br><span class="line">  <span class="string">"apiVersion"</span>: <span class="string">"v1"</span>,</span><br><span class="line">  <span class="string">"groupVersion"</span>: <span class="string">"metrics.k8s.io/v1beta1"</span>,</span><br><span class="line">  <span class="string">"resources"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"name"</span>: <span class="string">"nodes"</span>,</span><br><span class="line">      <span class="string">"singularName"</span>: <span class="string">""</span>,</span><br><span class="line">      <span class="string">"namespaced"</span>: <span class="literal">false</span>,</span><br><span class="line">      <span class="string">"kind"</span>: <span class="string">"NodeMetrics"</span>,</span><br><span class="line">      <span class="string">"verbs"</span>: [</span><br><span class="line">        <span class="string">"get"</span>,</span><br><span class="line">        <span class="string">"list"</span></span><br><span class="line">      ]</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"name"</span>: <span class="string">"pods"</span>,</span><br><span class="line">      <span class="string">"singularName"</span>: <span class="string">""</span>,</span><br><span class="line">      <span class="string">"namespaced"</span>: <span class="literal">true</span>,</span><br><span class="line">      <span class="string">"kind"</span>: <span class="string">"PodMetrics"</span>,</span><br><span class="line">      <span class="string">"verbs"</span>: [</span><br><span class="line">        <span class="string">"get"</span>,</span><br><span class="line">        <span class="string">"list"</span></span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl get --raw "/apis/metrics.k8s.io/v1beta1/nodes" | jq .</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"kind"</span>: <span class="string">"NodeMetricsList"</span>,</span><br><span class="line">  <span class="string">"apiVersion"</span>: <span class="string">"metrics.k8s.io/v1beta1"</span>,</span><br><span class="line">  <span class="string">"metadata"</span>: &#123;</span><br><span class="line">    <span class="string">"selfLink"</span>: <span class="string">"/apis/metrics.k8s.io/v1beta1/nodes"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"items"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"metadata"</span>: &#123;</span><br><span class="line">        <span class="string">"name"</span>: <span class="string">"172.21.16.204"</span>,</span><br><span class="line">        <span class="string">"selfLink"</span>: <span class="string">"/apis/metrics.k8s.io/v1beta1/nodes/172.21.16.204"</span>,</span><br><span class="line">        <span class="string">"creationTimestamp"</span>: <span class="string">"2019-09-04T07:00:44Z"</span></span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="string">"timestamp"</span>: <span class="string">"2019-09-04T07:00:40Z"</span>,</span><br><span class="line">      <span class="string">"window"</span>: <span class="string">"30s"</span>,</span><br><span class="line">      <span class="string">"usage"</span>: &#123;</span><br><span class="line">        <span class="string">"cpu"</span>: <span class="string">"63788460n"</span>,</span><br><span class="line">        <span class="string">"memory"</span>: <span class="string">"1033152Ki"</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"metadata"</span>: &#123;</span><br><span class="line">        <span class="string">"name"</span>: <span class="string">"172.21.16.240"</span>,</span><br><span class="line">        <span class="string">"selfLink"</span>: <span class="string">"/apis/metrics.k8s.io/v1beta1/nodes/172.21.16.240"</span>,</span><br><span class="line">        <span class="string">"creationTimestamp"</span>: <span class="string">"2019-09-04T07:00:44Z"</span></span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="string">"timestamp"</span>: <span class="string">"2019-09-04T07:00:40Z"</span>,</span><br><span class="line">      <span class="string">"window"</span>: <span class="string">"30s"</span>,</span><br><span class="line">      <span class="string">"usage"</span>: &#123;</span><br><span class="line">        <span class="string">"cpu"</span>: <span class="string">"41797865n"</span>,</span><br><span class="line">        <span class="string">"memory"</span>: <span class="string">"837420Ki"</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"metadata"</span>: &#123;</span><br><span class="line">        <span class="string">"name"</span>: <span class="string">"172.21.16.87"</span>,</span><br><span class="line">        <span class="string">"selfLink"</span>: <span class="string">"/apis/metrics.k8s.io/v1beta1/nodes/172.21.16.87"</span>,</span><br><span class="line">        <span class="string">"creationTimestamp"</span>: <span class="string">"2019-09-04T07:00:44Z"</span></span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="string">"timestamp"</span>: <span class="string">"2019-09-04T07:00:34Z"</span>,</span><br><span class="line">      <span class="string">"window"</span>: <span class="string">"30s"</span>,</span><br><span class="line">      <span class="string">"usage"</span>: &#123;</span><br><span class="line">        <span class="string">"cpu"</span>: <span class="string">"37347688n"</span>,</span><br><span class="line">        <span class="string">"memory"</span>: <span class="string">"851232Ki"</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>/apis/metrics.k8s.io/v1beta1/nodes 和 /apis/metrics.k8s.io/v1beta1/pods 返回的 usage 包含 CPU 和 Memory；</li></ul><h4 id="使用-kubectl-top"><a href="#使用-kubectl-top" class="headerlink" title="使用 kubectl top"></a>使用 kubectl top</h4><p>使用 kubectl top 命令查看集群节点资源使用情况,kubectl top 命令从 metrics-server 获取集群节点基本的指标信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl top node</span></span><br><span class="line">NAME            CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   </span><br><span class="line">172.21.16.204   69m          1%     1008Mi          13%       </span><br><span class="line">172.21.16.240   41m          2%     817Mi           23%       </span><br><span class="line">172.21.16.87    39m          1%     831Mi           23%</span><br></pre></td></tr></table></figure><p>metrics到这里就已经成功的部署，参数没有一一介绍，后期有时间在列出来</p><p>这里还有很多参考的文档没有一一列出来，主要是浏览器被关闭啦，感谢那些参考的文档，😊😊😊</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;metrics-server 通过 kube-apiserver 发现所有节点，然后调用 kubelet APIs（通过 https 接口）获得各节点（Node）和 Pod 的 CPU、Memory 等资源使用情况。Kubernetes 1.12 开始，kubernetes 的安装脚本移除了 Heapster，从 1.13 开始完全移除了对 Heapster 的支持，Heapster 不再被维护。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;替代方案如下:&lt;ul&gt;
&lt;li&gt;用于支持自动扩缩容的 CPU/memory HPA metrics：metrics-server&lt;/li&gt;
&lt;li&gt;通用的监控方案：使用第三方可以获取 Prometheus 格式监控指标的监控系统，如 Prometheus Operator&lt;/li&gt;
&lt;li&gt;事件传输：使用第三方工具来传输、归档 kubernetes events&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;使用 metrics-server 替代 Heapster，将无法在 dashboard 中以图形展示 Pod 的内存和 CPU 情况，需要通过 Prometheus、Grafana 等监控方案来弥补。&lt;/p&gt;
    
    </summary>
    
      <category term="kubernetes" scheme="https://xxlaila.github.io/categories/kubernetes/"/>
    
    
      <category term="metrics-server" scheme="https://xxlaila.github.io/tags/metrics-server/"/>
    
  </entry>
  
  <entry>
    <title>kubelet提供api请求接口</title>
    <link href="https://xxlaila.github.io/2019/09/04/kubelet%E6%8F%90%E4%BE%9Bapi%E8%AF%B7%E6%B1%82%E6%8E%A5%E5%8F%A3/"/>
    <id>https://xxlaila.github.io/2019/09/04/kubelet提供api请求接口/</id>
    <published>2019-09-04T02:04:30.000Z</published>
    <updated>2019-09-20T05:47:25.978Z</updated>
    
    <content type="html"><![CDATA[<h3 id="kubelet-提供的-API-接口认证"><a href="#kubelet-提供的-API-接口认证" class="headerlink" title="kubelet 提供的 API 接口认证"></a>kubelet 提供的 API 接口认证</h3><p><a href="https://xxlaila.github.io/2019/08/10/kubernetes-node%E8%8A%82%E7%82%B9%E5%AE%89%E8%A3%85/">node安装参考</a></p><p>kubelet 启动后监听多个端口，用于接收 kube-apiserver 或其它客户端发送的请求：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-3 ~]<span class="comment">#  netstat -lnpt|grep kubelet</span></span><br><span class="line">tcp        0      0 127.0.0.1:46395         0.0.0.0:*               LISTEN      8941/kubelet        </span><br><span class="line">tcp        0      0 127.0.0.1:10248         0.0.0.0:*               LISTEN      8941/kubelet        </span><br><span class="line">tcp6       0      0 :::10250                :::*                    LISTEN      8941/kubelet</span><br></pre></td></tr></table></figure><ul><li><strong>10248</strong>: healthz http 服务</li><li><strong>10250</strong>: https 服务，访问该端口时需要认证和授权（即使访问 /healthz 也需要）</li><li>未开启只读端口 10255</li><li>从 K8S v1.10 开始，去除了 –cadvisor-port 参数（默认 4194 端口），不支持访问 cAdvisor UI &amp; API</li></ul><a id="more"></a><p>kubelet 接收 10250 端口的 https 请求，可以访问如下资源：</p><ul><li>/pods、/runningpods</li><li>/metrics、/metrics/cadvisor、/metrics/probes</li><li>/spec</li><li>/stats、/stats/container</li><li>/logs</li><li>/run/、/exec/, /attach/, /portForward/, /containerLogs/<br><a href="https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/server/server.go#L434:3" target="_blank" rel="noopener">详情参考</a></li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;由于关闭了匿名认证，同时开启了 webhook 授权，所有访问 10250 端口 https API 的请求都需要被认证和授权。<br>&nbsp;&nbsp;&nbsp;&nbsp;预定义的 ClusterRole system:kubelet-api-admin 授予访问 kubelet 所有 API 的权限(kube-apiserver 使用的 kubernetes 证书 User 授予了该权限)：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl describe clusterrole system:kubelet-api-admin</span></span><br><span class="line">Name:         system:kubelet-api-admin</span><br><span class="line">Labels:       kubernetes.io/bootstrapping=rbac-defaults</span><br><span class="line">Annotations:  rbac.authorization.kubernetes.io/autoupdate: <span class="literal">true</span></span><br><span class="line">PolicyRule:</span><br><span class="line">  Resources      Non-Resource URLs  Resource Names  Verbs</span><br><span class="line">  ---------      -----------------  --------------  -----</span><br><span class="line">  nodes/<span class="built_in">log</span>      []                 []              [*]</span><br><span class="line">  nodes/metrics  []                 []              [*]</span><br><span class="line">  nodes/proxy    []                 []              [*]</span><br><span class="line">  nodes/spec     []                 []              [*]</span><br><span class="line">  nodes/stats    []                 []              [*]</span><br><span class="line">  nodes          []                 []              [get list watch proxy]</span><br></pre></td></tr></table></figure><h3 id="kubelet-api-认证和授权"><a href="#kubelet-api-认证和授权" class="headerlink" title="kubelet api 认证和授权"></a>kubelet api 认证和授权</h3><p>kubelet 配置了如下认证参数:</p><ul><li><strong>–anonymous-auth=false</strong>: 设置为 false，不允许匿名�访问 10250 端口</li><li><strong>–authentication-token-webhook=true</strong>: 指定签名客户端证书的 CA 证书，开启 HTTPs 证书认证</li><li><strong>–client-ca-file=/etc/kubernetes/ssl/kubernetes-ca.pem</strong>: 开启 HTTPs bearer token 认证</li><li><strong>–authorization-mode=Webhook</strong>: 开启 RBAC 授权</li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;kubelet 收到请求后，使用 clientCAFile 对证书签名进行认证，或者查询 bearer token 是否有效。如果两者都没通过，则拒绝请求，提示 Unauthorized</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master-01-2 ~]<span class="comment"># curl -s --cacert /etc/kubernetes/ssl/kubernetes-ca.pem https://172.21.16.204:10250/metrics</span></span><br><span class="line">Unauthorized</span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;通过认证后，kubelet 使用 SubjectAccessReview API 向 kube-apiserver 发送请求，查询证书或 token 对应的 user、group 是否有操作资源的权限(RBAC)；</p><h3 id="证书认证和授权"><a href="#证书认证和授权" class="headerlink" title="证书认证和授权"></a>证书认证和授权</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 权限不足</span></span><br><span class="line"><span class="comment"># curl -s --cacert /etc/kubernetes/ssl/kubernetes-ca.pem  --cert /etc/kubernetes/ssl/kube-controller-manager.pem --key /etc/kubernetes/ssl/kube-controller-manager-key.pem https://172.21.16.204:10250/metrics</span></span><br><span class="line">Forbidden (user=system:kube-controller-manager, verb=get, resource=nodes, subresource=metrics)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用部署 kubectl 命令行工具时创建的、具有最高权限的 admin 证书</span></span><br><span class="line"><span class="comment"># curl -s --cacert /etc/kubernetes/ssl/kubernetes-ca.pem --cert /etc/kubernetes/ssl/admin.pem --key /etc/kubernetes/ssl/admin-key.pem https://172.21.16.204:10250/metrics|head</span></span><br><span class="line"><span class="comment"># HELP apiserver_audit_event_total Counter of audit events generated and sent to the audit backend.</span></span><br><span class="line"><span class="comment"># TYPE apiserver_audit_event_total counter</span></span><br><span class="line">apiserver_audit_event_total 0</span><br><span class="line"><span class="comment"># HELP apiserver_audit_requests_rejected_total Counter of apiserver requests rejected due to an error in audit logging backend.</span></span><br><span class="line"><span class="comment"># TYPE apiserver_audit_requests_rejected_total counter</span></span><br><span class="line">apiserver_audit_requests_rejected_total 0</span><br><span class="line"><span class="comment"># HELP apiserver_client_certificate_expiration_seconds Distribution of the remaining lifetime on the certificate used to authenticate a request.</span></span><br><span class="line"><span class="comment"># TYPE apiserver_client_certificate_expiration_seconds histogram</span></span><br><span class="line">apiserver_client_certificate_expiration_seconds_bucket&#123;le=<span class="string">"0"</span>&#125; 0</span><br><span class="line">apiserver_client_certificate_expiration_seconds_bucket&#123;le=<span class="string">"21600"</span>&#125; 0</span><br></pre></td></tr></table></figure><ul><li><strong>注意</strong>: –cacert、–cert、–key 的参数值必须是文件路径，否则返回 401 Unauthorized；</li></ul><h4 id="bear-token-认证和授权"><a href="#bear-token-认证和授权" class="headerlink" title="bear token 认证和授权"></a>bear token 认证和授权</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;创建一个 ServiceAccount，将它和 ClusterRole system:kubelet-api-admin 绑定，从而具有调用 kubelet API 的权限</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">horization.k8s.io/kubelet-api-test created</span><br><span class="line"><span class="comment"># SECRET=$(kubectl get secrets | grep kubelet-api-test | awk '&#123;print $1&#125;')</span></span><br><span class="line"><span class="comment"># TOKEN=$(kubectl describe secret $&#123;SECRET&#125; | grep -E '^token' | awk '&#123;print $2&#125;')</span></span><br><span class="line"><span class="comment"># echo $&#123;TOKEN&#125;</span></span><br><span class="line">eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6Imt1YmVsZXQtYXBpLXRlc3QtdG9rZW4tNGJra3MiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoia3ViZWxldC1hcGktdGVzdCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6Ijk3MDRhZDEwLWNlYjQtMTFlOS04ZDIwLWZhMTYzZTVhZjgzMyIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0Omt1YmVsZXQtYXBpLXRlc3QifQ.ishvOaC5tppYKDNpEOXIiVhVtgjyqzjySZjzndot5Z5U9MkY9LN8ZSMWRe6lNsB1UuTgEWTsHlG3OIRfExnHehYhWIt59V9e39KKbeY17hHoT-RZSaD6GoB449t_vUdIJedd1FGZ8DckQvDr6X5fMuD7MSU3vRL077j-uls-y4IW5kaJHeAGJfc6eWoCnv96DCbI8mQ8yuYbwLFpfIPLb4u6FPkwMQL2KXy6FhWPY1va6zAh4LdjGWhH6IAkKleq0aqfMwvmlnk1_OUmnmBoGJGuB96IwqBATP0jFzrd-Sv6af3RsSYz2r8YzJUj3kat9bd__HNCCXampYYr8ffu8YEdn-J9p6HK13FWU4O9QSIDrRONNIOpUXclJ-ov3z6N1hiIcVq5UJU6xR2z4ccvPXmH9Sj7p8CquqKEuobZxK97TFtECGlb2Ex43u4t0UHRo23UCQA-qP2Zs4-U2Zmf_qu3I-Lm7jzuYzXFCAb27yZx_XOUY-ycnKhtM6PpUfVKhkcHfWBOYY-QtBEbYf6yHRqCWcjrsZ63C_B56qAYaU5ca3hAcr6RBuHmmHISGESlLbmrpGgJ_ajd5mrJSh3Z_qdqu-Xt0Ya0NLfXgAcGi5n8xWJLztRTeyHrFTj7g82MqERUfFk9bdLHcz77xmrNLnhRZ87GW9sTZLw8QRUjF1g</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># curl -s --cacert /etc/kubernetes/ssl/kubernetes-ca.pem -H "Authorization: Bearer $&#123;TOKEN&#125;" https://172.21.16.204:10250/metrics|head</span></span><br><span class="line"><span class="comment"># HELP apiserver_audit_event_total Counter of audit events generated and sent to the audit backend.</span></span><br><span class="line"><span class="comment"># TYPE apiserver_audit_event_total counter</span></span><br><span class="line">apiserver_audit_event_total 0</span><br><span class="line"><span class="comment"># HELP apiserver_audit_requests_rejected_total Counter of apiserver requests rejected due to an error in audit logging backend.</span></span><br><span class="line"><span class="comment"># TYPE apiserver_audit_requests_rejected_total counter</span></span><br><span class="line">apiserver_audit_requests_rejected_total 0</span><br><span class="line"><span class="comment"># HELP apiserver_client_certificate_expiration_seconds Distribution of the remaining lifetime on the certificate used to authenticate a request.</span></span><br><span class="line"><span class="comment"># TYPE apiserver_client_certificate_expiration_seconds histogram</span></span><br><span class="line">apiserver_client_certificate_expiration_seconds_bucket&#123;le=<span class="string">"0"</span>&#125; 0</span><br><span class="line">apiserver_client_certificate_expiration_seconds_bucket&#123;le=<span class="string">"21600"</span>&#125; 0</span><br></pre></td></tr></table></figure><h4 id="cadvisor-和-metrics"><a href="#cadvisor-和-metrics" class="headerlink" title="cadvisor 和 metrics"></a>cadvisor 和 metrics</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;cadvisor 是内嵌在 kubelet 二进制中的，统计所在节点各容器的资源(CPU、内存、磁盘、网卡)使用情况的服务.</p><blockquote><p>在访问api-server安全端口之前，我们需要做一些操作才能访问，否则无法进行访问</p></blockquote><h5 id="在浏览器访问kube-apiserver安全端口"><a href="#在浏览器访问kube-apiserver安全端口" class="headerlink" title="在浏览器访问kube-apiserver安全端口"></a>在浏览器访问kube-apiserver安全端口</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;提示证书不被信任,这是因为 kube-apiserver 的 server 证书是我们创建的根证书 ca.pem 签名的，需要将根证书 ca.pem 导入操作系统，并设置永久信任。<br><img src="http://zxc.kingxunlian.com/1567564092242.jpg" alt="img"></p><p>&nbsp;&nbsp;&nbsp;&nbsp;给浏览器生成一个 client 证书，访问 apiserver 的 6443 https 端口时使用。这里使用部署 kubectl 命令行工具时创建的 admin 证书、私钥和上面的 ca 证书，创建一个浏览器可以使用 PKCS#12/PFX 格式的证书：</p><ul><li><p>会提示输入密码，这里密码需要记住，一会倒入证书到浏览器的时候需要</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># openssl pkcs12 -export -out admin.pfx -inkey admin-key.pem -in admin.pem -certfile kubernetes-ca.pem</span></span><br><span class="line">Enter Export Password:</span><br><span class="line">Verifying - Enter Export Password:</span><br></pre></td></tr></table></figure></li><li><p>将创建的 admin.pfx 导入到系统的证书中,<br><img src="http://zxc.kingxunlian.com/1567564289155.jpg" alt="img"></p></li></ul><p>再次访问 apiserver 地址，提示选择一个浏览器证书，这里选中上面导入的 admin.pfx<br><img src="http://zxc.kingxunlian.com/1567564393274.jpg" alt="img"></p><ul><li><p>提示需要输入系统的密码,这里是mac的电脑<br><img src="http://zxc.kingxunlian.com/1567564441293.jpg" alt="img"></p></li><li><p>被授权访问 kube-apiserver 的安全端口<br><img src="http://zxc.kingxunlian.com/1567564526664.jpg" alt="img"></p></li></ul><h5 id="客户端选择证书的原理"><a href="#客户端选择证书的原理" class="headerlink" title="客户端选择证书的原理"></a>客户端选择证书的原理</h5><ul><li>证书选择是在客户端和服务端 SSL/TLS 握手协商阶段商定的；</li><li>服务端如果要求客户端提供证书，则在握手时会向客户端发送一个它接受的 CA 列表；</li><li>客户端查找它的证书列表(一般是操作系统的证书，对于 Mac 为 keychain)，看有没有被 CA 签名的证书，如果有，则将它们提供给用户选择（证书的私钥）；</li><li>用户选择一个证书私钥，然后客户端将使用它和服务端通信；</li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;在浏览器访问 <a href="https://172.21.16.204:10250/metrics" target="_blank" rel="noopener">https://172.21.16.204:10250/metrics</a> 和 <a href="https://172.21.16.204:10250/metrics/cadvisor" target="_blank" rel="noopener">https://172.21.16.204:10250/metrics/cadvisor</a> 分别返回 kubelet 和 cadvisor 的 metrics。<br><img src="http://zxc.kingxunlian.com/1567563858215.jpg" alt="img"><br><img src="http://zxc.kingxunlian.com/1567564733531.jpg" alt="img"></p><ul><li><strong>原因</strong>: kubelet配置文件设置<code>--anonymous-auth=false</code>不允许匿名证书访问 10250 的 https 服务,所以我们才需要配置证书来进行访问</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;kubelet-提供的-API-接口认证&quot;&gt;&lt;a href=&quot;#kubelet-提供的-API-接口认证&quot; class=&quot;headerlink&quot; title=&quot;kubelet 提供的 API 接口认证&quot;&gt;&lt;/a&gt;kubelet 提供的 API 接口认证&lt;/h3&gt;&lt;p&gt;&lt;a href=&quot;https://xxlaila.github.io/2019/08/10/kubernetes-node%E8%8A%82%E7%82%B9%E5%AE%89%E8%A3%85/&quot;&gt;node安装参考&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;kubelet 启动后监听多个端口，用于接收 kube-apiserver 或其它客户端发送的请求：&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;[root@k8s-3 ~]&lt;span class=&quot;comment&quot;&gt;#  netstat -lnpt|grep kubelet&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;tcp        0      0 127.0.0.1:46395         0.0.0.0:*               LISTEN      8941/kubelet        &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;tcp        0      0 127.0.0.1:10248         0.0.0.0:*               LISTEN      8941/kubelet        &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;tcp6       0      0 :::10250                :::*                    LISTEN      8941/kubelet&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;10248&lt;/strong&gt;: healthz http 服务&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;10250&lt;/strong&gt;: https 服务，访问该端口时需要认证和授权（即使访问 /healthz 也需要）&lt;/li&gt;
&lt;li&gt;未开启只读端口 10255&lt;/li&gt;
&lt;li&gt;从 K8S v1.10 开始，去除了 –cadvisor-port 参数（默认 4194 端口），不支持访问 cAdvisor UI &amp;amp; API&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="kubernetes" scheme="https://xxlaila.github.io/categories/kubernetes/"/>
    
    
      <category term="kubelet" scheme="https://xxlaila.github.io/tags/kubelet/"/>
    
  </entry>
  
  <entry>
    <title>centos-nfs-512错误</title>
    <link href="https://xxlaila.github.io/2019/09/03/centos-nfs-512%E9%94%99%E8%AF%AF/"/>
    <id>https://xxlaila.github.io/2019/09/03/centos-nfs-512错误/</id>
    <published>2019-09-03T03:29:13.000Z</published>
    <updated>2019-09-20T05:47:25.819Z</updated>
    
    <content type="html"><![CDATA[<p>nfs 错误kernel: NFS: nfs4_discover_server_trunking unhandled error -512. Exiting with error EIO</p><p>&nbsp;&nbsp;&nbsp;&nbsp;很久没挂载过nfs，忘记客户端怎么挂在nfs的了，服务端很早就安装好了，今天一台客户机需要挂载nfs，然后居然报错了，然后找了一圈居然没找到怎么解决，然后又重新看了一次centos nfs的配置，于是乎就搞定了</p><p>在挂载nfs的提示很慢，长时间无响应，强行结束看看是什么问题，查看日志</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo tail -f /var/<span class="built_in">log</span>/messages</span><br><span class="line">Sep  3 11:23:51 dev-application kernel: NFS: nfs4_discover_server_trunking unhandled error -512. Exiting with error EIO</span><br></pre></td></tr></table></figure><a id="more"></a><h3 id="1、在客户端查看rpcbind-service"><a href="#1、在客户端查看rpcbind-service" class="headerlink" title="1、在客户端查看rpcbind.service"></a>1、在客户端查看rpcbind.service</h3><p>在客户端查看<code>rpcbind.service</code>是否正常启动，没有正常启动，就需要启动，没有安装服务，就需要安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum -y install rpcbind.service</span><br><span class="line">$ sudo systemctl start rpcbind.service &amp;&amp; sudo systemctl <span class="built_in">enable</span> rpcbind.service</span><br></pre></td></tr></table></figure><ul><li>再次挂载的时候还是挂载补上，因为第一次挂载的时候客户端向服务器端发送了数据请求，而且还没有正常的断开链接，可以在服务端用<code>netstat -anp|grep tcp</code>查看有没有客户端的链接信息,状态是<code>ESTABLISHED</code>，有的话，我们需要把它结束掉</li></ul><h3 id="2、linux-结束tcp会话"><a href="#2、linux-结束tcp会话" class="headerlink" title="2、linux 结束tcp会话"></a>2、linux 结束tcp会话</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 目前不知道什么命令,后期更新</span></span><br><span class="line"><span class="comment"># 重启nfs服务</span></span><br><span class="line">$ sudo restart nfs.service</span><br></pre></td></tr></table></figure><h3 id="3、客户端挂载"><a href="#3、客户端挂载" class="headerlink" title="3、客户端挂载"></a>3、客户端挂载</h3><p>再次来到客户机挂载，可以成功的挂载</p><h3 id="4、rpcbind服务介绍"><a href="#4、rpcbind服务介绍" class="headerlink" title="4、rpcbind服务介绍"></a>4、rpcbind服务介绍</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;共享和加载NFS文件系统需要服务，红帽企业Linux使用核心级的支持和守护进程的组合来提供NFS文件共享.NFS依靠远程过程调用(RPC)在客户端和服务器端路由请求。在Linux下RPC服务由portmap服务控制。</p><h4 id="4-1、为了共享和加载NFS文件系统，下面的服务要一起工作"><a href="#4-1、为了共享和加载NFS文件系统，下面的服务要一起工作" class="headerlink" title="4.1、为了共享和加载NFS文件系统，下面的服务要一起工作:"></a>4.1、为了共享和加载NFS文件系统，下面的服务要一起工作:</h4><ul><li>nfs - 启动相应RPC服务进程来服务对于NFS文件系统的请求.</li><li>nfslock - 一个可选的服务，用于启动相应的RPC进程，允许NFS客户端在服务器上对文件加锁.</li><li>portmap - Linux的RPC服务,它响应RPC服务的请求和与请求的RPC服务建立连接.</li></ul><h4 id="4-2、RPC进程在后台一起工作服务于NFS服务"><a href="#4-2、RPC进程在后台一起工作服务于NFS服务" class="headerlink" title="4.2、RPC进程在后台一起工作服务于NFS服务"></a>4.2、RPC进程在后台一起工作服务于NFS服务</h4><ul><li>rpc.mountd - 这个进程接受来自NFS客户端的加载请求和验证请求的文件系统正在被输出.这个进程由NFS服务自动启动，不需要用户的配置.</li><li>rpc.nfsd - 这个进程是NFS服务器.它和Linux核心一起工作来满足NFS客户端的动态需求，例如提供为每个NFS客户端的每次请求服务器线程.这个进程对应于nfs服务.</li><li>rpc.lockd - 一个可选的进程，它允许NFS客户端在服务器上对文件加锁.这个进程对应于nfslock服务.</li><li>rpc.statd - 这个进程实现了网络状态监控(NSM)RPC协议,通知NFS客户端什么时候一个NFS服务器非正常重启动.这个进程被nfslock服务自动启动，不需要用户的配置.</li><li>rpc.rquotad - 这个进程对于远程用户提供用户配额信息. 这个进程被nfs服务自动启动，不需要用户的配置.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;nfs 错误kernel: NFS: nfs4_discover_server_trunking unhandled error -512. Exiting with error EIO&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;很久没挂载过nfs，忘记客户端怎么挂在nfs的了，服务端很早就安装好了，今天一台客户机需要挂载nfs，然后居然报错了，然后找了一圈居然没找到怎么解决，然后又重新看了一次centos nfs的配置，于是乎就搞定了&lt;/p&gt;
&lt;p&gt;在挂载nfs的提示很慢，长时间无响应，强行结束看看是什么问题，查看日志&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ sudo tail -f /var/&lt;span class=&quot;built_in&quot;&gt;log&lt;/span&gt;/messages&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Sep  3 11:23:51 dev-application kernel: NFS: nfs4_discover_server_trunking unhandled error -512. Exiting with error EIO&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="centos" scheme="https://xxlaila.github.io/categories/centos/"/>
    
    
      <category term="nfs" scheme="https://xxlaila.github.io/tags/nfs/"/>
    
  </entry>
  
  <entry>
    <title>k8s集群部署heapster</title>
    <link href="https://xxlaila.github.io/2019/09/02/k8s%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2heapster/"/>
    <id>https://xxlaila.github.io/2019/09/02/k8s集群部署heapster/</id>
    <published>2019-09-02T11:25:03.000Z</published>
    <updated>2019-09-20T05:47:25.959Z</updated>
    
    <content type="html"><![CDATA[<p>kubernetes集群启用tls认证部署heapster，在部署期间遇到了很多的坑，走过很多雷，这里记录一下,不过在新版本中heapster被metrics-server代替了，metrics-server后篇介绍和使用</p><h2 id="1、准备工作"><a href="#1、准备工作" class="headerlink" title="1、准备工作"></a>1、准备工作</h2><p>下载需要的文件，这里用之前k8s-heapster部署的文件拿来进行修改，<a href="https://github.com/xxlaila/kubernetes-yaml/" target="_blank" rel="noopener">文件地址</a></p><h3 id="2、执行文件创建"><a href="#2、执行文件创建" class="headerlink" title="2、执行文件创建"></a>2、执行文件创建</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cd kubernetes-yaml/heapster-influxdb-grafana</span></span><br><span class="line"><span class="comment"># kubectl apply -f ./</span></span><br></pre></td></tr></table></figure><a id="more"></a><ul><li><strong>错误提示</strong>: 这里在执行创建后，没有图像显示，查看pods日志发现错误<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get pods -n kube-system|grep he</span></span><br><span class="line">heapster-7b7b4754d-5p7tb                1/1     Running   0          17m</span><br><span class="line"><span class="comment"># kubectl logs heapster-7b7b4754d-5p7tb -n kube-system</span></span><br><span class="line">E0902 10:57:11.804543       1 reflector.go:190] k8s.io/heapster/metrics/processors/namespace_based_enricher.go:89: Failed to list *v1.Namespace: namespaces is forbidden: User <span class="string">"system:serviceaccount:kube-system:heapster"</span> cannot list resource <span class="string">"namespaces"</span> <span class="keyword">in</span> API group <span class="string">""</span> at the cluster scope: RBAC: clusterrole.rbac.authorization.k8s.io <span class="string">"system:heapster"</span> not found</span><br><span class="line">E0902 10:57:12.071117       1 reflector.go:190] k8s.io/heapster/metrics/util/util.go:30: Failed to list *v1.Node: nodes is forbidden: User <span class="string">"system:serviceaccount:kube-system:heapster"</span> cannot list resource <span class="string">"nodes"</span> <span class="keyword">in</span> API group <span class="string">""</span> at the cluster scope: RBAC: clusterrole.rbac.authorization.k8s.io <span class="string">"system:heapster"</span> not found</span><br></pre></td></tr></table></figure></li></ul><ul><li>排错<br>查看ClusterRole: system:heapster的权限,发现并没有</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl describe clusterrole system:heapster</span></span><br><span class="line">Error from server (NotFound): clusterroles.rbac.authorization.k8s.io <span class="string">"system:heapster"</span> not found</span><br></pre></td></tr></table></figure><p>提示这个错误，应该是我之前部署过，然后修改过权限，不小心给删掉啦，这里需要吧权限重建一次就好了，参考文献(<a href="https://www.cnblogs.com/vincenshen/p/9638162.html" target="_blank" rel="noopener">https://www.cnblogs.com/vincenshen/p/9638162.html</a>)</p><ul><li>新建heapster-clusterrole.yaml <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat heapster-clusterrole.yaml </span></span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRole</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    rbac.authorization.kubernetes.io/autoupdate: <span class="string">"true"</span></span><br><span class="line">  labels:</span><br><span class="line">    kubernetes.io/bootstrapping: rbac-defaults</span><br><span class="line">  name: system:heapster</span><br><span class="line">rules:</span><br><span class="line">- apiGroups:</span><br><span class="line">  - <span class="string">""</span></span><br><span class="line">  resources:</span><br><span class="line">  - events</span><br><span class="line">  - namespaces</span><br><span class="line">  - nodes</span><br><span class="line">  - pods</span><br><span class="line">  - nodes/stats</span><br><span class="line">  verbs:</span><br><span class="line">  - create</span><br><span class="line">  - get</span><br><span class="line">  - list</span><br><span class="line">  - watch</span><br><span class="line">- apiGroups:</span><br><span class="line">  - extensions</span><br><span class="line">  resources:</span><br><span class="line">  - deployments</span><br><span class="line">  verbs:</span><br><span class="line">  - get</span><br><span class="line">  - list</span><br><span class="line">  - watch</span><br></pre></td></tr></table></figure></li></ul><p>并执行 kubectl apply -f heapster-clusterrole.yaml,在次查看日志，发现之前的错误没有了，但是又出现了一个新的错误<br><code>E0902 11:27:05.025300       1 manager.go:101] Error in scraping containers from kubelet:172.21.16.204:10250: failed to get all container stats from Kubelet URL &quot;https://172.21.16.204:10250/stats/container/&quot;: request failed - &quot;401 Unauthorized&quot;, response: &quot;Unauthorized&quot;</code></p><ul><li><p>修改heapster-clusterrole.yaml文件，在文件里面我们添加几个权限</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat heapster-clusterrole.yaml </span></span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRole</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    rbac.authorization.kubernetes.io/autoupdate: <span class="string">"true"</span></span><br><span class="line">  labels:</span><br><span class="line">    kubernetes.io/bootstrapping: rbac-defaults</span><br><span class="line">  name: system:heapster</span><br><span class="line">rules:</span><br><span class="line">- apiGroups:</span><br><span class="line">  - <span class="string">""</span></span><br><span class="line">  resources:</span><br><span class="line">  - events</span><br><span class="line">  - namespaces</span><br><span class="line">  - nodes</span><br><span class="line">  - pods</span><br><span class="line">  - nodes/stats</span><br><span class="line">  verbs:</span><br><span class="line">  - create</span><br><span class="line">  - get</span><br><span class="line">  - list</span><br><span class="line">  - watch</span><br><span class="line">- apiGroups:</span><br><span class="line">  - extensions</span><br><span class="line">  resources:</span><br><span class="line">  - deployments</span><br><span class="line">  verbs:</span><br><span class="line">  - get</span><br><span class="line">  - list</span><br><span class="line">  - update</span><br><span class="line">  - watch</span><br><span class="line">- apiGroups:</span><br><span class="line">  - <span class="string">""</span></span><br><span class="line">  resources:</span><br><span class="line">  - nodes/stats</span><br><span class="line">  verbs:</span><br><span class="line">  - get</span><br></pre></td></tr></table></figure></li><li><p>执行创建</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl apply -f heapster-clusterrole.yaml</span></span><br><span class="line"><span class="comment"># 重新创建heapster</span></span><br><span class="line"><span class="comment"># kubectl delete -f heapster.yaml</span></span><br><span class="line"><span class="comment"># kubectl create -f heapster.yaml</span></span><br></pre></td></tr></table></figure></li></ul><p>完成以后我们继续看heapster pod的日志，发现日志里面还是出现<code>401 Unauthorized&quot;, response: &quot;Unauthorized&quot;</code>，我们需要修改node节点 kubelet 启动的配置参数，添加<code>--authentication-token-webhook</code>参数: 使用tokenreview API来进行令牌认证。Kubelet 在配置的 API server 上调用 TokenReview API 以确定来自 bearer token 的用户信息。<a href="https://k8smeetup.github.io/docs/admin/kubelet-authentication-authorization/" target="_blank" rel="noopener">官方参考</a>，<a href="https://github.com/kubernetes-retired/heapster/issues/1936" target="_blank" rel="noopener">github上错误解决</a>， <a href="https://jimmysong.io/posts/user-authentication-in-kubernetes/" target="_blank" rel="noopener">参数文章学习</a></p><p>到此为止: 错误没有啦，但是界面数据没出来，稍等片刻</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;kubernetes集群启用tls认证部署heapster，在部署期间遇到了很多的坑，走过很多雷，这里记录一下,不过在新版本中heapster被metrics-server代替了，metrics-server后篇介绍和使用&lt;/p&gt;
&lt;h2 id=&quot;1、准备工作&quot;&gt;&lt;a href=&quot;#1、准备工作&quot; class=&quot;headerlink&quot; title=&quot;1、准备工作&quot;&gt;&lt;/a&gt;1、准备工作&lt;/h2&gt;&lt;p&gt;下载需要的文件，这里用之前k8s-heapster部署的文件拿来进行修改，&lt;a href=&quot;https://github.com/xxlaila/kubernetes-yaml/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;文件地址&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;2、执行文件创建&quot;&gt;&lt;a href=&quot;#2、执行文件创建&quot; class=&quot;headerlink&quot; title=&quot;2、执行文件创建&quot;&gt;&lt;/a&gt;2、执行文件创建&lt;/h3&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# cd kubernetes-yaml/heapster-influxdb-grafana&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# kubectl apply -f ./&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="kubernetes" scheme="https://xxlaila.github.io/categories/kubernetes/"/>
    
    
      <category term="heapster" scheme="https://xxlaila.github.io/tags/heapster/"/>
    
  </entry>
  
  <entry>
    <title>k8s部署istio</title>
    <link href="https://xxlaila.github.io/2019/08/30/k8s%E9%83%A8%E7%BD%B2istio/"/>
    <id>https://xxlaila.github.io/2019/08/30/k8s部署istio/</id>
    <published>2019-08-30T03:21:08.000Z</published>
    <updated>2019-09-20T05:47:25.928Z</updated>
    
    <content type="html"><![CDATA[<h2 id="istio-介绍"><a href="#istio-介绍" class="headerlink" title="istio 介绍"></a>istio 介绍</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;istio代表的是Service Mesh的方案实现，Istio 有助于降低这些部署的复杂性，并减轻开发团队的压力。提供一种简单的方式来为已部署的服务建立网络，且提供具有负载均衡、服务间认证、监控、流量管理等功能。</p><h2 id="服务网格（Service-Mesh）"><a href="#服务网格（Service-Mesh）" class="headerlink" title="服务网格（Service Mesh）"></a>服务网格（Service Mesh）</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;服务网格（Service Mesh）用于描述构成这些应用程序的微服务网络以及应用之间的交互。随着规模和复杂性的增长，服务网格越来越难以理解和管理。它的需求包括服务发现、负载均衡、故障恢复、指标收集和监控以及通常更加复杂的运维需求，例如 A/B 测试、金丝雀发布、限流、访问控制和端到端认证等。而istio刚好提供了一套完整的解决方案，通过控制整个服务器网格提供行为洞察和操作控制来满足微服务应用的多样化</p><h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><p>Istio 服务网格逻辑上分为数据平面和控制平面。</p><a id="more"></a><ul><li>数据平面由一组以 sidecar 方式部署的智能代理（Envoy）组成。这些代理可以调节和控制微服务及 Mixer 之间所有的网络通信。</li><li>控制平面负责管理和配置代理来路由流量。此外控制平面配置 Mixer 以实施策略和收集遥测数据。<br>下图显示了构成每个面板的不同组件:<br><img src="http://zxc.kingxunlian.com/1567136153850.jpg" alt="img"></li></ul><h3 id="istio-组件"><a href="#istio-组件" class="headerlink" title="istio 组件"></a>istio 组件</h3><ul><li>Envoy: Istio 使用 Envoy 代理的扩展版本,用于调解服务网格中所有服务的所有入站和出站流量,属于数据层面</li><li>Mixer: 是一个独立于平台的组件，负责在服务网格上执行访问控制和使用策略，并从 Envoy 代理和其他服务收集遥测数据</li><li>Pilot: 为 Envoy sidecar 提供服务发现功能，为智能路由（例如 A/B 测试、金丝雀部署等）和弹性（超时、重试、熔断器等）提供流量管理功能</li><li>Citadel: 通过内置身份和凭证管理赋能强大的服务间和最终用户身份验证。可用于升级服务网格中未加密的流量，并为运维人员提供基于服务标识而不是网络控制的强制执行策略的能力</li><li>Galley: 代表其他的 Istio 控制平面组件，用来验证用户编写的 Istio API 配置。随着时间的推移，Galley 将接管 Istio 获取配置、 处理和分配组件的顶级责任</li></ul><p>详细信息查看<a href="https://istio.io/zh/docs/concepts/what-is-istio/" target="_blank" rel="noopener">官方</a>,官方支持中文，对于英语差的人来说，再好不过啦。</p><h2 id="1、下载istio包"><a href="#1、下载istio包" class="headerlink" title="1、下载istio包"></a>1、下载istio包</h2><p>执行下载和自动解压缩</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># curl -L https://git.io/getLatestIstio | ISTIO_VERSION=1.2.5 sh -</span></span><br><span class="line"><span class="comment"># cd istio-1.1.14/bin</span></span><br><span class="line"><span class="comment"># cp istioctl /usr/bin/</span></span><br><span class="line"><span class="comment"># 吧包分发给其他的node节点和master节点</span></span><br></pre></td></tr></table></figure><p>安装目录中包含：</p><ul><li><code>在 install/</code>:  目录中包含了 Kubernetes 安装所需的 .yaml 文件</li><li><code>samples/</code>:  目录中是示例应用</li><li><code>istioctl</code>:  客户端文件保存在 bin/ 目录之中。istioctl 的功能是手工进行 Envoy Sidecar 的注入。</li><li><code>istio.VERSION</code>:  配置文件</li></ul><h2 id="2、在kubernetes-集群中开始安装"><a href="#2、在kubernetes-集群中开始安装" class="headerlink" title="2、在kubernetes 集群中开始安装"></a>2、在kubernetes 集群中开始安装</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Istio 会被安装到自己的 istio-system 命名空间，并且能够对所有其他命名空间的服务进行管理。这里使用的宽容模式的<code>mutual TLS</code>来进行安装。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Istion这里采用helm进行安装，<a href="https://xxlaila.github.io/2019/09/04/k8s-helm/">helm安装参考</a></p><h3 id="2-1、使用-kubectl-apply-安装-Istio-的自定义资源定义（CRD"><a href="#2-1、使用-kubectl-apply-安装-Istio-的自定义资源定义（CRD" class="headerlink" title="2.1、使用 kubectl apply 安装 Istio 的自定义资源定义（CRD)"></a>2.1、使用 kubectl apply 安装 Istio 的自定义资源定义（CRD)</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># helm install install/kubernetes/helm/istio-init --name istio-init --namespace istio-system</span></span><br><span class="line"></span><br><span class="line">NAME:   istio-init</span><br><span class="line">LAST DEPLOYED: Wed Sep 18 14:01:23 2019</span><br><span class="line">NAMESPACE: istio-system</span><br><span class="line">STATUS: DEPLOYED</span><br><span class="line"></span><br><span class="line">RESOURCES:</span><br><span class="line">==&gt; v1/ClusterRole</span><br><span class="line">NAME                     AGE</span><br><span class="line">istio-init-istio-system  2s</span><br><span class="line"></span><br><span class="line">==&gt; v1/ClusterRoleBinding</span><br><span class="line">NAME                                        AGE</span><br><span class="line">istio-init-admin-role-binding-istio-system  2s</span><br><span class="line"></span><br><span class="line">==&gt; v1/ConfigMap</span><br><span class="line">NAME          DATA  AGE</span><br><span class="line">istio-crd-10  1     2s</span><br><span class="line">istio-crd-11  1     2s</span><br><span class="line">istio-crd-12  1     2s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Job</span><br><span class="line">NAME               COMPLETIONS  DURATION  AGE</span><br><span class="line">istio-init-crd-10  0/1          1s        2s</span><br><span class="line">istio-init-crd-11  0/1          1s        1s</span><br><span class="line">istio-init-crd-12  0/1          1s        1s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Pod(related)</span><br><span class="line">NAME                     READY  STATUS             RESTARTS  AGE</span><br><span class="line">istio-init-crd-10-mtw7q  0/1    ContainerCreating  0         1s</span><br><span class="line">istio-init-crd-11-hd56j  0/1    ContainerCreating  0         1s</span><br><span class="line">istio-init-crd-12-sflj5  0/1    ContainerCreating  0         1s</span><br><span class="line"></span><br><span class="line">==&gt; v1/ServiceAccount</span><br><span class="line">NAME                        SECRETS  AGE</span><br><span class="line">istio-init-service-account  1        2s</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl get pods -n istio-system</span></span><br><span class="line">NAME                      READY   STATUS      RESTARTS   AGE</span><br><span class="line">istio-init-crd-10-mtw7q   0/1     Completed   0          3m23s</span><br><span class="line">istio-init-crd-11-hd56j   0/1     Completed   0          3m23s</span><br><span class="line">istio-init-crd-12-sflj5   0/1     Completed   0          3m23s</span><br></pre></td></tr></table></figure><ul><li>查看安装的CRD<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get CustomResourceDefinition</span></span><br><span class="line">NAME                                   CREATED AT</span><br><span class="line">adapters.config.istio.io               2019-09-18T06:02:00Z</span><br><span class="line">attributemanifests.config.istio.io     2019-09-18T06:01:54Z</span><br><span class="line">authorizationpolicies.rbac.istio.io    2019-09-18T06:01:47Z</span><br><span class="line">clusterrbacconfigs.rbac.istio.io       2019-09-18T06:01:50Z</span><br><span class="line">destinationrules.networking.istio.io   2019-09-18T06:01:48Z</span><br><span class="line">envoyfilters.networking.istio.io       2019-09-18T06:01:49Z</span><br><span class="line">gateways.networking.istio.io           2019-09-18T06:01:49Z</span><br><span class="line">handlers.config.istio.io               2019-09-18T06:02:01Z</span><br><span class="line">httpapispecbindings.config.istio.io    2019-09-18T06:01:51Z</span><br><span class="line">httpapispecs.config.istio.io           2019-09-18T06:01:51Z</span><br><span class="line">instances.config.istio.io              2019-09-18T06:02:01Z</span><br><span class="line">meshpolicies.authentication.istio.io   2019-09-18T06:01:50Z</span><br><span class="line">policies.authentication.istio.io       2019-09-18T06:01:50Z</span><br><span class="line">quotaspecbindings.config.istio.io      2019-09-18T06:01:52Z</span><br><span class="line">quotaspecs.config.istio.io             2019-09-18T06:01:53Z</span><br><span class="line">rbacconfigs.rbac.istio.io              2019-09-18T06:01:56Z</span><br><span class="line">rules.config.istio.io                  2019-09-18T06:01:54Z</span><br><span class="line">serviceentries.networking.istio.io     2019-09-18T06:01:49Z</span><br><span class="line">servicerolebindings.rbac.istio.io      2019-09-18T06:01:59Z</span><br><span class="line">serviceroles.rbac.istio.io             2019-09-18T06:01:57Z</span><br><span class="line">sidecars.networking.istio.io           2019-09-18T06:01:35Z</span><br><span class="line">templates.config.istio.io              2019-09-18T06:02:01Z</span><br><span class="line">virtualservices.networking.istio.io    2019-09-18T06:01:47Z</span><br></pre></td></tr></table></figure></li></ul><h3 id="2-3、可以安装Istio"><a href="#2-3、可以安装Istio" class="headerlink" title="2.3、可以安装Istio"></a>2.3、可以安装Istio</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">helm install install/kubernetes/helm/istio \</span><br><span class="line">    --name istio \</span><br><span class="line">    --namespace istio-system \</span><br><span class="line">    --<span class="built_in">set</span> global.mtls.enabled=<span class="literal">true</span> \</span><br><span class="line">    --<span class="built_in">set</span> grafana.enabled=<span class="literal">true</span> \</span><br><span class="line">    --<span class="built_in">set</span> servicegraph.enabled=<span class="literal">true</span> \</span><br><span class="line">    --<span class="built_in">set</span> tracing.enabled=<span class="literal">true</span></span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;此命令会挂起几分钟，但它实际上是在后台安装所有内容。安装完成后，您将看到显示已安装的所有组件的输出。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这将创建istio-system名称空间以及所需的RBAC权限，并部署Istio-Pilot，Istio-Mixer，Istio-Ingress，Istio-Egress和Istio-CA（证书颁发机构）。</p><h3 id="2-4、确认各个组件的Pod正常运行"><a href="#2-4、确认各个组件的Pod正常运行" class="headerlink" title="2.4、确认各个组件的Pod正常运行"></a>2.4、确认各个组件的Pod正常运行</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get pod -n istio-system</span></span><br><span class="line">NAME                                      READY   STATUS      RESTARTS   AGE</span><br><span class="line">grafana-6fb9f8c5c7-6r2g7                  1/1     Running     0          11m</span><br><span class="line">istio-citadel-5cf47dbf7c-n2zsm            1/1     Running     0          11m</span><br><span class="line">istio-galley-7898b587db-5d72t             1/1     Running     0          11m</span><br><span class="line">istio-ingressgateway-7c6f8fd795-fwkpr     1/1     Running     0          11m</span><br><span class="line">istio-init-crd-10-mtw7q                   0/1     Completed   0          15m</span><br><span class="line">istio-init-crd-11-hd56j                   0/1     Completed   0          15m</span><br><span class="line">istio-init-crd-12-sflj5                   0/1     Completed   0          15m</span><br><span class="line">istio-pilot-5c4b6f576b-8dvzd              2/2     Running     1          11m</span><br><span class="line">istio-policy-769664fcf7-xt5zx             2/2     Running     6          11m</span><br><span class="line">istio-sidecar-injector-677bd5ccc5-5dxgv   1/1     Running     0          11m</span><br><span class="line">istio-telemetry-577c6f5b8c-nhv7d          2/2     Running     6          11m</span><br><span class="line">istio-tracing-5d8f57c8ff-5xcc7            1/1     Running     0          11m</span><br><span class="line">prometheus-776fdf7479-lbhg7               1/1     Running     0          11m</span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Istio 以一个项目的形式部署到 Kubernetes 集群中。我们可以看到，部署好的 pods 中，除了有 istio-citadel、istio-egressgateway、istio-ingressgateway、istio-pilot 等 Istio 本身的功能组件，还集成了微服务相关的监控工具，如：grafana、jaeger-agent、kiali、prometheus。正是这些功能丰富且强大的监控工具，帮助 Istio实现了微服务的可视化管理。</p><h3 id="2-5、查看svc服务"><a href="#2-5、查看svc服务" class="headerlink" title="2.5、查看svc服务"></a>2.5、查看svc服务</h3><p>首先，确保部署以下Kubernetes服务：istio-pilot，istio-mixer，istio-ingress和istio-egress。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get svc -n istio-system</span></span><br><span class="line">NAME                     TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)                                                                                                                                      AGE</span><br><span class="line">grafana                  ClusterIP      10.254.71.43     &lt;none&gt;        3000/TCP                                                                                                                                     12m</span><br><span class="line">istio-citadel            ClusterIP      10.254.49.243    &lt;none&gt;        8060/TCP,15014/TCP                                                                                                                           12m</span><br><span class="line">istio-galley             ClusterIP      10.254.104.163   &lt;none&gt;        443/TCP,15014/TCP,9901/TCP                                                                                                                   12m</span><br><span class="line">istio-ingressgateway     LoadBalancer   10.254.8.233     &lt;pending&gt;     15020:30781/TCP,80:31380/TCP,443:31390/TCP,31400:31400/TCP,15029:31398/TCP,15030:31020/TCP,15031:32709/TCP,15032:32354/TCP,15443:32498/TCP   12m</span><br><span class="line">istio-pilot              ClusterIP      10.254.175.118   &lt;none&gt;        15010/TCP,15011/TCP,8080/TCP,15014/TCP                                                                                                       12m</span><br><span class="line">istio-policy             ClusterIP      10.254.132.244   &lt;none&gt;        9091/TCP,15004/TCP,15014/TCP                                                                                                                 12m</span><br><span class="line">istio-sidecar-injector   ClusterIP      10.254.246.239   &lt;none&gt;        443/TCP                                                                                                                                      12m</span><br><span class="line">istio-telemetry          ClusterIP      10.254.88.168    &lt;none&gt;        9091/TCP,15004/TCP,15014/TCP,42422/TCP                                                                                                       12m</span><br><span class="line">jaeger-agent             ClusterIP      None             &lt;none&gt;        5775/UDP,6831/UDP,6832/UDP                                                                                                                   12m</span><br><span class="line">jaeger-collector         ClusterIP      10.254.188.179   &lt;none&gt;        14267/TCP,14268/TCP                                                                                                                          12m</span><br><span class="line">jaeger-query             ClusterIP      10.254.145.99    &lt;none&gt;        16686/TCP                                                                                                                                    12m</span><br><span class="line">prometheus               ClusterIP      10.254.197.218   &lt;none&gt;        9090/TCP                                                                                                                                     12m</span><br><span class="line">tracing                  ClusterIP      10.254.254.143   &lt;none&gt;        80/TCP                                                                                                                                       12m</span><br><span class="line">zipkin                   ClusterIP      10.254.39.195    &lt;none&gt;        9411/TCP                                                                                                                                     12m</span><br></pre></td></tr></table></figure><h2 id="3、运行示例Bookinfo"><a href="#3、运行示例Bookinfo" class="headerlink" title="3、运行示例Bookinfo"></a>3、运行示例Bookinfo</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;部署自己的应用或者示例应用程序如 Bookinfo。 注意：应用程序必须使用 HTTP/1.1 或 HTTP/2.0 协议来传递 HTTP 流量，因为 HTTP/1.0 已经不再支持。</p><p>Bookinfo 应用由四个单独的微服务构成，用来演示多种 Istio 特性，包含:</p><ul><li>productpage ：productpage 微服务会调用 details 和 reviews 两个微服务，用来生成页面。</li><li>details ：这个微服务包含了书籍的信息。</li><li>reviews ：这个微服务包含了书籍相关的评论。它还会调用 ratings 微服务。</li><li>ratings ：ratings 微服务中包含了由书籍评价组成的评级信息。</li></ul><p>reviews 微服务有 3 个版本：</p><ul><li>v1 版本不会调用 ratings 服务.</li><li>v2 版本会调用 ratings 服务，并使用 1 到 5 个黑色星形图标来显示评分信息</li><li>v3 版本会调用 ratings 服务，并使用 1 到 5 个红色星形图标来显示评分信息</li></ul><ul><li>下图展示了这个应用的端到端架构<br><img src alt="img"></li></ul><h3 id="3-1、运行示例bookinfo，并开启Sidecar自动注入。"><a href="#3-1、运行示例bookinfo，并开启Sidecar自动注入。" class="headerlink" title="3.1、运行示例bookinfo，并开启Sidecar自动注入。"></a>3.1、运行示例bookinfo，并开启Sidecar自动注入。</h3><h3 id="3-2、Istio安装使用自动边车注入。标记将托管应用程序的命名空间istio-injection-enabled"><a href="#3-2、Istio安装使用自动边车注入。标记将托管应用程序的命名空间istio-injection-enabled" class="headerlink" title="3.2、Istio安装使用自动边车注入。标记将托管应用程序的命名空间istio-injection=enabled"></a>3.2、Istio安装使用自动边车注入。标记将托管应用程序的命名空间istio-injection=enabled</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl label namespace default istio-injection=enabled</span></span><br></pre></td></tr></table></figure><ul><li><strong>注意:</strong> 此步骤先不执行，如果这这个执行了，在后面部署Bookinfo的时候会提示如下错误<code>Error creating: Internal error occurred: failed calling webhook &quot;sidecar-injector.istio.io&quot;: Post https://istio-sidecar-injector.istio-system.svc:443/inject?timeout=30s: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)</code>这一步有执行的可以执行以下命令进行删除</li></ul><ul><li>删除ns的label<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get ns --show-labels</span></span><br><span class="line">NAME              STATUS   AGE    LABELS</span><br><span class="line">default           Active   2d4h   istio-injection=enabled</span><br><span class="line">istio-system      Active   174m   &lt;none&gt;</span><br><span class="line">kube-node-lease   Active   2d4h   &lt;none&gt;</span><br><span class="line">kube-public       Active   2d4h   &lt;none&gt;</span><br><span class="line">kube-system       Active   2d4h   &lt;none&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl label namespace default istio-injection-</span></span><br><span class="line">namespace/default labeled</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl get ns --show-labels</span></span><br><span class="line">NAME              STATUS   AGE    LABELS</span><br><span class="line">default           Active   2d4h   &lt;none&gt;</span><br><span class="line">istio-system      Active   175m   &lt;none&gt;</span><br><span class="line">kube-node-lease   Active   2d4h   &lt;none&gt;</span><br><span class="line">kube-public       Active   2d4h   &lt;none&gt;</span><br><span class="line">kube-system       Active   2d4h   &lt;none&gt;</span><br></pre></td></tr></table></figure></li></ul><h3 id="3-3、部署Bookinfo"><a href="#3-3、部署Bookinfo" class="headerlink" title="3.3、部署Bookinfo"></a>3.3、部署Bookinfo</h3><p>使用kubectl create及其常规YAML部署文件直接部署我们的应用程序。我们将使用istioctl将Envoy容器注入应用程序窗格</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f &lt;(istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml)</span><br></pre></td></tr></table></figure><ul><li><p>确认已正确部署应用程序</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get services</span></span><br><span class="line">NAME          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE</span><br><span class="line">details       ClusterIP   10.254.185.191   &lt;none&gt;        9080/TCP   2m5s</span><br><span class="line">kubernetes    ClusterIP   10.254.0.1       &lt;none&gt;        443/TCP    2d4h</span><br><span class="line">productpage   ClusterIP   10.254.25.102    &lt;none&gt;        9080/TCP   2m2s</span><br><span class="line">ratings       ClusterIP   10.254.17.86     &lt;none&gt;        9080/TCP   2m5s</span><br><span class="line">reviews       ClusterIP   10.254.64.177    &lt;none&gt;        9080/TCP   2m3s</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl get pods</span></span><br><span class="line">NAME                             READY   STATUS    RESTARTS   AGE</span><br><span class="line">details-v1-6bd9dd69c8-xhbrw      2/2     Running   0          22m</span><br><span class="line">productpage-v1-6845658c8-jpvv6   2/2     Running   0          22m</span><br><span class="line">ratings-v1-5b877bbcd6-z8fkg      2/2     Running   0          11m</span><br><span class="line">reviews-v1-6b6c975f74-c9r8w      2/2     Running   0          22m</span><br><span class="line">reviews-v2-6fd65cd7c7-r8mss      2/2     Running   0          22m</span><br><span class="line">reviews-v3-7d45cbbb48-9j5gn      2/2     Running   0          22m</span><br></pre></td></tr></table></figure></li><li><p>确认Bookinfo应用程序正在运行，请通过curl某个pod中的命令向其发送请求</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl exec -it $(kubectl get pod -l app=ratings -o jsonpath='&#123;.items[0].metadata.name&#125;') -c ratings -- curl productpage:9080/productpage | grep -o "&lt;title&gt;.*&lt;/title&gt;"</span></span><br><span class="line">&lt;title&gt;Simple Bookstore App&lt;/title&gt;</span><br></pre></td></tr></table></figure></li></ul><p>Envoy注入每个服务，架构将如下所示：<br><img src alt="img"></p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bookinfo服务已启动并运行,从Kubernetes集群外部访问应用程序，例如，从浏览器访问。Istio Gateway就是用于此目的。</p><h3 id="3-4、应用程序定义入口网关"><a href="#3-4、应用程序定义入口网关" class="headerlink" title="3.4、应用程序定义入口网关"></a>3.4、应用程序定义入口网关</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><a href="https://jruels.github.io/fun-istio/labs/07-istio1/" target="_blank" rel="noopener">安装部署参考</a><br><a href="https://bbs.huaweicloud.com/blogs/112543" target="_blank" rel="noopener">排错参考</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;istio-介绍&quot;&gt;&lt;a href=&quot;#istio-介绍&quot; class=&quot;headerlink&quot; title=&quot;istio 介绍&quot;&gt;&lt;/a&gt;istio 介绍&lt;/h2&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;istio代表的是Service Mesh的方案实现，Istio 有助于降低这些部署的复杂性，并减轻开发团队的压力。提供一种简单的方式来为已部署的服务建立网络，且提供具有负载均衡、服务间认证、监控、流量管理等功能。&lt;/p&gt;
&lt;h2 id=&quot;服务网格（Service-Mesh）&quot;&gt;&lt;a href=&quot;#服务网格（Service-Mesh）&quot; class=&quot;headerlink&quot; title=&quot;服务网格（Service Mesh）&quot;&gt;&lt;/a&gt;服务网格（Service Mesh）&lt;/h2&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;服务网格（Service Mesh）用于描述构成这些应用程序的微服务网络以及应用之间的交互。随着规模和复杂性的增长，服务网格越来越难以理解和管理。它的需求包括服务发现、负载均衡、故障恢复、指标收集和监控以及通常更加复杂的运维需求，例如 A/B 测试、金丝雀发布、限流、访问控制和端到端认证等。而istio刚好提供了一套完整的解决方案，通过控制整个服务器网格提供行为洞察和操作控制来满足微服务应用的多样化&lt;/p&gt;
&lt;h3 id=&quot;架构&quot;&gt;&lt;a href=&quot;#架构&quot; class=&quot;headerlink&quot; title=&quot;架构&quot;&gt;&lt;/a&gt;架构&lt;/h3&gt;&lt;p&gt;Istio 服务网格逻辑上分为数据平面和控制平面。&lt;/p&gt;
    
    </summary>
    
      <category term="istio" scheme="https://xxlaila.github.io/categories/istio/"/>
    
    
      <category term="kubernetes" scheme="https://xxlaila.github.io/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>k8s配置Dashboard</title>
    <link href="https://xxlaila.github.io/2019/08/29/k8s%E9%85%8D%E7%BD%AEDashboard/"/>
    <id>https://xxlaila.github.io/2019/08/29/k8s配置Dashboard/</id>
    <published>2019-08-29T09:57:46.000Z</published>
    <updated>2019-09-20T05:47:25.900Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;K8S Dashboard是官方的一个基于WEB的用户界面，专门用来管理K8S集群，并可展示集群的状态。K8S集群安装好后默认没有包含Dashboard，我们需要额外创建它。</p><h3 id="1、安装dashboard"><a href="#1、安装dashboard" class="headerlink" title="1、安装dashboard"></a>1、安装dashboard</h3><h4 id="1-1、下载准备需要的文件"><a href="#1-1、下载准备需要的文件" class="headerlink" title="1.1、下载准备需要的文件"></a>1.1、下载准备需要的文件</h4><p>经过修改过后的文件，已经可以正常使用的<a href="https://github.com/xxlaila/kubernetes-yaml/" target="_blank" rel="noopener">文件</a></p><a id="more"></a><ul><li>创建dashboard<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master-01 dashboard]<span class="comment"># kubectl create -f kubernetes-dashboard.yaml </span></span><br><span class="line">secret/kubernetes-dashboard-certs created</span><br><span class="line">serviceaccount/kubernetes-dashboard created</span><br><span class="line">role.rbac.authorization.k8s.io/kubernetes-dashboard-minimal created</span><br><span class="line">rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard-minimal created</span><br><span class="line">deployment.apps/kubernetes-dashboard created</span><br><span class="line">service/kubernetes-dashboard created</span><br></pre></td></tr></table></figure></li></ul><h4 id="1-2、查看服务状态和pod"><a href="#1-2、查看服务状态和pod" class="headerlink" title="1.2、查看服务状态和pod"></a>1.2、查看服务状态和pod</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master-01 ~]<span class="comment"># kubectl get service --all-namespaces</span></span><br><span class="line">NAMESPACE     NAME                   TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)         AGE</span><br><span class="line">default       kubernetes             ClusterIP   10.254.0.1      &lt;none&gt;        443/TCP         18h</span><br><span class="line">kube-system   coredns                ClusterIP   10.254.0.10     &lt;none&gt;        53/UDP,53/TCP   16h</span><br><span class="line">kube-system   kubernetes-dashboard   NodePort    10.254.51.226   &lt;none&gt;        443:30001/TCP   15h</span><br></pre></td></tr></table></figure><ul><li><p>查看service描述</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master-01 ~]<span class="comment"># kubectl describe  service kubernetes-dashboard -n kube-system</span></span><br><span class="line">Name:                     kubernetes-dashboard</span><br><span class="line">Namespace:                kube-system</span><br><span class="line">Labels:                   k8s-app=kubernetes-dashboard</span><br><span class="line">Annotations:              &lt;none&gt;</span><br><span class="line">Selector:                 k8s-app=kubernetes-dashboard</span><br><span class="line">Type:                     NodePort</span><br><span class="line">IP:                       10.254.51.226</span><br><span class="line">Port:                     &lt;<span class="built_in">unset</span>&gt;  443/TCP</span><br><span class="line">TargetPort:               8443/TCP</span><br><span class="line">NodePort:                 &lt;<span class="built_in">unset</span>&gt;  30001/TCP</span><br><span class="line">Endpoints:                10.254.39.3:8443</span><br><span class="line">Session Affinity:         None</span><br><span class="line">External Traffic Policy:  Cluster</span><br><span class="line">Events:                   &lt;none&gt;</span><br></pre></td></tr></table></figure></li><li><p>查看pod描述</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master-01 ~]<span class="comment"># kubectl describe pod kubernetes-dashboard-6c655d9445-6zntr --namespace=kube-system</span></span><br><span class="line">Name:           kubernetes-dashboard-6c655d9445-6zntr</span><br><span class="line">Namespace:      kube-system</span><br><span class="line">Node:           172.21.17.31/172.21.17.31</span><br><span class="line">Start Time:     Thu, 29 Aug 2019 17:47:20 +0800</span><br><span class="line">Labels:         k8s-app=kubernetes-dashboard</span><br><span class="line">                pod-template-hash=6c655d9445</span><br><span class="line">Annotations:    &lt;none&gt;</span><br><span class="line">Status:         Running</span><br><span class="line">IP:             10.254.39.3</span><br></pre></td></tr></table></figure></li></ul><h3 id="2、授权Dashboard账户集群管理权限"><a href="#2、授权Dashboard账户集群管理权限" class="headerlink" title="2、授权Dashboard账户集群管理权限"></a>2、授权Dashboard账户集群管理权限</h3><p>若果不进行授权操作，打开dashboard会报错，如下图<br><img src="http://zxc.kingxunlian.com/WechatIMG28864.png" alt="img"></p><ul><li><p>新建kubrnetes-dashboard-admin-rbac.yaml</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat kubernetes-dashboard-admin-rbac.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: admin-user</span><br><span class="line">  namespace: kube-system</span><br><span class="line">---</span><br><span class="line"><span class="comment"># Create ClusterRoleBinding</span></span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: admin-user</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: cluster-admin</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: admin-user</span><br><span class="line">  namespace: kube-system</span><br></pre></td></tr></table></figure></li><li><p>执行创建</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl create -f kubernetes-dashboard-admin-rbac.yaml</span></span><br></pre></td></tr></table></figure></li></ul><p>找到kubernete-dashboard-admin的token，复制token在dashboard页面进行登录，</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master-01 dashboard]<span class="comment"># kubectl -n kube-system get secret | grep admin-user</span></span><br><span class="line">admin-user-token-qv49g             kubernetes.io/service-account-token   3      15h</span><br><span class="line"></span><br><span class="line">[root@k8s-master-01 dashboard]<span class="comment"># kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '&#123;print $1&#125;')</span></span><br><span class="line">Name:         admin-user-token-qv49g</span><br><span class="line">Namespace:    kube-system</span><br><span class="line">Labels:       &lt;none&gt;</span><br><span class="line">Annotations:  kubernetes.io/service-account.name: admin-user</span><br><span class="line">              kubernetes.io/service-account.uid: ea3f0e3f-ca42-11e9-8716-fa163effd55b</span><br><span class="line"></span><br><span class="line">Type:  kubernetes.io/service-account-token</span><br><span class="line"></span><br><span class="line">Data</span><br><span class="line">====</span><br><span class="line">ca.crt:     1359 bytes</span><br><span class="line">namespace:  11 bytes</span><br><span class="line">token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLXF2NDlnIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJlYTNmMGUzZi1jYTQyLTExZTktODcxNi1mYTE2M2VmZmQ1NWIiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.AbdsJdgi9d0rCYrmvoJkWf32HKSMT03OyOX55aRhPptjzIjDcGxxQYecT0w58N7Z_2L2RwTBfOrm4B3wTEDfFZKgYsnGJQOzJMtZDN9w5YJg2xGQ27E3KisTbbQzd_I5DgxSZWW75GwWf756_bIQpWuXNRO_KjheyWuNNv0tSEYRiXpcboSQpb-8R-Km-vP85mxke6s5cJFSk0WLMjFWow1vOF1ns23NZ5nslEmYOMZF3_Fxybh3LbiCyrpD4c0FtfRcXaBIBqACeyCPRriYMIIJq3OJjI-DzuqUedu1x2xH2prB4mNjxlKt2-7q0M1zCuvm5JhW_LzWgveu9ni2ig</span><br></pre></td></tr></table></figure><h3 id="3、配置文件修改说明"><a href="#3、配置文件修改说明" class="headerlink" title="3、配置文件修改说明"></a>3、配置文件修改说明</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;dashboard 文件被修改，默认的token失效的时间是900秒，15分钟，每15分钟就要进行一次认证，这样对于运维人员来说就不是特别的方便，我们可以通过修改token-ttl参数来设置，主要是修改dashborad的yaml文件，并重新建立即可</p><h4 id="3-1、在配置文件修改-添加"><a href="#3-1、在配置文件修改-添加" class="headerlink" title="3.1、在配置文件修改/添加"></a>3.1、在配置文件修改/添加</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ports:</span><br><span class="line">- containerPort: 8443</span><br><span class="line">  protocol: TCP</span><br><span class="line">args:</span><br><span class="line">  - --auto-generate-certificates</span><br><span class="line">  - --token-ttl=43200</span><br></pre></td></tr></table></figure><ul><li>重建pod<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master-01 dashboard]<span class="comment"># kubectl apply -f kubernetes-dashboard.yaml</span></span><br></pre></td></tr></table></figure></li></ul><p>我们可以输入任意节点的ip加30001端口就可以访问dashboard, https://{ip}:30001。</p><h3 id="其他操作"><a href="#其他操作" class="headerlink" title="其他操作"></a>其他操作</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;每天我们来公司要登录dashboard的时候都要去输入一次token，每次去获取token的时候都要输入很长的一串，这里为了方便，可以写一个脚本，要token的时候执行一下脚本，就可以。</p><ul><li><p>创建脚本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># vim kube-token</span><br><span class="line">#!/bin/bash</span><br><span class="line">kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk &apos;&#123;print $1&#125;&apos;)</span><br></pre></td></tr></table></figure></li><li><p>设置脚本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># chmod +x kube-token</span></span><br><span class="line"><span class="comment"># mv kube-token /usr/bin</span></span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;K8S Dashboard是官方的一个基于WEB的用户界面，专门用来管理K8S集群，并可展示集群的状态。K8S集群安装好后默认没有包含Dashboard，我们需要额外创建它。&lt;/p&gt;
&lt;h3 id=&quot;1、安装dashboard&quot;&gt;&lt;a href=&quot;#1、安装dashboard&quot; class=&quot;headerlink&quot; title=&quot;1、安装dashboard&quot;&gt;&lt;/a&gt;1、安装dashboard&lt;/h3&gt;&lt;h4 id=&quot;1-1、下载准备需要的文件&quot;&gt;&lt;a href=&quot;#1-1、下载准备需要的文件&quot; class=&quot;headerlink&quot; title=&quot;1.1、下载准备需要的文件&quot;&gt;&lt;/a&gt;1.1、下载准备需要的文件&lt;/h4&gt;&lt;p&gt;经过修改过后的文件，已经可以正常使用的&lt;a href=&quot;https://github.com/xxlaila/kubernetes-yaml/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;文件&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="kubernetes" scheme="https://xxlaila.github.io/categories/kubernetes/"/>
    
    
      <category term="kubernetes" scheme="https://xxlaila.github.io/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>k8s删除node重新加入</title>
    <link href="https://xxlaila.github.io/2019/08/29/k8s%E5%88%A0%E9%99%A4node%E9%87%8D%E6%96%B0%E5%8A%A0%E5%85%A5/"/>
    <id>https://xxlaila.github.io/2019/08/29/k8s删除node重新加入/</id>
    <published>2019-08-29T08:27:00.000Z</published>
    <updated>2019-09-20T05:47:25.940Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;有时候k8s node 在加入集群的时候不经意的时候弄错啦某些东西，这时候可以把这个node删除，然后重新加入，删除节点之前我们需要做一下常规化的操作，来保障运行在该节点的pod迁移到其他的node上。</p><a id="more"></a><ul><li><p>1、先驱赶上面的pod</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master-01-2 kubernetes]<span class="comment"># kubectl drain 172.21.110 --delete-local-data</span></span><br><span class="line">node/172.21.110 cordoned</span><br><span class="line">node/172.21.110 drained</span><br></pre></td></tr></table></figure></li><li><p>2、删除节点</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master-01-2 kubernetes]<span class="comment"># kubectl delete node 172.21.110</span></span><br><span class="line">node <span class="string">"172.21.110"</span> deleted</span><br></pre></td></tr></table></figure></li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;<code>kubectl delete</code> 命令本身是通用的，可以进行任何资源的删除<code>kubectl delete type typename</code>，type是资源类型，可以是<code>node, pod, rs, rc, deployment, service</code>等等，typename是这个资源的名称</p><ul><li><p>3、查看node是否被删除</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master-01-2 kubernetes]<span class="comment"># kubectl get nodes</span></span><br><span class="line">NAME           STATUS   ROLES    AGE   VERSION</span><br><span class="line">172.21.17.30   Ready    &lt;none&gt;   20m   v1.13.3</span><br><span class="line">172.21.17.31   Ready    &lt;none&gt;   10m   v1.13.3</span><br></pre></td></tr></table></figure></li><li><p>4、彻底删除node<br>进入该节点。删除<code>kubelet.kubeconfig</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master-01-3 kubernetes]<span class="comment"># rm -rf  kubelet.kubeconfig</span></span><br></pre></td></tr></table></figure></li><li><p>4、node重新加入集群<br>&nbsp;&nbsp;&nbsp;&nbsp;当我们的node执行删除以后，重新启动kubelet服务以后。node又会自动的加入到集群里面来，怎么彻底的删除，让后重启kubelet的时候重新像集群里面发出csr请求，集群重新通过该节点的csr请求吧该节点加入到集群来，<code>kubelet.kubeconfig</code>也重新生成</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master-01-2 kubernetes]<span class="comment"># kubectl get csr</span></span><br><span class="line">NAME                                                   AGE     REQUESTOR           CONDITION</span><br><span class="line">node-csr-H1CAqJw4VZYY67-tk4Akuso_uuPPwpj3d5jK3xcL88M   8m      kubelet-bootstrap   Approved,Issued</span><br><span class="line">node-csr-YPvpbITaxGBrOxuCpGiY7jrGpPNSZ4sdbKhSkUEcdnc   7m54s   kubelet-bootstrap   Approved,Issued</span><br><span class="line">node-csr-odhUT58g0mdVuZdUeclj7doEpUmWzv1YzaiJYQaPeek   5s      kubelet-bootstrap   Pending</span><br><span class="line">node-csr-u4oi5e0Upt-ZmejSDEFm9Q0RU3wZf9bThU_o51nclgg   17m     kubelet-bootstrap   Approved,Issued</span><br></pre></td></tr></table></figure></li><li><p>5、重新通过csr</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master-01-2 kubernetes]<span class="comment"># kubectl certificate approve node-csr-odhUT58g0mdVuZdUeclj7doEpUmWzv1YzaiJYQaPeek </span></span><br><span class="line">certificatesigningrequest.certificates.k8s.io/node-csr-odhUT58g0mdVuZdUeclj7doEpUmWzv1YzaiJYQaPeek approved</span><br></pre></td></tr></table></figure></li><li><p>6、在集群查看节点</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master-01-2 kubernetes]<span class="comment"># kubectl get nodes</span></span><br><span class="line">NAME            STATUS   ROLES    AGE   VERSION</span><br><span class="line">172.21.16.110   Ready    &lt;none&gt;   36s   v1.13.3</span><br><span class="line">172.21.17.30    Ready    &lt;none&gt;   28m   v1.13.3</span><br><span class="line">172.21.17.31    Ready    &lt;none&gt;   17m   v1.13.3</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;有时候k8s node 在加入集群的时候不经意的时候弄错啦某些东西，这时候可以把这个node删除，然后重新加入，删除节点之前我们需要做一下常规化的操作，来保障运行在该节点的pod迁移到其他的node上。&lt;/p&gt;
    
    </summary>
    
      <category term="常用命令" scheme="https://xxlaila.github.io/categories/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
    
    
      <category term="kubernetes" scheme="https://xxlaila.github.io/tags/kubernetes/"/>
    
  </entry>
  
</feed>
