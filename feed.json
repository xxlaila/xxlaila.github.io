{"title":"懒羊羊","description":"星际拾荒者","language":"zh-CN","link":"https://xxlaila.github.io","pubDate":"Wed, 04 Sep 2019 05:57:07 GMT","lastBuildDate":"Wed, 04 Sep 2019 09:54:26 GMT","generator":"hexo-generator-json-feed","webMaster":"xxlaila","items":[{"title":"metrics-server安装季","link":"https://xxlaila.github.io/2019/09/04/metrics-server安装季/","description":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;metrics-server 通过 kube-apiserver 发现所有节点，然后调用 kubelet APIs（通过 https 接口）获得各节点（Node）和 Pod 的 CPU、Memory 等资源使用情况。Kubernetes 1.12 开始，kubernetes 的安装脚本移除了 Heapster，从 1.13 开始完全移除了对 Heapster 的支持，Heapster 不再被维护。 替代方案如下: 用于支持自动扩缩容的 CPU/memory HPA metrics：metrics-server 通用的监控方案：使用第三方可以获取 Prometheus 格式监控指标的监控系统，如 Prometheus Operator 事件传输：使用第三方工具来传输、归档 kubernetes events &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;使用 metrics-server 替代 Heapster，将无法在 dashboard 中以图形展示 Pod 的内存和 CPU 情况，需要通过 Prometheus、Grafana 等监控方案来弥补。","pubDate":"Wed, 04 Sep 2019 05:57:07 GMT","guid":"https://xxlaila.github.io/2019/09/04/metrics-server安装季/","category":"kubernetes"},{"title":"kubelet提供api请求借口","link":"https://xxlaila.github.io/2019/09/04/kubelet提供api请求借口/","description":"kubelet 提供的 API 接口认证node安装参考 kubelet 启动后监听多个端口，用于接收 kube-apiserver 或其它客户端发送的请求： 1234[root@k8s-3 ~]# netstat -lnpt|grep kubelettcp 0 0 127.0.0.1:46395 0.0.0.0:* LISTEN 8941/kubelet tcp 0 0 127.0.0.1:10248 0.0.0.0:* LISTEN 8941/kubelet tcp6 0 0 :::10250 :::* LISTEN 8941/kubelet 10248: healthz http 服务 10250: https 服务，访问该端口时需要认证和授权（即使访问 /healthz 也需要） 未开启只读端口 10255 从 K8S v1.10 开始，去除了 –cadvisor-port 参数（默认 4194 端口），不支持访问 cAdvisor UI &amp; API","pubDate":"Wed, 04 Sep 2019 02:04:30 GMT","guid":"https://xxlaila.github.io/2019/09/04/kubelet提供api请求借口/","category":"kubernetes"},{"title":"centos-nfs-512错误","link":"https://xxlaila.github.io/2019/09/03/centos-nfs-512错误/","description":"nfs 错误kernel: NFS: nfs4_discover_server_trunking unhandled error -512. Exiting with error EIO &nbsp;&nbsp;&nbsp;&nbsp;很久没挂载过nfs，忘记客户端怎么挂在nfs的了，服务端很早就安装好了，今天一台客户机需要挂载nfs，然后居然报错了，然后找了一圈居然没找到怎么解决，然后又重新看了一次centos nfs的配置，于是乎就搞定了 在挂载nfs的提示很慢，长时间无响应，强行结束看看是什么问题，查看日志 12$ sudo tail -f /var/log/messagesSep 3 11:23:51 dev-application kernel: NFS: nfs4_discover_server_trunking unhandled error -512. Exiting with error EIO","pubDate":"Tue, 03 Sep 2019 03:29:13 GMT","guid":"https://xxlaila.github.io/2019/09/03/centos-nfs-512错误/","category":"centos"},{"title":"k8s集群部署heapster","link":"https://xxlaila.github.io/2019/09/02/k8s集群部署heapster/","description":"kubernetes集群启用tls认证部署heapster，在部署期间遇到了很多的坑，走过很多雷，这里记录一下,不过在新版本中heapster被metrics-server代替了，metrics-server后篇介绍和使用 1、准备工作下载需要的文件，这里用之前k8s-heapster部署的文件拿来进行修改，文件地址 2、执行文件创建12# cd kubernetes-yaml/heapster-influxdb-grafana# kubectl apply -f ./ 错误提示: 这里在执行创建后，没有图像显示，查看pods日志发现错误12345# kubectl get pods -n kube-system|grep heheapster-7b7b4754d-5p7tb 1/1 Running 0 17m# kubectl logs heapster-7b7b4754d-5p7tb -n kube-systemE0902 10:57:11.804543 1 reflector.go:190] k8s.io/heapster/metrics/processors/namespace_based_enricher.go:89: Failed to list *v1.Namespace: namespaces is forbidden: User \"system:serviceaccount:kube-system:heapster\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope: RBAC: clusterrole.rbac.authorization.k8s.io \"system:heapster\" not foundE0902 10:57:12.071117 1 reflector.go:190] k8s.io/heapster/metrics/util/util.go:30: Failed to list *v1.Node: nodes is forbidden: User \"system:serviceaccount:kube-system:heapster\" cannot list resource \"nodes\" in API group \"\" at the cluster scope: RBAC: clusterrole.rbac.authorization.k8s.io \"system:heapster\" not found 排错查看ClusterRole: system:heapster的权限,发现并没有 12# kubectl describe clusterrole system:heapsterError from server (NotFound): clusterroles.rbac.authorization.k8s.io \"system:heapster\" not found 提示这个错误，应该是我之前部署过，然后修改过权限，不小心给删掉啦，这里需要吧权限重建一次就好了，参考文献(https://www.cnblogs.com/vincenshen/p/9638162.html) 新建heapster-clusterrole.yaml 12345678910111213141516171819202122232425262728293031# cat heapster-clusterrole.yaml apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" labels: kubernetes.io/bootstrapping: rbac-defaults name: system:heapsterrules:- apiGroups: - \"\" resources: - events - namespaces - nodes - pods - nodes/stats verbs: - create - get - list - watch- apiGroups: - extensions resources: - deployments verbs: - get - list - watch 并执行 kubectl apply -f heapster-clusterrole.yaml,在次查看日志，发现之前的错误没有了，但是又出现了一个新的错误E0902 11:27:05.025300 1 manager.go:101] Error in scraping containers from kubelet:172.21.16.204:10250: failed to get all container stats from Kubelet URL &quot;https://172.21.16.204:10250/stats/container/&quot;: request failed - &quot;401 Unauthorized&quot;, response: &quot;Unauthorized&quot; 修改heapster-clusterrole.yaml文件，在文件里面我们添加几个权限 1234567891011121314151617181920212223242526272829303132333435363738# cat heapster-clusterrole.yaml apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" labels: kubernetes.io/bootstrapping: rbac-defaults name: system:heapsterrules:- apiGroups: - \"\" resources: - events - namespaces - nodes - pods - nodes/stats verbs: - create - get - list - watch- apiGroups: - extensions resources: - deployments verbs: - get - list - update - watch- apiGroups: - \"\" resources: - nodes/stats verbs: - get 执行创建 1234# kubectl apply -f heapster-clusterrole.yaml# 重新创建heapster# kubectl delete -f heapster.yaml# kubectl create -f heapster.yaml 完成以后我们继续看heapster pod的日志，发现日志里面还是出现401 Unauthorized&quot;, response: &quot;Unauthorized&quot;，我们需要修改node节点 kubelet 启动的配置参数，添加--authentication-token-webhook参数: 使用tokenreview API来进行令牌认证。Kubelet 在配置的 API server 上调用 TokenReview API 以确定来自 bearer token 的用户信息。官方参考，github上错误解决， 参数文章学习 到此为止: 错误没有啦，但是界面数据没出来，稍等片刻","pubDate":"Mon, 02 Sep 2019 11:25:03 GMT","guid":"https://xxlaila.github.io/2019/09/02/k8s集群部署heapster/","category":"kubernetes"},{"title":"k8s部署istio","link":"https://xxlaila.github.io/2019/08/30/k8s部署istio/","description":"istio 介绍&nbsp;&nbsp;&nbsp;&nbsp;istio代表的是Service Mesh的方案实现，Istio 有助于降低这些部署的复杂性，并减轻开发团队的压力。提供一种简单的方式来为已部署的服务建立网络，且提供具有负载均衡、服务间认证、监控、流量管理等功能。 服务网格（Service Mesh）&nbsp;&nbsp;&nbsp;&nbsp;服务网格（Service Mesh）用于描述构成这些应用程序的微服务网络以及应用之间的交互。随着规模和复杂性的增长，服务网格越来越难以理解和管理。它的需求包括服务发现、负载均衡、故障恢复、指标收集和监控以及通常更加复杂的运维需求，例如 A/B 测试、金丝雀发布、限流、访问控制和端到端认证等。而istio刚好提供了一套完整的解决方案，通过控制整个服务器网格提供行为洞察和操作控制来满足微服务应用的多样化 架构Istio 服务网格逻辑上分为数据平面和控制平面。","pubDate":"Fri, 30 Aug 2019 03:21:08 GMT","guid":"https://xxlaila.github.io/2019/08/30/k8s部署istio/","category":"istio"},{"title":"k8s配置Dashboard","link":"https://xxlaila.github.io/2019/08/29/k8s配置Dashboard/","description":"&nbsp;&nbsp;&nbsp;&nbsp;K8S Dashboard是官方的一个基于WEB的用户界面，专门用来管理K8S集群，并可展示集群的状态。K8S集群安装好后默认没有包含Dashboard，我们需要额外创建它。 1、安装dashboard1.1、下载准备需要的文件经过修改过后的文件，已经可以正常使用的文件","pubDate":"Thu, 29 Aug 2019 09:57:46 GMT","guid":"https://xxlaila.github.io/2019/08/29/k8s配置Dashboard/","category":"kubernetes"},{"title":"k8s删除node重新加入","link":"https://xxlaila.github.io/2019/08/29/k8s删除node重新加入/","description":"&nbsp;&nbsp;&nbsp;&nbsp;有时候k8s node 在加入集群的时候不经意的时候弄错啦某些东西，这时候可以把这个node删除，然后重新加入，删除节点之前我们需要做一下常规化的操作，来保障运行在该节点的pod迁移到其他的node上。","pubDate":"Thu, 29 Aug 2019 08:27:00 GMT","guid":"https://xxlaila.github.io/2019/08/29/k8s删除node重新加入/","category":"常用命令"},{"title":"Centos route策略","link":"https://xxlaila.github.io/2019/08/28/Centos-route策略/","description":"场景:&nbsp;&nbsp;&nbsp;&nbsp;公司业务在一个新的云平台上线，该云平台使用的是比较传统的VMware vSphere来做的虚拟化，而且该云平台网络也是我们不清楚的，反正就是不能设置(不能像现在主流云平台自定义网络或者是地址段)，是一个政府的云平台，具体各种奇葩的限制就不说啦，你懂的……","pubDate":"Wed, 28 Aug 2019 02:25:46 GMT","guid":"https://xxlaila.github.io/2019/08/28/Centos-route策略/","category":"Centos"},{"title":"k8s部署ingress","link":"https://xxlaila.github.io/2019/08/26/k8s部署ingress/","description":"在kubernetes 集群中，一个服务安装以后怎么对外提供访问，外部用户怎么来访问我们容器中业务。 1、Ingress 介绍&nbsp;&nbsp;&nbsp;&nbsp;Kubernetes 暴露服务的方式目前只有三种：LoadBlancer Service、NodePort Service、Ingress；本文主要通过Ingress来访问 2、Ingress 是什么&nbsp;&nbsp;&nbsp;&nbsp;Ingress 就是能利用 Nginx、Haproxy 啥的负载均衡器暴露集群内服务的工具问题来了，集群内服务想要暴露出去面临着几个问题：","pubDate":"Mon, 26 Aug 2019 08:24:55 GMT","guid":"https://xxlaila.github.io/2019/08/26/k8s部署ingress/","category":"kubernetes, Ingress"},{"title":"k8s部署Weave Scope","link":"https://xxlaila.github.io/2019/08/26/k8s部署Weave-Scope/","description":"1、Weave Scope介绍&nbsp;&nbsp;&nbsp;&nbsp;weave scope 是Docker和Kubernetes的故障排除和监控，自动生成应用程序的地图，能够直观地了解，监控和控制基于容器的，基于微服务的应用程序。可以实时的了解docker容器，选择容器基础架构的概述，或关注特定的微服务。轻松识别和纠正问题，确保集装箱化应用的稳定性和性能，查看容器的上下文指标，标记和元数据。轻松地在容器内的进程之间导航，以运行容器，在可扩展的可排序表中进行排列。使用给定主机或服务的最大CPU或内存轻松找到容器。直接与您的容器交互：暂停，重新启动和停止容器。启动命令行。全部不离开范围浏览器窗口。 2、部署weave scope初次学习，直接下载官方配置文件，没有经过任何修改，不过相信自己随着学习的进步，会逐渐深入。github地址，官方地址","pubDate":"Mon, 26 Aug 2019 08:08:15 GMT","guid":"https://xxlaila.github.io/2019/08/26/k8s部署Weave-Scope/","category":"kubernetes,Weave Scope"},{"title":"k8s部署zookeeper集群","link":"https://xxlaila.github.io/2019/08/24/k8s部署zookeeper集群/","description":"zk属于有状态服务，需要连接外部存储，吧数据存放在数据盘里面，否则容器挂了，数据没有了 准备工作准备zk的yaml文件","pubDate":"Sat, 24 Aug 2019 07:00:12 GMT","guid":"https://xxlaila.github.io/2019/08/24/k8s部署zookeeper集群/","category":"kubernetes"},{"title":"k8s部署coredns","link":"https://xxlaila.github.io/2019/08/24/k8s部署coredns/","description":"&nbsp;&nbsp;&nbsp;&nbsp;k8s集群中的应用通常是通过ingress实现微服务发布的，前文介绍过在K8S集群中使用traefik实现服务的自动发布，其实现方式是traefik通过集群的DNS服务来解析service对应的集群地址（clusterip），从而将用户的访问请求转发到集群地址上。因此，在部署完集群后的第一件事情应该是配置DNS服务，目前可选的方案有skydns, kube-dns, coredns。&nbsp;&nbsp;&nbsp;&nbsp;kubedns是Kubernetes中的一个内置插件，目前作为一个独立的开源项目维护，见https://github.com/kubernetes/dns。该DNS服务器利用SkyDNS的库来为Kubernetes pod和服务提供DNS请求。CoreDNS项目是SkyDNS2的作者，Miek Gieben采用更模块化，可扩展的框架构建,将此DNS服务器作为KubeDNS的替代品。CoreDNS作为CNCF中的托管的一个项目，在Kuberentes1.9版本中，使用kubeadm方式安装的集群可以通过以下命令直接安装CoreDNS。kubeadm init –feature-gates=CoreDNS=true 准备工作准备coredns的yaml文件","pubDate":"Sat, 24 Aug 2019 06:54:14 GMT","guid":"https://xxlaila.github.io/2019/08/24/k8s部署coredns/","category":"coredns"},{"title":"k8s部署mysql","link":"https://xxlaila.github.io/2019/08/24/k8s部署mysql/","description":"&nbsp;&nbsp;&nbsp;&nbsp;后端存储利用nfs来进行存储数据，nfs安装不阐述，需要注意注意的是在创建mysql 的共享目录的时候参数设定/data/mysql *(rw,sync,no_root_squash,no_subtree_check) 12$ sudo systemctl restart nfs.service$ sudo exportfs -arv 1、创建mysql存储12345678910111213141516171819202122232425262728# cat mysql-pvc.yamlapiVersion: v1kind: PersistentVolumemetadata: name: pvc001 namespace: kube-opsspec: capacity: storage: 10Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Delete nfs: server: 172.21.16.240 path: /data/mysql---kind: PersistentVolumeClaimapiVersion: v1metadata: name: mysql-pvc namespace: kube-opsspec: accessModes: - ReadWriteMany resources: requests: storage: 10Gi","pubDate":"Sat, 24 Aug 2019 06:43:07 GMT","guid":"https://xxlaila.github.io/2019/08/24/k8s部署mysql/","category":"kubernetes"},{"title":"k8s角色访问RBAC","link":"https://xxlaila.github.io/2019/08/24/k8s角色访问RBAC/","description":"1、rbac介绍&nbsp;&nbsp;&nbsp;&nbsp;Kubernetes中的两个用于配置信息的重要资源对象：ConfigMap和Secret，其实到这里我们基本上学习的内容已经覆盖到Kubernetes中一些重要的资源对象了，来部署一个应用程序是完全没有问题的了。在我们演示一个完整的示例之前，我们还需要给大家讲解一个重要的概念：RBAC - 基于角色的访问控制。&nbsp;&nbsp;&nbsp;&nbsp;RBAC使用rbac.authorization.k8s.io API Group 来实现授权决策，允许管理员通过 Kubernetes API 动态配置策略，要启用RBAC，需要在 apiserver 中添加参数–authorization-mode=RBAC，如果使用的kubeadm安装的集群，1.6 版本以上的都默认开启了RBAC，可以通过查看 Master 节点上 apiserver 的静态Pod定义文件： 2、 kubernetes 关于空间权限赋予1、获取并查看 Role ClusterRole RoleBinding ClusterRoleBinding","pubDate":"Sat, 24 Aug 2019 06:35:51 GMT","guid":"https://xxlaila.github.io/2019/08/24/k8s角色访问RBAC/","category":"RBAC"},{"title":"k8s heapster","link":"https://xxlaila.github.io/2019/08/24/k8s-heapster/","description":"1、heapster 介绍&nbsp;&nbsp;&nbsp;&nbsp;Heapster是容器集群监控和性能分析工具,支持Kubernetes和CoreOS。&nbsp;&nbsp;&nbsp;&nbsp;Kubernetes有个监控agent—cAdvisor。在每个kubernetes Node上都会运行cAdvisor,它会收集本机以及容器的监控数据(cpu,memory,filesystem,network,uptime)。在较新的版本中，K8S已经将cAdvisor功能集成到kubelet组件中。每个Node节点可以直接进行web访问。 2、heapster 安装下载heapster的yaml文件，下载完成后我们需要对文件修改，以满足我们的的需求. 2.1、grafana修改grafana添加nodePort: 30003让grafana支持外部访问，我们可以通过这个端口进行但单独的页面配置。","pubDate":"Sat, 24 Aug 2019 06:28:50 GMT","guid":"https://xxlaila.github.io/2019/08/24/k8s-heapster/","category":"kubernetes,heapster"},{"title":"k8s部署常规的jar服务","link":"https://xxlaila.github.io/2019/08/24/k8s部署常规的jar服务/","description":"1、Dockerfile 的配置123456789101112# cat Dockerfile FROM docker.io/xxlaila/centos7.6-jdk1.8:latestMAINTAINER xxlaila \"cq_xxlaila@163.com\"# Install dependent pluginADD target/&lt;BUILD_NAME&gt;.jar /opt/webapps/&lt;BUILD_NAME&gt;.jarWORKDIR /opt/webappsEXPOSE 8000ENTRYPOINT [\"java\", \"-jar\", \"-Dspring.profiles.active=$&#123;RUN_ENV&#125;\", \"&lt;BUILD_NAME&gt;.jar\"]","pubDate":"Sat, 24 Aug 2019 06:25:00 GMT","guid":"https://xxlaila.github.io/2019/08/24/k8s部署常规的jar服务/","category":"kubernetes"},{"title":"k8s部署eureka集群","link":"https://xxlaila.github.io/2019/08/24/k8s部署eureka集群/","description":"eureka 不阐述介绍，这里直接开始在kubernetes下部署eureka集群 1、配置文件的增加eureka 只一个有状态的服务，部署有状态服务我们可以使用StatefulSet 1.1、增加dockerfile12345678910111213$ cat DockerfileFROM docker.io/xxlaila/centos7.6-jdk1.8:latestMAINTAINER xxlaila \"cq_xxlaila@163.com\"# Install dependent pluginADD target/kxl-eureka.jar /opt/webapps/kxl-eureka.jarADD application.yaml /opt/webapps/application.yamlWORKDIR /opt/webappsEXPOSE 8080ENTRYPOINT [\"java\", \"-jar\", \"-Dspring.profiles.active=dev\", \"kxl-eureka.jar\"]","pubDate":"Sat, 24 Aug 2019 06:11:05 GMT","guid":"https://xxlaila.github.io/2019/08/24/k8s部署eureka集群/","category":"eureka,kubernetes"},{"title":"jira接入LDAP","link":"https://xxlaila.github.io/2019/08/24/jira接入LDAP/","description":"场景:&nbsp;&nbsp;&nbsp;&nbsp;之前介绍了jira 和confluence的账户结合，jira和confluence可以使用一个账户，有人员离职之后直接在jira吧用户禁用即可，一端操作，方便两端，但是随着公司人员越来越多，这样的方式已经不在适合这种了，来一个用户就需要去创建，对运维来说，这是重复的工作，提升不了任何效率，而且枯草无味。这里我们就可以使用ldap，jira和confluence都是支持ldap，ldap的好处，这里不阐述，下面来看看如何配置jira介入ldap。confluence还是接入jira，这样我们就只操作ladp和jira，简单省事。 问题点:&nbsp;&nbsp;&nbsp;&nbsp;由于在建立jira和confluence的时候还没有ldap，ldap是后期才接入的，所以这里就存在于怎么吧以前有jira登录的账户认证切换到ldap。而且不影响之前的文档，但是用户权限会影响，问题不大，可以添加。下面开始操作","pubDate":"Sat, 24 Aug 2019 03:19:52 GMT","guid":"https://xxlaila.github.io/2019/08/24/jira接入LDAP/","category":"jira"},{"title":"confluence与jira账户打通","link":"https://xxlaila.github.io/2019/08/24/confluence与jira账户打通/","description":"confluence安装 登录confluence点击用户管理","pubDate":"Sat, 24 Aug 2019 02:54:41 GMT","guid":"https://xxlaila.github.io/2019/08/24/confluence与jira账户打通/","category":"confluence"},{"title":"jira安装和配置","link":"https://xxlaila.github.io/2019/08/24/jira安装和配置/","description":"介绍&nbsp;&nbsp;&nbsp;&nbsp;JIRA是Atlassian公司出品的项目与事务跟踪工具，被广泛应用于缺陷跟踪、客户服务、需求收集、流程审批、任务跟踪、项目跟踪和敏捷管理等工作领域。 环境准备 应用 服务器配置 操作系统 插件 mysql 5.6 + 2/4G/50G centos 7.4 jira 4/8G/200G centos 7.4 jdk1.8","pubDate":"Sat, 24 Aug 2019 02:21:09 GMT","guid":"https://xxlaila.github.io/2019/08/24/jira安装和配置/","category":"jira"},{"title":"git清空commit记录方法","link":"https://xxlaila.github.io/2019/08/24/git清空commit记录方法/","description":"说明:&nbsp;&nbsp;&nbsp;&nbsp;例如将代码提交到git仓库，将一些敏感信息提交，所以需要删除提交记录以彻底清除提交信息，以得到一个干净的仓库且代码不变 1.Checkout1$ git checkout --orphan latest_branch 2. Add all the files1$ git add -A 3. Commit the changes1$ git commit -am \"commit message\"","pubDate":"Sat, 24 Aug 2019 01:36:36 GMT","guid":"https://xxlaila.github.io/2019/08/24/git清空commit记录方法/","category":"git"},{"title":"confluence_install","link":"https://xxlaila.github.io/2019/08/24/confluence-install/","description":"&nbsp;&nbsp;&nbsp;&nbsp;Confluence是一个专业的企业知识管理与协同软件，也可以用于构建企业wiki。使用简单，但它强大的编辑和站点管理特征能够帮助团队成员之间共享信息、文档协作、集体讨论，信息推送。为团队提供一个协作环境。在这里，团队成员齐心协力，各擅其能，协同地编写文档和管理项目。从此打破不同团队、不同部门以及个人之间信息孤岛的僵局，Confluence真正实现了组织资源共享。 环境准备 系统版本 插件 软件 版本 服务配置 centos 7.4 mysql 5.6+ 2/4G/50G centos 7.4 jdk 1.8 confluence 6.12.2 4/8G/200G confluence 6.12.2安装并破解，mysql 版本这里使用的是5.7.24","pubDate":"Sat, 24 Aug 2019 00:57:59 GMT","guid":"https://xxlaila.github.io/2019/08/24/confluence-install/","category":"confluence"},{"title":"nexus3搭建npm私服","link":"https://xxlaila.github.io/2019/08/23/nexus3搭建npm私服/","description":"介绍&nbsp;&nbsp;&nbsp;&nbsp;公司前端全是nodejs的，nodejs在install的时候往往是连接外网，或者是设置taobao源，即使是设置了taobao源，但是还是解决不了慢的问题，为此搭建了一个内部的npm私服，这里用google一下有很多都可以来进行搭建npm私服，然后也看到了nexus也可以来做，正好maven私服也是用的这个，都是3版本，为此选择了nexus来做npm的私服，和maven一套便于维护。 nexus安装&nbsp;&nbsp;&nbsp;&nbsp;不介绍，安装完成nexus后，在浏览器打开并进行登录，第一次安装登录nexus的默认用户admin,默认密码是admin123","pubDate":"Fri, 23 Aug 2019 01:41:41 GMT","guid":"https://xxlaila.github.io/2019/08/23/nexus3搭建npm私服/","category":"nexus,npm"},{"title":"nginx URL 斜杠问题","link":"https://xxlaila.github.io/2019/08/22/nginx-URL-斜杠问题/","description":"问题今天公司新上的一个前端应用遇到一个问题，那就是在微信登录界面扫码登录之后，微信回调给我们的地址多加了一个斜杠; 错误的地址:http://a.xxlaila.com/wx.html/?code=011amZet0h1IUf19Fvht0jg4ft0amZeN正确的地址:http://a.xxlaila.com/wx.html?code=011amZet0h1IUf19Fvht0jg4ft0amZeN 在nginx上配置需要吧这个斜杠删除掉。用户才能正常的访问； 实例在配置文件里面增加如下配置项","pubDate":"Thu, 22 Aug 2019 07:29:37 GMT","guid":"https://xxlaila.github.io/2019/08/22/nginx-URL-斜杠问题/","category":"nginx"},{"title":"kubernetes-ci/cd-(四)","link":"https://xxlaila.github.io/2019/08/20/kubernetes-ci-cd-四/","description":"1、Blue Ocean安装Blue Ocean插件 1.1、创建pipeline 配置代码库的地址 然后配置授权账户 在这儿之前git库里面必须存在于jenkinsfile文件，pipeline会自动去扫描代码库里面的分支，然后根据每一个分支建立一个类似于job的形式，然后我们可以根据每一个分支进行部署，可以执行定时触发，部署 这儿，只有一个分支存在于jenkinsfile，所以只显示一个分支，如下图：","pubDate":"Tue, 20 Aug 2019 06:54:48 GMT","guid":"https://xxlaila.github.io/2019/08/20/kubernetes-ci-cd-四/","category":"jenkins, ci/cd"}]}