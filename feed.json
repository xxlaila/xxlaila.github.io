{"title":"懒羊羊","description":"星际拾荒者","language":"zh-CN","link":"https://xxlaila.github.io","pubDate":"Sat, 24 Aug 2019 06:43:07 GMT","lastBuildDate":"Sat, 24 Aug 2019 06:48:02 GMT","generator":"hexo-generator-json-feed","webMaster":"xxlaila","items":[{"title":"k8s部署mysql","link":"https://xxlaila.github.io/2019/08/24/k8s部署mysql/","description":"后端存储利用nfs来进行存储数据，nfs安装不阐述，需要注意注意的是在创建mysql 的共享目录的时候参数设定/data/mysql *(rw,sync,no_root_squash,no_subtree_check) 12$ sudo systemctl restart nfs.service$ sudo exportfs -arv 1、创建mysql存储12345678910111213141516171819202122232425262728# cat mysql-pvc.yamlapiVersion: v1kind: PersistentVolumemetadata: name: pvc001 namespace: kube-opsspec: capacity: storage: 10Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Delete nfs: server: 172.21.16.240 path: /data/mysql---kind: PersistentVolumeClaimapiVersion: v1metadata: name: mysql-pvc namespace: kube-opsspec: accessModes: - ReadWriteMany resources: requests: storage: 10Gi","pubDate":"Sat, 24 Aug 2019 06:43:07 GMT","guid":"https://xxlaila.github.io/2019/08/24/k8s部署mysql/","category":"kubernetes"},{"title":"k8s角色访问RBAC","link":"https://xxlaila.github.io/2019/08/24/k8s角色访问RBAC/","description":"1、rbac介绍Kubernetes中的两个用于配置信息的重要资源对象：ConfigMap和Secret，其实到这里我们基本上学习的内容已经覆盖到Kubernetes中一些重要的资源对象了，来部署一个应用程序是完全没有问题的了。在我们演示一个完整的示例之前，我们还需要给大家讲解一个重要的概念：RBAC - 基于角色的访问控制。RBAC使用rbac.authorization.k8s.io API Group 来实现授权决策，允许管理员通过 Kubernetes API 动态配置策略，要启用RBAC，需要在 apiserver 中添加参数–authorization-mode=RBAC，如果使用的kubeadm安装的集群，1.6 版本以上的都默认开启了RBAC，可以通过查看 Master 节点上 apiserver 的静态Pod定义文件： 2、 kubernetes 关于空间权限赋予1、获取并查看Role/ClusterRole/RoleBinding/ClusterRoleBinding的信息","pubDate":"Sat, 24 Aug 2019 06:35:51 GMT","guid":"https://xxlaila.github.io/2019/08/24/k8s角色访问RBAC/","category":"kubernetes,RBAC"},{"title":"k8s heapster","link":"https://xxlaila.github.io/2019/08/24/k8s-heapster/","description":"1、heapster 介绍Heapster是容器集群监控和性能分析工具,支持Kubernetes和CoreOS。Kubernetes有个监控agent—cAdvisor。在每个kubernetes Node上都会运行cAdvisor,它会收集本机以及容器的监控数据(cpu,memory,filesystem,network,uptime)。在较新的版本中，K8S已经将cAdvisor功能集成到kubelet组件中。每个Node节点可以直接进行web访问。 2、heapster 安装下载heapster的yaml文件，下载完成后我们需要对文件修改，以满足我们的的需求. 2.1、grafana修改grafana添加nodePort: 30003让grafana支持外部访问，我们可以通过这个端口进行但单独的页面配置。","pubDate":"Sat, 24 Aug 2019 06:28:50 GMT","guid":"https://xxlaila.github.io/2019/08/24/k8s-heapster/","category":"kubernetes,heapster"},{"title":"k8s部署常规的jar服务","link":"https://xxlaila.github.io/2019/08/24/k8s部署常规的jar服务/","description":"1、Dockerfile 的配置123456789101112# cat Dockerfile FROM docker.io/xxlaila/centos7.6-jdk1.8:latestMAINTAINER xxlaila \"cq_xxlaila@163.com\"# Install dependent pluginADD target/&lt;BUILD_NAME&gt;.jar /opt/webapps/&lt;BUILD_NAME&gt;.jarWORKDIR /opt/webappsEXPOSE 8000ENTRYPOINT [\"java\", \"-jar\", \"-Dspring.profiles.active=$&#123;RUN_ENV&#125;\", \"&lt;BUILD_NAME&gt;.jar\"]","pubDate":"Sat, 24 Aug 2019 06:25:00 GMT","guid":"https://xxlaila.github.io/2019/08/24/k8s部署常规的jar服务/","category":"kubernetes"},{"title":"k8s部署eureka集群","link":"https://xxlaila.github.io/2019/08/24/k8s部署eureka集群/","description":"eureka 不阐述介绍，这里直接开始在kubernetes下部署eureka集群 1、配置文件的增加eureka 只一个有状态的服务，部署有状态服务我们可以使用StatefulSet 1.1、增加dockerfile12345678910111213$ cat DockerfileFROM docker.io/xxlaila/centos7.6-jdk1.8:latestMAINTAINER xxlaila \"cq_xxlaila@163.com\"# Install dependent pluginADD target/kxl-eureka.jar /opt/webapps/kxl-eureka.jarADD application.yaml /opt/webapps/application.yamlWORKDIR /opt/webappsEXPOSE 8080ENTRYPOINT [\"java\", \"-jar\", \"-Dspring.profiles.active=dev\", \"kxl-eureka.jar\"]","pubDate":"Sat, 24 Aug 2019 06:11:05 GMT","guid":"https://xxlaila.github.io/2019/08/24/k8s部署eureka集群/","category":"eureka,kubernetes"},{"title":"jira接入LDAP","link":"https://xxlaila.github.io/2019/08/24/jira接入LDAP/","description":"场景: 之前介绍了jira 和confluence的账户结合，jira和confluence可以使用一个账户，有人员离职之后直接在jira吧用户禁用即可，一端操作，方便两端，但是随着公司人员越来越多，这样的方式已经不在适合这种了，来一个用户就需要去创建，对运维来说，这是重复的工作，提升不了任何效率，而且枯草无味。这里我们就可以使用ldap，jira和confluence都是支持ldap，ldap的好处，这里不阐述，下面来看看如何配置jira介入ldap。confluence还是接入jira，这样我们就只操作ladp和jira，简单省事。 问题点: 由于在建立jira和confluence的时候还没有ldap，ldap是后期才接入的，所以这里就存在于怎么吧以前有jira登录的账户认证切换到ldap。而且不影响之前的文档，但是用户权限会影响，问题不大，可以添加。下面开始操作","pubDate":"Sat, 24 Aug 2019 03:19:52 GMT","guid":"https://xxlaila.github.io/2019/08/24/jira接入LDAP/","category":"jira"},{"title":"confluence与jira账户打通","link":"https://xxlaila.github.io/2019/08/24/confluence与jira账户打通/","description":"confluence安装 登录confluence点击用户管理","pubDate":"Sat, 24 Aug 2019 02:54:41 GMT","guid":"https://xxlaila.github.io/2019/08/24/confluence与jira账户打通/","category":"confluence"},{"title":"jira安装和配置","link":"https://xxlaila.github.io/2019/08/24/jira安装和配置/","description":"介绍JIRA是Atlassian公司出品的项目与事务跟踪工具，被广泛应用于缺陷跟踪、客户服务、需求收集、流程审批、任务跟踪、项目跟踪和敏捷管理等工作领域。 环境准备 应用 服务器配置 操作系统 插件 mysql 5.6 + 2/4G/50G centos 7.4 jira 4/8G/200G centos 7.4 jdk1.8","pubDate":"Sat, 24 Aug 2019 02:21:09 GMT","guid":"https://xxlaila.github.io/2019/08/24/jira安装和配置/","category":"jira"},{"title":"git清空commit记录方法","link":"https://xxlaila.github.io/2019/08/24/git清空commit记录方法/","description":"说明：例如将代码提交到git仓库，将一些敏感信息提交，所以需要删除提交记录以彻底清除提交信息，以得到一个干净的仓库且代码不变 1.Checkout1$ git checkout --orphan latest_branch 2. Add all the files1$ git add -A 3. Commit the changes1$ git commit -am \"commit message\"","pubDate":"Sat, 24 Aug 2019 01:36:36 GMT","guid":"https://xxlaila.github.io/2019/08/24/git清空commit记录方法/","category":"git"},{"title":"confluence_install","link":"https://xxlaila.github.io/2019/08/24/confluence-install/","description":"Confluence是一个专业的企业知识管理与协同软件，也可以用于构建企业wiki。使用简单，但它强大的编辑和站点管理特征能够帮助团队成员之间共享信息、文档协作、集体讨论，信息推送。为团队提供一个协作环境。在这里，团队成员齐心协力，各擅其能，协同地编写文档和管理项目。从此打破不同团队、不同部门以及个人之间信息孤岛的僵局，Confluence真正实现了组织资源共享。 环境准备 系统版本 插件 软件 版本 服务配置 centos 7.4 mysql 5.6+ 2/4G/50G centos 7.4 jdk 1.8 confluence 6.12.2 4/8G/200G confluence 6.12.2安装并破解，mysql 版本这里使用的是5.7.24","pubDate":"Sat, 24 Aug 2019 00:57:59 GMT","guid":"https://xxlaila.github.io/2019/08/24/confluence-install/","category":"confluence"},{"title":"nexus3搭建npm私服","link":"https://xxlaila.github.io/2019/08/23/nexus3搭建npm私服/","description":"介绍公司前端全是nodejs的，nodejs在install的时候往往是连接外网，或者是设置taobao源，即使是设置了taobao源，但是还是解决不了慢的问题，为此搭建了一个内部的npm私服，这里用google一下有很多都可以来进行搭建npm私服，然后也看到了nexus也可以来做，正好maven私服也是用的这个，都是3版本，为此选择了nexus来做npm的私服，和maven一套便于维护。 nexus安装不介绍，安装完成nexus后，在浏览器打开并进行登录，第一次安装登录nexus的默认用户admin,默认密码是admin123","pubDate":"Fri, 23 Aug 2019 01:41:41 GMT","guid":"https://xxlaila.github.io/2019/08/23/nexus3搭建npm私服/","category":"nexus,npm"},{"title":"nginx URL 斜杠问题","link":"https://xxlaila.github.io/2019/08/22/nginx-URL-斜杠问题/","description":"问题今天公司新上的一个前端应用遇到一个问题，那就是在微信登录界面扫码登录之后，微信回调给我们的地址多加了一个斜杠; 错误的地址:http://a.xxlaila.com/wx.html/?code=011amZet0h1IUf19Fvht0jg4ft0amZeN正确的地址:http://a.xxlaila.com/wx.html?code=011amZet0h1IUf19Fvht0jg4ft0amZeN 在nginx上配置需要吧这个斜杠删除掉。用户才能正常的访问； 实例在配置文件里面增加如下配置项","pubDate":"Thu, 22 Aug 2019 07:29:37 GMT","guid":"https://xxlaila.github.io/2019/08/22/nginx-URL-斜杠问题/","category":"nginx"},{"title":"kubernetes-ci/cd-(四)","link":"https://xxlaila.github.io/2019/08/20/kubernetes-ci-cd-四/","description":"1、Blue Ocean安装Blue Ocean插件 1.1、创建pipeline 配置代码库的地址 然后配置授权账户 在这儿之前git库里面必须存在于jenkinsfile文件，pipeline会自动去扫描代码库里面的分支，然后根据每一个分支建立一个类似于job的形式，然后我们可以根据每一个分支进行部署，可以执行定时触发，部署 这儿，只有一个分支存在于jenkinsfile，所以只显示一个分支，如下图：","pubDate":"Tue, 20 Aug 2019 06:54:48 GMT","guid":"https://xxlaila.github.io/2019/08/20/kubernetes-ci-cd-四/","category":"jenkins, ci/cd"},{"title":"kubernetes-ci/cd-(三)","link":"https://xxlaila.github.io/2019/08/20/kubernetes-ci-cd-三/","description":"jenkins 配置完成后，最终实现的是ci/cd，在编译的过程中，经常会遇到后端java的，前端nodejs的，这里就需要进行一个k8s在调度的时候生产pod来进行指定pod进行编译 1、制作容器自定义一个容器，里面包含了 java，nodejs的所需要的环境，同时需要同步容器的时间，包含来jenkins的node 12345678910111213141516171819202122# cat DockerfileFROM docker.io/centos:latestMAINTAINER xxlaila &quot;cq_xxlaila@163.com&quot;# Install dependent pluginENV VERSION v10.15.1RUN yum install -y wget \\ git \\ java-1.8.0-openjdk.x86_64 \\ &amp;&amp; curl -sL https://rpm.nodesource.com/setup_11.x | bash - \\ &amp;&amp; yum install -y gcc gcc-c++ make \\ &amp;&amp; yum install -y nodejs \\ &amp;&amp; yum clean all# System variable settingRUN echo &quot;LANG=zh_CN.UTF-8&quot; &gt;&gt; /etc/locale.conf \\ &amp;&amp; source /etc/locale.conf \\ &amp;&amp; cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \\ &amp;&amp; echo &quot;Asia/shanghai&quot; &gt;&gt; /etc/timezone \\ &amp;&amp; groupadd -g 10000 jenkins \\ &amp;&amp; useradd -g jenkins -u 10000 jenkinsEXPOSE 50000","pubDate":"Tue, 20 Aug 2019 06:17:48 GMT","guid":"https://xxlaila.github.io/2019/08/20/kubernetes-ci-cd-三/","category":"jenkins ci/cd"},{"title":"kubernetes-ci/cd-(二)","link":"https://xxlaila.github.io/2019/08/20/kubernetes-ci-cd-二/","description":"基于jenkins pipeline进行部署1、jenkins pipeline介绍要实现在 Jenkins 中的构建工作，可以有多种方式，我们这里采用比较常用的 Pipeline 这种方式。Pipeline，简单来说，就是一套运行在 Jenkins 上的工作流框架，将原来独立运行于单个或者多个节点的任务连接起来，实现单个任务难以完成的复杂流程编排和可视化的工作。 Jenkins Pipeline 有几个核心概念: Node：节点，一个 Node 就是一个 Jenkins 节点，Master 或者 Agent，是执行 Step 的具体运行环境，比如我们之前动态运行的 Jenkins Slave 就是一个 Node 节点 Stage：阶段，一个 Pipeline 可以划分为若干个 Stage，每个 Stage 代表一组操作，比如：Build、Test、Deploy，Stage 是一个逻辑分组的概念，可以跨多个 Node Step：步骤，Step 是最基本的操作单元，可以是打印一句话，也可以是构建一个 Docker 镜像，由各类 Jenkins 插件提供，比如命令：sh ‘make’，就相当于我们平时 shell 终端中执行 make 命令一样。 那么我们如何创建 Jenkins Pipline 呢？ Pipeline 脚本是由 Groovy 语言实现的，但是我们没必要单独去学习 Groovy，当然你会的话最好 Pipeline 支持两种语法：Declarative(声明式)和 Scripted Pipeline(脚本式)语法 Pipeline 也有两种创建方法：可以直接在 Jenkins 的 Web UI 界面中输入脚本；也可以通过创建一个 Jenkinsfile 脚本文件放入项目源码库中 一般我们都推荐在 Jenkins 中直接从源代码控制(SCMD)中直接载入 Jenkinsfile Pipeline 这种方法创建一个简单的 Pipeline 我们这里来给大家快速创建一个简单的 Pipeline，直接在 Jenkins 的 Web UI 界面中输入脚本运行。 新建 Job：在 Web UI 中点击 New Item -&gt; 输入名称：pipeline-demo -&gt; 选择下面的 Pipeline -&gt; 点击 OK 配置：在最下方的 Pipeline 区域输入如下 Script 脚本，然后点击保存。","pubDate":"Tue, 20 Aug 2019 05:46:14 GMT","guid":"https://xxlaila.github.io/2019/08/20/kubernetes-ci-cd-二/","category":"jenkins, ci/cd"},{"title":"zabbix企业微信告警","link":"https://xxlaila.github.io/2019/08/20/zabbix企业微信告警/","description":"Zabbix可以通过多种方式把告警信息发送到指定人，常用的有邮件，短信报警方式，但是越来越多的企业开始使用zabbix结合微信作为主要的告警方式，这样可以及时有效的把告警信息推送到接收人，方便告警的及时处理。微信企业号需要先在企业通信录新建该员工，该员工才能关注该企业号，这样就能实现告警信息的私密性。如果使用公众号，则只要所有关注了该公众号的人都能收到告警消息，容易造成信息泄露。而且员工数少于200人的企业号是不用钱的，也没有任何申请限制. 1、脚本存放目录/usr/lib/zabbix/alertscripts，脚本的权限是zabbix 账户，具有可执行权限","pubDate":"Tue, 20 Aug 2019 03:09:37 GMT","guid":"https://xxlaila.github.io/2019/08/20/zabbix企业微信告警/","category":"zabbix"},{"title":"帧中继配置","link":"https://xxlaila.github.io/2019/08/19/帧中继配置/","description":"点对点配置 RA配置: 1234567[RA]int s0/0[RA-s0/0]encap frame-relay 封装帧中继协议[RA-s0/0] frame-relay intf dte[RA-s0/0]frame-relay interface-dlci 102 设置本接口对应的INTERFACE-DLCI号[RA-s0/0]frame-relay map ip 172.16.1.2 102 建立对端协议地址与本地INTERFACE-DLCI号的映射关系[RA-s0/0]ip add 172.16.1.1 255.255.255.0 配置本接口IP地址[RA-s0/0]no sh 打开此物理接口 RB配置: 1234567[RB]int s0/0[RB-s0/0]encap frame-relay 封装帧中继协议[RA-s0/0] frame-relay intf dte[RB-s0/0]frame-relay interface-dlci 201 设置本接口对应的INTERFACE-DLCI号[RB-s0/0]frame-relay map ip 172.16.1.1 201 建立对端协议地址与本地INTERFACE-DLCI号的映射关系[RB-s0/0]ip add 172.16.1.2 255.255.255.0 配置本接口IP地址[RB-s0/0]no sh 打开此物理接口","pubDate":"Mon, 19 Aug 2019 13:43:35 GMT","guid":"https://xxlaila.github.io/2019/08/19/帧中继配置/","category":"帧中继, 网络设备"},{"title":"kubernetes ci/cd(一)","link":"https://xxlaila.github.io/2019/08/12/kubernetes-ci-cd-一/","description":"基于jenkins的CI/CD安装 jenkins一个流行的持续集成/发布工具，在Kubernetes使用,持续构建与发布是我们日常工作中必不可少的一个步骤，目前大多公司都采用 Jenkins 集群来搭建符合需求的 CI/CD 流程，然而传统的 Jenkins Slave 一主多从方式会存在一些痛点，比如：主 Master 发生单点故障时，整个流程都不可用了；每个 Slave 的配置环境不一样，来完成不同语言的编译打包等操作，但是这些差异化的配置导致管理起来非常不方便，维护起来也是比较费劲；资源分配不均衡，有的 Slave 要运行的 job 出现排队等待，而有的 Slave 处于空闲状态；最后资源有浪费，每台 Slave 可能是实体机或者 VM，当 Slave 处于空闲状态时，也不会完全释放掉资源。 提到基于Kubernete的CI/CD，可以使用的工具有很多，比如Jenkins、Gitlab CI已经新兴的drone之类的，我们这里会使用大家最为熟悉的Jenins来做CI/CD的工具。 优点: Jenkins 安装完成了，接下来我们不用急着就去使用，我们要了解下在 Kubernetes 环境下面使用 Jenkins 有什么好处。都知道持续构建与发布是我们日常工作中必不可少的一个步骤，目前大多公司都采用 Jenkins 集群来搭建符合需求的 CI/CD 流程，然而传统的 Jenkins Slave 一主多从方式会存在一些痛点，比如: E 主 Master 发生单点故障时，整个流程都不可用了。 E 每个 Slave 的配置环境不一样，来完成不同语言的编译打包等操作，但是这些差异化的配置导致管理起来非常不方便，维护起来也是比较费劲。 E 资源分配不均衡，有的 Slave 要运行的 job 出现排队等待，而有的 Slave 处于空闲状态。 E 资源有浪费，每台 Slave 可能是物理机或者虚拟机，当 Slave 处于空闲状态时，也不会完全释放掉资源。 正因为这些种种痛点，我们渴望一种更高效更可靠的方式来完成这个 CI/CD 流程，而 Docker 虚拟化容器技术能很好的解决这个痛点，又特别是在 Kubernetes 集群环境下面能够更好来解决上面的问题，下图是基于 Kubernetes 搭建 Jenkins 集群的简单示意图 可以看到 Jenkins Master 和 Jenkins Slave 以 Pod 形式运行在 Kubernetes 集群的 Node 上，Master 运行在其中一个节点，并且将其配置数据存储到一个 Volume 上去，Slave 运行在各个节点上，并且它不是一直处于运行状态，它会按照需求动态的创建并自动删除。","pubDate":"Mon, 12 Aug 2019 06:56:04 GMT","guid":"https://xxlaila.github.io/2019/08/12/kubernetes-ci-cd-一/","category":"kubernetes,k8s,ci/cd,jenkins"},{"title":"kube nfs 动态存储","link":"https://xxlaila.github.io/2019/08/12/kube-nfs-动态存储/","description":"nfs-client-provisioner是一个automatic provisioner，使用NFS作为存储，自动创建PV和对应的PVC，本身不提供NFS存储，需要外部先有一套NFS存储服务。 PV以 ${namespace}-${pvcName}-${pvName}的命名格式提供（在NFS服务器上） PV回收的时候以 archieved-${namespace}-${pvcName}-${pvName} 的命名格式（在NFS服务器上） 官方访问地址 1、权限体系构建1.1、创建serviceaccountServiceAccount也是一种账号, 供运行在pod中的进程使用, 为pod中的进程提供必要的身份证明. 123456# cat serviceaccount.yaml apiVersion: v1kind: ServiceAccountmetadata: name: nfs-client-provisioner namespace: kube-test","pubDate":"Mon, 12 Aug 2019 06:43:27 GMT","guid":"https://xxlaila.github.io/2019/08/12/kube-nfs-动态存储/","category":"kubernetes,nfs"},{"title":"pvc pv","link":"https://xxlaila.github.io/2019/08/12/pvc-pv/","description":"1、介绍PersistentVolume（pv）和PersistentVolumeClaim（pvc）是k8s提供的两种API资源，用于抽象存储细节。管理员关注于如何通过pv提供存储功能而无需关注用户如何使用，同样的用户只需要挂载pvc到容器中而不需要关注存储卷采用何种技术实现。pvc和pv的关系与pod和node关系类似，前者消耗后者的资源。pvc可以向pv申请指定大小的存储资源并设置访问模式,这就可以通过Provision -&gt; Claim 的方式，来对存储资源进行控制。 2、生命周期pv和pvc遵循以下生命周期： 供应准备。通过集群外的存储系统或者云平台来提供存储持久化支持。 静态提供：管理员手动创建多个PV，供PVC使用。 动态提供：动态创建PVC特定的PV，并绑定。 绑定。用户创建pvc并指定需要的资源和访问模式。在找到可用pv之前，pvc会保持未绑定状态。 使用。用户可在pod中像volume一样使用pvc。 释放。用户删除pvc来回收存储资源，pv将变成“released”状态。由于还保留着之前的数据，这些数据需要根据不同的策略来处理，否则这些存储资源无法被其他pvc使用。 回收(Reclaiming)。pv可以设置三种回收策略：保留（Retain），回收（Recycle）和删除（Delete）。 保留策略：允许人工处理保留的数据。 删除策略：将删除pv和外部关联的存储资源，需要插件支持。 回收策略：将执行清除操作，之后可以被新的pvc使用，需要插件支持。 目前只有NFS和HostPath类型卷支持回收策略，AWS EBS,GCE PD,Azure Disk和Cinder支持删除(Delete)策略。 2.1、Provisioning两种方式提供的PV资源供给","pubDate":"Mon, 12 Aug 2019 06:04:08 GMT","guid":"https://xxlaila.github.io/2019/08/12/pvc-pv/","category":"pvc,pv,kubernetes,存储"},{"title":"kubernetes 单机安装","link":"https://xxlaila.github.io/2019/08/12/kubernetes-单机安装/","description":"1.环境准备 一个master节点，四个node节点master节点ip 172.21.16.244node节点ip 172.21.16.24 172.21.16.231 172.21.16.202 172.21.16.55 以下是每一个节点上均进行操作 2、服务器添加阿里云yum源12345678910cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=0repo_gpgcheck=0gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF 3、重新建立yum缓存1# yum -y install epel-release &amp;&amp;yum clean all &amp;&amp;yum makecach 记得同步系统的时间 3、配置转发请求","pubDate":"Mon, 12 Aug 2019 03:06:47 GMT","guid":"https://xxlaila.github.io/2019/08/12/kubernetes-单机安装/","category":"kubernetes"},{"title":"Centos 5.5 安装DRBD","link":"https://xxlaila.github.io/2019/08/11/Centos-5-5-安装DRBD/","description":"Centos5.5 32bit安装DRBD 安装前准备 节点类型 IP地址规划 主机名 主用节点 192.168.1.101 node2 备用节点 192.168.1.102 node1 磁盘 两台10G磁盘 在主节点安装DRBD1[root@node2 ~]# yum -y install kmod-drbd83 drbd83 安装成功之后/sbin目录下面有drbdadm，drbdmeta，drbdsetup命令，以及/etc /init.d/drbd启动脚本。 备用节点安装DRBD 1[root@node1 ~]# yum -y install kmod-drbd83 drbd83 安装完成后。默认配置文件/etc/drbd.conf，以下是两台的主机配置实例:","pubDate":"Sat, 10 Aug 2019 23:42:34 GMT","guid":"https://xxlaila.github.io/2019/08/11/Centos-5-5-安装DRBD/","category":"DRBD"},{"title":"oracle ORA-12519","link":"https://xxlaila.github.io/2019/08/10/oracle-ORA-12519/","description":"oracle ORA-12519错误解决今天遇到做系统压力测试的时候，系统报了一个错误OERR: ORA-12519 TNS:no appropriate service handler found 在网上搜索了一下oralc的错误信息ORA-12519，解决办法挺多的，这里记录一下 登陆oracle的服务器，在登陆oracle数据库1sqlplus \"/as sysdba\" 首先检查process和session的使用情况","pubDate":"Sat, 10 Aug 2019 09:01:07 GMT","guid":"https://xxlaila.github.io/2019/08/10/oracle-ORA-12519/","category":"ORA-12519"},{"title":"nginx https","link":"https://xxlaila.github.io/2019/08/10/nginx-https/","description":"nginx http 强制跳转到https 方法一123if ($scheme = http ) &#123; return 301 https://$host$request_uri;&#125; 列子 123456789server &#123; listen 80; listen 443; server_name xxx.test.com; index index.html index.php index.htm; if ($scheme = http ) &#123; return 301 https://$host$request_uri; &#125;&#125; 方法二123if ($server_port = 80 ) &#123; return 301 https://$host$request_uri;&#125; 列子","pubDate":"Sat, 10 Aug 2019 08:00:14 GMT","guid":"https://xxlaila.github.io/2019/08/10/nginx-https/","category":"nginx"},{"title":"haproxy keepalived ","link":"https://xxlaila.github.io/2019/08/10/haproxy-keepalived/","description":"本文主要是代理kubernetes master的高可用。 安装haproxy和keepalived12# yum -y install keepalived.x86_64# yum -y install haproxy18u.x86_64 2、配置haproxy12345678910111213141516171819202122232425262728293031323334353637383940414243# cat /etc/haproxy/haproxy.cfgglobal log 127.0.0.1 local0 err maxconn 50000 uid 99 gid 99 #daemon nbproc 1 pidfile haproxy.piddefaults mode http log 127.0.0.1 local0 err maxconn 50000 retries 3 timeout connect 5s timeout client 30s timeout server 30s timeout check 2slisten admin_stats mode http bind 0.0.0.0:1080 log 127.0.0.1 local0 err stats refresh 30s stats uri /haproxy-status stats realm Haproxy\\ Statistics stats auth admin:admin1 stats hide-version stats admin if TRUEfrontend k8s-https bind 0.0.0.0:8443 mode tcp #maxconn 50000 default_backend k8s-httpsbackend k8s-https mode tcp balance roundrobin server k8s-master-01 172.21.17.4:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3 server k8s-master-02 172.21.16.231:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3 server k8s-master-03 172.21.16.240:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3","pubDate":"Sat, 10 Aug 2019 03:52:00 GMT","guid":"https://xxlaila.github.io/2019/08/10/haproxy-keepalived/","category":"haproxy,keepalived, haproxy+keepalived,kubernetes"}]}