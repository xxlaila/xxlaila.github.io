{"title":"懒羊羊","description":"星际拾荒者","language":"zh-CN","link":"https://xxlaila.github.io","pubDate":"Tue, 20 Aug 2019 11:53:33 GMT","lastBuildDate":"Thu, 22 Aug 2019 01:02:45 GMT","generator":"hexo-generator-json-feed","webMaster":"xxlaila","items":[{"title":"openstack认证服务","link":"https://xxlaila.github.io/2019/08/20/openstack认证服务/","description":"添加认证服务(Identity Service) Controller 节点 1、配置先决条件在配置OpenStack Identity服务之前，必须创建一个数据库和一个管理令牌 1234567# 数据库# mysql -u root -p&gt; CREATE DATABASE keystone;&gt; GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'localhost' IDENTIFIED BY '123456';&gt; GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'%' IDENTIFIED BY '123456';","pubDate":"Tue, 20 Aug 2019 11:53:33 GMT","guid":"https://xxlaila.github.io/2019/08/20/openstack认证服务/","category":"openstack"},{"title":"openstack环境准备","link":"https://xxlaila.github.io/2019/08/20/openstack环境准备/","description":"1、基础环境配置服务器做好read，安装好操作系统(centos7.4) 设置服务器的主机名 控制节点: controller 计算节点: computer 存储节点: block 主机信息列表","pubDate":"Tue, 20 Aug 2019 08:35:39 GMT","guid":"https://xxlaila.github.io/2019/08/20/openstack环境准备/","category":"openstack"},{"title":"kubernetes-ci/cd-(四)","link":"https://xxlaila.github.io/2019/08/20/kubernetes-ci-cd-四/","description":"1、Blue Ocean安装Blue Ocean插件 1.1、创建pipeline 配置代码库的地址 然后配置授权账户 在这儿之前git库里面必须存在于jenkinsfile文件，pipeline会自动去扫描代码库里面的分支，然后根据每一个分支建立一个类似于job的形式，然后我们可以根据每一个分支进行部署，可以执行定时触发，部署 这儿，只有一个分支存在于jenkinsfile，所以只显示一个分支，如下图：","pubDate":"Tue, 20 Aug 2019 06:54:48 GMT","guid":"https://xxlaila.github.io/2019/08/20/kubernetes-ci-cd-四/","category":"jenkins, ci/cd"},{"title":"kubernetes-ci/cd-(三)","link":"https://xxlaila.github.io/2019/08/20/kubernetes-ci-cd-三/","description":"jenkins 配置完成后，最终实现的是ci/cd，在编译的过程中，经常会遇到后端java的，前端nodejs的，这里就需要进行一个k8s在调度的时候生产pod来进行指定pod进行编译 1、制作容器自定义一个容器，里面包含了 java，nodejs的所需要的环境，同时需要同步容器的时间，包含来jenkins的node 12345678910111213141516171819202122# cat DockerfileFROM docker.io/centos:latestMAINTAINER xxlaila &quot;cq_xxlaila@163.com&quot;# Install dependent pluginENV VERSION v10.15.1RUN yum install -y wget \\ git \\ java-1.8.0-openjdk.x86_64 \\ &amp;&amp; curl -sL https://rpm.nodesource.com/setup_11.x | bash - \\ &amp;&amp; yum install -y gcc gcc-c++ make \\ &amp;&amp; yum install -y nodejs \\ &amp;&amp; yum clean all# System variable settingRUN echo &quot;LANG=zh_CN.UTF-8&quot; &gt;&gt; /etc/locale.conf \\ &amp;&amp; source /etc/locale.conf \\ &amp;&amp; cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \\ &amp;&amp; echo &quot;Asia/shanghai&quot; &gt;&gt; /etc/timezone \\ &amp;&amp; groupadd -g 10000 jenkins \\ &amp;&amp; useradd -g jenkins -u 10000 jenkinsEXPOSE 50000","pubDate":"Tue, 20 Aug 2019 06:17:48 GMT","guid":"https://xxlaila.github.io/2019/08/20/kubernetes-ci-cd-三/","category":"jenkins ci/cd"},{"title":"kubernetes-ci/cd-(二)","link":"https://xxlaila.github.io/2019/08/20/kubernetes-ci-cd-二/","description":"基于jenkins pipeline进行部署1、jenkins pipeline介绍要实现在 Jenkins 中的构建工作，可以有多种方式，我们这里采用比较常用的 Pipeline 这种方式。Pipeline，简单来说，就是一套运行在 Jenkins 上的工作流框架，将原来独立运行于单个或者多个节点的任务连接起来，实现单个任务难以完成的复杂流程编排和可视化的工作。 Jenkins Pipeline 有几个核心概念: Node：节点，一个 Node 就是一个 Jenkins 节点，Master 或者 Agent，是执行 Step 的具体运行环境，比如我们之前动态运行的 Jenkins Slave 就是一个 Node 节点 Stage：阶段，一个 Pipeline 可以划分为若干个 Stage，每个 Stage 代表一组操作，比如：Build、Test、Deploy，Stage 是一个逻辑分组的概念，可以跨多个 Node Step：步骤，Step 是最基本的操作单元，可以是打印一句话，也可以是构建一个 Docker 镜像，由各类 Jenkins 插件提供，比如命令：sh ‘make’，就相当于我们平时 shell 终端中执行 make 命令一样。 那么我们如何创建 Jenkins Pipline 呢？ Pipeline 脚本是由 Groovy 语言实现的，但是我们没必要单独去学习 Groovy，当然你会的话最好 Pipeline 支持两种语法：Declarative(声明式)和 Scripted Pipeline(脚本式)语法 Pipeline 也有两种创建方法：可以直接在 Jenkins 的 Web UI 界面中输入脚本；也可以通过创建一个 Jenkinsfile 脚本文件放入项目源码库中 一般我们都推荐在 Jenkins 中直接从源代码控制(SCMD)中直接载入 Jenkinsfile Pipeline 这种方法创建一个简单的 Pipeline 我们这里来给大家快速创建一个简单的 Pipeline，直接在 Jenkins 的 Web UI 界面中输入脚本运行。 新建 Job：在 Web UI 中点击 New Item -&gt; 输入名称：pipeline-demo -&gt; 选择下面的 Pipeline -&gt; 点击 OK 配置：在最下方的 Pipeline 区域输入如下 Script 脚本，然后点击保存。","pubDate":"Tue, 20 Aug 2019 05:46:14 GMT","guid":"https://xxlaila.github.io/2019/08/20/kubernetes-ci-cd-二/","category":"jenkins, ci/cd"},{"title":"zabbix企业微信告警","link":"https://xxlaila.github.io/2019/08/20/zabbix企业微信告警/","description":"Zabbix可以通过多种方式把告警信息发送到指定人，常用的有邮件，短信报警方式，但是越来越多的企业开始使用zabbix结合微信作为主要的告警方式，这样可以及时有效的把告警信息推送到接收人，方便告警的及时处理。微信企业号需要先在企业通信录新建该员工，该员工才能关注该企业号，这样就能实现告警信息的私密性。如果使用公众号，则只要所有关注了该公众号的人都能收到告警消息，容易造成信息泄露。而且员工数少于200人的企业号是不用钱的，也没有任何申请限制. 1、脚本存放目录/usr/lib/zabbix/alertscripts，脚本的权限是zabbix 账户，具有可执行权限","pubDate":"Tue, 20 Aug 2019 03:09:37 GMT","guid":"https://xxlaila.github.io/2019/08/20/zabbix企业微信告警/","category":"zabbix"},{"title":"帧中继配置","link":"https://xxlaila.github.io/2019/08/19/帧中继配置/","description":"点对点配置 RA配置: 1234567[RA]int s0/0[RA-s0/0]encap frame-relay 封装帧中继协议[RA-s0/0] frame-relay intf dte[RA-s0/0]frame-relay interface-dlci 102 设置本接口对应的INTERFACE-DLCI号[RA-s0/0]frame-relay map ip 172.16.1.2 102 建立对端协议地址与本地INTERFACE-DLCI号的映射关系[RA-s0/0]ip add 172.16.1.1 255.255.255.0 配置本接口IP地址[RA-s0/0]no sh 打开此物理接口 RB配置: 1234567[RB]int s0/0[RB-s0/0]encap frame-relay 封装帧中继协议[RA-s0/0] frame-relay intf dte[RB-s0/0]frame-relay interface-dlci 201 设置本接口对应的INTERFACE-DLCI号[RB-s0/0]frame-relay map ip 172.16.1.1 201 建立对端协议地址与本地INTERFACE-DLCI号的映射关系[RB-s0/0]ip add 172.16.1.2 255.255.255.0 配置本接口IP地址[RB-s0/0]no sh 打开此物理接口","pubDate":"Mon, 19 Aug 2019 13:43:35 GMT","guid":"https://xxlaila.github.io/2019/08/19/帧中继配置/","category":"帧中继, 网络设备"},{"title":"kubernetes ci/cd(一)","link":"https://xxlaila.github.io/2019/08/12/kubernetes-ci-cd-一/","description":"基于jenkins的CI/CD安装 jenkins一个流行的持续集成/发布工具，在Kubernetes使用,持续构建与发布是我们日常工作中必不可少的一个步骤，目前大多公司都采用 Jenkins 集群来搭建符合需求的 CI/CD 流程，然而传统的 Jenkins Slave 一主多从方式会存在一些痛点，比如：主 Master 发生单点故障时，整个流程都不可用了；每个 Slave 的配置环境不一样，来完成不同语言的编译打包等操作，但是这些差异化的配置导致管理起来非常不方便，维护起来也是比较费劲；资源分配不均衡，有的 Slave 要运行的 job 出现排队等待，而有的 Slave 处于空闲状态；最后资源有浪费，每台 Slave 可能是实体机或者 VM，当 Slave 处于空闲状态时，也不会完全释放掉资源。 提到基于Kubernete的CI/CD，可以使用的工具有很多，比如Jenkins、Gitlab CI已经新兴的drone之类的，我们这里会使用大家最为熟悉的Jenins来做CI/CD的工具。 优点: Jenkins 安装完成了，接下来我们不用急着就去使用，我们要了解下在 Kubernetes 环境下面使用 Jenkins 有什么好处。都知道持续构建与发布是我们日常工作中必不可少的一个步骤，目前大多公司都采用 Jenkins 集群来搭建符合需求的 CI/CD 流程，然而传统的 Jenkins Slave 一主多从方式会存在一些痛点，比如: E 主 Master 发生单点故障时，整个流程都不可用了。 E 每个 Slave 的配置环境不一样，来完成不同语言的编译打包等操作，但是这些差异化的配置导致管理起来非常不方便，维护起来也是比较费劲。 E 资源分配不均衡，有的 Slave 要运行的 job 出现排队等待，而有的 Slave 处于空闲状态。 E 资源有浪费，每台 Slave 可能是物理机或者虚拟机，当 Slave 处于空闲状态时，也不会完全释放掉资源。 正因为这些种种痛点，我们渴望一种更高效更可靠的方式来完成这个 CI/CD 流程，而 Docker 虚拟化容器技术能很好的解决这个痛点，又特别是在 Kubernetes 集群环境下面能够更好来解决上面的问题，下图是基于 Kubernetes 搭建 Jenkins 集群的简单示意图 可以看到 Jenkins Master 和 Jenkins Slave 以 Pod 形式运行在 Kubernetes 集群的 Node 上，Master 运行在其中一个节点，并且将其配置数据存储到一个 Volume 上去，Slave 运行在各个节点上，并且它不是一直处于运行状态，它会按照需求动态的创建并自动删除。","pubDate":"Mon, 12 Aug 2019 06:56:04 GMT","guid":"https://xxlaila.github.io/2019/08/12/kubernetes-ci-cd-一/","category":"kubernetes,k8s,ci/cd,jenkins"},{"title":"kube nfs 动态存储","link":"https://xxlaila.github.io/2019/08/12/kube-nfs-动态存储/","description":"nfs-client-provisioner是一个automatic provisioner，使用NFS作为存储，自动创建PV和对应的PVC，本身不提供NFS存储，需要外部先有一套NFS存储服务。 PV以 ${namespace}-${pvcName}-${pvName}的命名格式提供（在NFS服务器上） PV回收的时候以 archieved-${namespace}-${pvcName}-${pvName} 的命名格式（在NFS服务器上） 官方访问地址 1、权限体系构建1.1、创建serviceaccountServiceAccount也是一种账号, 供运行在pod中的进程使用, 为pod中的进程提供必要的身份证明. 123456# cat serviceaccount.yaml apiVersion: v1kind: ServiceAccountmetadata: name: nfs-client-provisioner namespace: kube-test","pubDate":"Mon, 12 Aug 2019 06:43:27 GMT","guid":"https://xxlaila.github.io/2019/08/12/kube-nfs-动态存储/","category":"kubernetes,nfs"},{"title":"pvc pv","link":"https://xxlaila.github.io/2019/08/12/pvc-pv/","description":"1、介绍PersistentVolume（pv）和PersistentVolumeClaim（pvc）是k8s提供的两种API资源，用于抽象存储细节。管理员关注于如何通过pv提供存储功能而无需关注用户如何使用，同样的用户只需要挂载pvc到容器中而不需要关注存储卷采用何种技术实现。pvc和pv的关系与pod和node关系类似，前者消耗后者的资源。pvc可以向pv申请指定大小的存储资源并设置访问模式,这就可以通过Provision -&gt; Claim 的方式，来对存储资源进行控制。 2、生命周期pv和pvc遵循以下生命周期： 供应准备。通过集群外的存储系统或者云平台来提供存储持久化支持。 静态提供：管理员手动创建多个PV，供PVC使用。 动态提供：动态创建PVC特定的PV，并绑定。 绑定。用户创建pvc并指定需要的资源和访问模式。在找到可用pv之前，pvc会保持未绑定状态。 使用。用户可在pod中像volume一样使用pvc。 释放。用户删除pvc来回收存储资源，pv将变成“released”状态。由于还保留着之前的数据，这些数据需要根据不同的策略来处理，否则这些存储资源无法被其他pvc使用。 回收(Reclaiming)。pv可以设置三种回收策略：保留（Retain），回收（Recycle）和删除（Delete）。 保留策略：允许人工处理保留的数据。 删除策略：将删除pv和外部关联的存储资源，需要插件支持。 回收策略：将执行清除操作，之后可以被新的pvc使用，需要插件支持。 目前只有NFS和HostPath类型卷支持回收策略，AWS EBS,GCE PD,Azure Disk和Cinder支持删除(Delete)策略。 2.1、Provisioning两种方式提供的PV资源供给","pubDate":"Mon, 12 Aug 2019 06:04:08 GMT","guid":"https://xxlaila.github.io/2019/08/12/pvc-pv/","category":"pvc,pv,kubernetes,存储"},{"title":"kubernetes 单机安装","link":"https://xxlaila.github.io/2019/08/12/kubernetes-单机安装/","description":"1.环境准备 一个master节点，四个node节点master节点ip 172.21.16.244node节点ip 172.21.16.24 172.21.16.231 172.21.16.202 172.21.16.55 以下是每一个节点上均进行操作 2、服务器添加阿里云yum源12345678910cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=0repo_gpgcheck=0gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF 3、重新建立yum缓存1# yum -y install epel-release &amp;&amp;yum clean all &amp;&amp;yum makecach 记得同步系统的时间 3、配置转发请求","pubDate":"Mon, 12 Aug 2019 03:06:47 GMT","guid":"https://xxlaila.github.io/2019/08/12/kubernetes-单机安装/","category":"kubernetes"},{"title":"Centos 5.5 安装DRBD","link":"https://xxlaila.github.io/2019/08/11/Centos-5-5-安装DRBD/","description":"Centos5.5 32bit安装DRBD 安装前准备 节点类型 IP地址规划 主机名 主用节点 192.168.1.101 node2 备用节点 192.168.1.102 node1 磁盘 两台10G磁盘 在主节点安装DRBD1[root@node2 ~]# yum -y install kmod-drbd83 drbd83 安装成功之后/sbin目录下面有drbdadm，drbdmeta，drbdsetup命令，以及/etc /init.d/drbd启动脚本。 备用节点安装DRBD 1[root@node1 ~]# yum -y install kmod-drbd83 drbd83 安装完成后。默认配置文件/etc/drbd.conf，以下是两台的主机配置实例:","pubDate":"Sat, 10 Aug 2019 23:42:34 GMT","guid":"https://xxlaila.github.io/2019/08/11/Centos-5-5-安装DRBD/","category":"DRBD"},{"title":"oracle ORA-12519","link":"https://xxlaila.github.io/2019/08/10/oracle-ORA-12519/","description":"oracle ORA-12519错误解决今天遇到做系统压力测试的时候，系统报了一个错误OERR: ORA-12519 TNS:no appropriate service handler found 在网上搜索了一下oralc的错误信息ORA-12519，解决办法挺多的，这里记录一下 登陆oracle的服务器，在登陆oracle数据库1sqlplus \"/as sysdba\" 首先检查process和session的使用情况","pubDate":"Sat, 10 Aug 2019 09:01:07 GMT","guid":"https://xxlaila.github.io/2019/08/10/oracle-ORA-12519/","category":"ORA-12519"},{"title":"nginx https","link":"https://xxlaila.github.io/2019/08/10/nginx-https/","description":"nginx http 强制跳转到https 方法一123if ($scheme = http ) &#123; return 301 https://$host$request_uri;&#125; 列子 123456789server &#123; listen 80; listen 443; server_name xxx.test.com; index index.html index.php index.htm; if ($scheme = http ) &#123; return 301 https://$host$request_uri; &#125;&#125; 方法二123if ($server_port = 80 ) &#123; return 301 https://$host$request_uri;&#125; 列子","pubDate":"Sat, 10 Aug 2019 08:00:14 GMT","guid":"https://xxlaila.github.io/2019/08/10/nginx-https/","category":"nginx"},{"title":"haproxy keepalived ","link":"https://xxlaila.github.io/2019/08/10/haproxy-keepalived/","description":"本文主要是代理kubernetes master的高可用。 安装haproxy和keepalived12# yum -y install keepalived.x86_64# yum -y install haproxy18u.x86_64 2、配置haproxy12345678910111213141516171819202122232425262728293031323334353637383940414243# cat /etc/haproxy/haproxy.cfgglobal log 127.0.0.1 local0 err maxconn 50000 uid 99 gid 99 #daemon nbproc 1 pidfile haproxy.piddefaults mode http log 127.0.0.1 local0 err maxconn 50000 retries 3 timeout connect 5s timeout client 30s timeout server 30s timeout check 2slisten admin_stats mode http bind 0.0.0.0:1080 log 127.0.0.1 local0 err stats refresh 30s stats uri /haproxy-status stats realm Haproxy\\ Statistics stats auth admin:admin1 stats hide-version stats admin if TRUEfrontend k8s-https bind 0.0.0.0:8443 mode tcp #maxconn 50000 default_backend k8s-httpsbackend k8s-https mode tcp balance roundrobin server k8s-master-01 172.21.17.4:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3 server k8s-master-02 172.21.16.231:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3 server k8s-master-03 172.21.16.240:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3","pubDate":"Sat, 10 Aug 2019 03:52:00 GMT","guid":"https://xxlaila.github.io/2019/08/10/haproxy-keepalived/","category":"haproxy,keepalived, haproxy+keepalived,kubernetes"},{"title":"k8s-prometheus","link":"https://xxlaila.github.io/2019/08/10/k8s-prometheus/","description":"Prometheus是一个集数据收集存储、数据查询和数据图表显示于一身的开源监控组件。本文主要讲解如何搭建Prometheus，并使用它监控Kubernetes集群。 1、下载相关yaml1234567891011# https://github.com/xxlaila/kubernetes-yaml/tree/master/prometheus-grafana# ls -lconfigmap.yamlgrafana-deploy.yamlgrafana-ingress.yamlgrafana-svc.yamlnode-exporter.yamlprometheus-deploy.yamlprometheus-svc.yamlrbac-setup.yamlprometheus-ingress.yaml 2、开始部署2.1、采用daemonset方式部署node-exporter组件1# kubectl create -f node-exporter.yaml","pubDate":"Sat, 10 Aug 2019 03:37:34 GMT","guid":"https://xxlaila.github.io/2019/08/10/k8s-prometheus/","category":"Prometheus, kubernetes"},{"title":"kubedns插件配置","link":"https://xxlaila.github.io/2019/08/10/kubedns插件配置/","description":"安装和配置kubedns插件1、配置文件准备下载官方的yaml文件目录：kubernetes/cluster/addons/dns。该插件直接使用kubernetes部署,yaml文件经过修改完成部署 1234567891011121314151617# git clone https://github.com/xxlaila/kubernetes-yaml.git# cd kubernetes-yaml/coredns# sed -i 's/10.96.0.10/10.254.0.2/g' coredns-service.yaml# kubectl create -f ./# kubectl get pods,svc,rs -n kube-systemNAME READY STATUS RESTARTS AGEpod/coredns-68676b6b88-l7b5g 1/1 Running 0 16mNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/coredns ClusterIP 10.254.0.2 &lt;none&gt; 53/UDP,53/TCP 16mNAME DESIRED CURRENT READY AGEreplicaset.extensions/coredns-68676b6b88 1 1 1 16m# kubectl get pods -o wide -n kube-systemNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEScoredns-68676b6b88-l7b5g 1/1 Running 0 40m 10.254.28.2 172.21.16.248 &lt;none&gt; &lt;none&gt; 2、安装和配置dashboard官方配置文件kubernetes/cluster/addons/dashboard，这里已经修改过了，经过测试部署，直接进入dashboard目录，修改inages参数进行部署","pubDate":"Sat, 10 Aug 2019 03:25:12 GMT","guid":"https://xxlaila.github.io/2019/08/10/kubedns插件配置/","category":"kube-dns,kubernetes, dashboard, kube-dashboard"},{"title":"kubernetes node节点安装","link":"https://xxlaila.github.io/2019/08/10/kubernetes-node节点安装/","description":"More: master节点安装请参考 1、部署kubernetes node节点Kubernetes node节点包含如下组件： Flanneld: 之前单机节点安装没有配置TLS，现在需要在service配置文件中增加TLS配置 Docker: version 18.06.2-ce kubelet kube-proxy node节点需要的文件 1234# ls /etc/kubernetes/sslbootstrap.kubeconfig kube-proxy.kubeconfig ssl token.csv# ls /etc/kubernetesadmin-key.pem admin.pem ca-key.pem ca.pem kube-proxy-key.pem kube-proxy.pem kubernetes-csr.json kubernetes-key.pem kubernetes.pem 1.1、安装flanneld123# mv kubernetes /etc/ &amp;&amp; chown -R root: /etc/kubernetes# wget https://github.com/coreos/flannel/releases/download/v0.11.0/flannel-v0.11.0-linux-amd64.tar.gz# tar zxf flannel-v0.11.0-linux-amd64.tar.gz &amp;&amp; mv flanneld mk-docker-opts.sh /usr/bin/ &amp;&amp; rm -rf flannel-v0.11.0-linux-amd64.tar.gz 1.1.1、flanneld启动配置文件1234567891011121314151617# cat /lib/systemd/system/flanneld.service[Unit]Description=Flanneld overlay address etcd agentAfter=network.targetAfter=network-online.targetWants=network-online.targetAfter=etcd.serviceBefore=docker.service[Service]Type=notifyEnvironmentFile=/etc/sysconfig/flanneldExecStart=/usr/bin/flanneld -etcd-endpoints=$&#123;FLANNEL_ETCD&#125; $FLANNEL_OPTIONSExecStartPost=/usr/bin/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/subnet.envRestart=on-failure[Install]WantedBy=multi-user.targetRequiredBy=docker.service","pubDate":"Fri, 09 Aug 2019 22:56:05 GMT","guid":"https://xxlaila.github.io/2019/08/10/kubernetes-node节点安装/","category":"kubernetes, kubernetes v13.3安装"},{"title":"kubernetes v1.13.3安装","link":"https://xxlaila.github.io/2019/08/09/kubernetes-v1-13-3安装/","description":"1、 环境准备 ip type docker os k8s version 172.21.17.4 master,etcd CentOS Linux release 7.4.1708 v1.13.3 172.21.16.230 master,etcd CentOS Linux release 7.4.1708 172.21.16.240 master,etcd CentOS Linux release 7.4.1708 172.21.16.244 node,flanneld,ha+kee 18.06.2-ce CentOS Linux release 7.4.1708 172.21.16.248 node,flanneld,ha+kee 18.06.2-ce CentOS Linux release 7.4.1708 172.21.16.45 vip CentOS Linux release 7.4.1708 2、增加docker 源123# yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo 2.1、根据实际查找当前版本 (可选) 1# yum list docker-ce --showduplicates | sort -r 2.2.如果确定了版本,直接安装,如果要装17。03直接修改下面数字即可 1# yum -y install docker-ce-18.06.2.ce-3.el7 # 主意版本填写包名的格式. 2.3 开启docker服务,和开机启动1# systemctl start docker &amp;&amp; systemctl enable docker 3、部署ETC集群","pubDate":"Fri, 09 Aug 2019 14:18:30 GMT","guid":"https://xxlaila.github.io/2019/08/09/kubernetes-v1-13-3安装/","category":"kubernetes"},{"title":"vsftpd安装","link":"https://xxlaila.github.io/2019/08/09/vsftpd安装/","description":"Centos下ftp的安装一般采用的是vsftpd，但是在ftp的模式中又有几个用户配置项需要注意，有些人喜欢用本地用户去登陆FTP，虽然在建立本地用户的时候加了/sbin/nologin参数，但是这个还是不够安全，而且这样权限控制也不是很好，他们都是统一的控制权限，这里采用虚拟用户前来配置。虚拟用户配合防火墙selinux还有单个用户的权限，这使得FTP有着足够的安全。而且权限控制特别灵活，修改一个用户的权限不会影响到其他用户。centos 系统版本(5.5、5.3、6.0、6.5) 首先我们安装vsftpd1[root@RAID1 ~]# yum –y install vsftpd 2、启动和加载vsftp12[root@RAID1 ~]# service vsftpd restart[root@RAID1 ~]# chkconfig –level 35 vsftpd on 3、开始配置vsftpdVsftpd的配置文件在/etc/vsftpd下面，在配置之前我们先cp一份做备份用以免发生意外(做什么都要随手备份，因为没有一万，只有万一。) 12[root@RAID1 ~]# cp /etc/vsftpd/vsftpd.conf /etc/vsftpd/vsftpd.conf.bak[root@RAID1 ~]# vim /etc/vsftpd/vsftpd.conf vsftpd的参数介绍","pubDate":"Fri, 09 Aug 2019 13:52:47 GMT","guid":"https://xxlaila.github.io/2019/08/09/vsftpd安装/","category":"vsftpd"},{"title":"TeamViewer mac破解","link":"https://xxlaila.github.io/2019/08/09/TeamViewer-mac破解/","description":"TeamViewer14.4 MAC破解在终端执行以下命令12sudo python TeamViewer-id-changer.py使用mac自带python2.7 执行即可","pubDate":"Fri, 09 Aug 2019 12:25:56 GMT","guid":"https://xxlaila.github.io/2019/08/09/TeamViewer-mac破解/","category":"TeamViewer"},{"title":"jenkins job管理","link":"https://xxlaila.github.io/2019/08/09/jenkins-job管理/","description":"需求 介绍: 由于公司的ci用于编译的环境比较多，为了更好的区分，为每一个环境建立了一个view 痛点: 运维人员在建立job的时候需要到对应的view下面建立，虽然这不是狠痛苦，但是还是不太方便。 解决: 人员登陆默认是在all view下面，每个运维人员在这下面建立job，然后每个view根据自己的规则吧对应的job添加进来。job规则自己提前定义好 1、安装jenkins插件view job 过滤插件view-job-filters，安装过程不累赘 2、配置view规则这里设置两个前端和一个后端实例","pubDate":"Fri, 09 Aug 2019 11:36:49 GMT","guid":"https://xxlaila.github.io/2019/08/09/jenkins-job管理/","category":"jenkins, jenkins job,"},{"title":"jenkins用户权限配置","link":"https://xxlaila.github.io/2019/08/09/jenkins用户权限配置/","description":"1、jenkins用户权限 可以集成gitlab、jenkins专有账户、LDAP、Servlet容器代理、Unix用户/组数据库 2、授权策略 Gitlab Commiter Authorization Strategy Role-Based Strategy 任何用户可以做任何事(没有任何限制) 安全矩阵 登录用户可以做任何事 遗留模式 项目矩阵授权策略 3、插件安装安装插件：Role-based Authorization Strategy","pubDate":"Fri, 09 Aug 2019 08:05:59 GMT","guid":"https://xxlaila.github.io/2019/08/09/jenkins用户权限配置/","category":"jenkins, ci/cd"},{"title":"mongodb_replica","link":"https://xxlaila.github.io/2019/08/09/mongodb-replica/","description":"Mongodb replica set安装加认证，这里使用的是keyFile进行认证，之前看过很多文章，坑一大堆，这里是看了两天的官方文档进行的安装，并用户生产，配置文件参数贴一部分,三个带有数据集的节点组成的复制集拥有，架构图如下，参考官方 一个主节点，两个从节点，这两个从节点都可以在选举中升级为主节点 环境三台服务器 123primary: 192.168.32.7secaodray: 192.168.32.11secondary: 192.168.32.14 1、安装mongodb1.1、每个节点都需要操作123456789# sudo vim /etc/yum.repos.d/mongodb-enterprise.repo[mongodb-enterprise]name=MongoDB Enterprise Repositorybaseurl=https://repo.mongodb.com/yum/redhat/$releasever/mongodb-enterprise/3.4/$basearch/gpgcheck=1enabled=1gpgkey=https://www.mongodb.org/static/pgp/server-3.4.asc # sudo yum install -y mongodb-enterprise 注意：如果采用源码包方式安装需要安装一下插件","pubDate":"Fri, 09 Aug 2019 06:46:44 GMT","guid":"https://xxlaila.github.io/2019/08/09/mongodb-replica/","category":"数据库"},{"title":"Hello World","link":"https://xxlaila.github.io/2019/08/09/hello-world/","description":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","pubDate":"Fri, 09 Aug 2019 03:55:49 GMT","guid":"https://xxlaila.github.io/2019/08/09/hello-world/","category":""}]}