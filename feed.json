{"title":"懒羊羊","description":"星际拾荒者","language":"zh-CN","link":"https://xxlaila.github.io","pubDate":"Wed, 25 Sep 2019 01:46:10 GMT","lastBuildDate":"Wed, 25 Sep 2019 01:57:35 GMT","generator":"hexo-generator-json-feed","webMaster":"xxlaila","items":[{"title":"pv pvc","link":"https://xxlaila.github.io/2019/09/25/pv-pvc/","description":"1、介绍&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PersistentVolume（pv）和PersistentVolumeClaim（pvc）是k8s提供的两种API资源，用于抽象存储细节。管理员关注于如何通过pv提供存储功能而无需关注用户如何使用，同样的用户只需要挂载pvc到容器中而不需要关注存储卷采用何种技术实现。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pvc和pv的关系与pod和node关系类似，前者消耗后者的资源。pvc可以向pv申请指定大小的存储资源并设置访问模式,这就可以通过Provision -&gt; Claim 的方式，来对存储资源进行控制。 2、生命周期pv和pvc遵循以下生命周期: 供应准备。通过集群外的存储系统或者云平台来提供存储持久化支持。 静态提供: 管理员手动创建多个PV，供PVC使用。 动态提供: 动态创建PVC特定的PV，并绑定。 绑定。用户创建pvc并指定需要的资源和访问模式。在找到可用pv之前，pvc会保持未绑定状态。 使用。用户可在pod中像volume一样使用pvc。 释放。用户删除pvc来回收存储资源，pv将变成“released”状态。由于还保留着之前的数据，这些数据需要根据不同的策略来处理，否则这些存储资源无法被其他pvc使用。 回收(Reclaiming)。pv可以设置三种回收策略：保留（Retain），回收（Recycle）和删除（Delete）。 保留策略: 允许人工处理保留的数据。 删除策略: 将删除pv和外部关联的存储资源，需要插件支持。 回收策略: 将执行清除操作，之后可以被新的pvc使用，需要插件支持。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;目前只有NFS和HostPath类型卷支持回收策略，AWS EBS,GCE PD,Azure Disk和Cinder支持删除(Delete)策略。 2.1、Provisioning两种方式提供的PV资源供给： static:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过集群管理者创建多个PV，为集群“使用者”提供存储能力而隐藏真实存储的细节。并且存在于kubenretes api中，可被直接使用。 dynamic:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;动态卷供给是kubernetes独有的功能，这一功能允许按需创建存储建。在此之前，集群管理员需要事先在集群外由存储提供者或者云提供商创建&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;存储卷，成功之后再创建PersistentVolume对象，才能够在kubernetes中使用。动态卷供给能让集群管理员不必进行预先创建存储卷，而是随着用户需求进行创建。在1.5版本提高了动态卷的弹性和可用性。 PV类型pv支持以下类型: GCEPersistentDisk AWSElasticBlockStore NFS iSCSI RBD (Ceph Block Device) Glusterfs AzureFile AzureDisk CephFS cinder FC FlexVolume Flocker PhotonPersistentDisk Quobyte VsphereVolume HostPath (single node testing only – local storage is not supported in any way and WILL NOT WORK in a multi-node cluster) 3.1、PV属性 访问模式,与pv的语义相同。在请求资源时使用特定模式。 资源,申请的存储资源数额。 3.2、PV卷阶段状态 Available – 资源尚未被claim使用 Bound – 卷已经被绑定到claim了 Released – claim被删除，卷处于释放状态，但未被集群回收。 Failed – 卷自动回收失败","pubDate":"Wed, 25 Sep 2019 01:46:10 GMT","guid":"https://xxlaila.github.io/2019/09/25/pv-pvc/","category":"kubernetes"},{"title":"利用NFS动态提供Kubernetes后端存储卷","link":"https://xxlaila.github.io/2019/09/24/利用NFS动态提供Kubernetes后端存储卷/","description":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nfs-client-provisioner是一个automatic provisioner，使用NFS作为存储，自动创建PV和对应的PVC，本身不提供NFS存储，需要外部先有一套NFS存储服务。 PV以 ${namespace}-${pvcName}-${pvName}的命名格式提供（在NFS服务器上） PV回收的时候以 archieved-${namespace}-${pvcName}-${pvName} 的命名格式（在NFS服务器上） 官方访问地址 1、权限体系构建1.1、创建serviceaccountServiceAccount也是一种账号, 供运行在pod中的进程使用, 为pod中的进程提供必要的身份证明 1234567$ cat &gt; serviceaccount.yaml &lt;&lt;EOFapiVersion: v1kind: ServiceAccountmetadata: name: nfs-client-provisioner namespace: kube-opsEOF 1.2、创建role123456789101112131415161718192021222324252627$ cat &gt;clusterrole.yaml&lt;&lt;EOFkind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: nfs-client-provisioner-runner namespace: kube-opsrules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"watch\", \"create\", \"update\", \"patch\"] - apiGroups: [\"\"] resources: [\"services\", \"endpoints\"] verbs: [\"get\", \"create\",\"list\", \"watch\",\"update\"] - apiGroups: [\"extensions\"] resources: [\"podsecuritypolicies\"] resourceNames: [\"nfs-client-provisioner\"] verbs: [\"use\"]EOF 1.3、账户和角色绑定123456789101112131415$ cat &gt;clusterrolebinding.yaml &lt;&lt;EOFkind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: namespace: kube-ops name: run-nfs-client-provisionersubjects: - kind: ServiceAccount name: nfs-client-provisioner namespace: kube-opsroleRef: kind: ClusterRole name: nfs-client-provisioner-runner apiGroup: rbac.authorization.k8s.ioEOF 1.4、执行创建1234$ kubectl create -f serviceaccount.yaml -f clusterrole.yaml -f clusterrolebinding.yamlserviceaccount/nfs-client-provisioner createdclusterrole.rbac.authorization.k8s.io/nfs-client-provisioner-runner createdclusterrolebinding.rbac.authorization.k8s.io/run-nfs-client-provisioner created 2、安装部署&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;下载deployment.yaml文件,需要修改NFS服务器所在的IP地址（10.10.10.60），以及NFS服务器共享的路径（/ifs/kubernetes），两处都需要修改为你实际的NFS服务器和共享目录 2.1、部署存储供应卷根据PVC的请求, 动态创建PV存储. 1234567891011121314151617181920212223242526272829303132333435363738394041$ cat &gt; deployment.yaml &lt;&lt;EOFapiVersion: v1kind: ServiceAccountmetadata: namespace: kube-ops name: nfs-client-provisioner---kind: DeploymentapiVersion: extensions/v1beta1metadata: namespace: kube-ops name: nfs-client-provisionerspec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner image: quay.io/external_storage/nfs-client-provisioner:latest volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: fuseim.pri/ifs - name: NFS_SERVER value: 172.21.17.39 - name: NFS_PATH value: /data volumes: - name: nfs-client-root nfs: server: /opt path: 172.21.17.39EOF 修改StorageClass文件并部署class.yaml 此处可以不修改，或者修改provisioner的名字，需要与上面的deployment的PROVISIONER_NAME名字一致 2.2、创建storageclass123456789$ cat &gt; class.yaml &lt;&lt;EOFapiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: managed-nfs-storageprovisioner: fuseim.pri/ifs # or choose another name, must match deployment's env PROVISIONER_NAME'parameters: archiveOnDelete: \"false\"EOF 2.3、执行创建123456$ kubectl apply -f deployment.yaml serviceaccount/nfs-client-provisioner createddeployment.extensions/nfs-client-provisioner created$ kubectl apply -f class.yaml storageclass.storage.k8s.io/managed-nfs-storage created 2.3.1、查看StorageClass123$ kubectl get storageclassNAME PROVISIONER AGEmanaged-nfs-storage fuseim.pri/ifs 18s 2.3.2、设置默认后端存储设置这个default名字的SC为Kubernetes的默认存储后端 12kubectl patch storageclass managed-nfs-storage -p '&#123;\"metadata\": &#123;\"annotations\":&#123;\"storageclass.kubernetes.io/is-default-class\":\"true\"&#125;&#125;&#125;'storageclass.storage.k8s.io/managed-nfs-storage patched storage.yaml (和上面一样)1234567891011$ cat &gt; storage.yaml &lt;&lt;EOFapiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: managed-nfs-storage annotations: storageclass.kubernetes.io/is-default-class: \"true\"provisioner: fuseim.pri/ifsparameters: archiveOnDelete: \"false\"EOF 2.3.3、查看验证123456789$ kubectl get all -n kube-opsNAME READY STATUS RESTARTS AGEpod/nfs-client-provisioner-77f678858b-8d2d6 1/1 Running 0 26mNAME READY UP-TO-DATE AVAILABLE AGEdeployment.apps/nfs-client-provisioner 1/1 1 1 29mNAME DESIRED CURRENT READY AGEreplicaset.apps/nfs-client-provisioner-77f678858b 1 1 1 26m 3、验证测试3.1、创建一个测试存储123456789101112131415$ cat &gt; test-claim.yaml &lt;&lt;EOFkind: PersistentVolumeClaimapiVersion: v1metadata: name: test-claim namespace: kube-ops annotations: volume.beta.kubernetes.io/storage-class: \"managed-nfs-storage\"spec: accessModes: - ReadWriteMany resources: requests: storage: 1MiEOF 3.2、启动测试PODPOD文件如下，作用就是在test-claim的PV里touch一个SUCCESS文件 1234567891011121314151617181920212223$ cat &gt; test-pod.yaml &lt;&lt;EOFkind: PodapiVersion: v1metadata: name: test-pod namespace: kube-opsspec: containers: - name: test-pod image: docker.io/busybox:1.24 command: - \"/bin/sh\" args: - \"-c\" - \"touch /mnt/SUCCESS &amp;&amp; exit 0 || exit 1\" volumeMounts: - name: nfs-pvc mountPath: \"/mnt\" restartPolicy: \"Never\" volumes: - name: nfs-pvc persistentVolumeClaim: claimName: test-claim 3.3、执行创建123$ kubectl apply -f ./persistentvolumeclaim/test-claim createdpod/test-pod created 3.4、查看验证1234$ kubectl get pod -n kube-opsNAME READY STATUS RESTARTS AGEnfs-client-provisioner-77f678858b-8d2d6 1/1 Running 0 35mtest-pod 0/1 Completed 0 54s 登录nfs服务器查看是否成功的创建目录12$ ls /opt/kube-ops-test-claim-pvc-2f0057b0-df35-11e9-ad62-fa163e53d4c8 3.5、更改PersistentVolumes 中的一个回收策略 查看集群中PersistentVolumes 123$ kubectl get pv -n kube-opsNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEpvc-2f0057b0-df35-11e9-ad62-fa163e53d4c8 1Mi RWX Delete Bound kube-ops/test-claim managed-nfs-storage 3m6s 更改PersistentVolumes 123456$ kubectl patch pv pvc-2f0057b0-df35-11e9-ad62-fa163e53d4c8 -p '&#123;\"spec\":&#123;\"persistentVolumeReclaimPolicy\":\"Retain\"&#125;&#125;'persistentvolume/pvc-2f0057b0-df35-11e9-ad62-fa163e53d4c8 patched$ kubectl get pv -n kube-opsNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEpvc-2f0057b0-df35-11e9-ad62-fa163e53d4c8 1Mi RWX Retain Bound kube-ops/test-claim managed-nfs-storage 3m54s","pubDate":"Tue, 24 Sep 2019 09:53:32 GMT","guid":"https://xxlaila.github.io/2019/09/24/利用NFS动态提供Kubernetes后端存储卷/","category":"kubernetes"},{"title":"k8s v1.14 prometheus","link":"https://xxlaila.github.io/2019/09/20/k8s-v1-14-prometheus/","description":"Prometheus、Grafana 部署&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Grafana是一个开源的度量分析与可视化套件。经常被用作基础设施的时间序列数据和应用程序分析的可视化，我们这里用它来做Kubernetes集群监控数据的可视化。 准备工作&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;截至当前，prometheus、grafana均采用最新的镜像包，在在第一次部署的时候grafana报了一个错误mkdir: cannot create directory &#39;/var/lib/grafana/plugins&#39;: No such file or directory,这是因为Grafana启动使用的用户和用户组都是472，造成对外挂存储没有权限。参考官方","pubDate":"Fri, 20 Sep 2019 08:12:48 GMT","guid":"https://xxlaila.github.io/2019/09/20/k8s-v1-14-prometheus/","category":"kubenertes"},{"title":"k8s v1.14 weave-scope","link":"https://xxlaila.github.io/2019/09/20/k8s-v1-14-weave-scope/","description":"前沿&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Kubernetes 集群并部署容器化应用只是第一步。一旦集群运行起来，我们需要确保一起正常，所有必要组件就位并各司其职，有足够的资源满足应用的需求。Kubernetes 是一个复杂系统，运维团队需要有一套工具帮助他们获知集群的实时状态，并为故障排查提供及时和准确的数据支持。 weave scope 介绍&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Weave Scope是Docker和Kubernetes的可视化和监控工具。它提供了一个自上而下的应用程序以及整个基础架构视图，并允许您在部署到云提供商时实时诊断分布式容器化应用程序的任何问题。","pubDate":"Fri, 20 Sep 2019 03:49:59 GMT","guid":"https://xxlaila.github.io/2019/09/20/k8s-v1-14-weave-scope/","category":"kubernetes"},{"title":"k8s v1.14 traefik部署","link":"https://xxlaila.github.io/2019/09/20/k8s-v1-14-traefik部署/","description":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;traefix 前篇是可以使用，这里k8s v1.14 之前的拿来用不上，然后折腾了一下，参考官方的折腾起来了 基于角色的访问控制配置（仅限Kubernetes 1.6+）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Kubernetes在1.6+中引入了基于角色的访问控制（RBAC），以允许对Kubernetes资源和API进行细粒度控制。群集配置了RBAC，则需要授权Traefik使用Kubernetes API。有两种方法可以设置适当的权限：通过特定于命名空间的RoleBindings或单个全局ClusterRoleBinding。","pubDate":"Fri, 20 Sep 2019 01:20:25 GMT","guid":"https://xxlaila.github.io/2019/09/20/k8s-v1-14-traefik部署/","category":"kubernetes"},{"title":"k8s v1.14 metrics-server","link":"https://xxlaila.github.io/2019/09/17/k8s-v1-14-metrics-server/","description":"metrics-server这里不详细介绍，可以参考metrics-server安装季 安装metrics-server&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里安装和之前的metrics-server安装季稍微有点不一样，之前集群安装没有使用https证书，后面去各种生成的证书和踩坑，这里是在安装的时候一开始就使用了https全证书,所有稍微有一点区别，这里只列出有区别的地方，其他的完全可以参考metrics-server安装季，这里https证书不需要重新生成；","pubDate":"Tue, 17 Sep 2019 01:06:18 GMT","guid":"https://xxlaila.github.io/2019/09/17/k8s-v1-14-metrics-server/","category":"kubernetes"},{"title":"k8s v1.14 dashboard","link":"https://xxlaila.github.io/2019/09/16/k8s-v1-14-dashboard/","description":"kuberntes 自带插件的 manifests yaml 文件使用 gcr.io 的 docker registry，国内被墙，需要手动替换为其它 registry 地址 修改配置文件将下载的 kubernetes-server-linux-amd64.tar.gz 解压后，再解压其中的 kubernetes-src.tar.gz 文件。 12# cd kubernetes# tar -xzvf kubernetes-src.tar.gz dashboard 对应的目录是：cluster/addons/dashboard： 1# cd cluster/addons/dashboard 修改 service 定义，指定端口类型为 NodePort，这样外界可以通过地址 NodeIP:NodePort 访问 dashboard；","pubDate":"Mon, 16 Sep 2019 09:46:10 GMT","guid":"https://xxlaila.github.io/2019/09/16/k8s-v1-14-dashboard/","category":"kubernetes"},{"title":"k8s v1.14 dns插件","link":"https://xxlaila.github.io/2019/09/16/k8s-v1-14-dns插件/","description":"部署 coredns 插件注意: kuberntes 自带插件的 manifests yaml 文件使用 gcr.io 的 docker registry，国内被墙，需要手动替换为其它 registry 地址; 修改配置文件将下载的 kubernetes-server-linux-amd64.tar.gz 解压后，再解压其中的 kubernetes-src.tar.gz 文件。 12# cd kubernetes# tar -xzvf kubernetes-src.tar.gz","pubDate":"Mon, 16 Sep 2019 09:37:06 GMT","guid":"https://xxlaila.github.io/2019/09/16/k8s-v1-14-dns插件/","category":"kubernetes"},{"title":"k8s v1.14集群验证","link":"https://xxlaila.github.io/2019/09/16/k8s-v1-14集群验证/","description":"验证集群功能检查节点状态12345# kubectl get nodesNAME STATUS ROLES AGE VERSION172.21.16.204 Ready &lt;none&gt; 5h50m v1.14.6172.21.16.240 Ready &lt;none&gt; 5h48m v1.14.6172.21.16.87 Ready &lt;none&gt; 5h45m v1.14.6 都为 Ready 时正常。","pubDate":"Mon, 16 Sep 2019 09:21:22 GMT","guid":"https://xxlaila.github.io/2019/09/16/k8s-v1-14集群验证/","category":"kubernetes"},{"title":"kubernetes-v1.14 node安装","link":"https://xxlaila.github.io/2019/09/16/kubernetes-v1-14-node安装/","description":"1、安装docker1.1、增加docker 源123yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo 1.2、安装docker1# yum -y install docker-ce","pubDate":"Mon, 16 Sep 2019 07:42:55 GMT","guid":"https://xxlaila.github.io/2019/09/16/kubernetes-v1-14-node安装/","category":"kubernetes"},{"title":"kubernetes-v1.14安装","link":"https://xxlaila.github.io/2019/09/11/kubernetes-v1-14安装/","description":"1、环境准备 ip type docker os k8s version 172.21.17.30 master,etcd CentOS Linux release 7.4.1708 v1.14.6 172.21.17.31 master,etcd CentOS Linux release 7.4.1708 172.21.16.110 master,etcd CentOS Linux release 7.4.1708 172.21.16.87 node,flanneld 18.06.2-ce CentOS Linux release 7.4.1708 172.21.16.240 node,flanneld,ha+kee 18.06.2-ce CentOS Linux release 7.4.1708 172.21.16.204 node,flanneld,ha+kee 18.06.2-ce CentOS Linux release 7.4.1708 172.21.16.45 vip CentOS Linux release 7.4.1708 2、初始化系统2.1、安装依赖包","pubDate":"Wed, 11 Sep 2019 08:20:26 GMT","guid":"https://xxlaila.github.io/2019/09/11/kubernetes-v1-14安装/","category":"kubernetes"},{"title":"路由器端口映射","link":"https://xxlaila.github.io/2019/09/10/路由器端口映射/","description":"好记性不如烂笔头，h3c MSR3620路由器做端口映射到后端服务器,包含单个端口和端口段的映射","pubDate":"Tue, 10 Sep 2019 00:56:24 GMT","guid":"https://xxlaila.github.io/2019/09/10/路由器端口映射/","category":"网络设备"},{"title":"traefik https应用","link":"https://xxlaila.github.io/2019/09/06/traefik-https应用/","description":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;之前已经使用traefik服务作为入口，测试并访问了tomcat应用，之前是通过http来访问的，而我们在yaml文件里面也添加8443端口用于https访问，在实际环境中我们也是需要https来进行访问应用，通过traefik实现https，traefik http应用 操作实践 这里我用了公司的证书，就是为了贴近真实，也满足测试需求， 创建一个secret，保存https证书","pubDate":"Fri, 06 Sep 2019 03:31:09 GMT","guid":"https://xxlaila.github.io/2019/09/06/traefik-https应用/","category":"kubernetes"},{"title":"traefik ingress使用","link":"https://xxlaila.github.io/2019/09/05/traefik-ingress使用/","description":"Traefik介绍&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;简单的说，ingress就是从kubernetes集群外访问集群的入口，将用户的URL请求转发到不同的service上。Ingress相当于nginx、apache等负载均衡反向代理服务器，其中还包括规则定义，即URL的路由信息。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Traefik是一款开源的反向代理与负载均衡工具。它最大的优点是能够与常见的微服务系统直接整合，实现自动化动态配置。Traefik通过不断地跟 kubernetes API 打交道，实时的感知后端 service、pod 等变化，比如pod，service 增加与减少等；当得到这些变化信息后，Ingress自动更新配置并热重载 ，达到服务发现的作用。","pubDate":"Thu, 05 Sep 2019 09:16:25 GMT","guid":"https://xxlaila.github.io/2019/09/05/traefik-ingress使用/","category":"kubernetes"},{"title":"k8s-helm","link":"https://xxlaila.github.io/2019/09/04/k8s-helm/","description":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Helm类似与linux下面的yum，Helm是一个用于kubernetes的包管理器，每一个包为一个chart，一个chart是一个目录，常常会对目录进行打包压缩，形成一个${name}-version.tgz的格式进行传输和存储。 对于应用发布者而言，可以通过Helm打包应用，管理应用依赖关系，管理应用版本并发布应用到软件仓库。 对于使用者而言，使用Helm后不用需要了解Kubernetes的Yaml语法并编写应用部署文件，可以通过Helm下载并在kubernetes上安装需要的应用。 Helm还提供了kubernetes上的软件部署，删除，升级，回滚应用的强大功能","pubDate":"Wed, 04 Sep 2019 10:52:26 GMT","guid":"https://xxlaila.github.io/2019/09/04/k8s-helm/","category":"kubernetes"},{"title":"metrics-server安装季","link":"https://xxlaila.github.io/2019/09/04/metrics-server安装季/","description":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;metrics-server 通过 kube-apiserver 发现所有节点，然后调用 kubelet APIs（通过 https 接口）获得各节点（Node）和 Pod 的 CPU、Memory 等资源使用情况。Kubernetes 1.12 开始，kubernetes 的安装脚本移除了 Heapster，从 1.13 开始完全移除了对 Heapster 的支持，Heapster 不再被维护。 替代方案如下: 用于支持自动扩缩容的 CPU/memory HPA metrics：metrics-server 通用的监控方案：使用第三方可以获取 Prometheus 格式监控指标的监控系统，如 Prometheus Operator 事件传输：使用第三方工具来传输、归档 kubernetes events &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;使用 metrics-server 替代 Heapster，将无法在 dashboard 中以图形展示 Pod 的内存和 CPU 情况，需要通过 Prometheus、Grafana 等监控方案来弥补。","pubDate":"Wed, 04 Sep 2019 05:57:07 GMT","guid":"https://xxlaila.github.io/2019/09/04/metrics-server安装季/","category":"kubernetes"},{"title":"kubelet提供api请求接口","link":"https://xxlaila.github.io/2019/09/04/kubelet提供api请求接口/","description":"kubelet 提供的 API 接口认证node安装参考 kubelet 启动后监听多个端口，用于接收 kube-apiserver 或其它客户端发送的请求： 1234[root@k8s-3 ~]# netstat -lnpt|grep kubelettcp 0 0 127.0.0.1:46395 0.0.0.0:* LISTEN 8941/kubelet tcp 0 0 127.0.0.1:10248 0.0.0.0:* LISTEN 8941/kubelet tcp6 0 0 :::10250 :::* LISTEN 8941/kubelet 10248: healthz http 服务 10250: https 服务，访问该端口时需要认证和授权（即使访问 /healthz 也需要） 未开启只读端口 10255 从 K8S v1.10 开始，去除了 –cadvisor-port 参数（默认 4194 端口），不支持访问 cAdvisor UI &amp; API","pubDate":"Wed, 04 Sep 2019 02:04:30 GMT","guid":"https://xxlaila.github.io/2019/09/04/kubelet提供api请求接口/","category":"kubernetes"},{"title":"centos-nfs-512错误","link":"https://xxlaila.github.io/2019/09/03/centos-nfs-512错误/","description":"nfs 错误kernel: NFS: nfs4_discover_server_trunking unhandled error -512. Exiting with error EIO &nbsp;&nbsp;&nbsp;&nbsp;很久没挂载过nfs，忘记客户端怎么挂在nfs的了，服务端很早就安装好了，今天一台客户机需要挂载nfs，然后居然报错了，然后找了一圈居然没找到怎么解决，然后又重新看了一次centos nfs的配置，于是乎就搞定了 在挂载nfs的提示很慢，长时间无响应，强行结束看看是什么问题，查看日志 12$ sudo tail -f /var/log/messagesSep 3 11:23:51 dev-application kernel: NFS: nfs4_discover_server_trunking unhandled error -512. Exiting with error EIO","pubDate":"Tue, 03 Sep 2019 03:29:13 GMT","guid":"https://xxlaila.github.io/2019/09/03/centos-nfs-512错误/","category":"centos"},{"title":"k8s集群部署heapster","link":"https://xxlaila.github.io/2019/09/02/k8s集群部署heapster/","description":"kubernetes集群启用tls认证部署heapster，在部署期间遇到了很多的坑，走过很多雷，这里记录一下,不过在新版本中heapster被metrics-server代替了，metrics-server后篇介绍和使用 1、准备工作下载需要的文件，这里用之前k8s-heapster部署的文件拿来进行修改，文件地址 2、执行文件创建12# cd kubernetes-yaml/heapster-influxdb-grafana# kubectl apply -f ./","pubDate":"Mon, 02 Sep 2019 11:25:03 GMT","guid":"https://xxlaila.github.io/2019/09/02/k8s集群部署heapster/","category":"kubernetes"},{"title":"k8s部署istio","link":"https://xxlaila.github.io/2019/08/30/k8s部署istio/","description":"istio 介绍&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;istio代表的是Service Mesh的方案实现，Istio 有助于降低这些部署的复杂性，并减轻开发团队的压力。提供一种简单的方式来为已部署的服务建立网络，且提供具有负载均衡、服务间认证、监控、流量管理等功能。 服务网格（Service Mesh）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;服务网格（Service Mesh）用于描述构成这些应用程序的微服务网络以及应用之间的交互。随着规模和复杂性的增长，服务网格越来越难以理解和管理。它的需求包括服务发现、负载均衡、故障恢复、指标收集和监控以及通常更加复杂的运维需求，例如 A/B 测试、金丝雀发布、限流、访问控制和端到端认证等。而istio刚好提供了一套完整的解决方案，通过控制整个服务器网格提供行为洞察和操作控制来满足微服务应用的多样化 架构Istio 服务网格逻辑上分为数据平面和控制平面。","pubDate":"Fri, 30 Aug 2019 03:21:08 GMT","guid":"https://xxlaila.github.io/2019/08/30/k8s部署istio/","category":"istio"},{"title":"k8s配置Dashboard","link":"https://xxlaila.github.io/2019/08/29/k8s配置Dashboard/","description":"&nbsp;&nbsp;&nbsp;&nbsp;K8S Dashboard是官方的一个基于WEB的用户界面，专门用来管理K8S集群，并可展示集群的状态。K8S集群安装好后默认没有包含Dashboard，我们需要额外创建它。 1、安装dashboard1.1、下载准备需要的文件经过修改过后的文件，已经可以正常使用的文件","pubDate":"Thu, 29 Aug 2019 09:57:46 GMT","guid":"https://xxlaila.github.io/2019/08/29/k8s配置Dashboard/","category":"kubernetes"},{"title":"k8s删除node重新加入","link":"https://xxlaila.github.io/2019/08/29/k8s删除node重新加入/","description":"&nbsp;&nbsp;&nbsp;&nbsp;有时候k8s node 在加入集群的时候不经意的时候弄错啦某些东西，这时候可以把这个node删除，然后重新加入，删除节点之前我们需要做一下常规化的操作，来保障运行在该节点的pod迁移到其他的node上。","pubDate":"Thu, 29 Aug 2019 08:27:00 GMT","guid":"https://xxlaila.github.io/2019/08/29/k8s删除node重新加入/","category":"常用命令"},{"title":"Centos route策略","link":"https://xxlaila.github.io/2019/08/28/Centos-route策略/","description":"场景:&nbsp;&nbsp;&nbsp;&nbsp;公司业务在一个新的云平台上线，该云平台使用的是比较传统的VMware vSphere来做的虚拟化，而且该云平台网络也是我们不清楚的，反正就是不能设置(不能像现在主流云平台自定义网络或者是地址段)，是一个政府的云平台，具体各种奇葩的限制就不说啦，你懂的……","pubDate":"Wed, 28 Aug 2019 02:25:46 GMT","guid":"https://xxlaila.github.io/2019/08/28/Centos-route策略/","category":"Centos"},{"title":"k8s部署ingress","link":"https://xxlaila.github.io/2019/08/26/k8s部署ingress/","description":"在kubernetes 集群中，一个服务安装以后怎么对外提供访问，外部用户怎么来访问我们容器中业务。 1、Ingress 介绍&nbsp;&nbsp;&nbsp;&nbsp;Kubernetes 暴露服务的方式目前只有三种：LoadBlancer Service、NodePort Service、Ingress；本文主要通过Ingress来访问 2、Ingress 是什么&nbsp;&nbsp;&nbsp;&nbsp;Ingress 就是能利用 Nginx、Haproxy 啥的负载均衡器暴露集群内服务的工具问题来了，集群内服务想要暴露出去面临着几个问题：","pubDate":"Mon, 26 Aug 2019 08:24:55 GMT","guid":"https://xxlaila.github.io/2019/08/26/k8s部署ingress/","category":"kubernetes, Ingress"},{"title":"k8s部署Weave Scope","link":"https://xxlaila.github.io/2019/08/26/k8s部署Weave-Scope/","description":"1、Weave Scope介绍&nbsp;&nbsp;&nbsp;&nbsp;weave scope 是Docker和Kubernetes的故障排除和监控，自动生成应用程序的地图，能够直观地了解，监控和控制基于容器的，基于微服务的应用程序。可以实时的了解docker容器，选择容器基础架构的概述，或关注特定的微服务。轻松识别和纠正问题，确保集装箱化应用的稳定性和性能，查看容器的上下文指标，标记和元数据。轻松地在容器内的进程之间导航，以运行容器，在可扩展的可排序表中进行排列。使用给定主机或服务的最大CPU或内存轻松找到容器。直接与您的容器交互：暂停，重新启动和停止容器。启动命令行。全部不离开范围浏览器窗口。 2、部署weave scope初次学习，直接下载官方配置文件，没有经过任何修改，不过相信自己随着学习的进步，会逐渐深入。github地址，官方地址","pubDate":"Mon, 26 Aug 2019 08:08:15 GMT","guid":"https://xxlaila.github.io/2019/08/26/k8s部署Weave-Scope/","category":"kubernetes,Weave Scope"}]}