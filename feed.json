{"title":"懒羊羊","description":"知识库","language":"zh-CN","link":"https://xxlaila.github.io","pubDate":"Fri, 09 Aug 2019 22:56:05 GMT","lastBuildDate":"Fri, 09 Aug 2019 23:03:54 GMT","generator":"hexo-generator-json-feed","webMaster":"xxlaila","items":[{"title":"kubernetes node节点安装","link":"https://xxlaila.github.io/2019/08/10/kubernetes-node节点安装/","description":"More: master节点安装请参考 1、部署kubernetes node节点Kubernetes node节点包含如下组件： Flanneld: 之前单机节点安装没有配置TLS，现在需要在service配置文件中增加TLS配置 Docker: version 18.06.2-ce kubelet kube-proxy node节点需要的文件1234# ls /etc/kubernetes/sslbootstrap.kubeconfig kube-proxy.kubeconfig ssl token.csv# ls /etc/kubernetesadmin-key.pem admin.pem ca-key.pem ca.pem kube-proxy-key.pem kube-proxy.pem kubernetes-csr.json kubernetes-key.pem kubernetes.pem","pubDate":"Fri, 09 Aug 2019 22:56:05 GMT","guid":"https://xxlaila.github.io/2019/08/10/kubernetes-node节点安装/","category":"kubernetes, kubernetes v13.3安装"},{"title":"kubernetes v1.13.3安装","link":"https://xxlaila.github.io/2019/08/09/kubernetes-v1-13-3安装/","description":"1、 环境准备 ip type docker os k8s version 172.21.17.4 master,etcd CentOS Linux release 7.4.1708 v1.13.3 172.21.16.230 master,etcd CentOS Linux release 7.4.1708 172.21.16.240 master,etcd CentOS Linux release 7.4.1708 172.21.16.244 node,flanneld,ha+kee 18.06.2-ce CentOS Linux release 7.4.1708 172.21.16.248 node,flanneld,ha+kee 18.06.2-ce CentOS Linux release 7.4.1708 172.21.16.45 vip CentOS Linux release 7.4.1708 2、增加docker 源12345678910# yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo2.1、根据实际查找当前版本 (可选)# yum list docker-ce --showduplicates | sort -r2.2、#4.如果确定了版本,直接安装,如果要装17。03直接修改下面数字即可# yum -y install docker-ce-18.06.2.ce-3.el7 # 主意版本填写包名的格式.2.3、#5.开启docker服务,和开机启动# systemctl start docker &amp;&amp; systemctl enable docker 3、部署ETC集群etcd的正常运行是k8s集群运行的提前条件，因此部署k8s集群首先部署etcd集群。安装CA证书，安装CFSSL证书管理工具。直接下载二进制安装包 3.1、下载cfssl12345# curl -LO https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -o cfssl# curl -LO https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -o cfssljson# curl -LO https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 -o cfssl-certinfo# chmod +x *# mv cfssl* /usr/bin/ 3.2、创建CA3.2.1、创建CA配置文件123456789101112131415161718192021222324# mkdir ssl &amp;&amp; cd ssl/# cfssl print-defaults config &gt; config.json# cfssl print-defaults csr &gt; csr.json* 根据config.json文件的格式创建如下的ca-config.json文件* 过期时间设置成了 87600h# cat ca-config.json&#123; \"signing\": &#123; \"default\": &#123; \"expiry\": \"87600h\" &#125;, \"profiles\": &#123; \"kubernetes\": &#123; \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ], \"expiry\": \"87600h\" &#125; &#125; &#125;&#125; 字段说明: ca-config.json: 可以定义多个profiles，分别指定不同的过期时间、使用场景等参数；后续在签发时使用某个porfile； signing: 表示该证书用于签名其他证书；生成ca.pem证书中CA=TRUE server auth: 表示client可以用该CA对server提供的证书进行验证 client auth: 表示server可以用该CA对client提供的证书进行验证 3.2.2、创建CA签名请求1234567891011121314151617# cat ca-csr.json&#123; \"CN\": \"kubernetes\", \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"L\": \"BeiJing\", \"ST\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" &#125; ]&#125; “CN”: Common Name，kube-apiserver 从该证书中提取该字段作为请求的用户名（User Name）;浏览器使用该字段验证网站合法性； “O”: Organization，kube-apiserver从该证书中提取该字段作为请求用户所属组（Group）； 3.2.3、生成CA证书和私匙123# cfssl gencert -initca ca-csr.json | cfssljson -bare ca# ls ca*ca-config.json ca-csr.json ca-key.pem ca.csr ca.pem 3.3、创建kubernetes证书3.3.1、创建kubernetes证书签名请求1234567891011121314151617181920212223242526272829# cat kubernetes-csr.json &#123; \"CN\": \"kubernetes\", \"hosts\": [ \"127.0.0.1\", \"172.21.17.4\", \"172.21.16.231\", \"172.21.16.240\", \"10.254.0.1\", \"172.21.16.45\", \"kubernetes\", \"kubernetes.default\", \"kubernetes.default.svc\", \"kubernetes.default.svc.cluster\", \"kubernetes.default.svc.cluster.local\" ], \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [&#123; \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" &#125;]&#125; 如果hosts字段不为空则需要指定授权使用该证书的ip或域名列表，由于该证书后续被etcd集群和kubenetes master集群使用，所以上面分别指定了etcd集群、kubenetes master集群的主机ip和kubenetes的服务ip（一般kube-apiserver指定的service-cluster-ip-range网段的第一个ip，如10.254.0.1）。 3.3.2、生成kubenetes证书和私钥123# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes# ls kubernetes*kubernetes-csr.json kubernetes-key.pem kubernetes.csr kubernetes.pem 3.3.3、配置admin证书创建admin证书签名请求 123456789101112131415161718# cat admin-csr.json &#123; \"CN\": \"admin\", \"hosts\": [], \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"system:masters\", \"OU\": \"System\" &#125; ]&#125; 后续kube-apiserver 使用RBAC对客户端（如kubectl、kube-proxy、pod）请求进行授权； kube-apiserver预定义一些RBAC使用RoleBindings，如cluster-admin将group system:master与Role cluster-admin绑定，该role授予了调用kube-apiserver的所有api权限； OU指定该证书的Group为system:masters，kubelet使用该证书访问kube-apiserver时，由于证书被CA签名，所以认证通过，同时由于证书用户组为经过授权的system:masters，所以被授权予访问所有的API权限； 3.3.3.1、生成admin证书和私钥123# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin# ls admin*admin-csr.json admin-key.pem admin.csr admin.pem 3.3.3、创建kube-proxy证书创建kube-proxy证书签名请求 123456789101112131415161718# cat kube-proxy-csr.json &#123; \"CN\": \"system:kube-proxy\", \"hosts\": [], \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" &#125; ]&#125; CN指定该证书的User为system:kube-proxy kube-apiserver预定义的RoleBinding cluster-admin将User system:kube-proxy与Role system:node-proxier绑定，该Role授予了调用kube-apiserver Proxy相关的API权限； 生成kube-proxy客户端证书和私钥 1# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy 3.4、校验证书以kuberntes证书为列 3.4.1、使用openssl命令1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# openssl x509 -noout -text -in kubernetes.pemCertificate: Data: Version: 3 (0x2) Serial Number: 79:07:56:50:51:dc:65:6d:09:54:fd:f4:04:42:55:d3:ca:30:ad:b9 Signature Algorithm: sha256WithRSAEncryption Issuer: C=CN, ST=BeiJing, L=BeiJing, O=k8s, OU=System, CN=kubernetes Validity Not Before: Feb 21 03:08:00 2019 GMT Not After : Feb 18 03:08:00 2029 GMT Subject: C=CN, ST=BeiJing, L=BeiJing, O=k8s, OU=System, CN=kubernetes Subject Public Key Info: Public Key Algorithm: rsaEncryption Public-Key: (2048 bit) Modulus: 00:bf:3c:d0:64:cd:86:43:89:16:b0:cd:db:d7:61: 84:a0:c4:d2:54:61:22:b1:c8:b5:04:fe:57:bf:ec: 5c:ae:8e:63:28:d3:b1:eb:08:31:19:e9:3d:66:ef: 0d:9c:92:09:b6:08:7f:74:48:31:ac:cb:0b:fa:19: e1:1e:27:95:ef:68:c9:e0:99:0f:25:a4:c5:49:9d: 14:f6:39:6c:e9:5c:9d:05:88:f0:f6:c3:19:43:3c: 19:2d:ba:3a:69:3e:23:fb:ff:3f:f7:a9:28:36:3f: 3b:bb:e2:74:c4:1a:b8:59:ef:90:6a:8b:7f:26:52: 28:17:5d:2b:53:45:d0:86:a4:cd:94:9f:e6:f8:0d: 81:24:ca:46:90:a8:7f:0c:10:4e:df:39:59:8b:bf: 75:92:c8:2d:dc:18:22:a5:be:a2:cd:dc:ae:af:f7: e2:e4:d0:f3:de:5a:86:04:76:89:8c:73:4a:3c:8d: 27:cc:39:c6:a4:bd:ce:bf:c7:72:63:0d:95:19:2b: 88:51:f6:6e:ba:21:5b:c4:e1:ad:9f:dc:2a:65:6f: 3f:65:1c:89:6f:49:ac:a6:c9:d0:34:74:ea:99:d0: e6:cc:7b:2f:65:0a:e1:e6:11:ef:f1:18:58:25:63: 4a:e6:39:08:56:f2:ab:39:e9:14:eb:2a:29:b0:f3: e8:b3 Exponent: 65537 (0x10001) X509v3 extensions: X509v3 Key Usage: critical Digital Signature, Key Encipherment X509v3 Extended Key Usage: TLS Web Server Authentication, TLS Web Client Authentication X509v3 Basic Constraints: critical CA:FALSE X509v3 Subject Key Identifier: 6F:02:BB:2D:22:7B:E4:B4:AC:E9:DF:F5:99:6D:BB:EC:C0:2F:E6:BB X509v3 Authority Key Identifier: keyid:70:A1:9B:81:7D:C9:43:72:C5:E3:8D:DF:9B:1A:4C:17:25:0B:99:B9 X509v3 Subject Alternative Name: DNS:kubernetes, DNS:kubernetes.default, DNS:kubernetes.default.svc, DNS:kubernetes.default.svc.cluster, DNS:kubernetes.default.svc.cluster.local, IP Address:127.0.0.1, IP Address:172.21.17.4, IP Address:172.21.16.231, IP Address:172.21.16.240, IP Address:10.254.0.1…………………………………… 确认Issuer字段的内容和ca-csr.json一致； 确认Subject字段内容和kuberetes-csr.json一致； 确认509v3 Subject Alternative Name字段内容和kuberetes-csr.json一致； 确认509v3 Key Usage、Extended Key Usage字段内容和ca-config.json中kubernetes profile一致； 3.4.2、使用cfssl-certinfo命令123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# cfssl-certinfo -cert kubernetes.pem&#123; \"subject\": &#123; \"common_name\": \"kubernetes\", \"country\": \"CN\", \"organization\": \"k8s\", \"organizational_unit\": \"System\", \"locality\": \"BeiJing\", \"province\": \"BeiJing\", \"names\": [ \"CN\", \"BeiJing\", \"BeiJing\", \"k8s\", \"System\", \"kubernetes\" ] &#125;, \"issuer\": &#123; \"common_name\": \"kubernetes\", \"country\": \"CN\", \"organization\": \"k8s\", \"organizational_unit\": \"System\", \"locality\": \"BeiJing\", \"province\": \"BeiJing\", \"names\": [ \"CN\", \"BeiJing\", \"BeiJing\", \"k8s\", \"System\", \"kubernetes\" ] &#125;, \"serial_number\": \"690951507474065611980073490741073050901299637689\", \"sans\": [ \"kubernetes\", \"kubernetes.default\", \"kubernetes.default.svc\", \"kubernetes.default.svc.cluster\", \"kubernetes.default.svc.cluster.local\", \"127.0.0.1\", \"172.21.17.4\", \"172.21.16.231\", \"172.21.16.240\", \"10.254.0.1\" ], \"not_before\": \"2019-02-21T03:08:00Z\", \"not_after\": \"2029-02-18T03:08:00Z\", \"sigalg\": \"SHA256WithRSA\",……………………………………………… 3.4.3、分发证书将生成的证书和秘钥文件（后缀名为.pem）拷贝到所有机器；kubernetes系统的各个组建需要使用tls证书对通信进行加密。 1）、生成的证书ca证书和秘钥文件如下： admin-key.pem admin.pem ca-key.pem ca.pem kube-proxy-key.pem kube-proxy.pem kubernetes-key.pem kubernetes.pem 2）、使用证书组建如下 etcd: 使用ca.pem、kubernetes-key.pem、kubernetes.pem； kube-apiserver: 使用ca.pem、kubernetes-key.pem、kubernetes.pem； kubelet: 使用ca.pem； kube-proxy: 使用ca.pem、kube-proxy.pem、kube-proxy-key.pem； kubectl: 使用ca.pem、admin-key.pem、admin.pem； kube-controller、kube-scheduler当前需要和kube-apiserver部署在一台服务器且使用非安全端口通信，故不需要证书。 3）、证书拷贝 master 节点拷贝 12345678# mkdir -p /etc/kubernetes/ssl &amp;&amp; mkdir -p /etc/etcd/ssl# cp ca.pem kubernetes-key.pem kubernetes.pem kube-proxy.pem kube-proxy-key.pem admin-key.pem admin.pem /etc/kubernetes/ssl/# cp ca.pem kubernetes-key.pem kubernetes.pem /etc/etcd/ssl/# cp ca* /etc/kubernetes/ssl# scp -r /etc/kubernetes/ssl k8s-master-02:/etc# scp -r /etc/kubernetes/ssl k8s-master-03:/etc# scp -r /etc/etcd k8s-master-02:/etc# scp -r /etc/etcd k8s-master-03:/etc node节点 1# cp admin-key.pem admin.pem ca.pem kube-proxy-key.pem kube-proxy.pem /etc/kubernetes/ssl/ 3.5、开始配置etcd3.5.1、下载etcd1234# wget https://github.com/etcd-io/etcd/releases/download/v3.3.12/etcd-v3.3.12-linux-amd64.tar.gz# tar zxf etcd-v3.3.12-linux-amd64.tar.gz# cd etcd-v3.3.12-linux-amd64# cp -arp etcd* /usr/bin/ 3.5.2、创建etcd的Systemd unit 文件Etcd 这里采用最新的 3.3.12 版本，安装方式直接复制二进制文件、systemd service 配置即可，不过需要注意相关用户权限问题，以下脚本配置等参考了 etcd rpm 安装包 3.5.3、配置etcd.conf k8s-master-01 123456789101112131415161718192021222324252627282930# cat /etc/etcd/etcd.conf# [member]ETCD_NAME=etcd1ETCD_DATA_DIR=\"/var/lib/etcd\"ETCD_SNAPSHOT_COUNT=\"100\"ETCD_HEARTBEAT_INTERVAL=\"100\"ETCD_ELECTION_TIMEOUT=\"1000\"ETCD_LISTEN_PEER_URLS=\"https://172.21.17.4:2380\"ETCD_LISTEN_CLIENT_URLS=\"https://172.21.17.4:2379,http://127.0.0.1:2379\"ETCD_MAX_SNAPSHOTS=\"5\"ETCD_MAX_WALS=\"5\"# [cluster]ETCD_INITIAL_ADVERTISE_PEER_URLS=\"https://172.21.17.4:2380\"ETCD_INITIAL_CLUSTER=\"etcd1=https://172.21.17.4:2380,etcd2=https://172.21.16.231:2380,etcd3=https://172.21.16.240:2380\"ETCD_INITIAL_CLUSTER_STATE=\"new\"ETCD_INITIAL_CLUSTER_TOKEN=\"etcd-cluster\"ETCD_ADVERTISE_CLIENT_URLS=\"https://172.21.17.4:2379\"# [security]ETCD_CERT_FILE=\"/etc/etcd/ssl/etcd.pem\"ETCD_KEY_FILE=\"/etc/etcd/ssl/etcd-key.pem\"ETCD_CLIENT_CERT_AUTH=\"true\"ETCD_TRUSTED_CA_FILE=\"/etc/kubernetes/ssl/ca.pem\"ETCD_AUTO_TLS=\"true\"ETCD_PEER_CERT_FILE=\"/etc/etcd/ssl/etcd.pem\"ETCD_PEER_KEY_FILE=\"/etc/etcd/ssl/etcd-key.pem\"ETCD_PEER_CLIENT_CERT_AUTH=\"true\"ETCD_PEER_TRUSTED_CA_FILE=\"/etc/kubernetes/ssl/ca.pem\"ETCD_PEER_AUTO_TLS=\"true\" k8s-master-02 123456789101112131415161718192021222324252627282930# cat /etc/etcd/etcd.conf# [member]ETCD_NAME=etcd2ETCD_DATA_DIR=\"/var/lib/etcd\"ETCD_SNAPSHOT_COUNT=\"100\"ETCD_HEARTBEAT_INTERVAL=\"100\"ETCD_ELECTION_TIMEOUT=\"1000\"ETCD_LISTEN_PEER_URLS=\"https://172.21.16.231:2380\"ETCD_LISTEN_CLIENT_URLS=\"https://172.21.16.231:2379,http://127.0.0.1:2379\"ETCD_MAX_SNAPSHOTS=\"5\"ETCD_MAX_WALS=\"5\"# [cluster]ETCD_INITIAL_ADVERTISE_PEER_URLS=\"https://172.21.16.231:2380\"ETCD_INITIAL_CLUSTER=\"etcd1=https://172.21.17.4:2380,etcd2=https://172.21.16.231:2380,etcd3=https://172.21.16.240:2380\"ETCD_INITIAL_CLUSTER_STATE=\"new\"ETCD_INITIAL_CLUSTER_TOKEN=\"etcd-cluster\"ETCD_ADVERTISE_CLIENT_URLS=\"https://172.21.16.231:2379\"# [security]ETCD_CERT_FILE=\"/etc/etcd/ssl/etcd.pem\"ETCD_KEY_FILE=\"/etc/etcd/ssl/etcd-key.pem\"ETCD_CLIENT_CERT_AUTH=\"true\"ETCD_TRUSTED_CA_FILE=\"/etc/kubernetes/ssl/ca.pem\"ETCD_AUTO_TLS=\"true\"ETCD_PEER_CERT_FILE=\"/etc/etcd/ssl/etcd.pem\"ETCD_PEER_KEY_FILE=\"/etc/etcd/ssl/etcd-key.pem\"ETCD_PEER_CLIENT_CERT_AUTH=\"true\"ETCD_PEER_TRUSTED_CA_FILE=\"/etc/kubernetes/ssl/ca.pem\"ETCD_PEER_AUTO_TLS=\"true\" k8s-master-03 123456789101112131415161718192021222324252627282930# cat /etc/etcd/etcd.conf# [member]ETCD_NAME=etcd3ETCD_DATA_DIR=\"/var/lib/etcd\"ETCD_SNAPSHOT_COUNT=\"100\"ETCD_HEARTBEAT_INTERVAL=\"100\"ETCD_ELECTION_TIMEOUT=\"1000\"ETCD_LISTEN_PEER_URLS=\"https://172.21.16.240:2380\"ETCD_LISTEN_CLIENT_URLS=\"https://172.21.16.240:2379,http://127.0.0.1:2379\"ETCD_MAX_SNAPSHOTS=\"5\"ETCD_MAX_WALS=\"5\"# [cluster]ETCD_INITIAL_ADVERTISE_PEER_URLS=\"https://172.21.16.240:2380\"ETCD_INITIAL_CLUSTER=\"etcd1=https://172.21.17.4:2380,etcd2=https://172.21.16.231:2380,etcd3=https://172.21.16.240:2380\"ETCD_INITIAL_CLUSTER_STATE=\"new\"ETCD_INITIAL_CLUSTER_TOKEN=\"etcd-cluster\"ETCD_ADVERTISE_CLIENT_URLS=\"https://172.21.16.240:2379\"# [security]ETCD_CERT_FILE=\"/etc/etcd/ssl/etcd.pem\"ETCD_KEY_FILE=\"/etc/etcd/ssl/etcd-key.pem\"ETCD_CLIENT_CERT_AUTH=\"true\"ETCD_TRUSTED_CA_FILE=\"/etc/kubernetes/ssl/ca.pem\"ETCD_AUTO_TLS=\"true\"ETCD_PEER_CERT_FILE=\"/etc/etcd/ssl/etcd.pem\"ETCD_PEER_KEY_FILE=\"/etc/etcd/ssl/etcd-key.pem\"ETCD_PEER_CLIENT_CERT_AUTH=\"true\"ETCD_PEER_TRUSTED_CA_FILE=\"/etc/kubernetes/ssl/ca.pem\"ETCD_PEER_AUTO_TLS=\"true\" 3.5.2、配置etcd启动文件12345678910111213141516171819# cat /lib/systemd/system/etcd.service [Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.target[Service]Type=notifyWorkingDirectory=/var/lib/etcd/EnvironmentFile=-/etc/etcd/etcd.confUser=etcd# set GOMAXPROCS to number of processorsExecStart=/bin/bash -c \"GOMAXPROCS=$(nproc) /usr/bin/etcd --name=\\\"$&#123;ETCD_NAME&#125;\\\" --data-dir=\\\"$&#123;ETCD_DATA_DIR&#125;\\\" --listen-client-urls=\\\"$&#123;ETCD_LISTEN_CLIENT_URLS&#125;\\\"\"Restart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target 3.5.4、etcd授权1234# groupadd -r etcd# useradd -r -g etcd -d /var/lib/etcd -s /sbin/nologin -c \"etcd user\" etcd# chown -R etcd:etcd /etc/etcd &amp;&amp; chmod -R 755 /etc/etcd/ssl &amp;&amp;chown -R etcd:etcd /var/lib/etcd# systemctl daemon-reload &amp;&amp; systemctl enable etcd &amp;&amp;systemctl start etcd 3.5.5、验证etcd 由于etcd使用了证书，所以etcd命令需要带上证书 1234# etcdctl --key-file /etc/etcd/ssl/etcd-key.pem --cert-file /etc/etcd/ssl/etcd.pem --ca-file /etc/kubernetes/ssl/ca.pem member list93c04a995ff8aa8: name=etcd3 peerURLs=https://172.21.16.240:2380 clientURLs=https://172.21.16.240:2379 isLeader=false7cc4daf6e4db3a8a: name=etcd2 peerURLs=https://172.21.16.231:2380 clientURLs=https://172.21.16.231:2379 isLeader=falseec7ea930930d012e: name=etcd1 peerURLs=https://172.21.17.4:2380 clientURLs=https://172.21.17.4:2379 isLeader=true 查看集群是否健康 12345# etcdctl --key-file /etc/etcd/ssl/etcd-key.pem --cert-file /etc/etcd/ssl/etcd.pem --ca-file /etc/kubernetes/ssl/ca.pem cluster-healthmember 93c04a995ff8aa8 is healthy: got healthy result from https://172.21.16.240:2379member 7cc4daf6e4db3a8a is healthy: got healthy result from https://172.21.16.231:2379member ec7ea930930d012e is healthy: got healthy result from https://172.21.17.4:2379cluster is healthy 4、创建kube config文件kubelet、kube-proxy等Node机器上的经常与master机器的kube-apiserver进程通信时需要认证和授权；kubernetes 1.4 开始支持有kube-apiserver为客户端生成tls证书的 TLS Bootstrapping功能，这样就不需要为每个客户端生成证书了，该功能当前仅支持为kuelet生成证书； 4.1、创建TLS Bootstrapping ToeknToken auth file，Token可以时任意的包含128bit的字符串，可以使用安全的随机数发生器生成； 1234# export BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d ' ')# cat &gt; token.csv &lt;&lt;EOF$&#123;BOOTSTRAP_TOKEN&#125;,kubelet-bootstrap,10001,\"system:kubelet-bootstrap\"EOF 将token.csv发放到所有的机器（master和node）的/etc/kubernetes目录； 4.2、创建kubelet bootstrapping kubeconfig文件在这之前我们需要下载kubernetes 相关的二进制包，把对应的工具和命令拷贝到/usr/bin目录下面;下载二进制包 12# wget https://dl.k8s.io/v1.13.3/kubernetes-server-linux-amd64.tar.gz# tar zxf kubernetes-server-linux-amd64.tar.gz &amp;&amp; cd kubernetes/server/bin master节点拷贝 1# mv apiextensions-apiserver cloud-controller-manager hyperkube kube-apiserver kube-controller-manager kube-proxy kube-scheduler kubectl kubelet mounter kubeadm /usr/bin/ &amp;&amp; cd &amp;&amp;rm -rf kubernetes kubernetes-server-linux-amd64.tar.gz node 节点拷贝 1# mv kubectl kube-proxy /usr/bin/ &amp;&amp; cd &amp;&amp;rm -rf kubernetes kubernetes-server-linux-amd64.tar.gz 4.2.1、生成文件bootstrapping master-01 config 是一个通用配置文件要连接本地的 6443 加密端口；而这个变量将会覆盖 kubeconfig 中指定的master_vip地址172.21.16.45:6443 地址 1# export KUBE_APISERVER=\"https://172.21.16.45:6443\" 设置集群参数 12345# kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=$&#123;KUBE_APISERVER&#125; \\ --kubeconfig=bootstrap.kubeconfig 设置客户端认证参数 123# kubectl config set-credentials kubelet-bootstrap \\--token=$&#123;BOOTSTRAP_TOKEN&#125; \\--kubeconfig=bootstrap.kubeconfig 设置上下文参数 1234# kubectl config set-context default \\--cluster=kubernetes \\--user=kubelet-bootstrap \\--kubeconfig=bootstrap.kubeconfig 设置默认上下 1# kubectl config use-context default --kubeconfig=bootstrap.kubeconfig –enbed-certs为true时表示将certificate-authority证书写入到生成的bootstrap.kubeconfig文件中 设置客户端认证参数时没有指定秘钥和证书，后续kube-apiserver自动生成 4.2.2、创建kube-proxy config文件1# export KUBE_APISERVER=\"https://172.21.16.45:6443\" 设置集群参数 12345# kubectl config set-cluster kubernetes \\--certificate-authority=/etc/kubernetes/ssl/ca.pem \\--embed-certs=true \\--server=$&#123;KUBE_APISERVER&#125; \\--kubeconfig=kube-proxy.kubeconfig 设置客户端认证参数 12345# kubectl config set-credentials kube-proxy \\--client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \\--client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \\--embed-certs=true \\--kubeconfig=kube-proxy.kubeconfig 设置上下文 1234# kubectl config set-context default \\--cluster=kubernetes \\--user=kube-proxy \\--kubeconfig=kube-proxy.kubeconfig 设置默认上下文 1# kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig 设置集群参数和客户端认证参数时–enbed-certs都为true，这将会certificate-authority、client-certificate和client-key都指向的证书文件内容写入到生成的kube-proxy.kubeconfig文件中； kube-proxy.pem证书中CN为system:kube-proxy，kube-apiserver预定义的RoleBinding cluster-admin将User system:kube-proxy与Role system:node-proxier绑定，该Role授予了调用kube-apiserver Proxy相关的API的权限； 1）、分开kubeconfig文件 将两个kubeconfig文件分发到所有的Node机器的/etc/kubernetes 目录 1# cp bootstrap.kubeconfig kube-proxy.kubeconfig /etc/kubernetes/ 5、配置和启动kube-apiserver5.1、设置启动文件 kube-apiserver.service12345678910111213141516171819202122232425# cat /usr/lib/systemd/system/kube-apiserver.service[Unit] Description=Kubernetes API Service Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target After=etcd.service[Service] EnvironmentFile=-/etc/kubernetes/config EnvironmentFile=-/etc/kubernetes/apiserver ExecStart=/usr/bin/kube-apiserver \\ $KUBE_LOGTOSTDERR \\ $KUBE_LOG_LEVEL \\ $KUBE_ETCD_SERVERS \\ $KUBE_API_ADDRESS \\ $KUBE_API_PORT \\ $KUBELET_PORT \\ $KUBE_ALLOW_PRIV \\ $KUBE_SERVICE_ADDRESSES \\ $KUBE_ADMISSION_CONTROL \\ $KUBE_API_ARGS Restart=on-failure Type=notify LimitNOFILE=65536[Install]WantedBy=multi-user.target 5.2、config配置文件12345678910111213141516171819# cat /etc/kubernetes/config # kubernetes system config## The following values are used to configure various aspects of all# kubernetes services, including## kube-apiserver.service# kube-controller-manager.service# kube-scheduler.service# kubelet.service# kube-proxy.service# logging to stderr means we get it in the systemd journalKUBE_LOGTOSTDERR=\"--logtostderr=true\"# journal message level, 0 is debugKUBE_LOG_LEVEL=\"--v=0\"# Should this cluster be allowed to run privileged docker containersKUBE_ALLOW_PRIV=\"--allow-privileged=true\"# How the controller-manager, scheduler, and proxy find the apiserverKUBE_MASTER=\"--master=http://127.0.0.1:8080\" 该配置文件同时被kube-apiserver、kube-controller-manager、kube-scheduler、kubelet、kube-proxy使用， 5.3、apiserver配置文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# cat /etc/kubernetes/apiserver## kubernetes system config#### The following values are used to configure the kube-apiserver##### The address on the local server to listen to.KUBE_API_ADDRESS=\"--advertise-address=&#123;master_ip&#125; --bind-address=&#123;master_ip&#125;\"# localhost 8080\"--insecure-bind-address=&#123;master_ip&#125;\"### The port on the local server to listen on.#KUBE_API_PORT=\"--port=8080\"### Port minions listen on#KUBELET_PORT=\"--kubelet-port=10250\"### Comma separated list of nodes in the etcd clusterKUBE_ETCD_SERVERS=\"--etcd-servers=https://172.21.17.4:2379,https://172.21.16.231:2379,https://172.21.16.240:2379\"### Address range to use for servicesKUBE_SERVICE_ADDRESSES=\"--service-cluster-ip-range=10.254.0.0/16\"### default admission control policiesKUBE_ADMISSION_CONTROL=\"--admission-control=ServiceAccount,NamespaceLifecycle,NamespaceExists,LimitRanger,ResourceQuota\"### Add your own!KUBE_API_ARGS=\"--authorization-mode=RBAC \\ --runtime-config=rbac.authorization.k8s.io/v1beta1 \\ --kubelet-https=true \\ --enable-bootstrap-token-auth \\ --token-auth-file=/etc/kubernetes/token.csv \\ --service-node-port-range=30000-50000 \\ --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \\ --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \\ --client-ca-file=/etc/kubernetes/ssl/ca.pem \\ --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --etcd-cafile=/etc/kubernetes/ssl/ca.pem \\ --etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem \\ --etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem \\ --enable-swagger-ui=true \\ --apiserver-count=3 \\ --audit-log-maxage=30 \\ --audit-log-maxbackup=3 \\ --audit-log-maxsize=100 \\ --audit-log-path=/var/lib/audit.log \\ --event-ttl=48h0m0s\" \\ --storage-backend=etcd3 –authorization-mode=RBAC 指定在安全端口使用RBAC授权模式，拒绝未通过授权请求； kube-scheduler kube-controller-manager 一般和kube-apiserver部署在同一台机器上，它们使用非安全端口和kube-apiserver通信； kubelet kube-proxy kubectl部署在其他Node节点上，如果通过安全端口访问kube-apiserver，则必须先通过tls证书认证，在通过RBAC授权； kube-proxy kubectl通过在使用证书里指定相关的User、Group来达到通过RABC授权的目的； 如果使用了kubelet TLS Boostrap机制，则不能再指定–kubelet-certificate- authority –kubelet-client-certificate和–kubelet-client-key选项，否则后续kube-apiserver校验kubelet证书时出现”x509: certificate signed by unknown authority”错误 –admission-control值必须包括ServiceAccount –bind-address 不能为127.0.0.1 runtime-config配置为rbac.authorization.k8s.io/v1beta1，表示运行时的apiversion –service-cluster-ip-range指定Service Cluster IP地址段，该地址段不能路由可达 缺省情况下kubernetes对象保存在etcd/registry路径下，可以通过–etcd-prefix参数进行调整 5.3.1、启动kube-apiserver12# systemctl daemon-reload# systemctl enable kube-apiserver &amp;&amp;systemctl start kube-apiserver &amp;&amp;systemctl stauts kube-apiserve 5.4、配置kube-controller-manager创建kube-controller-manager的service配置文件 5.4.1、配置kube-controller-manager启动文件12345678910111213141516# cat /usr/lib/systemd/system/kube-controller-manager.service[Unit]Description=Kubernetes Controller ManagerDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/controller-managerExecStart=/usr/bin/kube-controller-manager \\ $KUBE_LOGTOSTDERR \\ $KUBE_LOG_LEVEL \\ $KUBE_MASTER \\ $KUBE_CONTROLLER_MANAGER_ARGSRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target 5.4.2、配置controller-manager文件12345678910111213# cat /etc/kubernetes/controller-manager#### The following values are used to configure the kubernetes controller-manager# defaults from config and apiserver should be adequate# Add your own!KUBE_CONTROLLER_MANAGER_ARGS=\"--address=0.0.0.0 \\ --cluster-name=kubernetes \\ --service-cluster-ip-range=10.254.0.0/16 \\ --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem \\ --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --root-ca-file=/etc/kubernetes/ssl/ca.pem \\ --leader-elect=true\" –service-cluster-ip-range 参数指定Cluster中Service的CIDR范围，该网络在各Node间必须路由可达，必须和kube-apiserver中的参数一致。 –cluster-signing-* 指定的证书和私钥文件用来签名为TLS Bootstrap创建证书和私钥； –root-ca-file 用来对kube-apiserver证书进行校验，指定该参数后，才会在pod容器的SservieAccount中放置CA证书文件； –root-ca-file 值必须为127.0.0.1,因此当前kube-apiserver期望scheduler和controller-manager在同一台机器； 12345# kubectl get componentstatusesNAME STATUS MESSAGE ERRORscheduler Unhealthy Get http://127.0.0.1:10251/healthz: dial tcp 127.0.0.1:10251: connect: connection refused controller-manager Unhealthy Get http://127.0.0.1:10252/healthz: dial tcp 127.0.0.1:10252: connect: connection refused etcd-0 Healthy &#123;\"health\":\"true\"&#125; 5.4.3、启动kube-controller-manager123# systemctl daemon-reload# systemctl enable kube-controller-manager &amp;&amp;systemctl start kube-controller-manager &amp;&amp;systemctl status kube-controller-manager 5.5、配置kube-scheduler创建kube-scheduler的service配置文件 5.5.1、创建kube-scheduler启动文件12345678910111213141516# cat /lib/systemd/system/kube-scheduler.service[Unit]Description=Kubernetes Scheduler PluginDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/schedulerExecStart=/usr/bin/kube-scheduler \\ $KUBE_LOGTOSTDERR \\ $KUBE_LOG_LEVEL \\ $KUBE_MASTER \\ $KUBE_SCHEDULER_ARGSRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target 5.5.2、创建scheduler配置文件123456# cat /etc/kubernetes/scheduler#### kubernetes scheduler config# default config should be adequate# Add your own!KUBE_SCHEDULER_ARGS=\"--leader-elect=true --address=0.0.0.0\" 5.5.3、启动kube-scheduler12# systemctl daemon-reload# systemctl enable kube-scheduler &amp;&amp;systemctl start kube-scheduler &amp;&amp;systemctl status kube-scheduler 5.6、验证master节点1234567# kubectl get componentstatusesNAME STATUS MESSAGE ERRORscheduler Healthy ok controller-manager Healthy ok etcd-2 Healthy &#123;\"health\":\"true\"&#125; etcd-0 Healthy &#123;\"health\":\"true\"&#125; etcd-1 Healthy &#123;\"health\":\"true\"&#125; 至此master节点部署完毕","pubDate":"Fri, 09 Aug 2019 14:18:30 GMT","guid":"https://xxlaila.github.io/2019/08/09/kubernetes-v1-13-3安装/","category":"kubernetes"},{"title":"vsftpd安装","link":"https://xxlaila.github.io/2019/08/09/vsftpd安装/","description":"Centos下ftp的安装一般采用的是vsftpd，但是在ftp的模式中又有几个用户配置项需要注意，有些人喜欢用本地用户去登陆FTP，虽然在建立本地用户的时候加了/sbin/nologin参数，但是这个还是不够安全，而且这样权限控制也不是很好，他们都是统一的控制权限，这里采用虚拟用户前来配置。虚拟用户配合防火墙selinux还有单个用户的权限，这使得FTP有着足够的安全。而且权限控制特别灵活，修改一个用户的权限不会影响到其他用户。centos 系统版本(5.5、5.3、6.0、6.5) 首先我们安装vsftpd1[root@RAID1 ~]# yum –y install vsftpd 2、启动和加载vsftp12[root@RAID1 ~]# service vsftpd restart[root@RAID1 ~]# chkconfig –level 35 vsftpd on 3、开始配置vsftpdVsftpd的配置文件在/etc/vsftpd下面，在配置之前我们先cp一份做备份用以免发生意外(做什么都要随手备份，因为没有一万，只有万一。) 12[root@RAID1 ~]# cp /etc/vsftpd/vsftpd.conf /etc/vsftpd/vsftpd.conf.bak[root@RAID1 ~]# vim /etc/vsftpd/vsftpd.conf vsftpd的参数介绍 123456789101112131415161718192021222324reverse_lookup_enable=NO #添加此行，解决客户端登陆缓慢问题！重要！默认vsftpd开启了DNS反响解析！这里需要关闭，如果启动有错误，请注销！listen_port=21 #默认无此行，ftp端口为21，添加listen_port=2222把默认端口修改为2222，注意：防火墙同时要开启2222端口anonymous_enable=NO #禁止匿名用户local_enable=YES设定本地用户可以访问。注意：主要是为虚拟宿主用户，如果该项目设定为NO那么所有虚拟用户将无法访问write_enable=YES #全局设置，是否容许写入（无论是匿名用户还是本地用户，若要启用上传权限的话，就要开启他）local_umask=022 设定上传后文件的权限掩码。anon_upload_enable=NO 禁止匿名用户上传。anon_mkdir_write_enable=NO 禁止匿名用户建立目录。dirmessage_enable=YES 设定开启目录标语功能。xferlog_enable=YES 设定开启日志记录功能。connect_from_port_20=YES 设定端口20进行数据连接。chown_uploads=NO 设定禁止上传文件更改宿主。xferlog_file=/var/log/vsftpd.log 日志保存路径（先创建好文件）xferlog_std_format=YES #使用标准格式async_abor_enable=YES 设定支持异步传输功能。ascii_upload_enable=YESascii_download_enable=YES 设定支持ASCII模式的上传和下载功能。ftpd_banner=Welcome to Awei FTP servers 设定Vsftpd的登陆标语。chroot_local_user=YES 禁止本地用户登出自己的FTP主目录。pam_service_name=vsftpd 设定PAM服务下Vsftpd的验证配置文件名。因此，PAM验证将参考/etc/pam.d/下的vsftpd文件配置。userlist_enable=YES 设为YES的时候，如果一个用户名是在userlist_file参数指定的文件中， 那么在要求他们输入密码之前，会直接拒绝他们登陆。tcp_wrappers=YES 是否支持tcp_wrappers 以下是我使用的参数 123456789101112131415161718192021222324252627anonymous_enable=Nolisten_port=21local_enable=YESwrite_enable=YESlocal_umask=022dirmessage_enable=YESxferlog_enable=YESconnect_from_port_20=YES#chown_uploads=YESxferlog_file=/var/log/vsftpd.logxferlog_std_format=YESasync_abor_enable=YESascii_upload_enable=YESascii_download_enable=YESftpd_banner=Welcome to blah FTP service.chroot_list_enable=YESlisten=YESpam_service_name=vsftpduserlist_enable=YEStcp_wrappers=YESreverse_lookup_enable=Noguest_enable=YESguest_username=vsftpduser_config_dir=/etc/vsftpd/vconfvirtual_use_local_privs=YESpasv_min_port=9000pasv_max_port=9045 4、建立虚拟用户名单文件编辑虚拟用户的名单：（第一行用户名。第二行密码。不能使用root） 1234567[root@RAID1 ~]# vim /etc/vsftpd/xuniuserstest23123213test134dsfdstest2df43sd 5、开始建立生成虚拟用户数据文件这里需要安装db4,设置PAM文件权限，并制定虚拟用户数据库文件读取 123[root@RAID1 ~]# yum –y install db4-utils[root@RAID1 ~]# db_load -T -t hash -f /etc/vsftpd/xuniusers /etc/vsftpd/xuniusers.db[root@RAID1 ~]# chmod 600 /etc/vsftpd/xuniusers.db 在/etc/pam.d/vsftpd的文件头部加入以下信息（注*这里一定要在前面，不能再后面，刚开始我也加载到后面登陆的时候提示错误）,修改前先备份 1234[root@RAID1 ~]# cp /etc/pam.d/vsftpd /etc/pam.d/vsftpd.bak[root@RAID1 ~]# vi /etc/pam.d/vsftpdauth sufficient /lib/security/pam_userdb.so db=/etc/vsftpd/xuniusersaccount sufficient /lib/security/pam_userdb.so db=/etc/vsftpd/xuniusers 注*64位的操作系统，则上面lib改为64。不然配置也会无效 12auth sufficient /lib64/security/pam_userdb.so db=/etc/vsftpd/xuniusersaccount sufficient /lib64/security/pam_userdb.so db=/etc/vsftpd/xuniusers 建立一个系统用户vsftpd，用户的主目录可以自己设置，/home/wwwroot，设置用户登陆的终端为/bin/false 123[root@RAID1 ~]# useradd vsftpd -d /home/wwwroot -s /bin/false[root@RAID1 ~]# chown vsftpd:vsftpd /home/wwwroot -R[root@RAID1 ~]# chown www:www /home/wwwroot –R #如果虚拟用户的宿主用户为nginx，需要这样设置。 6、建立虚拟用户个人vsftp的配置文件123456789101112[root@RAID1 ~]# mkdir /etc/vsftpd/vconf[root@RAID1 ~]# cd /etc/vsftpd/vconftouch test1 test2 test3 #这里创建三个虚拟用户配置文件vi web1 #编辑用户test1配置文件，其他的跟这个配置文件类似[root@RAID1 ~]# vim test1local_root=/home/wwwroot/test1/write_enable=YESanon_umask=022anon_world_readable_only=NOanon_upload_enable=YESanon_mkdir_write_enable=YESanon_other_write_enable=YES 最后重启vsftpd服务,不关闭Selinux可以执行以下命令通过FTP。防火墙开放端口-P ftpd_disable_trans 1```1234上述配置完成后还可以通过#adduser -d /目录路径 -g vsftpd -s /sbin/nologin 用户名 这个命令来添加一个用户，不需要配置任何权限都可以进行FTP的访问,最后补充说明，需要安装的其他插件* 需要安装的的是pan``` bash[root@RAID1 ~]# yum install -y pam 这里我们还可以查看日志，可以根据提示的提示来判断。1[root@RAID1 ~]# cat /var/log/secure 测试用户登录虚拟用户只能看到自己本身的目录 ，不能去其他目录查看(到这里vsftpd配置结束)","pubDate":"Fri, 09 Aug 2019 13:52:47 GMT","guid":"https://xxlaila.github.io/2019/08/09/vsftpd安装/","category":"vsftpd"},{"title":"TeamViewer mac破解","link":"https://xxlaila.github.io/2019/08/09/TeamViewer-mac破解/","description":"TeamViewer14.4 MAC破解在终端执行以下命令12sudo python TeamViewer-id-changer.py使用mac自带python2.7 执行即可 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145$ vim TeamViewer-id-changer.py#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019/8/1 14:57# @Author : xxlaila# @Site : # @File : TeamViewer-id-changer.py# @Software: PyCharm#!/usr/bin/env python# -*- coding: utf-8 -*-import osimport platformimport randomimport reimport stringimport sysprint('''--------------------------------TeamViewer 14 ID Changer for MAC OSVersion: 0.2 2019--------------------------------''')if platform.system() != \"Darwin\": print(\"This script can be run only on MAC OS.\") sys.exit()if os.geteuid() != 0: print(\"This script must be run form root.\") sys.exit()if \"SUDO_USER\" in os.environ: USERNAME = os.environ[\"SUDO_USER\"] if USERNAME == \"root\": print(\"Can not find user name. Run this script via sudo from regular user\") sys.exit()else: print(\"Can not find user name. Run this script via sudo from regular user\") sys.exit()HOMEDIRLIB = \"/Users/\" + USERNAME + \"/library/preferences/\"GLOBALLIB = \"/library/preferences/\"CONFIGS = []# Find config filesdef listdir_fullpath(d): return [os.path.join(d, f) for f in os.listdir(d)]for file in listdir_fullpath(HOMEDIRLIB): if 'teamviewer' in file.lower(): CONFIGS.append(file)for file in listdir_fullpath(GLOBALLIB): if 'teamviewer' in file.lower(): CONFIGS.append(file)if not CONFIGS: print(''' There is no TemViewer configs found. Maybe you have deleted it manualy or never run TeamViewer after installation. Nothing to delete. ''')else: # Delete config files print(\"Configs found:\\n\") for file in CONFIGS: print(file) print(''' This files will be DELETED permanently. All TeamViewer settings will be lost ''') raw_input(\"Press Enter to continue or CTR+C to abort...\") for file in CONFIGS: try: os.remove(file) except: print(\"Cannot delete config files. Permission denied?\") sys.exit() print(\"Done.\")# Find binaryesTMBINARYES = [ '/Applications/TeamViewer.app/Contents/MacOS/TeamViewer', '/Applications/TeamViewer.app/Contents/MacOS/TeamViewer_Service', '/Applications/TeamViewer.app/Contents/Helpers/TeamViewer_Desktop', '/Applications/TeamViewer.app/Contents/Helpers/TeamViewer_Assignment']for file in TMBINARYES: if os.path.exists(file): pass else: print(\"File not found: \" + file) print (\"Install TeamViewer correctly\") sys.exit()# Patch filesdef idpatch(fpath, platf, serial): file = open(fpath, 'r+b') binary = file.read() PlatformPattern = \"IOPlatformExpert.&#123;6&#125;\" SerialPattern = \"IOPlatformSerialNumber%s%s%s\" binary = re.sub(PlatformPattern, platf, binary) binary = re.sub(SerialPattern % (chr(0), \"[0-9a-zA-Z]&#123;8,8&#125;\", chr(0)), SerialPattern % (chr(0), serial, chr(0)), binary) file = open(fpath, 'wb').write(binary) return Truedef random_generator(size=8, chars=string.ascii_uppercase + string.ascii_lowercase + string.digits): return ''.join(random.choice(chars) for _ in range(size))RANDOMSERIAL = random_generator(8)RANDOMPLATFORM = \"IOPlatformExpert\" + random_generator(6)for file in TMBINARYES: try: idpatch(file, RANDOMPLATFORM, RANDOMSERIAL) except: print(\"Error: can not patch file \" + file) sys.exit()print(\"PlatformDevice: \" + RANDOMPLATFORM)print(\"PlatformSerial: \" + RANDOMSERIAL)os.system(\"sudo codesign -f -s - /Applications/TeamViewer.app/\")print('''ID changed sucessfully.!!! Restart computer before using TeamViewer !!!!''')","pubDate":"Fri, 09 Aug 2019 12:25:56 GMT","guid":"https://xxlaila.github.io/2019/08/09/TeamViewer-mac破解/","category":"TeamViewer"},{"title":"jenkins job管理","link":"https://xxlaila.github.io/2019/08/09/jenkins-job管理/","description":"需求 介绍: 由于公司的ci用于编译的环境比较多，为了更好的区分，为每一个环境建立了一个view 痛点: 运维人员在建立job的时候需要到对应的view下面建立，虽然这不是狠痛苦，但是还是不太方便。 解决: 人员登陆默认是在all view下面，每个运维人员在这下面建立job，然后每个view根据自己的规则吧对应的job添加进来。job规则自己提前定义好 1、安装jenkins插件view job 过滤插件view-job-filters，安装过程不累赘 2、配置view规则这里设置两个前端和一个后端实例 2.1、前端1 2.2、前端testtest 我们用安装的这个插件来进行配置,点击Add Job Filter——&gt;会有很多的规则，可以根据不同的状态、栏目来进行却分，这里我们选择 这里添加了两条规则，这个是建立job的时候有点特殊性，用第一种方式实现就会有问题，第一条规则是现实所有test类的job，但是吧下面的一条给加进来了，不现实这类job。保持即可 后端java程序dev环境为例子","pubDate":"Fri, 09 Aug 2019 11:36:49 GMT","guid":"https://xxlaila.github.io/2019/08/09/jenkins-job管理/","category":"jenkins, jenkins job,"},{"title":"jenkins用户权限配置","link":"https://xxlaila.github.io/2019/08/09/jenkins用户权限配置/","description":"1、jenkins用户权限 可以集成gitlab、jenkins专有账户、LDAP、Servlet容器代理、Unix用户/组数据库 2、授权策略 Gitlab Commiter Authorization Strategy Role-Based Strategy 任何用户可以做任何事(没有任何限制) 安全矩阵 登录用户可以做任何事 遗留模式 项目矩阵授权策略 3、插件安装安装插件：Role-based Authorization Strategy 4、jenkins设置系统管理——&gt;全局安全配置——&gt; 回到系统管理界面，就可以看到多出来一个插件: Mangge and Assing Roles 5、权限设置进入Manager and Assign Roles——&gt;Manage Roles,这里建立了四个权限，分别来对应不同的人员 创建项目角色: 回到Manage and Assign Roles界面 6、配置角色选择Assign Roles,用户新建以后，根据用户不同类型的勾选不同的权限,","pubDate":"Fri, 09 Aug 2019 08:05:59 GMT","guid":"https://xxlaila.github.io/2019/08/09/jenkins用户权限配置/","category":"jenkins, ci/cd"},{"title":"mongodb_replica","link":"https://xxlaila.github.io/2019/08/09/mongodb-replica/","description":"Mongodb replica set安装加认证，这里使用的是keyFile进行认证，之前看过很多文章，坑一大堆，这里是看了两天的官方文档进行的安装，并用户生产，配置文件参数贴一部分,三个带有数据集的节点组成的复制集拥有，架构图如下，参考官方 一个主节点，两个从节点，这两个从节点都可以在选举中升级为主节点 环境三台服务器 123primary: 192.168.32.7secaodray: 192.168.32.11secondary: 192.168.32.14 1、安装mongodb1.1、每个节点都需要操作123456789# sudo vim /etc/yum.repos.d/mongodb-enterprise.repo[mongodb-enterprise]name=MongoDB Enterprise Repositorybaseurl=https://repo.mongodb.com/yum/redhat/$releasever/mongodb-enterprise/3.4/$basearch/gpgcheck=1enabled=1gpgkey=https://www.mongodb.org/static/pgp/server-3.4.asc # sudo yum install -y mongodb-enterprise 注意：如果采用源码包方式安装需要安装一下插件 1# sudo yum install cyrus-sasl cyrus-sasl-plain cyrus-sasl-gssapi krb5-libs lm_sensors-libs net-snmp-agent-libs net-snmp openssl rpm-libs tcp_wrappers-libs libcurl 2、修改mongodb的配置文件(每个节点均操作)自定义mongodb的目录12345678910111213141516# mkdir /opt/mongodb/&#123;data,conf,logs&#125; -p# sudo vim /etc/mongod.conf systemLog: destination: file logAppend: true path: /opt/mongodb/logs/mongod.logstorage: dbPath: /opt/mongodb/data journal: enabled: trueprocessManagement: fork: true pidFilePath: /opt/mongodb/logs/mongod.pidnet: port: 27017 bindIp: 0.0.0.0 3、生成密钥文件(在mong01上操作)1234# openssl rand -base64 756 &gt; ／opt/mongodb/conf/mongo-keyfile# sudo chmod 400 /opt/mongo/mongo-keyfile# scp –r mongo-keyfile user@192.168.32.11:/opt/mongodb/conf# scp –r mongo-keyfile user@192.168.32.14:/opt/mongodb/conf 4、修改mongodb的配置1234security: keyFile: /opt/mongodb/conf/mongo-keyfilereplication: replSetName: xxlaila01（可变化，自定义） 分别在三台服务器上启动mongodb 1# mongod --config /etc/mongod.conf 5、建立集群在你需要认为是主节点的服务器进行mongodb的登陆，和账户权限的建立，这里我选择的192.168.32.7 1mongo --shell --host 127.0.0.1 登陆进去以后可以进行一个简单的命令进行查看 6、把服务器加入副本集12MongoDB Enterprise &gt; config = &#123; _id:\"kxlprod01\",members:[ &#123;_id:0,host:\"192.168.32.7:27017\"&#125;,... &#123;_id:1,host:\"192.168.32.11:27017\"&#125; ,&#123;_id:2,host:\"192.168.32.14:27017\"&#125;] &#125; config = { _id:”kxlprod01”,members:[ {_id:0,host:”192.168.32.7:27017”},{_id:1,host:”192.168.32.11:27017”} ,{_id:2,host:”192.168.32.14:27017”}] }，增加内容 6.1 看当前副本集的状态利用rs.status()命令可以查看当前副本集的状态 1&gt; rs.status() 这里提示配置还没有加载到mongodb副本里面 6.2、加载配置到副本集1&gt; rs.initiate(config) 再次查看副本的状态就可以看到mongodb的副本集已建立，如果此时主节点未被选举出来，稍微等一会就成功 7、创建mongodb副本集认证下面两行我们可以看到第一次主节点没有选举成功，随即我们在回车PRIMARY节点选举成功了，下面我们创建一个管理员账户 12345678910111213141516MongoDB Enterprise xxlaila01:SECONDARY&gt; admin = db.getSiblingDB(\"admin\")MongoDB Enterprise xxlaila01:PRIMARY&gt;admin.createUser( &#123; user: \"root\", pwd: \"123456\", roles: [ &#123; role: \"userAdminAnyDatabase\", db: \"admin\" &#125; ] &#125;)db.getSiblingDB(\"admin\").auth(\"root\", \"123456\" ) 7.1、创建集群账户创建一个集群管理账户，集群账户具有管理整个副本集的 12345678910$ mongo -u \"root\" -p \"123456\" --authenticationDatabase \"admin\"db.getSiblingDB(\"admin\").createUser( &#123; \"user\" : \"manger\", \"pwd\" : \"123456\", roles: [ &#123; \"role\" : \"clusterAdmin\", \"db\" : \"admin\" &#125; ] &#125;) 7.2 创建一个程序连接的账户1234567db.getSiblingDB(\"admin\").createUser( &#123; \"user\" : \"systemprod\", \"pwd\" : \"123456\", roles: [ &#123; \"role\" : \"root\", \"db\" : \"admin\" &#125; ] &#125;) 至此mongodb的副本集创建完成。测试没有问题,登陆其中一台SECONDARY服务器进行测试,这里测试192.168.32.11服务器 1234567$ mongo&gt; rs.status() # 这里提示没有权限（登录进来以后如果不是主几点，mognodb就会默认显示未secondary）&gt; use admin #&gt; db.getSiblingDB(\"admin\").auth(\"manger\", \"123456\") 完成后我们在执行rs.status()就可以看到副本集的信息 7.3 测试程序连接账户12345MongoDB Enterprise xxlaila01:SECONDARY&gt; db.getSiblingDB(\"admin\").auth(\"systemprod\", \"123456\" )&gt; show dbs; #会提示 “not master and slaveok=false”&gt; db.getMongo().setSlaveOk()&gt; show dbs; #在次执行会显示出结果 副本集没有读的权限，需要执行db.getMongo().setSlaveOk()","pubDate":"Fri, 09 Aug 2019 06:46:44 GMT","guid":"https://xxlaila.github.io/2019/08/09/mongodb-replica/","category":"数据库"},{"title":"Hello World","link":"https://xxlaila.github.io/2019/08/09/hello-world/","description":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","pubDate":"Fri, 09 Aug 2019 03:55:49 GMT","guid":"https://xxlaila.github.io/2019/08/09/hello-world/","category":""}]}