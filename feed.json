{"title":"懒羊羊","description":"星际拾荒者","language":"zh-CN","link":"https://xxlaila.github.io","pubDate":"Thu, 29 Aug 2019 09:57:46 GMT","lastBuildDate":"Fri, 30 Aug 2019 01:26:12 GMT","generator":"hexo-generator-json-feed","webMaster":"xxlaila","items":[{"title":"k8s配置Dashboard","link":"https://xxlaila.github.io/2019/08/29/k8s配置Dashboard/","description":"&nbsp;&nbsp;&nbsp;&nbsp;K8S Dashboard是官方的一个基于WEB的用户界面，专门用来管理K8S集群，并可展示集群的状态。K8S集群安装好后默认没有包含Dashboard，我们需要额外创建它。 1、安装dashboard1.1、下载准备需要的文件经过修改过后的文件，已经可以正常使用的文件 创建dashboard1234567[root@k8s-master-01 dashboard]# kubectl create -f kubernetes-dashboard.yaml secret/kubernetes-dashboard-certs createdserviceaccount/kubernetes-dashboard createdrole.rbac.authorization.k8s.io/kubernetes-dashboard-minimal createdrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard-minimal createddeployment.apps/kubernetes-dashboard createdservice/kubernetes-dashboard created 1.2、查看服务状态和pod12345[root@k8s-master-01 ~]# kubectl get service --all-namespacesNAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEdefault kubernetes ClusterIP 10.254.0.1 &lt;none&gt; 443/TCP 18hkube-system coredns ClusterIP 10.254.0.10 &lt;none&gt; 53/UDP,53/TCP 16hkube-system kubernetes-dashboard NodePort 10.254.51.226 &lt;none&gt; 443:30001/TCP 15h 查看service描述 123456789101112131415[root@k8s-master-01 ~]# kubectl describe service kubernetes-dashboard -n kube-systemName: kubernetes-dashboardNamespace: kube-systemLabels: k8s-app=kubernetes-dashboardAnnotations: &lt;none&gt;Selector: k8s-app=kubernetes-dashboardType: NodePortIP: 10.254.51.226Port: &lt;unset&gt; 443/TCPTargetPort: 8443/TCPNodePort: &lt;unset&gt; 30001/TCPEndpoints: 10.254.39.3:8443Session Affinity: NoneExternal Traffic Policy: ClusterEvents: &lt;none&gt; 查看pod描述 12345678910[root@k8s-master-01 ~]# kubectl describe pod kubernetes-dashboard-6c655d9445-6zntr --namespace=kube-systemName: kubernetes-dashboard-6c655d9445-6zntrNamespace: kube-systemNode: 172.21.17.31/172.21.17.31Start Time: Thu, 29 Aug 2019 17:47:20 +0800Labels: k8s-app=kubernetes-dashboard pod-template-hash=6c655d9445Annotations: &lt;none&gt;Status: RunningIP: 10.254.39.3 2、授权Dashboard账户集群管理权限若果不进行授权操作，打开dashboard会报错，如下图 新建kubrnetes-dashboard-admin-rbac.yaml 1234567891011121314151617181920# cat kubernetes-dashboard-admin-rbac.yaml apiVersion: v1kind: ServiceAccountmetadata: name: admin-user namespace: kube-system---# Create ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: admin-userroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects:- kind: ServiceAccount name: admin-user namespace: kube-system 执行创建 1# kubectl create -f kubernetes-dashboard-admin-rbac.yaml 找到kubernete-dashboard-admin的token，复制token在dashboard页面进行登录， 1234567891011121314151617[root@k8s-master-01 dashboard]# kubectl -n kube-system get secret | grep admin-useradmin-user-token-qv49g kubernetes.io/service-account-token 3 15h[root@k8s-master-01 dashboard]# kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '&#123;print $1&#125;')Name: admin-user-token-qv49gNamespace: kube-systemLabels: &lt;none&gt;Annotations: kubernetes.io/service-account.name: admin-user kubernetes.io/service-account.uid: ea3f0e3f-ca42-11e9-8716-fa163effd55bType: kubernetes.io/service-account-tokenData====ca.crt: 1359 bytesnamespace: 11 bytestoken: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLXF2NDlnIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJlYTNmMGUzZi1jYTQyLTExZTktODcxNi1mYTE2M2VmZmQ1NWIiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.AbdsJdgi9d0rCYrmvoJkWf32HKSMT03OyOX55aRhPptjzIjDcGxxQYecT0w58N7Z_2L2RwTBfOrm4B3wTEDfFZKgYsnGJQOzJMtZDN9w5YJg2xGQ27E3KisTbbQzd_I5DgxSZWW75GwWf756_bIQpWuXNRO_KjheyWuNNv0tSEYRiXpcboSQpb-8R-Km-vP85mxke6s5cJFSk0WLMjFWow1vOF1ns23NZ5nslEmYOMZF3_Fxybh3LbiCyrpD4c0FtfRcXaBIBqACeyCPRriYMIIJq3OJjI-DzuqUedu1x2xH2prB4mNjxlKt2-7q0M1zCuvm5JhW_LzWgveu9ni2ig 3、配置文件修改说明&nbsp;&nbsp;&nbsp;&nbsp;dashboard 文件被修改，默认的token失效的时间是900秒，15分钟，每15分钟就要进行一次认证，这样对于运维人员来说就不是特别的方便，我们可以通过修改token-ttl参数来设置，主要是修改dashborad的yaml文件，并重新建立即可 3.1、在配置文件修改/添加123456ports:- containerPort: 8443 protocol: TCPargs: - --auto-generate-certificates - --token-ttl=43200 重建pod1[root@k8s-master-01 dashboard]# kubectl apply -f kubernetes-dashboard.yaml 我们可以输入任意节点的ip加30001端口就可以访问dashboard, https://{ip}:30001。 其他操作&nbsp;&nbsp;&nbsp;&nbsp;每天我们来公司要登录dashboard的时候都要去输入一次token，每次去获取token的时候都要输入很长的一串，这里为了方便，可以写一个脚本，要token的时候执行一下脚本，就可以。 创建脚本 123# vim kube-token!#/bin/bashkubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk &apos;&#123;print $1&#125;&apos;) 设置脚本 12# chmod +x kube-token# mv kube-token /usr/bin","pubDate":"Thu, 29 Aug 2019 09:57:46 GMT","guid":"https://xxlaila.github.io/2019/08/29/k8s配置Dashboard/","category":"kubernetes"},{"title":"k8s删除node重新加入","link":"https://xxlaila.github.io/2019/08/29/k8s删除node重新加入/","description":"&nbsp;&nbsp;&nbsp;&nbsp;有时候k8s node 在加入集群的时候不经意的时候弄错啦某些东西，这时候可以把这个node删除，然后重新加入，删除节点之前我们需要做一下常规化的操作，来保障运行在该节点的pod迁移到其他的node上。","pubDate":"Thu, 29 Aug 2019 08:27:00 GMT","guid":"https://xxlaila.github.io/2019/08/29/k8s删除node重新加入/","category":"常用命令"},{"title":"Centos route策略","link":"https://xxlaila.github.io/2019/08/28/Centos-route策略/","description":"场景:&nbsp;&nbsp;&nbsp;&nbsp;公司业务在一个新的云平台上线，该云平台使用的是比较传统的VMware vSphere来做的虚拟化，而且该云平台网络也是我们不清楚的，反正就是不能设置(不能像现在主流云平台自定义网络或者是地址段)，是一个政府的云平台，具体各种奇葩的限制就不说啦，你懂的……","pubDate":"Wed, 28 Aug 2019 02:25:46 GMT","guid":"https://xxlaila.github.io/2019/08/28/Centos-route策略/","category":"Centos"},{"title":"k8s部署ingress","link":"https://xxlaila.github.io/2019/08/26/k8s部署ingress/","description":"在kubernetes 集群中，一个服务安装以后怎么对外提供访问，外部用户怎么来访问我们容器中业务。 1、Ingress 介绍&nbsp;&nbsp;&nbsp;&nbsp;Kubernetes 暴露服务的方式目前只有三种：LoadBlancer Service、NodePort Service、Ingress；本文主要通过Ingress来访问 2、Ingress 是什么&nbsp;&nbsp;&nbsp;&nbsp;Ingress 就是能利用 Nginx、Haproxy 啥的负载均衡器暴露集群内服务的工具问题来了，集群内服务想要暴露出去面临着几个问题：","pubDate":"Mon, 26 Aug 2019 08:24:55 GMT","guid":"https://xxlaila.github.io/2019/08/26/k8s部署ingress/","category":"kubernetes, Ingress"},{"title":"k8s部署Weave Scope","link":"https://xxlaila.github.io/2019/08/26/k8s部署Weave-Scope/","description":"1、Weave Scope介绍&nbsp;&nbsp;&nbsp;&nbsp;weave scope 是Docker和Kubernetes的故障排除和监控，自动生成应用程序的地图，能够直观地了解，监控和控制基于容器的，基于微服务的应用程序。可以实时的了解docker容器，选择容器基础架构的概述，或关注特定的微服务。轻松识别和纠正问题，确保集装箱化应用的稳定性和性能，查看容器的上下文指标，标记和元数据。轻松地在容器内的进程之间导航，以运行容器，在可扩展的可排序表中进行排列。使用给定主机或服务的最大CPU或内存轻松找到容器。直接与您的容器交互：暂停，重新启动和停止容器。启动命令行。全部不离开范围浏览器窗口。 2、部署weave scope初次学习，直接下载官方配置文件，没有经过任何修改，不过相信自己随着学习的进步，会逐渐深入。github地址，官方地址","pubDate":"Mon, 26 Aug 2019 08:08:15 GMT","guid":"https://xxlaila.github.io/2019/08/26/k8s部署Weave-Scope/","category":"kubernetes,Weave Scope"},{"title":"k8s部署zookeeper集群","link":"https://xxlaila.github.io/2019/08/24/k8s部署zookeeper集群/","description":"zk属于有状态服务，需要连接外部存储，吧数据存放在数据盘里面，否则容器挂了，数据没有了 准备工作准备zk的yaml文件","pubDate":"Sat, 24 Aug 2019 07:00:12 GMT","guid":"https://xxlaila.github.io/2019/08/24/k8s部署zookeeper集群/","category":"kubernetes"},{"title":"k8s部署coredns","link":"https://xxlaila.github.io/2019/08/24/k8s部署coredns/","description":"&nbsp;&nbsp;&nbsp;&nbsp;k8s集群中的应用通常是通过ingress实现微服务发布的，前文介绍过在K8S集群中使用traefik实现服务的自动发布，其实现方式是traefik通过集群的DNS服务来解析service对应的集群地址（clusterip），从而将用户的访问请求转发到集群地址上。因此，在部署完集群后的第一件事情应该是配置DNS服务，目前可选的方案有skydns, kube-dns, coredns。&nbsp;&nbsp;&nbsp;&nbsp;kubedns是Kubernetes中的一个内置插件，目前作为一个独立的开源项目维护，见https://github.com/kubernetes/dns。该DNS服务器利用SkyDNS的库来为Kubernetes pod和服务提供DNS请求。CoreDNS项目是SkyDNS2的作者，Miek Gieben采用更模块化，可扩展的框架构建,将此DNS服务器作为KubeDNS的替代品。CoreDNS作为CNCF中的托管的一个项目，在Kuberentes1.9版本中，使用kubeadm方式安装的集群可以通过以下命令直接安装CoreDNS。kubeadm init –feature-gates=CoreDNS=true 准备工作准备coredns的yaml文件","pubDate":"Sat, 24 Aug 2019 06:54:14 GMT","guid":"https://xxlaila.github.io/2019/08/24/k8s部署coredns/","category":"coredns"},{"title":"k8s部署mysql","link":"https://xxlaila.github.io/2019/08/24/k8s部署mysql/","description":"&nbsp;&nbsp;&nbsp;&nbsp;后端存储利用nfs来进行存储数据，nfs安装不阐述，需要注意注意的是在创建mysql 的共享目录的时候参数设定/data/mysql *(rw,sync,no_root_squash,no_subtree_check) 12$ sudo systemctl restart nfs.service$ sudo exportfs -arv 1、创建mysql存储12345678910111213141516171819202122232425262728# cat mysql-pvc.yamlapiVersion: v1kind: PersistentVolumemetadata: name: pvc001 namespace: kube-opsspec: capacity: storage: 10Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Delete nfs: server: 172.21.16.240 path: /data/mysql---kind: PersistentVolumeClaimapiVersion: v1metadata: name: mysql-pvc namespace: kube-opsspec: accessModes: - ReadWriteMany resources: requests: storage: 10Gi","pubDate":"Sat, 24 Aug 2019 06:43:07 GMT","guid":"https://xxlaila.github.io/2019/08/24/k8s部署mysql/","category":"kubernetes"},{"title":"k8s角色访问RBAC","link":"https://xxlaila.github.io/2019/08/24/k8s角色访问RBAC/","description":"1、rbac介绍&nbsp;&nbsp;&nbsp;&nbsp;Kubernetes中的两个用于配置信息的重要资源对象：ConfigMap和Secret，其实到这里我们基本上学习的内容已经覆盖到Kubernetes中一些重要的资源对象了，来部署一个应用程序是完全没有问题的了。在我们演示一个完整的示例之前，我们还需要给大家讲解一个重要的概念：RBAC - 基于角色的访问控制。&nbsp;&nbsp;&nbsp;&nbsp;RBAC使用rbac.authorization.k8s.io API Group 来实现授权决策，允许管理员通过 Kubernetes API 动态配置策略，要启用RBAC，需要在 apiserver 中添加参数–authorization-mode=RBAC，如果使用的kubeadm安装的集群，1.6 版本以上的都默认开启了RBAC，可以通过查看 Master 节点上 apiserver 的静态Pod定义文件： 2、 kubernetes 关于空间权限赋予1、获取并查看 Role ClusterRole RoleBinding ClusterRoleBinding","pubDate":"Sat, 24 Aug 2019 06:35:51 GMT","guid":"https://xxlaila.github.io/2019/08/24/k8s角色访问RBAC/","category":"RBAC"},{"title":"k8s heapster","link":"https://xxlaila.github.io/2019/08/24/k8s-heapster/","description":"1、heapster 介绍&nbsp;&nbsp;&nbsp;&nbsp;Heapster是容器集群监控和性能分析工具,支持Kubernetes和CoreOS。&nbsp;&nbsp;&nbsp;&nbsp;Kubernetes有个监控agent—cAdvisor。在每个kubernetes Node上都会运行cAdvisor,它会收集本机以及容器的监控数据(cpu,memory,filesystem,network,uptime)。在较新的版本中，K8S已经将cAdvisor功能集成到kubelet组件中。每个Node节点可以直接进行web访问。 2、heapster 安装下载heapster的yaml文件，下载完成后我们需要对文件修改，以满足我们的的需求. 2.1、grafana修改grafana添加nodePort: 30003让grafana支持外部访问，我们可以通过这个端口进行但单独的页面配置。","pubDate":"Sat, 24 Aug 2019 06:28:50 GMT","guid":"https://xxlaila.github.io/2019/08/24/k8s-heapster/","category":"kubernetes,heapster"},{"title":"k8s部署常规的jar服务","link":"https://xxlaila.github.io/2019/08/24/k8s部署常规的jar服务/","description":"1、Dockerfile 的配置123456789101112# cat Dockerfile FROM docker.io/xxlaila/centos7.6-jdk1.8:latestMAINTAINER xxlaila \"cq_xxlaila@163.com\"# Install dependent pluginADD target/&lt;BUILD_NAME&gt;.jar /opt/webapps/&lt;BUILD_NAME&gt;.jarWORKDIR /opt/webappsEXPOSE 8000ENTRYPOINT [\"java\", \"-jar\", \"-Dspring.profiles.active=$&#123;RUN_ENV&#125;\", \"&lt;BUILD_NAME&gt;.jar\"]","pubDate":"Sat, 24 Aug 2019 06:25:00 GMT","guid":"https://xxlaila.github.io/2019/08/24/k8s部署常规的jar服务/","category":"kubernetes"},{"title":"k8s部署eureka集群","link":"https://xxlaila.github.io/2019/08/24/k8s部署eureka集群/","description":"eureka 不阐述介绍，这里直接开始在kubernetes下部署eureka集群 1、配置文件的增加eureka 只一个有状态的服务，部署有状态服务我们可以使用StatefulSet 1.1、增加dockerfile12345678910111213$ cat DockerfileFROM docker.io/xxlaila/centos7.6-jdk1.8:latestMAINTAINER xxlaila \"cq_xxlaila@163.com\"# Install dependent pluginADD target/kxl-eureka.jar /opt/webapps/kxl-eureka.jarADD application.yaml /opt/webapps/application.yamlWORKDIR /opt/webappsEXPOSE 8080ENTRYPOINT [\"java\", \"-jar\", \"-Dspring.profiles.active=dev\", \"kxl-eureka.jar\"]","pubDate":"Sat, 24 Aug 2019 06:11:05 GMT","guid":"https://xxlaila.github.io/2019/08/24/k8s部署eureka集群/","category":"eureka,kubernetes"},{"title":"jira接入LDAP","link":"https://xxlaila.github.io/2019/08/24/jira接入LDAP/","description":"场景:&nbsp;&nbsp;&nbsp;&nbsp;之前介绍了jira 和confluence的账户结合，jira和confluence可以使用一个账户，有人员离职之后直接在jira吧用户禁用即可，一端操作，方便两端，但是随着公司人员越来越多，这样的方式已经不在适合这种了，来一个用户就需要去创建，对运维来说，这是重复的工作，提升不了任何效率，而且枯草无味。这里我们就可以使用ldap，jira和confluence都是支持ldap，ldap的好处，这里不阐述，下面来看看如何配置jira介入ldap。confluence还是接入jira，这样我们就只操作ladp和jira，简单省事。 问题点:&nbsp;&nbsp;&nbsp;&nbsp;由于在建立jira和confluence的时候还没有ldap，ldap是后期才接入的，所以这里就存在于怎么吧以前有jira登录的账户认证切换到ldap。而且不影响之前的文档，但是用户权限会影响，问题不大，可以添加。下面开始操作","pubDate":"Sat, 24 Aug 2019 03:19:52 GMT","guid":"https://xxlaila.github.io/2019/08/24/jira接入LDAP/","category":"jira"},{"title":"confluence与jira账户打通","link":"https://xxlaila.github.io/2019/08/24/confluence与jira账户打通/","description":"confluence安装 登录confluence点击用户管理","pubDate":"Sat, 24 Aug 2019 02:54:41 GMT","guid":"https://xxlaila.github.io/2019/08/24/confluence与jira账户打通/","category":"confluence"},{"title":"jira安装和配置","link":"https://xxlaila.github.io/2019/08/24/jira安装和配置/","description":"介绍&nbsp;&nbsp;&nbsp;&nbsp;JIRA是Atlassian公司出品的项目与事务跟踪工具，被广泛应用于缺陷跟踪、客户服务、需求收集、流程审批、任务跟踪、项目跟踪和敏捷管理等工作领域。 环境准备 应用 服务器配置 操作系统 插件 mysql 5.6 + 2/4G/50G centos 7.4 jira 4/8G/200G centos 7.4 jdk1.8","pubDate":"Sat, 24 Aug 2019 02:21:09 GMT","guid":"https://xxlaila.github.io/2019/08/24/jira安装和配置/","category":"jira"},{"title":"git清空commit记录方法","link":"https://xxlaila.github.io/2019/08/24/git清空commit记录方法/","description":"说明:&nbsp;&nbsp;&nbsp;&nbsp;例如将代码提交到git仓库，将一些敏感信息提交，所以需要删除提交记录以彻底清除提交信息，以得到一个干净的仓库且代码不变 1.Checkout1$ git checkout --orphan latest_branch 2. Add all the files1$ git add -A 3. Commit the changes1$ git commit -am \"commit message\"","pubDate":"Sat, 24 Aug 2019 01:36:36 GMT","guid":"https://xxlaila.github.io/2019/08/24/git清空commit记录方法/","category":"git"},{"title":"confluence_install","link":"https://xxlaila.github.io/2019/08/24/confluence-install/","description":"&nbsp;&nbsp;&nbsp;&nbsp;Confluence是一个专业的企业知识管理与协同软件，也可以用于构建企业wiki。使用简单，但它强大的编辑和站点管理特征能够帮助团队成员之间共享信息、文档协作、集体讨论，信息推送。为团队提供一个协作环境。在这里，团队成员齐心协力，各擅其能，协同地编写文档和管理项目。从此打破不同团队、不同部门以及个人之间信息孤岛的僵局，Confluence真正实现了组织资源共享。 环境准备 系统版本 插件 软件 版本 服务配置 centos 7.4 mysql 5.6+ 2/4G/50G centos 7.4 jdk 1.8 confluence 6.12.2 4/8G/200G confluence 6.12.2安装并破解，mysql 版本这里使用的是5.7.24","pubDate":"Sat, 24 Aug 2019 00:57:59 GMT","guid":"https://xxlaila.github.io/2019/08/24/confluence-install/","category":"confluence"},{"title":"nexus3搭建npm私服","link":"https://xxlaila.github.io/2019/08/23/nexus3搭建npm私服/","description":"介绍&nbsp;&nbsp;&nbsp;&nbsp;公司前端全是nodejs的，nodejs在install的时候往往是连接外网，或者是设置taobao源，即使是设置了taobao源，但是还是解决不了慢的问题，为此搭建了一个内部的npm私服，这里用google一下有很多都可以来进行搭建npm私服，然后也看到了nexus也可以来做，正好maven私服也是用的这个，都是3版本，为此选择了nexus来做npm的私服，和maven一套便于维护。 nexus安装&nbsp;&nbsp;&nbsp;&nbsp;不介绍，安装完成nexus后，在浏览器打开并进行登录，第一次安装登录nexus的默认用户admin,默认密码是admin123","pubDate":"Fri, 23 Aug 2019 01:41:41 GMT","guid":"https://xxlaila.github.io/2019/08/23/nexus3搭建npm私服/","category":"nexus,npm"},{"title":"nginx URL 斜杠问题","link":"https://xxlaila.github.io/2019/08/22/nginx-URL-斜杠问题/","description":"问题今天公司新上的一个前端应用遇到一个问题，那就是在微信登录界面扫码登录之后，微信回调给我们的地址多加了一个斜杠; 错误的地址:http://a.xxlaila.com/wx.html/?code=011amZet0h1IUf19Fvht0jg4ft0amZeN正确的地址:http://a.xxlaila.com/wx.html?code=011amZet0h1IUf19Fvht0jg4ft0amZeN 在nginx上配置需要吧这个斜杠删除掉。用户才能正常的访问； 实例在配置文件里面增加如下配置项","pubDate":"Thu, 22 Aug 2019 07:29:37 GMT","guid":"https://xxlaila.github.io/2019/08/22/nginx-URL-斜杠问题/","category":"nginx"},{"title":"kubernetes-ci/cd-(四)","link":"https://xxlaila.github.io/2019/08/20/kubernetes-ci-cd-四/","description":"1、Blue Ocean安装Blue Ocean插件 1.1、创建pipeline 配置代码库的地址 然后配置授权账户 在这儿之前git库里面必须存在于jenkinsfile文件，pipeline会自动去扫描代码库里面的分支，然后根据每一个分支建立一个类似于job的形式，然后我们可以根据每一个分支进行部署，可以执行定时触发，部署 这儿，只有一个分支存在于jenkinsfile，所以只显示一个分支，如下图：","pubDate":"Tue, 20 Aug 2019 06:54:48 GMT","guid":"https://xxlaila.github.io/2019/08/20/kubernetes-ci-cd-四/","category":"jenkins, ci/cd"},{"title":"kubernetes-ci/cd-(三)","link":"https://xxlaila.github.io/2019/08/20/kubernetes-ci-cd-三/","description":"&nbsp;&nbsp;&nbsp;&nbsp;jenkins 配置完成后，最终实现的是ci/cd，在编译的过程中，经常会遇到后端java的，前端nodejs的，这里就需要进行一个k8s在调度的时候生产pod来进行指定pod进行编译 1、制作容器自定义一个容器，里面包含了 java，nodejs的所需要的环境，同时需要同步容器的时间，包含来jenkins的node 12345678910111213141516171819202122# cat DockerfileFROM docker.io/centos:latestMAINTAINER xxlaila &quot;cq_xxlaila@163.com&quot;# Install dependent pluginENV VERSION v10.15.1RUN yum install -y wget \\ git \\ java-1.8.0-openjdk.x86_64 \\ &amp;&amp; curl -sL https://rpm.nodesource.com/setup_11.x | bash - \\ &amp;&amp; yum install -y gcc gcc-c++ make \\ &amp;&amp; yum install -y nodejs \\ &amp;&amp; yum clean all# System variable settingRUN echo &quot;LANG=zh_CN.UTF-8&quot; &gt;&gt; /etc/locale.conf \\ &amp;&amp; source /etc/locale.conf \\ &amp;&amp; cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \\ &amp;&amp; echo &quot;Asia/shanghai&quot; &gt;&gt; /etc/timezone \\ &amp;&amp; groupadd -g 10000 jenkins \\ &amp;&amp; useradd -g jenkins -u 10000 jenkinsEXPOSE 50000","pubDate":"Tue, 20 Aug 2019 06:17:48 GMT","guid":"https://xxlaila.github.io/2019/08/20/kubernetes-ci-cd-三/","category":"jenkins ci/cd"},{"title":"kubernetes-ci/cd-(二)","link":"https://xxlaila.github.io/2019/08/20/kubernetes-ci-cd-二/","description":"基于jenkins pipeline进行部署1、jenkins pipeline介绍&nbsp;&nbsp;&nbsp;&nbsp;要实现在 Jenkins 中的构建工作，可以有多种方式，我们这里采用比较常用的 Pipeline 这种方式。Pipeline，简单来说，就是一套运行在 Jenkins 上的工作流框架，将原来独立运行于单个或者多个节点的任务连接起来，实现单个任务难以完成的复杂流程编排和可视化的工作。 Jenkins Pipeline 有几个核心概念: Node：节点，一个 Node 就是一个 Jenkins 节点，Master 或者 Agent，是执行 Step 的具体运行环境，比如我们之前动态运行的 Jenkins Slave 就是一个 Node 节点 Stage：阶段，一个 Pipeline 可以划分为若干个 Stage，每个 Stage 代表一组操作，比如：Build、Test、Deploy，Stage 是一个逻辑分组的概念，可以跨多个 Node Step：步骤，Step 是最基本的操作单元，可以是打印一句话，也可以是构建一个 Docker 镜像，由各类 Jenkins 插件提供，比如命令：sh ‘make’，就相当于我们平时 shell 终端中执行 make 命令一样。 那么我们如何创建 Jenkins Pipline 呢？ Pipeline 脚本是由 Groovy 语言实现的，但是我们没必要单独去学习 Groovy，当然你会的话最好 Pipeline 支持两种语法：Declarative(声明式)和 Scripted Pipeline(脚本式)语法 Pipeline 也有两种创建方法：可以直接在 Jenkins 的 Web UI 界面中输入脚本；也可以通过创建一个 Jenkinsfile 脚本文件放入项目源码库中 一般我们都推荐在 Jenkins 中直接从源代码控制(SCMD)中直接载入 Jenkinsfile Pipeline 这种方法创建一个简单的 Pipeline 我们这里来给大家快速创建一个简单的 Pipeline，直接在 Jenkins 的 Web UI 界面中输入脚本运行。 新建 Job：在 Web UI 中点击 New Item -&gt; 输入名称：pipeline-demo -&gt; 选择下面的 Pipeline -&gt; 点击 OK 配置：在最下方的 Pipeline 区域输入如下 Script 脚本，然后点击保存。","pubDate":"Tue, 20 Aug 2019 05:46:14 GMT","guid":"https://xxlaila.github.io/2019/08/20/kubernetes-ci-cd-二/","category":"jenkins, ci/cd"},{"title":"zabbix企业微信告警","link":"https://xxlaila.github.io/2019/08/20/zabbix企业微信告警/","description":"&nbsp;&nbsp;&nbsp;&nbsp;Zabbix可以通过多种方式把告警信息发送到指定人，常用的有邮件，短信报警方式，但是越来越多的企业开始使用zabbix结合微信作为主要的告警方式，这样可以及时有效的把告警信息推送到接收人，方便告警的及时处理。&nbsp;&nbsp;&nbsp;&nbsp;微信企业号需要先在企业通信录新建该员工，该员工才能关注该企业号，这样就能实现告警信息的私密性。如果使用公众号，则只要所有关注了该公众号的人都能收到告警消息，容易造成信息泄露。而且员工数少于200人的企业号是不用钱的，也没有任何申请限制. 1、脚本存放目录/usr/lib/zabbix/alertscripts，脚本的权限是zabbix 账户，具有可执行权限","pubDate":"Tue, 20 Aug 2019 03:09:37 GMT","guid":"https://xxlaila.github.io/2019/08/20/zabbix企业微信告警/","category":"zabbix"},{"title":"帧中继配置","link":"https://xxlaila.github.io/2019/08/19/帧中继配置/","description":"点对点配置 RA配置: 1234567[RA]int s0/0[RA-s0/0]encap frame-relay 封装帧中继协议[RA-s0/0] frame-relay intf dte[RA-s0/0]frame-relay interface-dlci 102 设置本接口对应的INTERFACE-DLCI号[RA-s0/0]frame-relay map ip 172.16.1.2 102 建立对端协议地址与本地INTERFACE-DLCI号的映射关系[RA-s0/0]ip add 172.16.1.1 255.255.255.0 配置本接口IP地址[RA-s0/0]no sh 打开此物理接口 RB配置: 1234567[RB]int s0/0[RB-s0/0]encap frame-relay 封装帧中继协议[RA-s0/0] frame-relay intf dte[RB-s0/0]frame-relay interface-dlci 201 设置本接口对应的INTERFACE-DLCI号[RB-s0/0]frame-relay map ip 172.16.1.1 201 建立对端协议地址与本地INTERFACE-DLCI号的映射关系[RB-s0/0]ip add 172.16.1.2 255.255.255.0 配置本接口IP地址[RB-s0/0]no sh 打开此物理接口","pubDate":"Mon, 19 Aug 2019 13:43:35 GMT","guid":"https://xxlaila.github.io/2019/08/19/帧中继配置/","category":"帧中继, 网络设备"},{"title":"kubernetes ci/cd(一)","link":"https://xxlaila.github.io/2019/08/12/kubernetes-ci-cd-一/","description":"基于jenkins的CI/CD安装 &nbsp;&nbsp;&nbsp;&nbsp; jenkins一个流行的持续集成/发布工具，在Kubernetes使用,持续构建与发布是我们日常工作中必不可少的一个步骤，目前大多公司都采用 Jenkins 集群来搭建符合需求的 CI/CD 流程，然而传统的 Jenkins Slave 一主多从方式会存在一些痛点，比如：主 Master 发生单点故障时，整个流程都不可用了；每个 Slave 的配置环境不一样，来完成不同语言的编译打包等操作，但是这些差异化的配置导致管理起来非常不方便，维护起来也是比较费劲；资源分配不均衡，有的 Slave 要运行的 job 出现排队等待，而有的 Slave 处于空闲状态；最后资源有浪费，每台 Slave 可能是实体机或者 VM，当 Slave 处于空闲状态时，也不会完全释放掉资源。 &nbsp;&nbsp;&nbsp;&nbsp;提到基于Kubernete的CI/CD，可以使用的工具有很多，比如Jenkins、Gitlab CI已经新兴的drone之类的，我们这里会使用大家最为熟悉的Jenins来做CI/CD的工具。 优点: Jenkins 安装完成了，接下来我们不用急着就去使用，我们要了解下在 Kubernetes 环境下面使用 Jenkins 有什么好处。都知道持续构建与发布是我们日常工作中必不可少的一个步骤，目前大多公司都采用 Jenkins 集群来搭建符合需求的 CI/CD 流程，然而传统的 Jenkins Slave 一主多从方式会存在一些痛点，比如: E 主 Master 发生单点故障时，整个流程都不可用了。 E 每个 Slave 的配置环境不一样，来完成不同语言的编译打包等操作，但是这些差异化的配置导致管理起来非常不方便，维护起来也是比较费劲。 E 资源分配不均衡，有的 Slave 要运行的 job 出现排队等待，而有的 Slave 处于空闲状态。 E 资源有浪费，每台 Slave 可能是物理机或者虚拟机，当 Slave 处于空闲状态时，也不会完全释放掉资源。 正因为这些种种痛点，我们渴望一种更高效更可靠的方式来完成这个 CI/CD 流程，而 Docker 虚拟化容器技术能很好的解决这个痛点，又特别是在 Kubernetes 集群环境下面能够更好来解决上面的问题，下图是基于 Kubernetes 搭建 Jenkins 集群的简单示意图 &nbsp;&nbsp;&nbsp;&nbsp;可以看到 Jenkins Master 和 Jenkins Slave 以 Pod 形式运行在 Kubernetes 集群的 Node 上，Master 运行在其中一个节点，并且将其配置数据存储到一个 Volume 上去，Slave 运行在各个节点上，并且它不是一直处于运行状态，它会按照需求动态的创建并自动删除。","pubDate":"Mon, 12 Aug 2019 06:56:04 GMT","guid":"https://xxlaila.github.io/2019/08/12/kubernetes-ci-cd-一/","category":"ci/cd"}]}