{"title":"懒羊羊","description":"星际拾荒者","language":"zh-CN","link":"https://xxlaila.github.io","pubDate":"Fri, 30 Aug 2019 03:21:08 GMT","lastBuildDate":"Mon, 02 Sep 2019 07:03:57 GMT","generator":"hexo-generator-json-feed","webMaster":"xxlaila","items":[{"title":"k8s部署istio","link":"https://xxlaila.github.io/2019/08/30/k8s部署istio/","description":"istio 介绍&nbsp;&nbsp;&nbsp;&nbsp;istio代表的是Service Mesh的方案实现，Istio 有助于降低这些部署的复杂性，并减轻开发团队的压力。提供一种简单的方式来为已部署的服务建立网络，且提供具有负载均衡、服务间认证、监控、流量管理等功能。 服务网格（Service Mesh）&nbsp;&nbsp;&nbsp;&nbsp;服务网格（Service Mesh）用于描述构成这些应用程序的微服务网络以及应用之间的交互。随着规模和复杂性的增长，服务网格越来越难以理解和管理。它的需求包括服务发现、负载均衡、故障恢复、指标收集和监控以及通常更加复杂的运维需求，例如 A/B 测试、金丝雀发布、限流、访问控制和端到端认证等。而istio刚好提供了一套完整的解决方案，通过控制整个服务器网格提供行为洞察和操作控制来满足微服务应用的多样化 架构Istio 服务网格逻辑上分为数据平面和控制平面。 数据平面由一组以 sidecar 方式部署的智能代理（Envoy）组成。这些代理可以调节和控制微服务及 Mixer 之间所有的网络通信。 控制平面负责管理和配置代理来路由流量。此外控制平面配置 Mixer 以实施策略和收集遥测数据。下图显示了构成每个面板的不同组件: istio 组件 Envoy: Istio 使用 Envoy 代理的扩展版本,用于调解服务网格中所有服务的所有入站和出站流量,属于数据层面 Mixer: 是一个独立于平台的组件，负责在服务网格上执行访问控制和使用策略，并从 Envoy 代理和其他服务收集遥测数据 Pilot: 为 Envoy sidecar 提供服务发现功能，为智能路由（例如 A/B 测试、金丝雀部署等）和弹性（超时、重试、熔断器等）提供流量管理功能 Citadel: 通过内置身份和凭证管理赋能强大的服务间和最终用户身份验证。可用于升级服务网格中未加密的流量，并为运维人员提供基于服务标识而不是网络控制的强制执行策略的能力 Galley: 代表其他的 Istio 控制平面组件，用来验证用户编写的 Istio API 配置。随着时间的推移，Galley 将接管 Istio 获取配置、 处理和分配组件的顶级责任 详细信息查看官方,官方支持中文，对于英语差的人来说，再好不过啦。 1、下载istio包执行下载和自动解压缩 1234# curl -L https://git.io/getLatestIstio | ISTIO_VERSION=1.2.5 sh -# cd istio-1.1.14/bin# cp istioctl /usr/bin/# 吧包分发给其他的node节点和master节点 安装目录中包含： 在 install/: 目录中包含了 Kubernetes 安装所需的 .yaml 文件 samples/: 目录中是示例应用 istioctl: 客户端文件保存在 bin/ 目录之中。istioctl 的功能是手工进行 Envoy Sidecar 的注入。 istio.VERSION: 配置文件 2、在kubernetes 集群中开始安装Istio 会被安装到自己的 istio-system 命名空间，并且能够对所有其他命名空间的服务进行管理。这里使用的宽容模式的mutual TLS来进行安装。 2.1、使用 kubectl apply 安装 Istio 的自定义资源定义（CRD)1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# for i in install/kubernetes/helm/istio-init/files/crd*yaml; do kubectl apply -f $i; donecustomresourcedefinition.apiextensions.k8s.io/virtualservices.networking.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/destinationrules.networking.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/serviceentries.networking.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/gateways.networking.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/envoyfilters.networking.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/clusterrbacconfigs.rbac.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/policies.authentication.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/meshpolicies.authentication.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/httpapispecbindings.config.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/httpapispecs.config.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/quotaspecbindings.config.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/quotaspecs.config.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/rules.config.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/attributemanifests.config.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/bypasses.config.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/circonuses.config.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/deniers.config.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/fluentds.config.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/kubernetesenvs.config.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/listcheckers.config.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/memquotas.config.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/noops.config.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/opas.config.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/prometheuses.config.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/rbacs.config.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/redisquotas.config.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/signalfxs.config.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/solarwindses.config.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/stackdrivers.config.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/statsds.config.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/stdios.config.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/apikeys.config.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/authorizations.config.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/checknothings.config.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/kuberneteses.config.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/listentries.config.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/logentries.config.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/edges.config.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/metrics.config.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/quotas.config.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/reportnothings.config.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/tracespans.config.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/rbacconfigs.rbac.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/serviceroles.rbac.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/servicerolebindings.rbac.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/adapters.config.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/instances.config.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/templates.config.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/handlers.config.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/cloudwatches.config.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/dogstatsds.config.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/sidecars.networking.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/zipkins.config.istio.io createdcustomresourcedefinition.apiextensions.k8s.io/clusterissuers.certmanager.k8s.io createdcustomresourcedefinition.apiextensions.k8s.io/issuers.certmanager.k8s.io createdcustomresourcedefinition.apiextensions.k8s.io/certificates.certmanager.k8s.io createdcustomresourcedefinition.apiextensions.k8s.io/orders.certmanager.k8s.io createdcustomresourcedefinition.apiextensions.k8s.io/challenges.certmanager.k8s.io created 查看安装的CRD123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# kubectl get CustomResourceDefinitionNAME CREATED ATadapters.config.istio.io 2019-08-30T07:14:26Zapikeys.config.istio.io 2019-08-30T07:14:21Zattributemanifests.config.istio.io 2019-08-30T07:14:14Zauthorizations.config.istio.io 2019-08-30T07:14:22Zbypasses.config.istio.io 2019-08-30T07:14:15Zcertificates.certmanager.k8s.io 2019-08-30T07:14:31Zchallenges.certmanager.k8s.io 2019-08-30T07:14:32Zchecknothings.config.istio.io 2019-08-30T07:14:22Zcirconuses.config.istio.io 2019-08-30T07:14:15Zcloudwatches.config.istio.io 2019-08-30T07:14:28Zclusterissuers.certmanager.k8s.io 2019-08-30T07:14:30Zclusterrbacconfigs.rbac.istio.io 2019-08-30T07:14:12Zdeniers.config.istio.io 2019-08-30T07:14:15Zdestinationrules.networking.istio.io 2019-08-30T07:14:11Zdogstatsds.config.istio.io 2019-08-30T07:14:29Zedges.config.istio.io 2019-08-30T07:14:23Zenvoyfilters.networking.istio.io 2019-08-30T07:14:12Zfluentds.config.istio.io 2019-08-30T07:14:16Zgateways.networking.istio.io 2019-08-30T07:14:12Zhandlers.config.istio.io 2019-08-30T07:14:27Zhttpapispecbindings.config.istio.io 2019-08-30T07:14:13Zhttpapispecs.config.istio.io 2019-08-30T07:14:14Zinstances.config.istio.io 2019-08-30T07:14:26Zissuers.certmanager.k8s.io 2019-08-30T07:14:30Zkubernetesenvs.config.istio.io 2019-08-30T07:14:16Zkuberneteses.config.istio.io 2019-08-30T07:14:22Zlistcheckers.config.istio.io 2019-08-30T07:14:17Zlistentries.config.istio.io 2019-08-30T07:14:23Zlogentries.config.istio.io 2019-08-30T07:14:23Zmemquotas.config.istio.io 2019-08-30T07:14:18Zmeshpolicies.authentication.istio.io 2019-08-30T07:14:13Zmetrics.config.istio.io 2019-08-30T07:14:23Znoops.config.istio.io 2019-08-30T07:14:18Zopas.config.istio.io 2019-08-30T07:14:18Zorders.certmanager.k8s.io 2019-08-30T07:14:31Zpolicies.authentication.istio.io 2019-08-30T07:14:12Zprometheuses.config.istio.io 2019-08-30T07:14:19Zquotas.config.istio.io 2019-08-30T07:14:24Zquotaspecbindings.config.istio.io 2019-08-30T07:14:14Zquotaspecs.config.istio.io 2019-08-30T07:14:14Zrbacconfigs.rbac.istio.io 2019-08-30T07:14:25Zrbacs.config.istio.io 2019-08-30T07:14:19Zredisquotas.config.istio.io 2019-08-30T07:14:19Zreportnothings.config.istio.io 2019-08-30T07:14:24Zrules.config.istio.io 2019-08-30T07:14:14Zserviceentries.networking.istio.io 2019-08-30T07:14:11Zservicerolebindings.rbac.istio.io 2019-08-30T07:14:25Zserviceroles.rbac.istio.io 2019-08-30T07:14:25Zsidecars.networking.istio.io 2019-08-30T07:14:29Zsignalfxs.config.istio.io 2019-08-30T07:14:20Zsolarwindses.config.istio.io 2019-08-30T07:14:20Zstackdrivers.config.istio.io 2019-08-30T07:14:20Zstatsds.config.istio.io 2019-08-30T07:14:21Zstdios.config.istio.io 2019-08-30T07:14:21Ztemplates.config.istio.io 2019-08-30T07:14:27Ztracespans.config.istio.io 2019-08-30T07:14:24Zvirtualservices.networking.istio.io 2019-08-30T07:14:10Zzipkins.config.istio.io 2019-08-30T07:14:29Z 2.2、安装一个demo环境1# kubectl apply -f install/kubernetes/istio-demo.yaml","pubDate":"Fri, 30 Aug 2019 03:21:08 GMT","guid":"https://xxlaila.github.io/2019/08/30/k8s部署istio/","category":"istio"},{"title":"k8s配置Dashboard","link":"https://xxlaila.github.io/2019/08/29/k8s配置Dashboard/","description":"&nbsp;&nbsp;&nbsp;&nbsp;K8S Dashboard是官方的一个基于WEB的用户界面，专门用来管理K8S集群，并可展示集群的状态。K8S集群安装好后默认没有包含Dashboard，我们需要额外创建它。 1、安装dashboard1.1、下载准备需要的文件经过修改过后的文件，已经可以正常使用的文件","pubDate":"Thu, 29 Aug 2019 09:57:46 GMT","guid":"https://xxlaila.github.io/2019/08/29/k8s配置Dashboard/","category":"kubernetes"},{"title":"k8s删除node重新加入","link":"https://xxlaila.github.io/2019/08/29/k8s删除node重新加入/","description":"&nbsp;&nbsp;&nbsp;&nbsp;有时候k8s node 在加入集群的时候不经意的时候弄错啦某些东西，这时候可以把这个node删除，然后重新加入，删除节点之前我们需要做一下常规化的操作，来保障运行在该节点的pod迁移到其他的node上。","pubDate":"Thu, 29 Aug 2019 08:27:00 GMT","guid":"https://xxlaila.github.io/2019/08/29/k8s删除node重新加入/","category":"常用命令"},{"title":"Centos route策略","link":"https://xxlaila.github.io/2019/08/28/Centos-route策略/","description":"场景:&nbsp;&nbsp;&nbsp;&nbsp;公司业务在一个新的云平台上线，该云平台使用的是比较传统的VMware vSphere来做的虚拟化，而且该云平台网络也是我们不清楚的，反正就是不能设置(不能像现在主流云平台自定义网络或者是地址段)，是一个政府的云平台，具体各种奇葩的限制就不说啦，你懂的……","pubDate":"Wed, 28 Aug 2019 02:25:46 GMT","guid":"https://xxlaila.github.io/2019/08/28/Centos-route策略/","category":"Centos"},{"title":"k8s部署ingress","link":"https://xxlaila.github.io/2019/08/26/k8s部署ingress/","description":"在kubernetes 集群中，一个服务安装以后怎么对外提供访问，外部用户怎么来访问我们容器中业务。 1、Ingress 介绍&nbsp;&nbsp;&nbsp;&nbsp;Kubernetes 暴露服务的方式目前只有三种：LoadBlancer Service、NodePort Service、Ingress；本文主要通过Ingress来访问 2、Ingress 是什么&nbsp;&nbsp;&nbsp;&nbsp;Ingress 就是能利用 Nginx、Haproxy 啥的负载均衡器暴露集群内服务的工具问题来了，集群内服务想要暴露出去面临着几个问题：","pubDate":"Mon, 26 Aug 2019 08:24:55 GMT","guid":"https://xxlaila.github.io/2019/08/26/k8s部署ingress/","category":"kubernetes, Ingress"},{"title":"k8s部署Weave Scope","link":"https://xxlaila.github.io/2019/08/26/k8s部署Weave-Scope/","description":"1、Weave Scope介绍&nbsp;&nbsp;&nbsp;&nbsp;weave scope 是Docker和Kubernetes的故障排除和监控，自动生成应用程序的地图，能够直观地了解，监控和控制基于容器的，基于微服务的应用程序。可以实时的了解docker容器，选择容器基础架构的概述，或关注特定的微服务。轻松识别和纠正问题，确保集装箱化应用的稳定性和性能，查看容器的上下文指标，标记和元数据。轻松地在容器内的进程之间导航，以运行容器，在可扩展的可排序表中进行排列。使用给定主机或服务的最大CPU或内存轻松找到容器。直接与您的容器交互：暂停，重新启动和停止容器。启动命令行。全部不离开范围浏览器窗口。 2、部署weave scope初次学习，直接下载官方配置文件，没有经过任何修改，不过相信自己随着学习的进步，会逐渐深入。github地址，官方地址","pubDate":"Mon, 26 Aug 2019 08:08:15 GMT","guid":"https://xxlaila.github.io/2019/08/26/k8s部署Weave-Scope/","category":"kubernetes,Weave Scope"},{"title":"k8s部署zookeeper集群","link":"https://xxlaila.github.io/2019/08/24/k8s部署zookeeper集群/","description":"zk属于有状态服务，需要连接外部存储，吧数据存放在数据盘里面，否则容器挂了，数据没有了 准备工作准备zk的yaml文件","pubDate":"Sat, 24 Aug 2019 07:00:12 GMT","guid":"https://xxlaila.github.io/2019/08/24/k8s部署zookeeper集群/","category":"kubernetes"},{"title":"k8s部署coredns","link":"https://xxlaila.github.io/2019/08/24/k8s部署coredns/","description":"&nbsp;&nbsp;&nbsp;&nbsp;k8s集群中的应用通常是通过ingress实现微服务发布的，前文介绍过在K8S集群中使用traefik实现服务的自动发布，其实现方式是traefik通过集群的DNS服务来解析service对应的集群地址（clusterip），从而将用户的访问请求转发到集群地址上。因此，在部署完集群后的第一件事情应该是配置DNS服务，目前可选的方案有skydns, kube-dns, coredns。&nbsp;&nbsp;&nbsp;&nbsp;kubedns是Kubernetes中的一个内置插件，目前作为一个独立的开源项目维护，见https://github.com/kubernetes/dns。该DNS服务器利用SkyDNS的库来为Kubernetes pod和服务提供DNS请求。CoreDNS项目是SkyDNS2的作者，Miek Gieben采用更模块化，可扩展的框架构建,将此DNS服务器作为KubeDNS的替代品。CoreDNS作为CNCF中的托管的一个项目，在Kuberentes1.9版本中，使用kubeadm方式安装的集群可以通过以下命令直接安装CoreDNS。kubeadm init –feature-gates=CoreDNS=true 准备工作准备coredns的yaml文件","pubDate":"Sat, 24 Aug 2019 06:54:14 GMT","guid":"https://xxlaila.github.io/2019/08/24/k8s部署coredns/","category":"coredns"},{"title":"k8s部署mysql","link":"https://xxlaila.github.io/2019/08/24/k8s部署mysql/","description":"&nbsp;&nbsp;&nbsp;&nbsp;后端存储利用nfs来进行存储数据，nfs安装不阐述，需要注意注意的是在创建mysql 的共享目录的时候参数设定/data/mysql *(rw,sync,no_root_squash,no_subtree_check) 12$ sudo systemctl restart nfs.service$ sudo exportfs -arv 1、创建mysql存储12345678910111213141516171819202122232425262728# cat mysql-pvc.yamlapiVersion: v1kind: PersistentVolumemetadata: name: pvc001 namespace: kube-opsspec: capacity: storage: 10Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Delete nfs: server: 172.21.16.240 path: /data/mysql---kind: PersistentVolumeClaimapiVersion: v1metadata: name: mysql-pvc namespace: kube-opsspec: accessModes: - ReadWriteMany resources: requests: storage: 10Gi","pubDate":"Sat, 24 Aug 2019 06:43:07 GMT","guid":"https://xxlaila.github.io/2019/08/24/k8s部署mysql/","category":"kubernetes"},{"title":"k8s角色访问RBAC","link":"https://xxlaila.github.io/2019/08/24/k8s角色访问RBAC/","description":"1、rbac介绍&nbsp;&nbsp;&nbsp;&nbsp;Kubernetes中的两个用于配置信息的重要资源对象：ConfigMap和Secret，其实到这里我们基本上学习的内容已经覆盖到Kubernetes中一些重要的资源对象了，来部署一个应用程序是完全没有问题的了。在我们演示一个完整的示例之前，我们还需要给大家讲解一个重要的概念：RBAC - 基于角色的访问控制。&nbsp;&nbsp;&nbsp;&nbsp;RBAC使用rbac.authorization.k8s.io API Group 来实现授权决策，允许管理员通过 Kubernetes API 动态配置策略，要启用RBAC，需要在 apiserver 中添加参数–authorization-mode=RBAC，如果使用的kubeadm安装的集群，1.6 版本以上的都默认开启了RBAC，可以通过查看 Master 节点上 apiserver 的静态Pod定义文件： 2、 kubernetes 关于空间权限赋予1、获取并查看 Role ClusterRole RoleBinding ClusterRoleBinding","pubDate":"Sat, 24 Aug 2019 06:35:51 GMT","guid":"https://xxlaila.github.io/2019/08/24/k8s角色访问RBAC/","category":"RBAC"},{"title":"k8s heapster","link":"https://xxlaila.github.io/2019/08/24/k8s-heapster/","description":"1、heapster 介绍&nbsp;&nbsp;&nbsp;&nbsp;Heapster是容器集群监控和性能分析工具,支持Kubernetes和CoreOS。&nbsp;&nbsp;&nbsp;&nbsp;Kubernetes有个监控agent—cAdvisor。在每个kubernetes Node上都会运行cAdvisor,它会收集本机以及容器的监控数据(cpu,memory,filesystem,network,uptime)。在较新的版本中，K8S已经将cAdvisor功能集成到kubelet组件中。每个Node节点可以直接进行web访问。 2、heapster 安装下载heapster的yaml文件，下载完成后我们需要对文件修改，以满足我们的的需求. 2.1、grafana修改grafana添加nodePort: 30003让grafana支持外部访问，我们可以通过这个端口进行但单独的页面配置。","pubDate":"Sat, 24 Aug 2019 06:28:50 GMT","guid":"https://xxlaila.github.io/2019/08/24/k8s-heapster/","category":"kubernetes,heapster"},{"title":"k8s部署常规的jar服务","link":"https://xxlaila.github.io/2019/08/24/k8s部署常规的jar服务/","description":"1、Dockerfile 的配置123456789101112# cat Dockerfile FROM docker.io/xxlaila/centos7.6-jdk1.8:latestMAINTAINER xxlaila \"cq_xxlaila@163.com\"# Install dependent pluginADD target/&lt;BUILD_NAME&gt;.jar /opt/webapps/&lt;BUILD_NAME&gt;.jarWORKDIR /opt/webappsEXPOSE 8000ENTRYPOINT [\"java\", \"-jar\", \"-Dspring.profiles.active=$&#123;RUN_ENV&#125;\", \"&lt;BUILD_NAME&gt;.jar\"]","pubDate":"Sat, 24 Aug 2019 06:25:00 GMT","guid":"https://xxlaila.github.io/2019/08/24/k8s部署常规的jar服务/","category":"kubernetes"},{"title":"k8s部署eureka集群","link":"https://xxlaila.github.io/2019/08/24/k8s部署eureka集群/","description":"eureka 不阐述介绍，这里直接开始在kubernetes下部署eureka集群 1、配置文件的增加eureka 只一个有状态的服务，部署有状态服务我们可以使用StatefulSet 1.1、增加dockerfile12345678910111213$ cat DockerfileFROM docker.io/xxlaila/centos7.6-jdk1.8:latestMAINTAINER xxlaila \"cq_xxlaila@163.com\"# Install dependent pluginADD target/kxl-eureka.jar /opt/webapps/kxl-eureka.jarADD application.yaml /opt/webapps/application.yamlWORKDIR /opt/webappsEXPOSE 8080ENTRYPOINT [\"java\", \"-jar\", \"-Dspring.profiles.active=dev\", \"kxl-eureka.jar\"]","pubDate":"Sat, 24 Aug 2019 06:11:05 GMT","guid":"https://xxlaila.github.io/2019/08/24/k8s部署eureka集群/","category":"eureka,kubernetes"},{"title":"jira接入LDAP","link":"https://xxlaila.github.io/2019/08/24/jira接入LDAP/","description":"场景:&nbsp;&nbsp;&nbsp;&nbsp;之前介绍了jira 和confluence的账户结合，jira和confluence可以使用一个账户，有人员离职之后直接在jira吧用户禁用即可，一端操作，方便两端，但是随着公司人员越来越多，这样的方式已经不在适合这种了，来一个用户就需要去创建，对运维来说，这是重复的工作，提升不了任何效率，而且枯草无味。这里我们就可以使用ldap，jira和confluence都是支持ldap，ldap的好处，这里不阐述，下面来看看如何配置jira介入ldap。confluence还是接入jira，这样我们就只操作ladp和jira，简单省事。 问题点:&nbsp;&nbsp;&nbsp;&nbsp;由于在建立jira和confluence的时候还没有ldap，ldap是后期才接入的，所以这里就存在于怎么吧以前有jira登录的账户认证切换到ldap。而且不影响之前的文档，但是用户权限会影响，问题不大，可以添加。下面开始操作","pubDate":"Sat, 24 Aug 2019 03:19:52 GMT","guid":"https://xxlaila.github.io/2019/08/24/jira接入LDAP/","category":"jira"},{"title":"confluence与jira账户打通","link":"https://xxlaila.github.io/2019/08/24/confluence与jira账户打通/","description":"confluence安装 登录confluence点击用户管理","pubDate":"Sat, 24 Aug 2019 02:54:41 GMT","guid":"https://xxlaila.github.io/2019/08/24/confluence与jira账户打通/","category":"confluence"},{"title":"jira安装和配置","link":"https://xxlaila.github.io/2019/08/24/jira安装和配置/","description":"介绍&nbsp;&nbsp;&nbsp;&nbsp;JIRA是Atlassian公司出品的项目与事务跟踪工具，被广泛应用于缺陷跟踪、客户服务、需求收集、流程审批、任务跟踪、项目跟踪和敏捷管理等工作领域。 环境准备 应用 服务器配置 操作系统 插件 mysql 5.6 + 2/4G/50G centos 7.4 jira 4/8G/200G centos 7.4 jdk1.8","pubDate":"Sat, 24 Aug 2019 02:21:09 GMT","guid":"https://xxlaila.github.io/2019/08/24/jira安装和配置/","category":"jira"},{"title":"git清空commit记录方法","link":"https://xxlaila.github.io/2019/08/24/git清空commit记录方法/","description":"说明:&nbsp;&nbsp;&nbsp;&nbsp;例如将代码提交到git仓库，将一些敏感信息提交，所以需要删除提交记录以彻底清除提交信息，以得到一个干净的仓库且代码不变 1.Checkout1$ git checkout --orphan latest_branch 2. Add all the files1$ git add -A 3. Commit the changes1$ git commit -am \"commit message\"","pubDate":"Sat, 24 Aug 2019 01:36:36 GMT","guid":"https://xxlaila.github.io/2019/08/24/git清空commit记录方法/","category":"git"},{"title":"confluence_install","link":"https://xxlaila.github.io/2019/08/24/confluence-install/","description":"&nbsp;&nbsp;&nbsp;&nbsp;Confluence是一个专业的企业知识管理与协同软件，也可以用于构建企业wiki。使用简单，但它强大的编辑和站点管理特征能够帮助团队成员之间共享信息、文档协作、集体讨论，信息推送。为团队提供一个协作环境。在这里，团队成员齐心协力，各擅其能，协同地编写文档和管理项目。从此打破不同团队、不同部门以及个人之间信息孤岛的僵局，Confluence真正实现了组织资源共享。 环境准备 系统版本 插件 软件 版本 服务配置 centos 7.4 mysql 5.6+ 2/4G/50G centos 7.4 jdk 1.8 confluence 6.12.2 4/8G/200G confluence 6.12.2安装并破解，mysql 版本这里使用的是5.7.24","pubDate":"Sat, 24 Aug 2019 00:57:59 GMT","guid":"https://xxlaila.github.io/2019/08/24/confluence-install/","category":"confluence"},{"title":"nexus3搭建npm私服","link":"https://xxlaila.github.io/2019/08/23/nexus3搭建npm私服/","description":"介绍&nbsp;&nbsp;&nbsp;&nbsp;公司前端全是nodejs的，nodejs在install的时候往往是连接外网，或者是设置taobao源，即使是设置了taobao源，但是还是解决不了慢的问题，为此搭建了一个内部的npm私服，这里用google一下有很多都可以来进行搭建npm私服，然后也看到了nexus也可以来做，正好maven私服也是用的这个，都是3版本，为此选择了nexus来做npm的私服，和maven一套便于维护。 nexus安装&nbsp;&nbsp;&nbsp;&nbsp;不介绍，安装完成nexus后，在浏览器打开并进行登录，第一次安装登录nexus的默认用户admin,默认密码是admin123","pubDate":"Fri, 23 Aug 2019 01:41:41 GMT","guid":"https://xxlaila.github.io/2019/08/23/nexus3搭建npm私服/","category":"nexus,npm"},{"title":"nginx URL 斜杠问题","link":"https://xxlaila.github.io/2019/08/22/nginx-URL-斜杠问题/","description":"问题今天公司新上的一个前端应用遇到一个问题，那就是在微信登录界面扫码登录之后，微信回调给我们的地址多加了一个斜杠; 错误的地址:http://a.xxlaila.com/wx.html/?code=011amZet0h1IUf19Fvht0jg4ft0amZeN正确的地址:http://a.xxlaila.com/wx.html?code=011amZet0h1IUf19Fvht0jg4ft0amZeN 在nginx上配置需要吧这个斜杠删除掉。用户才能正常的访问； 实例在配置文件里面增加如下配置项","pubDate":"Thu, 22 Aug 2019 07:29:37 GMT","guid":"https://xxlaila.github.io/2019/08/22/nginx-URL-斜杠问题/","category":"nginx"},{"title":"kubernetes-ci/cd-(四)","link":"https://xxlaila.github.io/2019/08/20/kubernetes-ci-cd-四/","description":"1、Blue Ocean安装Blue Ocean插件 1.1、创建pipeline 配置代码库的地址 然后配置授权账户 在这儿之前git库里面必须存在于jenkinsfile文件，pipeline会自动去扫描代码库里面的分支，然后根据每一个分支建立一个类似于job的形式，然后我们可以根据每一个分支进行部署，可以执行定时触发，部署 这儿，只有一个分支存在于jenkinsfile，所以只显示一个分支，如下图：","pubDate":"Tue, 20 Aug 2019 06:54:48 GMT","guid":"https://xxlaila.github.io/2019/08/20/kubernetes-ci-cd-四/","category":"jenkins, ci/cd"},{"title":"kubernetes-ci/cd-(三)","link":"https://xxlaila.github.io/2019/08/20/kubernetes-ci-cd-三/","description":"&nbsp;&nbsp;&nbsp;&nbsp;jenkins 配置完成后，最终实现的是ci/cd，在编译的过程中，经常会遇到后端java的，前端nodejs的，这里就需要进行一个k8s在调度的时候生产pod来进行指定pod进行编译 1、制作容器自定义一个容器，里面包含了 java，nodejs的所需要的环境，同时需要同步容器的时间，包含来jenkins的node 12345678910111213141516171819202122# cat DockerfileFROM docker.io/centos:latestMAINTAINER xxlaila &quot;cq_xxlaila@163.com&quot;# Install dependent pluginENV VERSION v10.15.1RUN yum install -y wget \\ git \\ java-1.8.0-openjdk.x86_64 \\ &amp;&amp; curl -sL https://rpm.nodesource.com/setup_11.x | bash - \\ &amp;&amp; yum install -y gcc gcc-c++ make \\ &amp;&amp; yum install -y nodejs \\ &amp;&amp; yum clean all# System variable settingRUN echo &quot;LANG=zh_CN.UTF-8&quot; &gt;&gt; /etc/locale.conf \\ &amp;&amp; source /etc/locale.conf \\ &amp;&amp; cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \\ &amp;&amp; echo &quot;Asia/shanghai&quot; &gt;&gt; /etc/timezone \\ &amp;&amp; groupadd -g 10000 jenkins \\ &amp;&amp; useradd -g jenkins -u 10000 jenkinsEXPOSE 50000","pubDate":"Tue, 20 Aug 2019 06:17:48 GMT","guid":"https://xxlaila.github.io/2019/08/20/kubernetes-ci-cd-三/","category":"jenkins ci/cd"},{"title":"kubernetes-ci/cd-(二)","link":"https://xxlaila.github.io/2019/08/20/kubernetes-ci-cd-二/","description":"基于jenkins pipeline进行部署1、jenkins pipeline介绍&nbsp;&nbsp;&nbsp;&nbsp;要实现在 Jenkins 中的构建工作，可以有多种方式，我们这里采用比较常用的 Pipeline 这种方式。Pipeline，简单来说，就是一套运行在 Jenkins 上的工作流框架，将原来独立运行于单个或者多个节点的任务连接起来，实现单个任务难以完成的复杂流程编排和可视化的工作。 Jenkins Pipeline 有几个核心概念: Node：节点，一个 Node 就是一个 Jenkins 节点，Master 或者 Agent，是执行 Step 的具体运行环境，比如我们之前动态运行的 Jenkins Slave 就是一个 Node 节点 Stage：阶段，一个 Pipeline 可以划分为若干个 Stage，每个 Stage 代表一组操作，比如：Build、Test、Deploy，Stage 是一个逻辑分组的概念，可以跨多个 Node Step：步骤，Step 是最基本的操作单元，可以是打印一句话，也可以是构建一个 Docker 镜像，由各类 Jenkins 插件提供，比如命令：sh ‘make’，就相当于我们平时 shell 终端中执行 make 命令一样。 那么我们如何创建 Jenkins Pipline 呢？ Pipeline 脚本是由 Groovy 语言实现的，但是我们没必要单独去学习 Groovy，当然你会的话最好 Pipeline 支持两种语法：Declarative(声明式)和 Scripted Pipeline(脚本式)语法 Pipeline 也有两种创建方法：可以直接在 Jenkins 的 Web UI 界面中输入脚本；也可以通过创建一个 Jenkinsfile 脚本文件放入项目源码库中 一般我们都推荐在 Jenkins 中直接从源代码控制(SCMD)中直接载入 Jenkinsfile Pipeline 这种方法创建一个简单的 Pipeline 我们这里来给大家快速创建一个简单的 Pipeline，直接在 Jenkins 的 Web UI 界面中输入脚本运行。 新建 Job：在 Web UI 中点击 New Item -&gt; 输入名称：pipeline-demo -&gt; 选择下面的 Pipeline -&gt; 点击 OK 配置：在最下方的 Pipeline 区域输入如下 Script 脚本，然后点击保存。","pubDate":"Tue, 20 Aug 2019 05:46:14 GMT","guid":"https://xxlaila.github.io/2019/08/20/kubernetes-ci-cd-二/","category":"jenkins, ci/cd"},{"title":"zabbix企业微信告警","link":"https://xxlaila.github.io/2019/08/20/zabbix企业微信告警/","description":"&nbsp;&nbsp;&nbsp;&nbsp;Zabbix可以通过多种方式把告警信息发送到指定人，常用的有邮件，短信报警方式，但是越来越多的企业开始使用zabbix结合微信作为主要的告警方式，这样可以及时有效的把告警信息推送到接收人，方便告警的及时处理。&nbsp;&nbsp;&nbsp;&nbsp;微信企业号需要先在企业通信录新建该员工，该员工才能关注该企业号，这样就能实现告警信息的私密性。如果使用公众号，则只要所有关注了该公众号的人都能收到告警消息，容易造成信息泄露。而且员工数少于200人的企业号是不用钱的，也没有任何申请限制. 1、脚本存放目录/usr/lib/zabbix/alertscripts，脚本的权限是zabbix 账户，具有可执行权限","pubDate":"Tue, 20 Aug 2019 03:09:37 GMT","guid":"https://xxlaila.github.io/2019/08/20/zabbix企业微信告警/","category":"zabbix"},{"title":"帧中继配置","link":"https://xxlaila.github.io/2019/08/19/帧中继配置/","description":"点对点配置 RA配置: 1234567[RA]int s0/0[RA-s0/0]encap frame-relay 封装帧中继协议[RA-s0/0] frame-relay intf dte[RA-s0/0]frame-relay interface-dlci 102 设置本接口对应的INTERFACE-DLCI号[RA-s0/0]frame-relay map ip 172.16.1.2 102 建立对端协议地址与本地INTERFACE-DLCI号的映射关系[RA-s0/0]ip add 172.16.1.1 255.255.255.0 配置本接口IP地址[RA-s0/0]no sh 打开此物理接口 RB配置: 1234567[RB]int s0/0[RB-s0/0]encap frame-relay 封装帧中继协议[RA-s0/0] frame-relay intf dte[RB-s0/0]frame-relay interface-dlci 201 设置本接口对应的INTERFACE-DLCI号[RB-s0/0]frame-relay map ip 172.16.1.1 201 建立对端协议地址与本地INTERFACE-DLCI号的映射关系[RB-s0/0]ip add 172.16.1.2 255.255.255.0 配置本接口IP地址[RB-s0/0]no sh 打开此物理接口","pubDate":"Mon, 19 Aug 2019 13:43:35 GMT","guid":"https://xxlaila.github.io/2019/08/19/帧中继配置/","category":"帧中继, 网络设备"}]}