{"title":"懒羊羊","description":"星际拾荒者","language":"zh-CN","link":"https://xxlaila.github.io","pubDate":"Thu, 17 Oct 2019 03:23:17 GMT","lastBuildDate":"Thu, 17 Oct 2019 08:15:27 GMT","generator":"hexo-generator-json-feed","webMaster":"xxlaila","items":[{"title":"elasticserch","link":"https://xxlaila.github.io/2019/10/17/elasticserch日常维护/","description":"days 1","pubDate":"Thu, 17 Oct 2019 03:23:17 GMT","guid":"https://xxlaila.github.io/2019/10/17/elasticserch日常维护/","category":"elasticserch"},{"title":"nexus配置ldap","link":"https://xxlaila.github.io/2019/10/15/nexus配置ldap/","description":"配置nexus&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;登录nexus在设置页，点击ldap，","pubDate":"Tue, 15 Oct 2019 09:28:34 GMT","guid":"https://xxlaila.github.io/2019/10/15/nexus配置ldap/","category":"nexus"},{"title":"jenkins配置备份","link":"https://xxlaila.github.io/2019/10/15/jenkins配置备份/","description":"jenkins 备份&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当jenkins在用起来的时候，我们难保他不会出故障，但是出了故障我们怎么做到快速的恢复呢，这时备份就显得尤为重要了，备份可以多样化，一种是我们直接到jenkins的目录下面手动备份jenkins目录。一种是我们就jenkins自带的插件thinBackup进行备份恢复，下面重点介绍thinBackup","pubDate":"Tue, 15 Oct 2019 00:55:19 GMT","guid":"https://xxlaila.github.io/2019/10/15/jenkins配置备份/","category":"jenkins"},{"title":"jenkins配置ldap","link":"https://xxlaila.github.io/2019/10/14/jenkins配置ldap/","description":"背景&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;公司研发人员和测试人员，还有运维人员有时候登录jenkins去查看一些job的状态或者是其他的东西，虽然有企业微信的通知，但是感觉还是不能满足，比如job错误了，企业微信虽然吧错误发给了研发人员，但是研发还是要登录jenkins上去看，就感觉要舒服一点，测试上做的一些自动化测试，有时候失败了他们也会去看或者是去建立一些自动化的job。之前建立了公共的账号，开发和测试人员都去登录，但是有时候他们误操作了，导致一些其他的东西失败或者错误，虽然做了权限控制，但是他们还是死不承认，所以这里介入ldap。谁动的就知道了，这样就不怕了。","pubDate":"Mon, 14 Oct 2019 03:49:32 GMT","guid":"https://xxlaila.github.io/2019/10/14/jenkins配置ldap/","category":"jenkins"},{"title":"java应用部署","link":"https://xxlaila.github.io/2019/10/12/java应用部署/","description":"Welcome to my blog, enter password to read.","pubDate":"Sat, 12 Oct 2019 03:38:32 GMT","guid":"https://xxlaila.github.io/2019/10/12/java应用部署/","category":"kubernetes"},{"title":"harbor 使用","link":"https://xxlaila.github.io/2019/10/10/harbor-使用/","description":"days(2019-10-10)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;前面文章介绍了harbor的部署，今天第一次学习入门使用。","pubDate":"Thu, 10 Oct 2019 08:49:41 GMT","guid":"https://xxlaila.github.io/2019/10/10/harbor-使用/","category":"kubrnetes"},{"title":"HPA认识","link":"https://xxlaila.github.io/2019/10/09/hpa/","description":"Pod 自动扩缩容&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Kubernetes提供了这样一个资源对象: Horizontal Pod Autoscaling Pod水平自动伸缩），简称HPA。HAP通过监控分析RC或者Deployment控制的所有Pod的负载变化情况来确定是否需要调整Pod的副本数量，这是HPA最基本的原理。","pubDate":"Wed, 09 Oct 2019 07:12:23 GMT","guid":"https://xxlaila.github.io/2019/10/09/hpa/","category":"kubernetes"},{"title":"Deployment使用","link":"https://xxlaila.github.io/2019/10/09/Deployment使用/","description":"Deployment和rc的对比&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;首先RC是Kubernetes的一个核心概念，当我们把应用部署到集群之后，需要保证应用能够持续稳定的运行，RC就是这个保证的关键，主要功能如: 确保Pod数量: 它会确保Kubernetes中有指定数量的Pod在运行，如果少于指定数量的Pod，RC就会创建新的，反之这会删除多余的，保证Pod的副本数量不变。 确保Pod健康: 当Pod不健康，比如运行出错了，总之无法提供正常服务时，RC也会杀死不健康的Pod，重新创建新的。 弹性伸缩: 在业务高峰或者低峰的时候，可以用过RC来动态的调整Pod数量来提供资源的利用率，当然我们也提到过如果使用HPA这种资源对象的话可以做到自动伸缩。 滚动升级: 滚动升级是一种平滑的升级方式，通过逐步替换的策略，保证整体系统的稳定性","pubDate":"Wed, 09 Oct 2019 01:59:08 GMT","guid":"https://xxlaila.github.io/2019/10/09/Deployment使用/","category":"kubernetes"},{"title":"harbor私有仓库部署","link":"https://xxlaila.github.io/2019/09/30/harbor私有仓库部署/","description":"介绍&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Harbor是一个用于存储和分发Docker镜像的企业级Registry服务器，通过添加一些企业必需的功能特性，例如安全、标识和管理等，扩展了开源Docker Distribution。作为一个企业级私有Registry服务器，Harbor提供了更好的性能和安全。提升用户使用Registry构建和运行环境传输镜像的效率。Harbor支持安装在多个Registry节点的镜像资源复制，镜像全部保存在私有Registry中， 确保数据和知识产权在公司内部网络中管控。另外，Harbor也提供了高级的安全特性，诸如用户管理，访问控制和活动审计等。","pubDate":"Mon, 30 Sep 2019 06:55:28 GMT","guid":"https://xxlaila.github.io/2019/09/30/harbor私有仓库部署/","category":"kubernetes"},{"title":"k8s pod健康检测","link":"https://xxlaila.github.io/2019/09/27/k8s-pod健康检测/","description":"Pod健康检测机制对于Pod的健康状态检测，kubernetes提供了两类探针(Probe)来执行对Pod的健康状态检测: LivenessProbe探针:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;用于判断容器是否存活，即Pod是否为running状态，如果LivenessProbe探针探测到容器不健康，则kubelet将kill掉容器，并根据容器的重启策略是否重启，如果一个容器不包含LivenessProbe探针，则Kubelet认为容器的LivenessProbe探针的返回值永远成功.","pubDate":"Fri, 27 Sep 2019 05:37:53 GMT","guid":"https://xxlaila.github.io/2019/09/27/k8s-pod健康检测/","category":"kubernetes"},{"title":"EFK","link":"https://xxlaila.github.io/2019/09/25/EFK/","description":"初始化配置文件准备&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;将下载的 kubernetes-server-linux-amd64.tar.gz 解压后，再解压其中的 kubernetes-src.tar.gz 文件。kubernetes/cluster/addons/fluentd-elasticsearch这是文件所在的路径 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;es 数据默认的存储在docker里面，在用的是node节点的空间，而node节点我们不可能都准备很大的空间，那样很浪费资源，所以这里我们需要准备外部的nfs存储空间，然后通过pv的模式进行挂载，数据存储到nfs服务器上，这样保障了es收集数据的可用性。","pubDate":"Wed, 25 Sep 2019 07:24:18 GMT","guid":"https://xxlaila.github.io/2019/09/25/EFK/","category":"kubernetes"},{"title":"网络状态监控","link":"https://xxlaila.github.io/2019/09/25/网络状态监控/","description":"需求&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;监控IDC机房网络质量情况，本地区到其他地区，其他地区到本节点，或者各省市时间网络、运营商网络状态，监视网络性能，包括常规的 ping，用 fping、echoping、tracert 监视 www 服务器性能，监视 dns 查询性能，监视 ssh 性能等。底层也是 rrdtool 做支持，特点是画的图非常漂亮，网络丢包和延迟用颜色和阴影来表示。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Smokeping。最新版本的 Smokeping 支持多个节点的检测结果从一个图上画出来","pubDate":"Wed, 25 Sep 2019 05:27:04 GMT","guid":"https://xxlaila.github.io/2019/09/25/网络状态监控/","category":"监控"},{"title":"Linux 常用命令学习","link":"https://xxlaila.github.io/2019/09/25/Linux常用命令学习/","description":"查找文件使用命令 查找目录下面大小超过5M的文件 1find /home/ -size +5M 查找目录下100天之前修改过的文件 1find /home/ -mtime +100 查找目录下60天未被访问过的文件 1find /home/ \\! atime -60","pubDate":"Wed, 25 Sep 2019 03:38:07 GMT","guid":"https://xxlaila.github.io/2019/09/25/Linux常用命令学习/","category":"linux"},{"title":"iptables","link":"https://xxlaila.github.io/2019/09/25/iptables/","description":"Iptables&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Iptalbes 是用来设置、维护和检查Linux内核的IP包过滤规则的。可以定义不同的表，每个表都包含几个内部的链，也能包含用户定义的链。每个链都是一个规则列表，对对应的包进行匹配：每条规则指定应当如何处理与之相匹配的包。这被称作’target’（目标），也可以跳向同一个表内的用户定义的链。","pubDate":"Wed, 25 Sep 2019 02:19:11 GMT","guid":"https://xxlaila.github.io/2019/09/25/iptables/","category":"linux"},{"title":"交换机做端口聚合","link":"https://xxlaila.github.io/2019/09/25/交换机做端口聚合/","description":"应用场景：h3c s5500 (Switch A)。huawei s5720S-SI-AC（Switch B） Switch A 作为上行交换机，Switch B作为下行交换机 组网：两个交换机的id、vlan号这里使用的是相同","pubDate":"Wed, 25 Sep 2019 02:09:47 GMT","guid":"https://xxlaila.github.io/2019/09/25/交换机做端口聚合/","category":"网络设备"},{"title":"pv pvc","link":"https://xxlaila.github.io/2019/09/25/pv-pvc/","description":"1、介绍&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PersistentVolume（pv）和PersistentVolumeClaim（pvc）是k8s提供的两种API资源，用于抽象存储细节。管理员关注于如何通过pv提供存储功能而无需关注用户如何使用，同样的用户只需要挂载pvc到容器中而不需要关注存储卷采用何种技术实现。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pvc和pv的关系与pod和node关系类似，前者消耗后者的资源。pvc可以向pv申请指定大小的存储资源并设置访问模式,这就可以通过Provision -&gt; Claim 的方式，来对存储资源进行控制。","pubDate":"Wed, 25 Sep 2019 01:46:10 GMT","guid":"https://xxlaila.github.io/2019/09/25/pv-pvc/","category":"kubernetes"},{"title":"利用NFS动态提供Kubernetes后端存储卷","link":"https://xxlaila.github.io/2019/09/24/利用NFS动态提供Kubernetes后端存储卷/","description":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nfs-client-provisioner是一个automatic provisioner，使用NFS作为存储，自动创建PV和对应的PVC，本身不提供NFS存储，需要外部先有一套NFS存储服务。 PV以 ${namespace}-${pvcName}-${pvName}的命名格式提供（在NFS服务器上） PV回收的时候以 archieved-${namespace}-${pvcName}-${pvName} 的命名格式（在NFS服务器上） 官方访问地址","pubDate":"Tue, 24 Sep 2019 09:53:32 GMT","guid":"https://xxlaila.github.io/2019/09/24/利用NFS动态提供Kubernetes后端存储卷/","category":"kubernetes"},{"title":"k8s v1.14 prometheus","link":"https://xxlaila.github.io/2019/09/20/k8s-v1-14-prometheus/","description":"Prometheus、Grafana 部署&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Grafana是一个开源的度量分析与可视化套件。经常被用作基础设施的时间序列数据和应用程序分析的可视化，我们这里用它来做Kubernetes集群监控数据的可视化。 准备工作&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;截至当前，prometheus、grafana均采用最新的镜像包，在在第一次部署的时候grafana报了一个错误mkdir: cannot create directory &#39;/var/lib/grafana/plugins&#39;: No such file or directory,这是因为Grafana启动使用的用户和用户组都是472，造成对外挂存储没有权限。参考官方","pubDate":"Fri, 20 Sep 2019 08:12:48 GMT","guid":"https://xxlaila.github.io/2019/09/20/k8s-v1-14-prometheus/","category":"kubenertes"},{"title":"k8s v1.14 weave-scope","link":"https://xxlaila.github.io/2019/09/20/k8s-v1-14-weave-scope/","description":"前沿&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Kubernetes 集群并部署容器化应用只是第一步。一旦集群运行起来，我们需要确保一起正常，所有必要组件就位并各司其职，有足够的资源满足应用的需求。Kubernetes 是一个复杂系统，运维团队需要有一套工具帮助他们获知集群的实时状态，并为故障排查提供及时和准确的数据支持。 weave scope 介绍&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Weave Scope是Docker和Kubernetes的可视化和监控工具。它提供了一个自上而下的应用程序以及整个基础架构视图，并允许您在部署到云提供商时实时诊断分布式容器化应用程序的任何问题。","pubDate":"Fri, 20 Sep 2019 03:49:59 GMT","guid":"https://xxlaila.github.io/2019/09/20/k8s-v1-14-weave-scope/","category":"kubernetes"},{"title":"k8s v1.14 traefik部署","link":"https://xxlaila.github.io/2019/09/20/k8s-v1-14-traefik部署/","description":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;traefix 前篇是可以使用，这里k8s v1.14 之前的拿来用不上，然后折腾了一下，参考官方的折腾起来了 基于角色的访问控制配置（仅限Kubernetes 1.6+）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Kubernetes在1.6+中引入了基于角色的访问控制（RBAC），以允许对Kubernetes资源和API进行细粒度控制。群集配置了RBAC，则需要授权Traefik使用Kubernetes API。有两种方法可以设置适当的权限：通过特定于命名空间的RoleBindings或单个全局ClusterRoleBinding。","pubDate":"Fri, 20 Sep 2019 01:20:25 GMT","guid":"https://xxlaila.github.io/2019/09/20/k8s-v1-14-traefik部署/","category":"kubernetes"},{"title":"k8s v1.14 metrics-server","link":"https://xxlaila.github.io/2019/09/17/k8s-v1-14-metrics-server/","description":"metrics-server这里不详细介绍，可以参考metrics-server安装季 安装metrics-server&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里安装和之前的metrics-server安装季稍微有点不一样，之前集群安装没有使用https证书，后面去各种生成的证书和踩坑，这里是在安装的时候一开始就使用了https全证书,所有稍微有一点区别，这里只列出有区别的地方，其他的完全可以参考metrics-server安装季，这里https证书不需要重新生成；","pubDate":"Tue, 17 Sep 2019 01:06:18 GMT","guid":"https://xxlaila.github.io/2019/09/17/k8s-v1-14-metrics-server/","category":"kubernetes"},{"title":"k8s v1.14 dashboard","link":"https://xxlaila.github.io/2019/09/16/k8s-v1-14-dashboard/","description":"kuberntes 自带插件的 manifests yaml 文件使用 gcr.io 的 docker registry，国内被墙，需要手动替换为其它 registry 地址 修改配置文件将下载的 kubernetes-server-linux-amd64.tar.gz 解压后，再解压其中的 kubernetes-src.tar.gz 文件。 12# cd kubernetes# tar -xzvf kubernetes-src.tar.gz dashboard 对应的目录是：cluster/addons/dashboard： 1# cd cluster/addons/dashboard 修改 service 定义，指定端口类型为 NodePort，这样外界可以通过地址 NodeIP:NodePort 访问 dashboard；","pubDate":"Mon, 16 Sep 2019 09:46:10 GMT","guid":"https://xxlaila.github.io/2019/09/16/k8s-v1-14-dashboard/","category":"kubernetes"},{"title":"k8s v1.14 dns插件","link":"https://xxlaila.github.io/2019/09/16/k8s-v1-14-dns插件/","description":"部署 coredns 插件注意: kuberntes 自带插件的 manifests yaml 文件使用 gcr.io 的 docker registry，国内被墙，需要手动替换为其它 registry 地址; 修改配置文件将下载的 kubernetes-server-linux-amd64.tar.gz 解压后，再解压其中的 kubernetes-src.tar.gz 文件。 12# cd kubernetes# tar -xzvf kubernetes-src.tar.gz","pubDate":"Mon, 16 Sep 2019 09:37:06 GMT","guid":"https://xxlaila.github.io/2019/09/16/k8s-v1-14-dns插件/","category":"kubernetes"},{"title":"k8s v1.14集群验证","link":"https://xxlaila.github.io/2019/09/16/k8s-v1-14集群验证/","description":"验证集群功能检查节点状态12345# kubectl get nodesNAME STATUS ROLES AGE VERSION172.21.16.204 Ready &lt;none&gt; 5h50m v1.14.6172.21.16.240 Ready &lt;none&gt; 5h48m v1.14.6172.21.16.87 Ready &lt;none&gt; 5h45m v1.14.6 都为 Ready 时正常。","pubDate":"Mon, 16 Sep 2019 09:21:22 GMT","guid":"https://xxlaila.github.io/2019/09/16/k8s-v1-14集群验证/","category":"kubernetes"},{"title":"kubernetes-v1.14 node安装","link":"https://xxlaila.github.io/2019/09/16/kubernetes-v1-14-node安装/","description":"1、安装docker1.1、增加docker 源123yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo 1.2、安装docker1# yum -y install docker-ce","pubDate":"Mon, 16 Sep 2019 07:42:55 GMT","guid":"https://xxlaila.github.io/2019/09/16/kubernetes-v1-14-node安装/","category":"kubernetes"}]}