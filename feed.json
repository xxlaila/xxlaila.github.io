{"title":"懒羊羊","description":"星际拾荒者","language":"zh-CN","link":"https://xxlaila.github.io","pubDate":"Fri, 20 Sep 2019 03:49:59 GMT","lastBuildDate":"Fri, 20 Sep 2019 06:24:32 GMT","generator":"hexo-generator-json-feed","webMaster":"xxlaila","items":[{"title":"k8s v1.14 weave-scope","link":"https://xxlaila.github.io/2019/09/20/k8s-v1-14-weave-scope/","description":"前沿&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Kubernetes 集群并部署容器化应用只是第一步。一旦集群运行起来，我们需要确保一起正常，所有必要组件就位并各司其职，有足够的资源满足应用的需求。Kubernetes 是一个复杂系统，运维团队需要有一套工具帮助他们获知集群的实时状态，并为故障排查提供及时和准确的数据支持。 weave scope 介绍&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Weave Scope是Docker和Kubernetes的可视化和监控工具。它提供了一个自上而下的应用程序以及整个基础架构视图，并允许您在部署到云提供商时实时诊断分布式容器化应用程序的任何问题。","pubDate":"Fri, 20 Sep 2019 03:49:59 GMT","guid":"https://xxlaila.github.io/2019/09/20/k8s-v1-14-weave-scope/","category":"kubernetes"},{"title":"k8s v1.14 traefik部署","link":"https://xxlaila.github.io/2019/09/20/k8s-v1-14-traefik部署/","description":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;traefix 前篇是可以使用，这里k8s v1.14 之前的拿来用不上，然后折腾了一下，参考官方的折腾起来了 基于角色的访问控制配置（仅限Kubernetes 1.6+）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Kubernetes在1.6+中引入了基于角色的访问控制（RBAC），以允许对Kubernetes资源和API进行细粒度控制。群集配置了RBAC，则需要授权Traefik使用Kubernetes API。有两种方法可以设置适当的权限：通过特定于命名空间的RoleBindings或单个全局ClusterRoleBinding。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;每个命名空间的RoleBinding可以限制授予权限，只有Traefik正在监视的名称空间才能使用，从而遵循最小权限原则。如果Traefik不应该监视所有名称空间，并且名称空间集不会动态更改，那么这是首选方法。否则，必须使用单个ClusterRoleBinding。 traefik学习traefik官方 准备工作下载trarfix代码，然后切换到v1.7的分支 1234567891011121314151617181920212223# git clone https://github.com/containous/traefik.git# git branch --all* master remotes/origin/HEAD -&gt; origin/master remotes/origin/add-plugin-support remotes/origin/gh-pages remotes/origin/master remotes/origin/v1.0 remotes/origin/v1.1 remotes/origin/v1.2 remotes/origin/v1.3 remotes/origin/v1.4 remotes/origin/v1.5 remotes/origin/v1.6 remotes/origin/v1.7 remotes/origin/v2.0# git checkout v1.7Branch 'v1.7' set up to track remote branch 'v1.7' from 'origin'.Switched to a new branch 'v1.7'# /root/traefik/examples/k8s 安装部署使用ClusterRoleBinding123# kubectl apply -f traefik-rbac.yaml clusterrole.rbac.authorization.k8s.io/traefik-ingress-controller createdclusterrolebinding.rbac.authorization.k8s.io/traefik-ingress-controller created 对于命名空间限制，每个监视命名空间需要一个RoleBinding以及Traefik kubernetes.namespaces参数的相应配置。 使用Deployments部署或部署DaemonSet可以将Traefik与Deployment或DaemonSet对象一起使用，而这两个选项各有利弊： 使用部署时，可伸缩性可以更好，因为在使用DaemonSet时您将拥有每个节点的Single-Pod模型，而在使用部署时，可能需要更少的基于环境的副本。 当节点加入群集时，DaemonSet会自动扩展到新节点，而部署窗格仅在需要时在新节点上进行调度。 DaemonSets确保只有一个pod副本在任何单个节点上运行。如果要确保两个pod不在同一节点上，则部署需要关联设置 可以使用该NET_BIND_SERVICE功能运行DaemonSet ，这将允许它绑定到每个主机上的端口80/443 / etc。这将允许绕过kube-proxy，并减少流量跳跃。请注意，这违反了Kubernetes最佳实践指南，并提出了调度/扩展问题的可能性。尽管存在潜在问题，但这仍然是大多数入口控制器的选择。 Deployments部署1234# kubectl apply -f traefik-deployment.yamlserviceaccount/traefik-ingress-controller createddeployment.extensions/traefik-ingress-controller createdservice/traefik-ingress-service created DaemonSets 部署(可选)1# kubectl apply -f traefik-ds.yaml Deployments和DaemonSets之间存在一些显着差异: 部署具有更容易的向上和向下扩展可能性。它可以实现完整的pod生命周期，并支持Kubernetes 1.2的滚动更新。运行部署至少需要一个Pod。 DaemonSet会自动扩展到满足特定选择器的所有节点，并保证一次填充一个节点。Kubernetes 1.7也完全支持滚动更新，适用于DaemonSets 检查部署 查看pod 1234567# kubectl --namespace=kube-system get podsNAME READY STATUS RESTARTS AGEcoredns-5579b8778b-xw8m9 1/1 Running 2 3d21hkubernetes-dashboard-65dfbf6f4f-hcgbb 1/1 Running 0 2d16hmetrics-server-94ff5d4cc-b97l5 1/1 Running 1 3dtiller-deploy-5cbcf75545-rbzld 1/1 Running 0 17htraefik-ingress-controller-c595665d6-cm7kh 1/1 Running 0 3m20s 查看services 1234567# kubectl get services --namespace=kube-systemNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkube-dns ClusterIP 10.254.0.2 &lt;none&gt; 53/UDP,53/TCP,9153/TCP 3d21hkubernetes-dashboard NodePort 10.254.214.153 &lt;none&gt; 443:32533/TCP 3d21hmetrics-server ClusterIP 10.254.61.132 &lt;none&gt; 443/TCP 3dtiller-deploy ClusterIP 10.254.207.227 &lt;none&gt; 44134/TCP 17htraefik-ingress-service NodePort 10.254.246.158 &lt;none&gt; 80:32146/TCP,8080:30455/TCP 3m53s 这里使用的是nodeport模式进行部署的，可以看到端口为32146，这里访问会返回404 page not found,那是因为我们还没有给Traefik任何配置。 创建一个服务和一个将公开Traefik Web UI的Ingres123# kubectl apply -f ui.yaml service/traefik-web-ui createdingress.extensions/traefik-web-ui created 在/etc/hosts 文件设置一个路由条目traefik-ui.minikube 在浏览器进行访问可以看到Traefik Web UI","pubDate":"Fri, 20 Sep 2019 01:20:25 GMT","guid":"https://xxlaila.github.io/2019/09/20/k8s-v1-14-traefik部署/","category":"kubernetes"},{"title":"k8s v1.14 metrics-server","link":"https://xxlaila.github.io/2019/09/17/k8s-v1-14-metrics-server/","description":"metrics-server这里不详细介绍，可以参考metrics-server安装季 安装metrics-server&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里安装和之前的metrics-server安装季稍微有点不一样，之前集群安装没有使用https证书，后面去各种生成的证书和踩坑，这里是在安装的时候一开始就使用了https全证书,所有稍微有一点区别，这里只列出有区别的地方，其他的完全可以参考metrics-server安装季，这里https证书不需要重新生成；","pubDate":"Tue, 17 Sep 2019 01:06:18 GMT","guid":"https://xxlaila.github.io/2019/09/17/k8s-v1-14-metrics-server/","category":"kubernetes"},{"title":"k8s v1.14 dashboard","link":"https://xxlaila.github.io/2019/09/16/k8s-v1-14-dashboard/","description":"kuberntes 自带插件的 manifests yaml 文件使用 gcr.io 的 docker registry，国内被墙，需要手动替换为其它 registry 地址 修改配置文件将下载的 kubernetes-server-linux-amd64.tar.gz 解压后，再解压其中的 kubernetes-src.tar.gz 文件。 12# cd kubernetes# tar -xzvf kubernetes-src.tar.gz dashboard 对应的目录是：cluster/addons/dashboard： 1# cd cluster/addons/dashboard 修改 service 定义，指定端口类型为 NodePort，这样外界可以通过地址 NodeIP:NodePort 访问 dashboard；","pubDate":"Mon, 16 Sep 2019 09:46:10 GMT","guid":"https://xxlaila.github.io/2019/09/16/k8s-v1-14-dashboard/","category":"kubernetes"},{"title":"k8s v1.14 dns插件","link":"https://xxlaila.github.io/2019/09/16/k8s-v1-14-dns插件/","description":"部署 coredns 插件注意: kuberntes 自带插件的 manifests yaml 文件使用 gcr.io 的 docker registry，国内被墙，需要手动替换为其它 registry 地址; 修改配置文件将下载的 kubernetes-server-linux-amd64.tar.gz 解压后，再解压其中的 kubernetes-src.tar.gz 文件。 12# cd kubernetes# tar -xzvf kubernetes-src.tar.gz","pubDate":"Mon, 16 Sep 2019 09:37:06 GMT","guid":"https://xxlaila.github.io/2019/09/16/k8s-v1-14-dns插件/","category":"kubernetes"},{"title":"k8s v1.14集群验证","link":"https://xxlaila.github.io/2019/09/16/k8s-v1-14集群验证/","description":"验证集群功能检查节点状态12345# kubectl get nodesNAME STATUS ROLES AGE VERSION172.21.16.204 Ready &lt;none&gt; 5h50m v1.14.6172.21.16.240 Ready &lt;none&gt; 5h48m v1.14.6172.21.16.87 Ready &lt;none&gt; 5h45m v1.14.6 都为 Ready 时正常。","pubDate":"Mon, 16 Sep 2019 09:21:22 GMT","guid":"https://xxlaila.github.io/2019/09/16/k8s-v1-14集群验证/","category":"kubernetes"},{"title":"kubernetes-v1.14 node安装","link":"https://xxlaila.github.io/2019/09/16/kubernetes-v1-14-node安装/","description":"1、安装docker1.1、增加docker 源123yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo 1.2、安装docker1# yum -y install docker-ce","pubDate":"Mon, 16 Sep 2019 07:42:55 GMT","guid":"https://xxlaila.github.io/2019/09/16/kubernetes-v1-14-node安装/","category":"kubernetes"},{"title":"kubernetes-v1.14安装","link":"https://xxlaila.github.io/2019/09/11/kubernetes-v1-14安装/","description":"1、环境准备 ip type docker os k8s version 172.21.17.30 master,etcd CentOS Linux release 7.4.1708 v1.14.6 172.21.17.31 master,etcd CentOS Linux release 7.4.1708 172.21.16.110 master,etcd CentOS Linux release 7.4.1708 172.21.16.87 node,flanneld 18.06.2-ce CentOS Linux release 7.4.1708 172.21.16.240 node,flanneld,ha+kee 18.06.2-ce CentOS Linux release 7.4.1708 172.21.16.204 node,flanneld,ha+kee 18.06.2-ce CentOS Linux release 7.4.1708 172.21.16.45 vip CentOS Linux release 7.4.1708 2、初始化系统2.1、安装依赖包","pubDate":"Wed, 11 Sep 2019 08:20:26 GMT","guid":"https://xxlaila.github.io/2019/09/11/kubernetes-v1-14安装/","category":"kubernetes"},{"title":"路由器端口映射","link":"https://xxlaila.github.io/2019/09/10/路由器端口映射/","description":"好记性不如烂笔头，h3c MSR3620路由器做端口映射到后端服务器,包含单个端口和端口段的映射","pubDate":"Tue, 10 Sep 2019 00:56:24 GMT","guid":"https://xxlaila.github.io/2019/09/10/路由器端口映射/","category":"网络设备"},{"title":"traefik https应用","link":"https://xxlaila.github.io/2019/09/06/traefik-https应用/","description":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;之前已经使用traefik服务作为入口，测试并访问了tomcat应用，之前是通过http来访问的，而我们在yaml文件里面也添加8443端口用于https访问，在实际环境中我们也是需要https来进行访问应用，通过traefik实现https，traefik http应用 操作实践 这里我用了公司的证书，就是为了贴近真实，也满足测试需求， 创建一个secret，保存https证书","pubDate":"Fri, 06 Sep 2019 03:31:09 GMT","guid":"https://xxlaila.github.io/2019/09/06/traefik-https应用/","category":"kubernetes"},{"title":"traefik ingress使用","link":"https://xxlaila.github.io/2019/09/05/traefik-ingress使用/","description":"Traefik介绍&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;简单的说，ingress就是从kubernetes集群外访问集群的入口，将用户的URL请求转发到不同的service上。Ingress相当于nginx、apache等负载均衡反向代理服务器，其中还包括规则定义，即URL的路由信息。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Traefik是一款开源的反向代理与负载均衡工具。它最大的优点是能够与常见的微服务系统直接整合，实现自动化动态配置。Traefik通过不断地跟 kubernetes API 打交道，实时的感知后端 service、pod 等变化，比如pod，service 增加与减少等；当得到这些变化信息后，Ingress自动更新配置并热重载 ，达到服务发现的作用。","pubDate":"Thu, 05 Sep 2019 09:16:25 GMT","guid":"https://xxlaila.github.io/2019/09/05/traefik-ingress使用/","category":"kubernetes"},{"title":"k8s-helm","link":"https://xxlaila.github.io/2019/09/04/k8s-helm/","description":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Helm类似与linux下面的yum，Helm是一个用于kubernetes的包管理器，每一个包为一个chart，一个chart是一个目录，常常会对目录进行打包压缩，形成一个${name}-version.tgz的格式进行传输和存储。 对于应用发布者而言，可以通过Helm打包应用，管理应用依赖关系，管理应用版本并发布应用到软件仓库。 对于使用者而言，使用Helm后不用需要了解Kubernetes的Yaml语法并编写应用部署文件，可以通过Helm下载并在kubernetes上安装需要的应用。 Helm还提供了kubernetes上的软件部署，删除，升级，回滚应用的强大功能","pubDate":"Wed, 04 Sep 2019 10:52:26 GMT","guid":"https://xxlaila.github.io/2019/09/04/k8s-helm/","category":"kubernetes"},{"title":"metrics-server安装季","link":"https://xxlaila.github.io/2019/09/04/metrics-server安装季/","description":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;metrics-server 通过 kube-apiserver 发现所有节点，然后调用 kubelet APIs（通过 https 接口）获得各节点（Node）和 Pod 的 CPU、Memory 等资源使用情况。Kubernetes 1.12 开始，kubernetes 的安装脚本移除了 Heapster，从 1.13 开始完全移除了对 Heapster 的支持，Heapster 不再被维护。 替代方案如下: 用于支持自动扩缩容的 CPU/memory HPA metrics：metrics-server 通用的监控方案：使用第三方可以获取 Prometheus 格式监控指标的监控系统，如 Prometheus Operator 事件传输：使用第三方工具来传输、归档 kubernetes events &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;使用 metrics-server 替代 Heapster，将无法在 dashboard 中以图形展示 Pod 的内存和 CPU 情况，需要通过 Prometheus、Grafana 等监控方案来弥补。","pubDate":"Wed, 04 Sep 2019 05:57:07 GMT","guid":"https://xxlaila.github.io/2019/09/04/metrics-server安装季/","category":"kubernetes"},{"title":"kubelet提供api请求接口","link":"https://xxlaila.github.io/2019/09/04/kubelet提供api请求接口/","description":"kubelet 提供的 API 接口认证node安装参考 kubelet 启动后监听多个端口，用于接收 kube-apiserver 或其它客户端发送的请求： 1234[root@k8s-3 ~]# netstat -lnpt|grep kubelettcp 0 0 127.0.0.1:46395 0.0.0.0:* LISTEN 8941/kubelet tcp 0 0 127.0.0.1:10248 0.0.0.0:* LISTEN 8941/kubelet tcp6 0 0 :::10250 :::* LISTEN 8941/kubelet 10248: healthz http 服务 10250: https 服务，访问该端口时需要认证和授权（即使访问 /healthz 也需要） 未开启只读端口 10255 从 K8S v1.10 开始，去除了 –cadvisor-port 参数（默认 4194 端口），不支持访问 cAdvisor UI &amp; API","pubDate":"Wed, 04 Sep 2019 02:04:30 GMT","guid":"https://xxlaila.github.io/2019/09/04/kubelet提供api请求接口/","category":"kubernetes"},{"title":"centos-nfs-512错误","link":"https://xxlaila.github.io/2019/09/03/centos-nfs-512错误/","description":"nfs 错误kernel: NFS: nfs4_discover_server_trunking unhandled error -512. Exiting with error EIO &nbsp;&nbsp;&nbsp;&nbsp;很久没挂载过nfs，忘记客户端怎么挂在nfs的了，服务端很早就安装好了，今天一台客户机需要挂载nfs，然后居然报错了，然后找了一圈居然没找到怎么解决，然后又重新看了一次centos nfs的配置，于是乎就搞定了 在挂载nfs的提示很慢，长时间无响应，强行结束看看是什么问题，查看日志 12$ sudo tail -f /var/log/messagesSep 3 11:23:51 dev-application kernel: NFS: nfs4_discover_server_trunking unhandled error -512. Exiting with error EIO","pubDate":"Tue, 03 Sep 2019 03:29:13 GMT","guid":"https://xxlaila.github.io/2019/09/03/centos-nfs-512错误/","category":"centos"},{"title":"k8s集群部署heapster","link":"https://xxlaila.github.io/2019/09/02/k8s集群部署heapster/","description":"kubernetes集群启用tls认证部署heapster，在部署期间遇到了很多的坑，走过很多雷，这里记录一下,不过在新版本中heapster被metrics-server代替了，metrics-server后篇介绍和使用 1、准备工作下载需要的文件，这里用之前k8s-heapster部署的文件拿来进行修改，文件地址 2、执行文件创建12# cd kubernetes-yaml/heapster-influxdb-grafana# kubectl apply -f ./","pubDate":"Mon, 02 Sep 2019 11:25:03 GMT","guid":"https://xxlaila.github.io/2019/09/02/k8s集群部署heapster/","category":"kubernetes"},{"title":"k8s部署istio","link":"https://xxlaila.github.io/2019/08/30/k8s部署istio/","description":"istio 介绍&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;istio代表的是Service Mesh的方案实现，Istio 有助于降低这些部署的复杂性，并减轻开发团队的压力。提供一种简单的方式来为已部署的服务建立网络，且提供具有负载均衡、服务间认证、监控、流量管理等功能。 服务网格（Service Mesh）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;服务网格（Service Mesh）用于描述构成这些应用程序的微服务网络以及应用之间的交互。随着规模和复杂性的增长，服务网格越来越难以理解和管理。它的需求包括服务发现、负载均衡、故障恢复、指标收集和监控以及通常更加复杂的运维需求，例如 A/B 测试、金丝雀发布、限流、访问控制和端到端认证等。而istio刚好提供了一套完整的解决方案，通过控制整个服务器网格提供行为洞察和操作控制来满足微服务应用的多样化 架构Istio 服务网格逻辑上分为数据平面和控制平面。","pubDate":"Fri, 30 Aug 2019 03:21:08 GMT","guid":"https://xxlaila.github.io/2019/08/30/k8s部署istio/","category":"istio"},{"title":"k8s配置Dashboard","link":"https://xxlaila.github.io/2019/08/29/k8s配置Dashboard/","description":"&nbsp;&nbsp;&nbsp;&nbsp;K8S Dashboard是官方的一个基于WEB的用户界面，专门用来管理K8S集群，并可展示集群的状态。K8S集群安装好后默认没有包含Dashboard，我们需要额外创建它。 1、安装dashboard1.1、下载准备需要的文件经过修改过后的文件，已经可以正常使用的文件","pubDate":"Thu, 29 Aug 2019 09:57:46 GMT","guid":"https://xxlaila.github.io/2019/08/29/k8s配置Dashboard/","category":"kubernetes"},{"title":"k8s删除node重新加入","link":"https://xxlaila.github.io/2019/08/29/k8s删除node重新加入/","description":"&nbsp;&nbsp;&nbsp;&nbsp;有时候k8s node 在加入集群的时候不经意的时候弄错啦某些东西，这时候可以把这个node删除，然后重新加入，删除节点之前我们需要做一下常规化的操作，来保障运行在该节点的pod迁移到其他的node上。","pubDate":"Thu, 29 Aug 2019 08:27:00 GMT","guid":"https://xxlaila.github.io/2019/08/29/k8s删除node重新加入/","category":"常用命令"},{"title":"Centos route策略","link":"https://xxlaila.github.io/2019/08/28/Centos-route策略/","description":"场景:&nbsp;&nbsp;&nbsp;&nbsp;公司业务在一个新的云平台上线，该云平台使用的是比较传统的VMware vSphere来做的虚拟化，而且该云平台网络也是我们不清楚的，反正就是不能设置(不能像现在主流云平台自定义网络或者是地址段)，是一个政府的云平台，具体各种奇葩的限制就不说啦，你懂的……","pubDate":"Wed, 28 Aug 2019 02:25:46 GMT","guid":"https://xxlaila.github.io/2019/08/28/Centos-route策略/","category":"Centos"},{"title":"k8s部署ingress","link":"https://xxlaila.github.io/2019/08/26/k8s部署ingress/","description":"在kubernetes 集群中，一个服务安装以后怎么对外提供访问，外部用户怎么来访问我们容器中业务。 1、Ingress 介绍&nbsp;&nbsp;&nbsp;&nbsp;Kubernetes 暴露服务的方式目前只有三种：LoadBlancer Service、NodePort Service、Ingress；本文主要通过Ingress来访问 2、Ingress 是什么&nbsp;&nbsp;&nbsp;&nbsp;Ingress 就是能利用 Nginx、Haproxy 啥的负载均衡器暴露集群内服务的工具问题来了，集群内服务想要暴露出去面临着几个问题：","pubDate":"Mon, 26 Aug 2019 08:24:55 GMT","guid":"https://xxlaila.github.io/2019/08/26/k8s部署ingress/","category":"kubernetes, Ingress"},{"title":"k8s部署Weave Scope","link":"https://xxlaila.github.io/2019/08/26/k8s部署Weave-Scope/","description":"1、Weave Scope介绍&nbsp;&nbsp;&nbsp;&nbsp;weave scope 是Docker和Kubernetes的故障排除和监控，自动生成应用程序的地图，能够直观地了解，监控和控制基于容器的，基于微服务的应用程序。可以实时的了解docker容器，选择容器基础架构的概述，或关注特定的微服务。轻松识别和纠正问题，确保集装箱化应用的稳定性和性能，查看容器的上下文指标，标记和元数据。轻松地在容器内的进程之间导航，以运行容器，在可扩展的可排序表中进行排列。使用给定主机或服务的最大CPU或内存轻松找到容器。直接与您的容器交互：暂停，重新启动和停止容器。启动命令行。全部不离开范围浏览器窗口。 2、部署weave scope初次学习，直接下载官方配置文件，没有经过任何修改，不过相信自己随着学习的进步，会逐渐深入。github地址，官方地址","pubDate":"Mon, 26 Aug 2019 08:08:15 GMT","guid":"https://xxlaila.github.io/2019/08/26/k8s部署Weave-Scope/","category":"kubernetes,Weave Scope"},{"title":"k8s部署zookeeper集群","link":"https://xxlaila.github.io/2019/08/24/k8s部署zookeeper集群/","description":"zk属于有状态服务，需要连接外部存储，吧数据存放在数据盘里面，否则容器挂了，数据没有了 准备工作准备zk的yaml文件","pubDate":"Sat, 24 Aug 2019 07:00:12 GMT","guid":"https://xxlaila.github.io/2019/08/24/k8s部署zookeeper集群/","category":"kubernetes"},{"title":"k8s部署coredns","link":"https://xxlaila.github.io/2019/08/24/k8s部署coredns/","description":"&nbsp;&nbsp;&nbsp;&nbsp;k8s集群中的应用通常是通过ingress实现微服务发布的，前文介绍过在K8S集群中使用traefik实现服务的自动发布，其实现方式是traefik通过集群的DNS服务来解析service对应的集群地址（clusterip），从而将用户的访问请求转发到集群地址上。因此，在部署完集群后的第一件事情应该是配置DNS服务，目前可选的方案有skydns, kube-dns, coredns。&nbsp;&nbsp;&nbsp;&nbsp;kubedns是Kubernetes中的一个内置插件，目前作为一个独立的开源项目维护，见https://github.com/kubernetes/dns。该DNS服务器利用SkyDNS的库来为Kubernetes pod和服务提供DNS请求。CoreDNS项目是SkyDNS2的作者，Miek Gieben采用更模块化，可扩展的框架构建,将此DNS服务器作为KubeDNS的替代品。CoreDNS作为CNCF中的托管的一个项目，在Kuberentes1.9版本中，使用kubeadm方式安装的集群可以通过以下命令直接安装CoreDNS。kubeadm init –feature-gates=CoreDNS=true 准备工作准备coredns的yaml文件","pubDate":"Sat, 24 Aug 2019 06:54:14 GMT","guid":"https://xxlaila.github.io/2019/08/24/k8s部署coredns/","category":"coredns"},{"title":"k8s部署mysql","link":"https://xxlaila.github.io/2019/08/24/k8s部署mysql/","description":"&nbsp;&nbsp;&nbsp;&nbsp;后端存储利用nfs来进行存储数据，nfs安装不阐述，需要注意注意的是在创建mysql 的共享目录的时候参数设定/data/mysql *(rw,sync,no_root_squash,no_subtree_check) 12$ sudo systemctl restart nfs.service$ sudo exportfs -arv 1、创建mysql存储12345678910111213141516171819202122232425262728# cat mysql-pvc.yamlapiVersion: v1kind: PersistentVolumemetadata: name: pvc001 namespace: kube-opsspec: capacity: storage: 10Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Delete nfs: server: 172.21.16.240 path: /data/mysql---kind: PersistentVolumeClaimapiVersion: v1metadata: name: mysql-pvc namespace: kube-opsspec: accessModes: - ReadWriteMany resources: requests: storage: 10Gi","pubDate":"Sat, 24 Aug 2019 06:43:07 GMT","guid":"https://xxlaila.github.io/2019/08/24/k8s部署mysql/","category":"kubernetes"}]}