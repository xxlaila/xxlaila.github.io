<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>istio部署</title>
    <url>/2019/10/29/istio%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:23 GMT+0800 (China Standard Time) --><h3 id="Istio介绍"><a href="#Istio介绍" class="headerlink" title="Istio介绍"></a>Istio介绍</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;istio代表的是Service Mesh的方案实现，Istio 有助于降低这些部署的复杂性，并减轻开发团队的压力。提供一种简单的方式来为已部署的服务建立网络，且提供具有负载均衡、服务间认证、监控、流量管理等功能。</p><a id="more"></a><h3 id="服务网格（Service-Mesh）"><a href="#服务网格（Service-Mesh）" class="headerlink" title="服务网格（Service Mesh）"></a>服务网格（Service Mesh）</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;服务网格（Service Mesh）用于描述构成这些应用程序的微服务网络以及应用之间的交互。随着规模和复杂性的增长，服务网格越来越难以理解和管理。它的需求包括服务发现、负载均衡、故障恢复、指标收集和监控以及通常更加复杂的运维需求，例如 A/B 测试、金丝雀发布、限流、访问控制和端到端认证等。而istio刚好提供了一套完整的解决方案，通过控制整个服务器网格提供行为洞察和操作控制来满足微服务应用的多样化</p><h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Istio 服务网格逻辑上分为数据平面和控制平面。</p><ul><li>数据平面由一组以 sidecar 方式部署的智能代理（Envoy）组成。这些代理可以调节和控制微服务及 Mixer 之间所有的网络通信。</li><li>控制平面负责管理和配置代理来路由流量。此外控制平面配置 Mixer 以实施策略和收集遥测数据。</li></ul><p>构成每个面板的不同组件:<br><img src="https://img.xxlaila.cn/1567136153850.jpg" alt="img"></p><h4 id="istio-组件"><a href="#istio-组件" class="headerlink" title="istio 组件"></a>istio 组件</h4><ul><li>Envoy: Istio 使用 Envoy 代理的扩展版本，用于调解服务网格中所有服务的所有入站和出站流量，属于数据层面。Istio利用Envoy的内置功能实现如下指标:<ul><li>动态服务发现</li><li>负载均衡</li><li>TLS终止</li><li>HTTP/2和gRPC代理</li><li>断路器</li><li>健康检查</li><li>分阶段推出，按百分比分配流量</li><li>故障注入</li><li>丰富的指标</li></ul></li><li>Mixer: 是一个独立于平台的组件，负责在服务网格上执行访问控制和使用策略，并从 Envoy 代理和其他服务收集遥测数据</li><li>Pilot: 为 Envoy sidecar 提供服务发现功能，为智能路由（例如 A/B 测试、金丝雀部署等）和弹性（超时、重试、熔断器等）提供流量管理功能</li><li>Citadel: 通过内置身份和凭证管理赋能强大的服务间和最终用户身份验证。可用于升级服务网格中未加密的流量，并为运维人员提供基于服务标识而不是网络控制的强制执行策略的能力</li><li>Galley: 代表其他的 Istio 控制平面组件，用来验证用户编写的 Istio API 配置。随着时间的推移，Galley 将接管 Istio 获取配置、 处理和分配组件的顶级责任</li></ul><h3 id="Istion-安装"><a href="#Istion-安装" class="headerlink" title="Istion 安装"></a>Istion 安装</h3><h4 id="下载istio包"><a href="#下载istio包" class="headerlink" title="下载istio包"></a>下载istio包</h4><p>执行下载和自动解压缩</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># curl -L https://git.io/getLatestIstio | ISTIO_VERSION=1.2.8 sh -</span></span><br><span class="line"><span class="comment"># cd istio-1.2.8/bin</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># cp istioctl /usr/bin/</span></span><br></pre></td></tr></table></figure><p>安装目录中包含：</p><ul><li><code>在 install/</code>: 目录中包含了 Kubernetes 安装所需的 .yaml 文件</li><li><code>samples/</code>: 目录中是示例应用</li><li><code>istioctl</code>: istioctl客户端二进制文件。手动将Envoy作为Sidecar代理注入并创建路由规则和策略时，将使用此工具。</li><li><code>istio.VERSION</code>: 配置文件</li></ul><h3 id="在kubernetes-集群中安装"><a href="#在kubernetes-集群中安装" class="headerlink" title="在kubernetes 集群中安装"></a>在kubernetes 集群中安装</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Istio 会被安装到自己的 istio-system 命名空间，并且能够对所有其他命名空间的服务进行管理。这里采用helm进行安装，<a href="https://xxlaila.github.io/2019/09/04/k8s-helm/" target="_blank" rel="noopener">helm安装参考</a>，我们需要为Kiali设置身份验证凭据（监视）。用于后面的登录认证</p><h4 id="设置用户名和密码的环境变量"><a href="#设置用户名和密码的环境变量" class="headerlink" title="设置用户名和密码的环境变量"></a>设置用户名和密码的环境变量</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># KIALI_USERNAME=$(read -p 'Kiali Username: ' uval &amp;&amp; echo -n $uval | base64)</span></span><br><span class="line"><span class="comment"># KIALI_PASSPHRASE=$(read -sp 'Kiali Passphrase: ' pval &amp;&amp; echo -n $pval | base64)</span></span><br></pre></td></tr></table></figure><h4 id="创建命名空间"><a href="#创建命名空间" class="headerlink" title="创建命名空间"></a>创建命名空间</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># NAMESPACE=istio-system</span></span><br><span class="line"><span class="comment"># kubectl create namespace $NAMESPACE</span></span><br></pre></td></tr></table></figure><ul><li>创建用于存储上面设置的用户名/密码的机密<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF | kubectl apply -f -</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  name: kiali</span><br><span class="line">  namespace: <span class="variable">$NAMESPACE</span></span><br><span class="line">  labels:</span><br><span class="line">    app: kiali</span><br><span class="line"><span class="built_in">type</span>: Opaque</span><br><span class="line">data:</span><br><span class="line">  username: <span class="variable">$KIALI_USERNAME</span></span><br><span class="line">  passphrase: <span class="variable">$KIALI_PASSPHRASE</span></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></li></ul><h4 id="使用helm安装istio-CRD"><a href="#使用helm安装istio-CRD" class="headerlink" title="使用helm安装istio CRD"></a>使用helm安装istio CRD</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># helm install install/kubernetes/helm/istio-init --name istio-init --namespace istio-system</span></span><br><span class="line">NAME:   istio-init</span><br><span class="line">LAST DEPLOYED: Fri Nov  1 10:13:22 2019</span><br><span class="line">NAMESPACE: istio-system</span><br><span class="line">STATUS: DEPLOYED</span><br><span class="line"></span><br><span class="line">RESOURCES:</span><br><span class="line">==&gt; v1/ClusterRole</span><br><span class="line">NAME                     AGE</span><br><span class="line">istio-init-istio-system  0s</span><br><span class="line"></span><br><span class="line">==&gt; v1/ClusterRoleBinding</span><br><span class="line">NAME                                        AGE</span><br><span class="line">istio-init-admin-role-binding-istio-system  0s</span><br><span class="line"></span><br><span class="line">==&gt; v1/ConfigMap</span><br><span class="line">NAME          DATA  AGE</span><br><span class="line">istio-crd-10  1     0s</span><br><span class="line">istio-crd-11  1     0s</span><br><span class="line">istio-crd-12  1     0s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Job</span><br><span class="line">NAME                     COMPLETIONS  DURATION  AGE</span><br><span class="line">istio-init-crd-10-1.2.8  0/1          0s</span><br><span class="line">istio-init-crd-11-1.2.8  0/1          0s</span><br><span class="line">istio-init-crd-12-1.2.8  0/1          0s</span><br><span class="line"></span><br><span class="line">==&gt; v1/ServiceAccount</span><br><span class="line">NAME                        SECRETS  AGE</span><br><span class="line">istio-init-service-account  0        0s</span><br></pre></td></tr></table></figure><h5 id="查看安装的CRD和pod"><a href="#查看安装的CRD和pod" class="headerlink" title="查看安装的CRD和pod"></a>查看安装的CRD和pod</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上述安装会把istio的23个crd都提交给kubernetes api 服务器。如果启用了证书管理，crd计数器为28个。我这里未启用证书管理，只有23个。还生成三个pod</p><ul><li><p>CRD</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get CustomResourceDefinition</span></span><br><span class="line">NAME                                   CREATED AT</span><br><span class="line">adapters.config.istio.io               2019-10-29T08:41:31Z</span><br><span class="line">attributemanifests.config.istio.io     2019-10-29T08:41:30Z</span><br><span class="line">authorizationpolicies.rbac.istio.io    2019-10-29T08:41:36Z</span><br><span class="line">certificates.certmanager.k8s.io        2019-10-29T08:41:38Z</span><br><span class="line">challenges.certmanager.k8s.io          2019-10-29T08:41:40Z</span><br><span class="line">clusterissuers.certmanager.k8s.io      2019-10-29T08:41:37Z</span><br><span class="line">clusterrbacconfigs.rbac.istio.io       2019-10-29T08:41:26Z</span><br><span class="line">destinationrules.networking.istio.io   2019-10-29T08:41:25Z</span><br><span class="line">envoyfilters.networking.istio.io       2019-10-29T08:41:26Z</span><br><span class="line">gateways.networking.istio.io           2019-10-29T08:41:26Z</span><br><span class="line">handlers.config.istio.io               2019-10-29T08:41:33Z</span><br><span class="line">httpapispecbindings.config.istio.io    2019-10-29T08:41:27Z</span><br><span class="line">httpapispecs.config.istio.io           2019-10-29T08:41:28Z</span><br><span class="line">instances.config.istio.io              2019-10-29T08:41:32Z</span><br><span class="line">issuers.certmanager.k8s.io             2019-10-29T08:41:37Z</span><br><span class="line">meshpolicies.authentication.istio.io   2019-10-29T08:41:27Z</span><br><span class="line">orders.certmanager.k8s.io              2019-10-29T08:41:40Z</span><br><span class="line">policies.authentication.istio.io       2019-10-29T08:41:27Z</span><br><span class="line">quotaspecbindings.config.istio.io      2019-10-29T08:41:28Z</span><br><span class="line">quotaspecs.config.istio.io             2019-10-29T08:41:29Z</span><br><span class="line">rbacconfigs.rbac.istio.io              2019-10-29T08:41:31Z</span><br><span class="line">rules.config.istio.io                  2019-10-29T08:41:30Z</span><br><span class="line">serviceentries.networking.istio.io     2019-10-29T08:41:25Z</span><br><span class="line">servicerolebindings.rbac.istio.io      2019-10-29T08:41:31Z</span><br><span class="line">serviceroles.rbac.istio.io             2019-10-29T08:41:31Z</span><br><span class="line">sidecars.networking.istio.io           2019-10-29T08:41:34Z</span><br><span class="line">templates.config.istio.io              2019-10-29T08:41:32Z</span><br><span class="line">virtualservices.networking.istio.io    2019-10-29T08:41:25Z</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl get crds | grep 'istio.io\|certmanager.k8s.io' | wc -l</span></span><br><span class="line">23</span><br></pre></td></tr></table></figure></li><li><p>pod</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get pods -n istio-system</span></span><br><span class="line">NAME                                      READY   STATUS      RESTARTS   AGE</span><br><span class="line">istio-init-crd-10-1.2.8-wxxjn             0/1     Completed   0          3h20m</span><br><span class="line">istio-init-crd-11-1.2.8-brjhh             0/1     Completed   0          3h20m</span><br><span class="line">istio-init-crd-12-1.2.8-w8wnc             0/1     Completed   0          3h20m</span><br></pre></td></tr></table></figure></li></ul><h4 id="使用helm安装各个组件"><a href="#使用helm安装各个组件" class="headerlink" title="使用helm安装各个组件"></a>使用helm安装各个组件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">helm install install/kubernetes/helm/istio \</span><br><span class="line">    --name istio \</span><br><span class="line">    --namespace istio-system \</span><br><span class="line">    --<span class="built_in">set</span> global.mtls.enabled=<span class="literal">true</span> \</span><br><span class="line">    --<span class="built_in">set</span> kiali.enabled=<span class="literal">true</span> \</span><br><span class="line">    --<span class="built_in">set</span> tracing.enabled=<span class="literal">true</span> \</span><br><span class="line">    --<span class="built_in">set</span> grafana.enabled=<span class="literal">true</span> \</span><br><span class="line">    --<span class="built_in">set</span> servicegraph.enabled=<span class="literal">true</span> \</span><br><span class="line">    --<span class="built_in">set</span> <span class="string">"kiali.dashboard.jaegerURL=http://jaeger-query:16686"</span> \</span><br><span class="line">    --<span class="built_in">set</span> <span class="string">"kiali.dashboard.grafanaURL=http://grafana:3000"</span> \</span><br><span class="line">    --<span class="built_in">set</span> gateways.istio-ingressgateway.type=NodePort \</span><br><span class="line">    --<span class="built_in">set</span> gateways.istio-egressgateway.type=NodePort</span><br></pre></td></tr></table></figure><h4 id="验证安装"><a href="#验证安装" class="headerlink" title="验证安装"></a>验证安装</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;验证文件里面的服务是否都部署在kubernetes 服务中。确保部署的pod 在对应的kubernetes namespace 里面，并正常启动。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这期间将创建所需的RBAC权限，并部署Istio-Pilot，Istio-Mixer，Istio-Ingress，Istio-Egress和Istio-CA（证书颁发机构）。</p><h5 id="服务器验证"><a href="#服务器验证" class="headerlink" title="服务器验证"></a>服务器验证</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;确保部署了以下Kubernetes服务：istio-pilot，istio-mixer，istio-ingress。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get svc -n istio-system</span></span><br><span class="line">NAME                     TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                                                                                                                      AGE</span><br><span class="line">grafana                  ClusterIP   10.254.113.150   &lt;none&gt;        3000/TCP                                                                                                                                     3h22m</span><br><span class="line">istio-citadel            ClusterIP   10.254.27.143    &lt;none&gt;        8060/TCP,15014/TCP                                                                                                                           3h22m</span><br><span class="line">istio-galley             ClusterIP   10.254.155.177   &lt;none&gt;        443/TCP,15014/TCP,9901/TCP                                                                                                                   3h22m</span><br><span class="line">istio-ingressgateway     NodePort    10.254.170.109   &lt;none&gt;        15020:31952/TCP,80:31380/TCP,443:31390/TCP,31400:31400/TCP,15029:32532/TCP,15030:31518/TCP,15031:31525/TCP,15032:30404/TCP,15443:30309/TCP   3h22m</span><br><span class="line">istio-pilot              ClusterIP   10.254.228.182   &lt;none&gt;        15010/TCP,15011/TCP,8080/TCP,15014/TCP                                                                                                       3h22m</span><br><span class="line">istio-policy             ClusterIP   10.254.13.184    &lt;none&gt;        9091/TCP,15004/TCP,15014/TCP                                                                                                                 3h22m</span><br><span class="line">istio-sidecar-injector   ClusterIP   10.254.154.169   &lt;none&gt;        443/TCP                                                                                                                                      3h22m</span><br><span class="line">istio-telemetry          ClusterIP   10.254.71.72     &lt;none&gt;        9091/TCP,15004/TCP,15014/TCP,42422/TCP                                                                                                       3h22m</span><br><span class="line">jaeger-agent             ClusterIP   None             &lt;none&gt;        5775/UDP,6831/UDP,6832/UDP                                                                                                                   3h22m</span><br><span class="line">jaeger-collector         ClusterIP   10.254.100.29    &lt;none&gt;        14267/TCP,14268/TCP                                                                                                                          3h22m</span><br><span class="line">jaeger-query             ClusterIP   10.254.18.117    &lt;none&gt;        16686/TCP                                                                                                                                    3h22m</span><br><span class="line">kiali                    ClusterIP   10.254.156.117   &lt;none&gt;        20001/TCP                                                                                                                                    3h22m</span><br><span class="line">prometheus               ClusterIP   10.254.145.181   &lt;none&gt;        9090/TCP                                                                                                                                     3h22m</span><br><span class="line">tracing                  ClusterIP   10.254.87.72     &lt;none&gt;        80/TCP                                                                                                                                       3h22m</span><br><span class="line">zipkin                   ClusterIP   10.254.39.22     &lt;none&gt;        9411/TCP                                                                                                                                     3h22m</span><br></pre></td></tr></table></figure><h5 id="pod-验证"><a href="#pod-验证" class="headerlink" title="pod 验证"></a>pod 验证</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;确保已部署相应的Kubernetes Pod，并且所有容器都已启动并正在运行</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get pods -n istio-system</span></span><br><span class="line">NAME                                      READY   STATUS      RESTARTS   AGE</span><br><span class="line">grafana-6fb9f8c5c7-n2plk                  1/1     Running     0          3h19m</span><br><span class="line">istio-citadel-7c9b84ddb6-n5h2n            1/1     Running     0          3h19m</span><br><span class="line">istio-galley-64f7d8cc97-zdbb6             1/1     Running     0          3h19m</span><br><span class="line">istio-grafana-post-install-1.2.8-98grv    0/1     Completed   0          3h19m</span><br><span class="line">istio-ingressgateway-65c7498b78-dfmfp     1/1     Running     0          3h19m</span><br><span class="line">istio-init-crd-10-1.2.8-wxxjn             0/1     Completed   0          3h20m</span><br><span class="line">istio-init-crd-11-1.2.8-brjhh             0/1     Completed   0          3h20m</span><br><span class="line">istio-init-crd-12-1.2.8-w8wnc             0/1     Completed   0          3h20m</span><br><span class="line">istio-pilot-569499d666-vhgn5              2/2     Running     0          3h19m</span><br><span class="line">istio-policy-5dbbc56db5-dmr4p             2/2     Running     3          3h19m</span><br><span class="line">istio-sidecar-injector-747cf74498-99drh   1/1     Running     0          3h19m</span><br><span class="line">istio-telemetry-7db5dd4c57-zngq7          2/2     Running     4          3h19m</span><br><span class="line">istio-tracing-5d8f57c8ff-vt2kn            1/1     Running     0          3h19m</span><br><span class="line">kiali-7d749f9dcb-68tlt                    1/1     Running     0          3h19m</span><br><span class="line">prometheus-776fdf7479-zbrxl               1/1     Running     0          3h19m</span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Istio 以一个项目的形式部署到 Kubernetes 集群中。我们可以看到，部署好的 pods 中，除了有 istio-citadel、、istio-ingressgateway、istio-pilot 等 Istio 本身的功能组件，还集成了微服务相关的监控工具，，如：grafana、jaeger-query、kiali、prometheus。这些功能丰富且强大的监控工具，帮助 Istio实现了微服务的可视化管理。</p><h3 id="部署BookInfo用程序"><a href="#部署BookInfo用程序" class="headerlink" title="部署BookInfo用程序"></a>部署BookInfo用程序</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;现在开始部署 Bookinfo 示例程序。部署Bookinfo条件是集群中至少有4个节点，而且每个节点的内存不得低于4G。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;可以部署安装随附的示例应用程序之一-BookInfo。这是一个简单的模拟书店应用程序，由四个服务组成，这些服务提供一个Web产品页面，书籍详细信息，评论（带有多个版本的评论服务）和评分-所有这些都使用Istio进行管理。</p><ul><li><p>BookInfo应用程序分为四个单独的微服务:</p><ul><li>productpage ：productpage 微服务会调用 details 和 reviews 两个微服务，用来生成页面。</li><li>details ：这个微服务包含了书籍的信息。</li><li>reviews ：这个微服务包含了书籍相关的评论。它还会调用 ratings 微服务。</li><li>ratings ：ratings 微服务中包含了由书籍评价组成的评级信息。</li></ul></li><li><p>reviews 微服务有 3 个版本：</p><ul><li>v1 版本不会调用 ratings 服务.</li><li>v2 版本会调用 ratings 服务，并使用 1 到 5 个黑色星形图标来显示评分信息</li><li>v3 版本会调用 ratings 服务，并使用 1 到 5 个红色星形图标来显示评分信息</li></ul></li><li><p>下图展示了这个应用的端到端架构<br><img src="https://img.xxlaila.cn/1572576628250.jpg" alt="img"></p></li></ul><h4 id="打标签"><a href="#打标签" class="headerlink" title="打标签"></a>打标签</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为 default 命名空间打上标签 istio-injection=enabled，实现 Sidecar 自动注入。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl label namespace default istio-injection=enabled</span></span><br><span class="line">namespace/default labeled</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl get namespace --show-labels</span></span><br><span class="line">NAME              STATUS   AGE   LABELS</span><br><span class="line">default           Active   43d   istio-injection=enabled</span><br><span class="line">istio-system      Active   29m   &lt;none&gt;</span><br><span class="line">kube-node-lease   Active   43d   &lt;none&gt;</span><br><span class="line">kube-public       Active   43d   &lt;none&gt;</span><br><span class="line">kube-system       Active   43d   &lt;none&gt;</span><br><span class="line">monitoring        Active   35d   &lt;none&gt;</span><br><span class="line">weave             Active   35d   &lt;none&gt;</span><br></pre></td></tr></table></figure><ul><li><strong>注意:</strong> 此步骤先不执行，如果这这个执行了，在后面部署Bookinfo的时候会提示如下错误<code>Error creating: Internal error occurred: failed calling webhook &quot;sidecar-injector.istio.io&quot;: Post https://istio-sidecar-injector.istio-system.svc:443/inject?timeout=30s: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)</code>这一步有执行的可以执行以下命令进行删除</li></ul><ul><li>删除ns的label<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get ns --show-labels</span></span><br><span class="line">NAME              STATUS   AGE    LABELS</span><br><span class="line">default           Active   2d4h   istio-injection=enabled</span><br><span class="line">istio-system      Active   174m   &lt;none&gt;</span><br><span class="line">kube-node-lease   Active   2d4h   &lt;none&gt;</span><br><span class="line">kube-public       Active   2d4h   &lt;none&gt;</span><br><span class="line">kube-system       Active   2d4h   &lt;none&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl label namespace default istio-injection-</span></span><br><span class="line">namespace/default labeled</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl get ns --show-labels</span></span><br><span class="line">NAME              STATUS   AGE    LABELS</span><br><span class="line">default           Active   2d4h   &lt;none&gt;</span><br><span class="line">istio-system      Active   175m   &lt;none&gt;</span><br><span class="line">kube-node-lease   Active   2d4h   &lt;none&gt;</span><br><span class="line">kube-public       Active   2d4h   &lt;none&gt;</span><br><span class="line">kube-system       Active   2d4h   &lt;none&gt;</span><br></pre></td></tr></table></figure></li></ul><h4 id="部署Bookinfo"><a href="#部署Bookinfo" class="headerlink" title="部署Bookinfo"></a>部署Bookinfo</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;直接使用kubectl create其常规的YAML部署文件来部署我们的应用程序。将使用istioctl将Envoy容器注入到应用程序容器中：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl create -f &lt;(istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml)</span></span><br><span class="line">service/details created</span><br><span class="line">serviceaccount/bookinfo-details created</span><br><span class="line">deployment.apps/details-v1 created</span><br><span class="line">service/ratings created</span><br><span class="line">serviceaccount/bookinfo-ratings created</span><br><span class="line">deployment.apps/ratings-v1 created</span><br><span class="line">service/reviews created</span><br><span class="line">serviceaccount/bookinfo-reviews created</span><br><span class="line">deployment.apps/reviews-v1 created</span><br><span class="line">deployment.apps/reviews-v2 created</span><br><span class="line">deployment.apps/reviews-v3 created</span><br><span class="line">service/productpage created</span><br><span class="line">serviceaccount/bookinfo-productpage created</span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;该命令将启动bookinfo应用程序体系结构图中显示的所有四个服务。已启动评论服务的所有3个版本，即v1，v2和v3。而在实际部署中，随着时间的推移会部署新版本的微服务，而不是同时部署所有版本。</p><h4 id="检查部署"><a href="#检查部署" class="headerlink" title="检查部署"></a>检查部署</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;确认所有服务和Pod均已正确定义并正在运行。</p><ul><li><p>检查 services</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get services</span></span><br><span class="line">NAME          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE</span><br><span class="line">details       ClusterIP   10.254.61.113    &lt;none&gt;        9080/TCP   2m27s</span><br><span class="line">kubernetes    ClusterIP   10.254.0.1       &lt;none&gt;        443/TCP    43d</span><br><span class="line">productpage   ClusterIP   10.254.130.5     &lt;none&gt;        9080/TCP   2m23s</span><br><span class="line">ratings       ClusterIP   10.254.186.181   &lt;none&gt;        9080/TCP   2m26s</span><br><span class="line">reviews       ClusterIP   10.254.200.107   &lt;none&gt;        9080/TCP   2m25s</span><br></pre></td></tr></table></figure></li><li><p>检查 pod</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get pods</span></span><br><span class="line">NAME                              READY   STATUS    RESTARTS   AGE</span><br><span class="line">details-v1-c5b5f496d-lphgd        1/1     Running   0          15h</span><br><span class="line">load-generator-7fbcc7489f-vbpnx   1/1     Running   2          20d</span><br><span class="line">nginx-deploy-d494b9564-vx97s      1/1     Running   1          20d</span><br><span class="line">productpage-v1-c7765c886-97spj    1/1     Running   0          15h</span><br><span class="line">ratings-v1-f745cf57b-mdgxr        1/1     Running   0          15h</span><br><span class="line">reviews-v1-75b979578c-ghqqm       1/1     Running   0          15h</span><br><span class="line">reviews-v2-597bf96c8f-r659w       1/1     Running   0          15h</span><br><span class="line">reviews-v3-54c6c64795-tvsmq       1/1     Running   0          15h</span><br></pre></td></tr></table></figure></li><li><p>确认Bookinfo应用程序正在运行，请通过curl某个pod中的命令向其发送请求</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl exec -it $(kubectl get pod -l app=ratings -o jsonpath='&#123;.items[0].metadata.name&#125;') -c ratings -- curl productpage:9080/productpage | grep -o "&lt;title&gt;.*&lt;/title&gt;"</span></span><br><span class="line">&lt;title&gt;Simple Bookstore App&lt;/title&gt;</span><br></pre></td></tr></table></figure></li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在每个服务旁边都注入了Envoy，架构将如下<br><img src="https://img.xxlaila.cn/1572577460804.jpg" alt="img"></p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bookinfo服务已启动并正在运行，您需要使该应用程序可以从Kubernetes集群外部访问，例如，从浏览器访问。Istio网关用于此目的。但是我在部署 bookinfo-gateway 的时候出现错误，错误如下；然后看了一下 bookinfo-gateway就是提供一个web访问的程序，既然是提供的一个web访问，我就使用了Traefix来提供这个服务。</p><figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line"><span class="builtin-name">Error</span> <span class="keyword">from</span><span class="built_in"> server </span>(Timeout): <span class="builtin-name">error</span> when creating <span class="string">"samples/bookinfo/networking/bookinfo-gateway.yaml"</span>: Timeout: request did <span class="keyword">not</span> complete within requested timeout 30s</span><br><span class="line"><span class="builtin-name">Error</span> <span class="keyword">from</span><span class="built_in"> server </span>(Timeout): <span class="builtin-name">error</span> when creating <span class="string">"samples/bookinfo/networking/bookinfo-gateway.yaml"</span>: Timeout: request did <span class="keyword">not</span> complete within requested timeout 30s</span><br></pre></td></tr></table></figure><h4 id="创建-bookinfo-gateway"><a href="#创建-bookinfo-gateway" class="headerlink" title="创建 bookinfo-gateway"></a>创建 bookinfo-gateway</h4><ul><li>istio-Ingress.yaml<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat &gt;istio-Ingress.yaml &lt;&lt;EOF</span></span><br><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: istio-web-ui</span><br><span class="line">  namespace: </span><br><span class="line">  annotations:</span><br><span class="line">    kubernetes.io/ingress.class: traefik</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: istio.xxlaila.cn</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: /</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: productpage</span><br><span class="line">          servicePort: 9080</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在浏览器输入<code>http://istio.xxlaila.cn</code> 来访问。用 productpage以查看BookInfo网页。如果您多次刷新页面，您应该会看到产品页面上显示的评论版本不同，并以循环方式显示（红色星星，黑色星星，无星星），因为我们尚未使用Istio来控制版本路由<br><img src="https://img.xxlaila.cn/1572578398765.jpg" alt="img"><br><img src="https://img.xxlaila.cn/1572578189667.jpg" alt="img"></p><p>基本道这里，动态更改请求路由学习中，😂😂😂</p><h3 id="监控方式"><a href="#监控方式" class="headerlink" title="监控方式"></a>监控方式</h3><h4 id="生成服务图"><a href="#生成服务图" class="headerlink" title="生成服务图"></a>生成服务图</h4><p>要验证Kiali是否在您的集群中运行</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl -n istio-system get svc kiali</span></span><br><span class="line">NAME    TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)     AGE</span><br><span class="line">kiali   ClusterIP   10.254.156.117   &lt;none&gt;        20001/TCP   4h38m</span><br></pre></td></tr></table></figure><p>流量发送到网格，有三种选择:<br>1.在网络浏览器中访问<a href="http://istio.xxlaila.cn/productpage" target="_blank" rel="noopener">http://istio.xxlaila.cn/productpage</a><br>2.多次使用以下命令</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># curl http://istio.xxlaila.cn/productpage</span></span><br></pre></td></tr></table></figure><p>3.使用以下watch命令连续发送请求：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># watch -n 1 curl -o /dev/null -s -w %&#123;http_code&#125; http://istio.xxlaila.cn/productpage</span></span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里需要配置Kiali UI，我们同样适用Traefix来进行配置</p><ul><li>kiali–Ingress.yaml<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat &gt; kiali--Ingress.yaml &lt;&lt;EOF</span></span><br><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: kiali-web-ui</span><br><span class="line">  namespace: istio-system </span><br><span class="line">  annotations:</span><br><span class="line">    kubernetes.io/ingress.class: traefik</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: istio-kiali.xxlaila.cn</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: /</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: kiali</span><br><span class="line">          servicePort: 20001</span><br></pre></td></tr></table></figure></li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在浏览器打开<a href="http://istio-kiali.xxlaila.cn" target="_blank" rel="noopener">http://istio-kiali.xxlaila.cn</a> ， 要登录Kiali UI，请转到Kiali登录屏幕，然后输入存储在Kiali机密中的用户名和密码。账户密码是前面我们设置的</p><h4 id="1-网格概述"><a href="#1-网格概述" class="headerlink" title="1.网格概述"></a>1.网格概述</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;登录后立即显示的“概述”页面中查看网格的概述。“概述”页面显示了网格中具有服务的所有名称空间。以下屏幕截图显示了类似的页面<br><img src="https://img.xxlaila.cn/1572578943386.jpg" alt="img"></p><h4 id="2-空间图"><a href="#2-空间图" class="headerlink" title="2.空间图"></a>2.空间图</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;要查看名称空间图，请在bookinfoBookinfo名称空间卡中单击图图标。图形图标位于名称空间卡的左下方，看起来像是一组相连的圈子。该页面类似于<br><img src="https://img.xxlaila.cn/1572579048298.jpg" alt="img"></p><h3 id="分布式跟踪系统"><a href="#分布式跟踪系统" class="headerlink" title="分布式跟踪系统"></a>分布式跟踪系统</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;启用Istio的应用程序可以配置为使用流行的Jaeger分布式跟踪系统来收集跟踪范围。分布式跟踪使您可以查看用户在系统中发出的请求流，而Istio的模型则允许这样做，而与构建应用程序所使用的语言/框架/平台无关。使用Traefix来提供这个服务。</p><ul><li><p>Jaeger-Ingress.yaml</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat &gt; Jaeger-Ingress.yaml  &lt;&lt;EOF</span></span><br><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: jaeger-web-ui</span><br><span class="line">  namespace: istio-system </span><br><span class="line">  annotations:</span><br><span class="line">    kubernetes.io/ingress.class: traefik</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: jaeger.xxlaila.cn</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: /</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: jaeger-query</span><br><span class="line">          servicePort: 16686</span><br></pre></td></tr></table></figure></li><li><p>执行创建</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl apply -f Jaeger-Ingress.yaml </span></span><br><span class="line">ingress.extensions/jaeger-web-ui unchanged</span><br></pre></td></tr></table></figure></li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在浏览器打开<a href="http://jaeger.xxlaila.cn" target="_blank" rel="noopener">http://jaeger.xxlaila.cn</a> ， 使用Bookinfo示例生成跟踪，要查看跟踪数据，必须将请求发送到服务。请求数量取决于Istio的采样率。您在安装Istio时设置此速率。默认采样率为1％。您需要至少发送100个请求，才能显示第一条跟踪。要将100个请求发送到productpage服务，请使用以下命令：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># for i in `seq 1 100`; do curl -s -o /dev/null http://istio.xxlaila.cn/productpage; done</span></span><br></pre></td></tr></table></figure><ul><li><p>在仪表板的左侧窗格中，从“服务”下拉列表中选择productpage.default，然后单击“查找跟踪”<br><img src="https://img.xxlaila.cn/1572592255728.jpg" alt="img"></p></li><li><p>单击顶部的最新跟踪以查看与对/ productpage的最新请求相对应的详细信息<br><img src="https://img.xxlaila.cn/1572592385675.jpg" alt="img"></p></li></ul><h3 id="监视Istio"><a href="#监视Istio" class="headerlink" title="监视Istio"></a>监视Istio</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如何设置和使用Istio仪表板监视网格流量。作为监控的一部分，需要将安装Grafana Istio插件，并使用基于Web的界面查看服务网格流量数据。Grafana将用于可视化普罗米修斯数据。在执行部署的时候也部署了这两个服务。</p><h4 id="创建grafana-Ingress"><a href="#创建grafana-Ingress" class="headerlink" title="创建grafana Ingress"></a>创建grafana Ingress</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat &gt;grafana-istio-Ingress.yaml &lt;&lt;EOF</span></span><br><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: grafana-istio-web-ui</span><br><span class="line">  namespace: istio-system</span><br><span class="line">  annotations:</span><br><span class="line">    kubernetes.io/ingress.class: traefik</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: grafana-istio.xxlaila.cn</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: /</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: grafana</span><br><span class="line">          servicePort: 3000</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li>执行创建，这里我们可以在以前的grafana里面添加数据库源，就不用在新起一个域名来进行访问<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;再次加载Bookinfo应用程序（<a href="http://istio.xxlaila.cn/productpage）" target="_blank" rel="noopener">http://istio.xxlaila.cn/productpage）</a> ， 刷新页面几次（或发送命令几次）以产生少量流量。再次查看Istio仪表板。它应该反映所产生的流量。<br><img src="https://img.xxlaila.cn/1572593852626.jpg" alt="img"></li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;istio 还提供了网格的全局视图以及网格中的服务和工作负载。您可以通过导航到特定的仪表板来获取有关服务和工作负载的更多详细信息。<br><img src="https://img.xxlaila.cn/1572594150893.jpg" alt="img"></p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;提供了有关服务指标的详细信息，然后是该服务的客户端工作负载（正在调用此服务的工作负载）和服务工作负载（正在提供该服务的工作负载）。<br><img src="https://img.xxlaila.cn/1572594261333.jpg" alt="img"></p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Istio 在grafana 提供了很多的监控指标，可以分别点击看看<br><img src="https://img.xxlaila.cn/1572594330246.jpg" alt="img"></p><h3 id="查询Istio指标"><a href="#查询Istio指标" class="headerlink" title="查询Istio指标"></a>查询Istio指标</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Istio的数据是存储在prometheus里面的，这里我们通过prometheus进行直接数据的查询</p><h4 id="查看prometheus服务"><a href="#查看prometheus服务" class="headerlink" title="查看prometheus服务"></a>查看prometheus服务</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl -n istio-system get svc prometheus</span></span><br><span class="line">NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE</span><br><span class="line">prometheus   ClusterIP   10.254.145.181   &lt;none&gt;        9090/TCP   5h35m</span><br></pre></td></tr></table></figure><h4 id="prometheus-traefix"><a href="#prometheus-traefix" class="headerlink" title="prometheus traefix"></a>prometheus traefix</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过traefix 来代理prometheus，然后我们将流量发送到网格。</p><ul><li><p>prometheus-istio.yaml</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat &gt; prometheus-istio-Ingress.yaml &lt;&lt;EOF</span></span><br><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: prometheus-istio-web-ui</span><br><span class="line">  namespace: istio-system</span><br><span class="line">  annotations:</span><br><span class="line">    kubernetes.io/ingress.class: traefik</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: prometheus-istio.xxlaila.cn</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: /</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: prometheus</span><br><span class="line">          servicePort: 9090</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></li><li><p>执行创建</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl apply -f prometheus-istio-Ingress.yaml </span></span><br><span class="line">ingress.extensions/prometheus-istio-web-ui created</span><br></pre></td></tr></table></figure></li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在浏览器打开<a href="http://prometheus-istio.xxlaila.cn" target="_blank" rel="noopener">http://prometheus-istio.xxlaila.cn</a> ，可以在输入框里面输入表达式来获取指，输入文本：istio_requests_total<br><img src="https://img.xxlaila.cn/1572594888435.jpg" alt="img"></p><ul><li><p>其他查询尝试：</p><ul><li><p>对productpage服务的所有请求总数：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">istio_requests_total&#123;destination_service=<span class="string">"productpage.default.svc.cluster.local"</span>&#125;</span><br></pre></td></tr></table></figure></li><li><p>对v3版本的评论服务的所有请求总数：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">istio_requests_total&#123;destination_service=<span class="string">"reviews.default.svc.cluster.local"</span>, destination_version=<span class="string">"v3"</span>&#125;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>该查询将所有请求的当前总数返回到评论服务的v3。</p><ul><li>过去5分钟内对productpage服务所有实例的请求率：<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rate(istio_requests_total&#123;destination_service=~<span class="string">"productpage.*"</span>, response_code=<span class="string">"200"</span>&#125;[5m])</span><br></pre></td></tr></table></figure></li></ul></li></ul><!-- rebuild by neat -->]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>istio</tag>
      </tags>
  </entry>
  <entry>
    <title>pipeline核心高级篇</title>
    <url>/2019/10/26/pipeline%E9%AB%98%E7%BA%A7%E7%AF%87/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:23 GMT+0800 (China Standard Time) --><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;前面的两篇文章介绍了pipeline的基本使用和一些实际使用的例子，看似很不错，但是在实际应用也会出现很多的不足和问题，随之系统的庞大、服务的增加、人员的参差不齐会导致很多的问题。<a id="more"></a>届时会带来很大的维护成本和一些改动，所以我们在做事情之前就要考虑进去，一些意外事件的发生、或者是在将来即将会发生和需要改变的事情我们都要想到或者是预留口子，这样才在今后扩展、修改、引入都能有很好可塑性。</p><h3 id="jenkins-job介绍"><a href="#jenkins-job介绍" class="headerlink" title="jenkins job介绍"></a>jenkins job介绍</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;大多数情况下我们都是使用jenkins的普通job，普通的job好处是配置简单，结构化可以复杂，也可以单一。在使用jenkins job的时候我们分为两种：一种是单一job，一种是具有耦合性的。下面对两种情况进行对比和比较。</p><h4 id="jenkins-单一job"><a href="#jenkins-单一job" class="headerlink" title="jenkins 单一job"></a>jenkins 单一job</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在jenkins的传统模式下，单一的的job可以让维护人员可以很好的查看里面的逻辑步骤，job里面所有的任务都在这个所属的空间里面执行，它里面包含了：代码pull、编译、打包、复制包、发布包（使用内置的shell模块来写shell，这种应该不存在）。种单一job服务算得上是服务周到，不影响其他人，自己管理好自己的一亩三分地。好处是当出错以后影响范围小，容易控制。如下图：<br><img src="https://img.xxlaila.cn/1572064519037.jpg" alt="img"></p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在这中模式下，维护人员前期用看似比较轻松的工作建立起了整个发布流程。但是到了后期就不行了。之前我在的这家公司前期也是这么这么做的。开发完成后提交git，然后自动触发、构建、制品库、发布，在一个job里面就完成了。后来我们准备推行更好的devops方案的时候；发现以前的这个job建立有问题，一想到几百个微服务，几百个job需要去进行改造。顿时我们运维脸线一黑，虽然我们自己写了一个快速在jenkins上建立job，但是一想到几百个还是不好。为了解决这个问题，我们使用了job之间的任务关联，然后通过参数传递完成整个流程服务。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这种模式下的弊端就如上面所说的一样，但什么时候好的服务呢？好的服务又是什么样子的呢？这里也可以嵌套一些微服务的概念理论。如果我们要做到什么时候好的服务，我们得了解了解一下: 低耦合和高内聚。了解这个东西有助于我们在接下来的pipeline 流水线的设计，包括在后期devops的设计以及撸码都有很大的帮助。</p><h3 id="耦合性"><a href="#耦合性" class="headerlink" title="耦合性"></a>耦合性</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;首先我们来了解这一概念: “高内聚低耦合”。在软件设计中通常用耦合度和内聚度作为衡量模块独立程度的标准。划分模块的一个准则是高内聚低耦合。从模块粒度来看，高内聚：尽可能类的每个成员方法只完成一件事（最大限度的聚合）；低耦合：减少类内部，一个成员方法调用另一个成员方法。从类角度来看，高内聚低耦合：减少类内部，对其他类的调用；从功能块来看，高内聚低耦合：减少模块之间的交互复杂度（接口数量，参数数据）即横向：类与类之间、模块与模块之间；纵向：层次之间；尽可能，内容内聚，数据耦合。</p><h4 id="低耦合"><a href="#低耦合" class="headerlink" title="低耦合"></a>低耦合</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;不同模块相互依赖多少？模块应尽可能独立于其他模块，以使对模块的更改不会严重影响其他模块。</p><h4 id="高耦合"><a href="#高耦合" class="headerlink" title="高耦合"></a>高耦合</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;高耦合将意味着您的模块对其他模块的内部运作了解太多。对其他模块了解太多的模块会使更改难以协调，并使模块能力变弱。如果模块A对模块B的了解过多，则对模块B内部的更改可能会破坏模块A的功能。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过实现低耦合，可以轻松更改模块内部，不必担心它们对系统中其他模块的影响。低耦合还使我们的模块彼此之间不相互依赖，因此更易于设计，编写和测试代码。我们还获得了易于重用和可组合的模块的优势。问题也被隔离到小的，独立的代码单元中。</p><p><strong>好处:</strong></p><ul><li>可维护性: 更改限制在一个模块中</li><li>可测试性: 单元测试中涉及的模块可以限制在最低限度</li><li>可读性: 需要分析的类减少</li></ul><h4 id="高内聚"><a href="#高内聚" class="headerlink" title="高内聚"></a>高内聚</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;内聚性通常是指模块的元素如何相互组合。相关代码应彼此接近，以使其具有高度的凝聚力。易于维护的代码通常具有很高的内聚性。模块中的元素与该模块要提供的功能直接相关。如果需要修改一个功能，最好是在一个地方进行修改，然后可以尽快的发布。如果很多不同的地方要进行修改，就有可能需要发布多个微服务才能交互这个功能。在很多地方进行修改，不仅修改速度很慢，同时部署多个微服务也提高了风险。所以在找到问题域的边界域后可以确保相关的行为能放在同一个地方，并且它们会和其它边界以尽量低耦合的形式进行通信。</p><p><strong>好处:</strong></p><ul><li>可读性: 功能包含在单个模块中</li><li>可维护性: 调试往往包含在单个模块中</li><li>可重用性: 具有集中功能不会被无用的干扰</li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;内聚性低意味着组成某些功能的代码会散布在您的整个代码库中。不仅很难发现与您的模块相关的代码，而且很难在不同的模块之间跳转并跟踪的所有代码。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通俗的来讲，内聚是从功能角度来度量模块内的联系，好的内聚模块应恰好做一件事。描述的是模块内的功能联系。耦合是软件结构中各模块之间相互连接的一种度量，耦合强弱取决于模块间接口的复杂程度、进入或访问一个模块点以及通过接口的数据。</p><h4 id="可维护的代码"><a href="#可维护的代码" class="headerlink" title="可维护的代码"></a>可维护的代码</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;一般在编写可维护的代码有助于提高开发人员的生产力。具有高度可维护的代码使设计新功能和编写代码变得更加容易。模块化，基于组件的分层代码可提高生产率并降低进行更改时的风险。通过使代码保持松散耦合，可以在一个模块内编写代码，而不会影响其他模块。通过保持代码的内聚性，我们可以更轻松地编写易于使用的DRY代码。</p><p><strong>问题</strong>: 当我们遇到问题时，请评估修复、修改程序的程度。是更改一个模块，还是更改分散在整个系统中？在进行更改时，它是否可以解决所有的问题，还是会产生其他一些不可预知的问题？</p><p>在编写和使用代码库时:</p><ul><li>我要修复和创建的此功能模块是多少？</li><li>此更改是要在几个不同的地方进行？</li><li>我能否独立测试代码，测试整个代码有多难？</li><li>我们是否可以使代码更松散地耦合来改善？可以使用高内聚来改善我们的代码吗？</li></ul><h3 id="Jenkins-设计"><a href="#Jenkins-设计" class="headerlink" title="Jenkins 设计"></a>Jenkins 设计</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;有了上面的的理论与概念。根据这里理论和概念我们就可以设计出一套更好的devops流程。本文将kubernetes平台上来做这一套设计，并在实际的环境中应用。涉及的功能如下: 服务 Job、Code Job、Release、Notice四个功能任务。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;每一个环境有错误，就会执行告警任务模块，告警目前使用的是<a href="https://github.com/xxlaila/jenkins-wechat-notice" target="_blank" rel="noopener">企业微信</a>。job之间需要传递JOB_NAME，env，version三个参数。在之前的devops设计里面整个job的调用设计还要多。形成了一个通用体系。在这个设计里面，当还需要增加一个任务流程，我们只需要修改pipeline，然后增加一个job，在下次构建的时候就会把我们新增加的流程给加进去，非常的方便。设计图如下：<br><img src="https://img.xxlaila.cn/1572081425995.jpg" alt="img"></p><h4 id="Project-Name"><a href="#Project-Name" class="headerlink" title="Project Name"></a>Project Name</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;此job一般就是服务，job名称以服务的名称进行命名。里面包含了四个功能.</p><ul><li>Clone Code: clone 代码。</li><li>Build Code: 就是对开发提交的代码进行编译。</li><li>Env Version: 获取本次提交的hash，以hash为版本，结合环境来做一个版本记录，这里需要进行判断。uat/prod环境不需要env前缀。</li><li>Build Docker: 把编译完成后的二进制文件，打包成一个docker镜像。</li></ul><h4 id="Code-Test"><a href="#Code-Test" class="headerlink" title="Code Test"></a>Code Test</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;用于测试进行对代码的自动化测试；自动化流程、性能等测试</p><h4 id="Release"><a href="#Release" class="headerlink" title="Release"></a>Release</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;主要是进行发布服务。当接受到上游job传递来的参数信息后，结合参数信息来进行对应的发布到kubernetes中namespace中，主要包含了以下功能</p><ul><li>Push Docker: 把前面打包的docker镜像推送到harbor</li><li>Edit Files: 修改发布的脚本</li><li>Release: 执行<code>kubectl</code>进行发布<ul><li>当发布到kubernetes中，kubernetes 会执行<a href="https://xxlaila.github.io/2019/09/27/k8s-pod%E5%81%A5%E5%BA%B7%E6%A3%80%E6%B5%8B/" target="_blank" rel="noopener">health检测</a>，如果启动失败，会进行通知</li></ul></li></ul><h4 id="Notice"><a href="#Notice" class="headerlink" title="Notice"></a>Notice</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;此job主要用于通知。当接受到规则的告警通知以后，就会进行触发通知相关的人员。</p><!-- rebuild by neat -->]]></content>
      <categories>
        <category>jenkins</category>
      </categories>
      <tags>
        <tag>pipeline</tag>
      </tags>
  </entry>
  <entry>
    <title>pipeline多分支gitlab触发</title>
    <url>/2019/10/25/pipeline%E5%A4%9A%E5%88%86%E6%94%AFgitlab%E8%A7%A6%E5%8F%91/</url>
    <content><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <div class="hbe-input-container">
  <input type="password" id="hbePass" placeholder="enter password to read." />
    <label for="hbePass">enter password to read.</label>
    <div class="bottom-line"></div>
  </div>
  <script id="hbeData" type="hbeData" data-hmacdigest="63d7f763d0e0c56f9f2b1fcc9e94e9e559752980106663f4de94be3bb8d5c212">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</script>
</div>
<script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>jenkins</category>
      </categories>
      <tags>
        <tag>pipeline</tag>
      </tags>
  </entry>
  <entry>
    <title>jenkins运维篇</title>
    <url>/2019/10/22/jenkins%E8%BF%90%E7%BB%B4%E7%AF%87/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><h3 id="jenkins-可视化构建"><a href="#jenkins-可视化构建" class="headerlink" title="jenkins 可视化构建"></a>jenkins 可视化构建</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在之前遇到过开发提交代码后，完全不管提交的代码是否发布成功，及时后来加入了企业微信的告警机制，但是依然有人不会去关注这个。<a id="more"></a> 只有在测试人员在反馈xxx你的代码提交了没有，这时候研发人员才回去看，有时候一个触发构建失败了，摆在那里很久，如有下一个开发人员要对这个工程修改提交的时候发现过不了，这时候再来解决，成本就有点大。这里可以借助看板的形式让研发人员可以随时关注到自己的提交的工程，结合告警来做，效果还是不错的。</p><h4 id="插件安装"><a href="#插件安装" class="headerlink" title="插件安装"></a>插件安装</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;安装Build Monitor View 插件，然后在主页面添加<code>+</code>一个视图<br><img src="https://img.xxlaila.cn/1571707794737.jpg" alt="img"><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;可以根据job的类型或者根据自己的条件进行<a href="https://xxlaila.github.io/2019/08/09/jenkins-job%E7%AE%A1%E7%90%86/" target="_blank" rel="noopener">过滤job</a>来生成看板。</p><ul><li>Build Monitor - View Settings: 根据job的一些状态来进行排序<br><img src="https://img.xxlaila.cn/1571708048469.jpg" alt="img"></li></ul><h3 id="jenkins-监控"><a href="#jenkins-监控" class="headerlink" title="jenkins 监控"></a>jenkins 监控</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;有时候我们没有监控，但是有时候需要看看jenkins的一些监控信息，如：内存、cpu、系统负债、http响应时间、系统进程数、线程数等，有懒得安装监控，这时候我们可以借助jenkins自带的一个插件<code>Monitoring</code>。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;插件安装完成后，我们可以在系统管理菜单下面看到<code>Monitoring of Jenkins master</code><br><img src="https://img.xxlaila.cn/1571708499625.jpg" alt="img"></p><p>点击进入以后我们可以看到<br><img src="https://img.xxlaila.cn/1571708561404.jpg" alt="img"><br>页面显示乱码，这个可以自己google解决</p><h3 id="Build-Trigger-Badge"><a href="#Build-Trigger-Badge" class="headerlink" title="Build Trigger Badge"></a>Build Trigger Badge</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;此插件直接在构建历史记录中显示代表构建原因的图标。它可以让您快速知道是哪个原因触发了构建。如果没有此插件，您有时可能会想知道是什么触发了构建历史中显示的&gt;&gt;特定构建。要知道这一点，您必须单独打开每个链接，这可能很麻烦。<br><img src="https://img.xxlaila.cn/1572059619062.jpg" alt="img"></p><!-- rebuild by neat -->]]></content>
      <categories>
        <category>jenkins</category>
      </categories>
      <tags>
        <tag>jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title>pipeline语法</title>
    <url>/2019/10/21/pipeline%E8%AF%AD%E6%B3%95/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:23 GMT+0800 (China Standard Time) --><h3 id="pipeline"><a href="#pipeline" class="headerlink" title="pipeline"></a>pipeline</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;最近在测试k8s上的ci/cd，之前的ci/cd其实也能满足目前先业务的需求，但是想尝试改进一下，优化以前的job，希望在登录ci的时候更加的简洁，<a id="more"></a> 而且查找job的时候，点击一个job就能查看完整的信息，不需要job之间的来回切换，等等各种理由，😁😁。这里使用jenkins pipeline，起初测试的时候使用pipeline，没问题以后，使用jenkinsfile。</p><h3 id="pipeline-常用介绍"><a href="#pipeline-常用介绍" class="headerlink" title="pipeline 常用介绍"></a>pipeline 常用介绍</h3><h4 id="清理历史build"><a href="#清理历史build" class="headerlink" title="清理历史build"></a>清理历史build</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;普通job的时候清理和保留历史job的build 很简单，勾勾就可以啦，但是pipeline就的使用一下方式，而且还的写在最前面，不然识别不了，会报错的</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">options &#123;</span><br><span class="line">        buildDiscarder(logRotar(numToKeepStr: <span class="string">'8'</span>))</span><br><span class="line">        disableConcurrentBuilds()</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><ul><li>buildDiscarder: 保持构建的最大个数</li><li>disableConcurrentBuilds: 禁止并发构建</li></ul><p>详细参数:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">buildDiscarder(logRotator(numToKeepStr: <span class="string">'8'</span>, artifactNumToKeepStr: <span class="string">'8'</span>, daysToKeepStr: <span class="string">'8'</span>, artifactDaysToKeepStr: <span class="string">'7'</span>))</span><br></pre></td></tr></table></figure><ul><li>artifactDaysToKeepStr: 发布包保留天数</li><li>artifactNumToKeepStr: 发布包最大保留#个构建</li><li>daysToKeepStr: 保持构建的天数</li><li>numToKeepStr: 保持构建的最大个数</li></ul><h4 id="gitlab事件触发"><a href="#gitlab事件触发" class="headerlink" title="gitlab事件触发"></a>gitlab事件触发</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;之前的我们的ci/cd都是开发提交到某一个分支，然后jenkins会自动触发编译、发布，而且配置这个步骤也需要好几步才能实现，但在pipeline中也可以通过代码形式最这种触发器(勾子)进行配置。这样让每个项目都和jenkins进行耦合；运维人员只需要专注的维护Jenkinsfile，创建对应的项目即可。gitlab触发jenkins的构建需要依赖Gitlab插件。这里需要自行安装</p><ul><li><p>接受固定的分支</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">triggers &#123;</span><br><span class="line">        gitlab(triggersOnPush: <span class="literal">true</span>,</span><br><span class="line">              triggersOnMergeRequest: <span class="literal">true</span>,</span><br><span class="line">              branchFilterType: <span class="string">"NameBasedFilter"</span>,</span><br><span class="line">              includeBranchesSpec: <span class="string">"dev,test,master"</span>,</span><br><span class="line">              secretToken: <span class="string">"<span class="variable">$&#123;env.git_token&#125;</span>"</span>)</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></li><li><p>triggerOnPush: 当Gitlab触发push事件时，是否执行构建</p></li><li><p>triggerOnMergeRequest: 当Gitlab触发mergeRequest事件时，是否执行构建</p></li><li><p>branchFilterType: 只有符合条件的分支才会触发构建，必选，否则无法实现触发。</p><ul><li>All: 所有分支</li><li>NameBasedFilter: 基于分支名进行过滤，多个分支名使用逗号分隔<ul><li>includeBranchesSpec: 基于branchFilterType值，输入期望包括的分支的规则</li><li>excludeBranchesSpec: 基于branchFilterType值，输入期望排除的分支的规则</li></ul></li><li>RegexBasedFilter: 基于正则表达式对分支名进行过滤<ul><li>sourceBranchRegex: 定义期望的通过正则表达式限制的分支规则</li></ul></li></ul></li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;所以分支不阐述，其他的两个选项是最实用的，我们在正式使用的时候一定会用到这个，上面的例子是一个接受固定的几个分支</p><ul><li>匹配的方式<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">triggers &#123;</span><br><span class="line">        gitlab(triggersOnPush: <span class="literal">true</span>,</span><br><span class="line">              triggersOnMergeRequest: <span class="literal">true</span>,</span><br><span class="line">              branchFilterType: <span class="string">"RegexBasedFilter"</span>,</span><br><span class="line">              sourceBranchRegex: <span class="string">"dev.*"</span>,</span><br><span class="line">              secretToken: <span class="string">"<span class="variable">$&#123;env.git_token&#125;</span>"</span>)</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里的git_token需要在jenkins的全局变量里面添加一个<code>Environment variables</code>对应的一个键值即可。</p><p><strong>注</strong>: 所有的触发器都需要先手动执行一次，让jenkins家在其中的配置，对应的指令才会生效。</p><ul><li><p>jenkins 验证<br><img src="https://img.xxlaila.cn/1571644117201.jpg" alt="img"></p></li><li><p>gitlab验证<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;需要将项目回调地址写入到Gitlab钩子当中才可以。经过测试一个pipeline的job可以管理多个分支的触发，避免之前的每一个分支的job进行触发。</p></li></ul><h4 id="parameters-模块"><a href="#parameters-模块" class="headerlink" title="parameters 模块"></a>parameters 模块</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;该模块需要安装，parameters指令提供用户在触发Pipeline时应提供的参数列表。这些用户指定的参数的值通过该params对象可用于Pipeline步骤。研发经常会有打出一个特性分支，这个分支用于hotfix，这个时候就要给研发提交一个可以选择的分支，然他们去部署到对应的环境。</p><ul><li><p>字符串参数<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;定义一个字符串参数，用户可以在Jenkins UI上输入字符串，常见使用这个参数的场景有，用户名，收件人邮箱，文件网络路径，主机名称的或者url等</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">parameters &#123;</span><br><span class="line">    string(name: <span class="string">'DEPLOY_ENV'</span>, defaultValue: <span class="string">'staging'</span>, description: <span class="string">''</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>布尔值参数<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;定义一个布尔类型参数，用户可以在Jenkins UI上选择是还是否，选择是表示代码会执行这部分，如果选择否，会跳过这部分。一般需要使用布尔值的场景有，执行一些特定集成的脚本或则工作，或者事后清除环境，例如清楚Jenkins的workspace这样的动作</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">parameters &#123;</span><br><span class="line">    booleanParam(name: <span class="string">'DEBUG_BUILD'</span>, defaultValue: <span class="literal">true</span>, description: <span class="string">''</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>选择参数<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;选择（choice）的参数就是支持用户从多个选择项中，选择一个值用来表示这个变量的值。工作中常用的场景，有选择服务器类型，选择版本号等。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">parameters &#123;</span><br><span class="line">    choice(name: <span class="string">'ENV_TYPE'</span>, choices: [<span class="string">'dev'</span>, <span class="string">'test'</span>, <span class="string">'product'</span>], description: <span class="string">'dev env test'</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当然parameters模块我们用的最多的是在手动的时候我们可以手动点击进行构建部署，至于其他的目前我暂时未用到</p><ul><li>选择分支部署<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pipeline &#123;</span><br><span class="line">    agent &#123;label <span class="string">'agent-node'</span>&#125;</span><br><span class="line">    parameters &#123;</span><br><span class="line">        gitParameter branchFilter: <span class="string">'origin/(.*)'</span>, defaultValue: <span class="string">'dev'</span>, name: <span class="string">'BRANCH'</span>, <span class="built_in">type</span>: <span class="string">'PT_BRANCH'</span></span><br><span class="line">    &#125;</span><br><span class="line">    stages &#123;</span><br><span class="line">        stage(<span class="string">'gitlib code'</span>) &#123;</span><br><span class="line">            steps&#123;</span><br><span class="line">                git branch:<span class="string">"<span class="variable">$&#123;params.BRANCH&#125;</span>"</span>, credentialsId:<span class="string">'gitlabUser'</span>, url: <span class="string">"http://gitlab.xxlaila.cn/xxx/kxl-eureka.git"</span></span><br><span class="line">                script &#123;</span><br><span class="line">                    build_tag = sh(returnStdout: <span class="literal">true</span>, script: <span class="string">'git rev-parse --short HEAD'</span>).trim()</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><p>parameters<a href="https://wiki.jenkins.io/display/JENKINS/Git+Parameter+Plugin" target="_blank" rel="noopener">官方参考</a>，介绍得挺详细的，<a href="https://mohamicorp.atlassian.net/wiki/spaces/DOC/pages/136740885/Triggering+Jenkins+Based+on+New+Tags" target="_blank" rel="noopener">辅助参考</a><br><img src="https://img.xxlaila.cn/1571651950634.jpg" alt="img"></p><ul><li>还可以写成<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">parameters &#123;</span><br><span class="line">    gitParameter(</span><br><span class="line">        branch: <span class="string">''</span>,</span><br><span class="line">        branchFilter: <span class="string">'origin/(.*)'</span>,</span><br><span class="line">        defaultValue: <span class="string">'dev'</span>,</span><br><span class="line">        description: <span class="string">'test code'</span>,</span><br><span class="line">        name: <span class="string">'BRANCH'</span>,</span><br><span class="line">        quickFilterEnabled: <span class="literal">false</span>,</span><br><span class="line">        selectedValue: <span class="string">'NONE'</span>,</span><br><span class="line">        sortMode: <span class="string">'NONE'</span>,</span><br><span class="line">        tagFilter: <span class="string">'*'</span>,</span><br><span class="line">        <span class="built_in">type</span>: <span class="string">'PT_BRANCH'</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里有一个问题：当这里设置了可以选择分支的时候，然后在之前的自动触发就会有问题，就是在去分支拉去代码的时候就一只是dev分支，而不是其他的分支，这里仍然在探索的测试中。<br>编辑job可以看到<br><img src="https://img.xxlaila.cn/1571903055002.jpg" alt="img"></p><h3 id="多分支pipeline"><a href="#多分支pipeline" class="headerlink" title="多分支pipeline"></a>多分支pipeline</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;按照上面的又要支持用户可以选择分支，又要适合自动触发功能。用单分支pipeline来管理项目，又要回到我们最初的模式，而在实际过程中，我们可以用到多分支同时进行开发。这样就满足了我们的实际需求。多分支任务这里不做过多的详细介绍，这里阐述两个功能点；分别是分支的扫描策略和孤儿项策略(Orphaned Item)。</p><h4 id="分支的扫描策略"><a href="#分支的扫描策略" class="headerlink" title="分支的扫描策略"></a>分支的扫描策略</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分支扫描是jenkins根据一定的策略去代码仓库扫描分支，如果有新分支就创建一个以新分支命名的任务，如果发现分支被删除，就删除对应的jenkins任务。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在”扫描多分支流水线触发器(Scan Multibranch Pipeline Triggers)”下有一个: Periodically if not otherwise run（没有手动触发，就定期扫描分支）。选择此项，设置一个扫描间隔时长。可以根据项目分支的频繁程度设置周期的长短，也可以在任务页面手动触发jenkins进行扫描。<br><img src="https://img.xxlaila.cn/1571973819297.jpg" alt="img"></p><h4 id="孤儿项策略-Orphaned-Item"><a href="#孤儿项策略-Orphaned-Item" class="headerlink" title="孤儿项策略(Orphaned Item)"></a>孤儿项策略(Orphaned Item)</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;该功能是在代码仓库中删除了release分支，那么在多任务页面上，该分支在jenkins上的任务也应该对应删除。什么时候删除，取决于下次分支扫描时间。如果代码仓库中的分支被删除，而jenkins上响应的任务没有被删除，那么这个任务就是所说的孤儿任务。对于分支任务的历史记录，保存多长时间设置</p><ul><li><p>界面配置<br><img src="https://img.xxlaila.cn/1571974190710.jpg" alt="img"></p></li><li><p>pipeline 写法</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">orphanedItemStrategy &#123;</span><br><span class="line">    discardolditems &#123;</span><br><span class="line">        daysTokeep(10)</span><br><span class="line">        numToKeep(5)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><p><strong>注</strong>: 这里孤儿策略pipeline 需要另外一种方式来支持，<a href="https://gitee.com/jenkins-zh/gitlab-branch-source-plugin" target="_blank" rel="noopener">Setting up GitLab Server Configuration on Jenkins</a>，这里没有用到这个，不做过多的阐述。<a href="https://github.com/jenkinsci/job-dsl-plugin/wiki/Migration" target="_blank" rel="noopener">github参考</a></p><h3 id="多分支的自动触发"><a href="#多分支的自动触发" class="headerlink" title="多分支的自动触发"></a>多分支的自动触发</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分支的触好处是多多的，自然在多分支面前自动触发肯定也少不了。多分支的触发有两种模式，分别是前面提到的Gitlab trigger和Generic Webhook Trigger。下面分别对两种模式进行阐述和实际的测试</p><h4 id="Generic-Webhook-Trigger"><a href="#Generic-Webhook-Trigger" class="headerlink" title="Generic Webhook Trigger"></a>Generic Webhook Trigger</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Generic Webhook Trigger 插件需要提前安装，GenericTrigger触发条件是由GWT插件提供，GenericTrigger触发的条件分为5个部分。<a href="https://wiki.jenkins.io/display/JENKINS/Generic+Webhook+Trigger+Plugin" target="_blank" rel="noopener">GenericTrigger官方参考</a></p><ul><li>从HTTP POST请求中提取参数</li><li>token，GWT插件用于标识jenkins项目的唯一性</li><li>根据请求参数值判断是否触发Jenkins项目执行</li><li>日志控制打印</li><li>webhook响应控制</li></ul><h4 id="GerenericTrigger-的写法"><a href="#GerenericTrigger-的写法" class="headerlink" title="GerenericTrigger 的写法"></a>GerenericTrigger 的写法</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">triggers &#123;</span><br><span class="line">    GenericTrigger(</span><br><span class="line">        genericVariables:[</span><br><span class="line">            [key: <span class="string">'ref'</span>, value: <span class="string">'$.ref'</span>]</span><br><span class="line">        ],</span><br><span class="line"></span><br><span class="line">        token: env.JOB_NAME,</span><br><span class="line">        regexpFilterText: <span class="string">'$ref'</span>,</span><br><span class="line">        regexpFilterExpression: <span class="string">'refs/heads/'</span> + env.BRANCH_NAME</span><br><span class="line">    )</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;env.BRANCH_NAME 这里指的是分支名。当然这样修改以后是不行的，是达不到自动触发的，需要自行去gitlab上添加钩子，这里经过测试流程：用户修改dev分支，push到gitlab dev分支可以触发任务的dev分支自动构建；合并到test分支，也可以触发test分支自动构建；在合并到master分支也能自动触发任务的master分支自动构建。<br><img src="https://img.xxlaila.cn/1571984557618.jpg" alt="img"></p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们要实现这块，要理解知道这个东西，首先要知道gitlab push 数据的格式，知道了gitlab push格式，我们才知道应该怎么操作，<a href="https://docs.gitlab.com/ee/user/project/integrations/webhooks.html#webhooks" target="_blank" rel="noopener">gitlab push数据的格式参考</a>，</p><ul><li>参考<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="string">"object_kind"</span>: <span class="string">"push"</span>,</span><br><span class="line">  <span class="string">"before"</span>: <span class="string">"95790bf891e76fee5e1747ab589903a6a1f80f22"</span>,</span><br><span class="line">  <span class="string">"after"</span>: <span class="string">"da1560886d4f094c3e6c9ef40349f7d38b5d27d7"</span>,</span><br><span class="line">  <span class="string">"ref"</span>: <span class="string">"refs/heads/master"</span>,</span><br><span class="line">  <span class="string">"checkout_sha"</span>: <span class="string">"da1560886d4f094c3e6c9ef40349f7d38b5d27d7"</span>,</span><br><span class="line">  <span class="string">"user_id"</span>: 4,</span><br><span class="line">  <span class="string">"user_name"</span>: <span class="string">"John Smith"</span>,</span><br><span class="line">  <span class="string">"user_username"</span>: <span class="string">"jsmith"</span>,</span><br><span class="line">  <span class="string">"user_email"</span>: <span class="string">"john@example.com"</span>,</span><br><span class="line">  <span class="string">"user_avatar"</span>: <span class="string">"https://s.gravatar.com/avatar/d4c74594d841139328695756648b6bd6?s=8://s.gravatar.com/avatar/d4c74594d841139328695756648b6bd6?s=80"</span>,</span><br><span class="line">  <span class="string">"project_id"</span>: 15,</span><br><span class="line">  <span class="string">"project"</span>:&#123;</span><br><span class="line">    <span class="string">"id"</span>: 15,</span><br><span class="line">    <span class="string">"name"</span>:<span class="string">"Diaspora"</span>,</span><br><span class="line">    <span class="string">"description"</span>:<span class="string">""</span>,</span><br><span class="line">    <span class="string">"web_url"</span>:<span class="string">"http://example.com/mike/diaspora"</span>,</span><br><span class="line">    <span class="string">"avatar_url"</span>:null,</span><br><span class="line">    <span class="string">"git_ssh_url"</span>:<span class="string">"git@example.com:mike/diaspora.git"</span>,</span><br><span class="line">    <span class="string">"git_http_url"</span>:<span class="string">"http://example.com/mike/diaspora.git"</span>,</span><br><span class="line">    <span class="string">"namespace"</span>:<span class="string">"Mike"</span>,</span><br><span class="line">    <span class="string">"visibility_level"</span>:0,</span><br><span class="line">    <span class="string">"path_with_namespace"</span>:<span class="string">"mike/diaspora"</span>,</span><br><span class="line">    <span class="string">"default_branch"</span>:<span class="string">"master"</span>,</span><br><span class="line">    <span class="string">"homepage"</span>:<span class="string">"http://example.com/mike/diaspora"</span>,</span><br><span class="line">    <span class="string">"url"</span>:<span class="string">"git@example.com:mike/diaspora.git"</span>,</span><br><span class="line">    <span class="string">"ssh_url"</span>:<span class="string">"git@example.com:mike/diaspora.git"</span>,</span><br><span class="line">    <span class="string">"http_url"</span>:<span class="string">"http://example.com/mike/diaspora.git"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"repository"</span>:&#123;</span><br><span class="line">    <span class="string">"name"</span>: <span class="string">"Diaspora"</span>,</span><br><span class="line">    <span class="string">"url"</span>: <span class="string">"git@example.com:mike/diaspora.git"</span>,</span><br><span class="line">    <span class="string">"description"</span>: <span class="string">""</span>,</span><br><span class="line">    <span class="string">"homepage"</span>: <span class="string">"http://example.com/mike/diaspora"</span>,</span><br><span class="line">    <span class="string">"git_http_url"</span>:<span class="string">"http://example.com/mike/diaspora.git"</span>,</span><br><span class="line">    <span class="string">"git_ssh_url"</span>:<span class="string">"git@example.com:mike/diaspora.git"</span>,</span><br><span class="line">    <span class="string">"visibility_level"</span>:0</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"commits"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"id"</span>: <span class="string">"b6568db1bc1dcd7f8b4d5a946b0b91f9dacd7327"</span>,</span><br><span class="line">      <span class="string">"message"</span>: <span class="string">"Update Catalan translation to e38cb41."</span>,</span><br><span class="line">      <span class="string">"timestamp"</span>: <span class="string">"2011-12-12T14:27:31+02:00"</span>,</span><br><span class="line">      <span class="string">"url"</span>: <span class="string">"http://example.com/mike/diaspora/commit/b6568db1bc1dcd7f8b4d5a946b0b91f9dacd7327"</span>,</span><br><span class="line">      <span class="string">"author"</span>: &#123;</span><br><span class="line">        <span class="string">"name"</span>: <span class="string">"Jordi Mallach"</span>,</span><br><span class="line">        <span class="string">"email"</span>: <span class="string">"jordi@softcatala.org"</span></span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="string">"added"</span>: [<span class="string">"CHANGELOG"</span>],</span><br><span class="line">      <span class="string">"modified"</span>: [<span class="string">"app/controller/application.rb"</span>],</span><br><span class="line">      <span class="string">"removed"</span>: []</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"id"</span>: <span class="string">"da1560886d4f094c3e6c9ef40349f7d38b5d27d7"</span>,</span><br><span class="line">      <span class="string">"message"</span>: <span class="string">"fixed readme"</span>,</span><br><span class="line">      <span class="string">"timestamp"</span>: <span class="string">"2012-01-03T23:36:29+02:00"</span>,</span><br><span class="line">      <span class="string">"url"</span>: <span class="string">"http://example.com/mike/diaspora/commit/da1560886d4f094c3e6c9ef40349f7d38b5d27d7"</span>,</span><br><span class="line">      <span class="string">"author"</span>: &#123;</span><br><span class="line">        <span class="string">"name"</span>: <span class="string">"GitLab dev user"</span>,</span><br><span class="line">        <span class="string">"email"</span>: <span class="string">"gitlabdev@dv6700.(none)"</span></span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="string">"added"</span>: [<span class="string">"CHANGELOG"</span>],</span><br><span class="line">      <span class="string">"modified"</span>: [<span class="string">"app/controller/application.rb"</span>],</span><br><span class="line">      <span class="string">"removed"</span>: []</span><br><span class="line">    &#125;</span><br><span class="line">  ],</span><br><span class="line">  <span class="string">"total_commits_count"</span>: 4</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果我们想根据不同的分支提交来触发jenkins的构建，那就应该知道post数据哪一个属性代表了不同的分支，我们可以在第四行看到</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="string">"ref"</span>: <span class="string">"refs/heads/master"</span>,</span><br></pre></td></tr></table></figure><p><strong>注释</strong>: 也可以通过IDEA工具提交的时候看到提交的选项。可以看到我们用ref可以很好的区分不同分支，这里就是为什么要填写ref的原因。我们可以通过pipeline代码的生成器来生成</p><ul><li>pipeline 代码生成器<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">triggers &#123;</span><br><span class="line">  GenericTrigger causeString: <span class="string">'Generic Cause'</span>, genericVariables: [[defaultValue: <span class="string">''</span>, key: <span class="string">'ref'</span>, regexpFilter: <span class="string">''</span>, value: <span class="string">'$.ref'</span>]], printContributedVariables: <span class="literal">true</span>, printPostContent: <span class="literal">true</span>, regexpFilterExpression: <span class="string">'\'</span>refs/heads/\<span class="string">' + evn.BRANCH_NAME'</span>, regexpFilterText: <span class="string">'$ref'</span>, token: <span class="string">'env.JOB_NAME'</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><p><img src="https://img.xxlaila.cn/1571982583457.jpg" alt="img"><br><img src="https://img.xxlaila.cn/1571982622070.jpg" alt="img"></p><p><strong>注</strong>: token参数的作用是标识一个pipeline在jenkins中的唯一性，这个参数的重要性就得提起GWT插件的原理。当jenkins收到generic-webhook-trgger/invoke接口的请求时，会将请求代理给GWT插件处理，GWT插件内容会从jenkins实例对象中取出所有的参数化jenkins项目，包括pipeline，然后进行遍历。如果我们在参数化项目中Generic Trigger配置token的值与webhook请求时的token一致，就会触发改项目。如果多个参数化项目的token一样，则都会进行触发，所以这里的token最好时JOB_NAME项目名，因为这个是在项目或者是在为服务领域他都是唯一的。</p><ul><li>参数介绍:<ul><li>regexpFilterText: 需要进行匹配的key，例子中，使用从post body中提取的ref变量值。</li><li>regexpFilterExpression: <a href="https://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html" target="_blank" rel="noopener">正则表达式</a>；如果regexpFilterText参数符合regexpFilterExpression参数的正则表达式，则触发执行。</li><li>printPostContent: 布尔值，将webhook请求信息打印到日志上</li><li>printContributedVariables: 布尔值，将提取后的变量名及变量值打印出来</li><li>causeString: 字符串型，触发原因，可以直接应用提取后的变量，如 causeString: ‘Triggered on $msg’</li><li>Silent response: 布尔型，在正常情况下，当webhook请求成功后，GWT插件会返回HTTP 200状态码和触发结果给对方调用，但是当Silentresponse设置为true时，就只返回HTTP 200状态码，不反悔触发结果</li></ul></li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上面的看的出来，我们只要是提交了分支都可以进行触发构建，但是呢，在实际生产中，我们定义了dev——&gt;test——master 分支，就是只想要这几个进行触发构建，其他的不进行触发，让开发自己去点击。</p><ul><li><p>指定分支构建</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">triggers &#123;</span><br><span class="line">  GenericTrigger causeString: <span class="string">'Triggered on $msg'</span>, genericVariables: [[defaultValue: <span class="string">''</span>, key: <span class="string">'ref'</span>, regexpFilter: <span class="string">''</span>, value: <span class="string">'$.ref'</span>]], printContributedVariables: <span class="literal">true</span>, printPostContent: <span class="literal">true</span>, regexpFilterExpression: <span class="string">'\'</span>refs/heads/(dev|<span class="built_in">test</span>|master)\<span class="string">''</span>, regexpFilterText: <span class="string">'$ref'</span>, token: <span class="string">'env.JOB_NAME'</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>多分支Gitlab trigger<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;多分支的Gitlab trigger和我们前面介绍的gitlab事件触发一样的，没有任何区别，这里我测试了一个job，没有任何问题。同时新建了一个分支，jenkins会自动的扫描新建一个以分支为名的任务，进行自动触发。当我删除了某一个分支，就会触发自动扫描，然后查看分支为删除。</p></li><li><p>删除分支<br><img src="https://img.xxlaila.cn/1571996378764.jpg" alt="img"><br><img src="https://img.xxlaila.cn/1571996257688.jpg" alt="img"></p></li><li><p>整体效果图<br><img src="https://img.xxlaila.cn/1571990331005.jpg" alt="img"></p></li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里介绍一下部署这块，根据branch来进行判断，不同的branch部署到不同的环境，当设定的值不在branch范围内，就需要人为的制定部署环境。当人员三分钟内没有来进行环境部署的选择，系统就会断开，对该分支标记为结束。</p><p><a href="http://xxlaila.github.io/2019/10/25/pipeline%E5%A4%9A%E5%88%86%E6%94%AFgitlab%E8%A7%A6%E5%8F%91/" target="_blank" rel="noopener">完整文件</a><br><a href="https://jenkinsci.github.io/job-dsl-plugin/#path/buildPipelineView" target="_blank" rel="noopener">推荐学习参考地址</a></p><!-- rebuild by neat -->]]></content>
      <categories>
        <category>jenkins</category>
      </categories>
      <tags>
        <tag>pipeline</tag>
      </tags>
  </entry>
  <entry>
    <title>elasticserch</title>
    <url>/2019/10/17/elasticserch%E6%97%A5%E5%B8%B8%E7%BB%B4%E6%8A%A4/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><h3 id="days-1"><a href="#days-1" class="headerlink" title="days 1"></a>days 1</h3><a id="more"></a><h4 id="elasticserch-索引和数据操作"><a href="#elasticserch-索引和数据操作" class="headerlink" title="elasticserch 索引和数据操作"></a>elasticserch 索引和数据操作</h4><ul><li><p>查看索引</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># curl -XGET 'http://127.0.0.1:9200/_cat/indices?v'</span></span><br></pre></td></tr></table></figure></li><li><p>删除索引</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># curl -XGET 'http://127.0.0.1:9200/_cat/indices?v' |grep "red"|awk '&#123;print $3&#125;'|uniq &gt;l</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># for i in `cat a`;do  curl -XDELETE http://127.0.0.1:9200/$&#123;i&#125;;done</span></span><br></pre></td></tr></table></figure></li><li><p>查看shards</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># curl -XGET http://127.0.0.1:9200/_cat/shards</span></span><br></pre></td></tr></table></figure></li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;shards 有几种类型，这里说一下<code>UNASSIGNED</code>，es 集群里面的分片是分配在多台node上的，为的就是高可用，比如你的某台机器crash了，那么集群就会让其他副本顶上来，避免出现某个分片不能提供服务的情况，但是难免还是会出现 UNASSIGNED shards 的错误。</p><ul><li>删除shards UNASSIGNED<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># curl -XGET 'http://127.0.0.1:9200/_cat/shards'|grep "UNASSIGNED"|awk '&#123;print $1&#125;'|uniq &gt;l</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># for i in `cat l`;do curl -XDELETE http://127.0.0.1:9200/$i;done</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="elasticserch验证集群"><a href="#elasticserch验证集群" class="headerlink" title="elasticserch验证集群"></a>elasticserch验证集群</h4><ul><li><p>集群相关API</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ curl http://127.0.0.1:9200/_cat</span><br><span class="line">=^.^=</span><br><span class="line">/_cat/allocation</span><br><span class="line">/_cat/shards</span><br><span class="line">/_cat/shards/&#123;index&#125;</span><br><span class="line">/_cat/master</span><br><span class="line">/_cat/nodes</span><br><span class="line">/_cat/tasks</span><br><span class="line">/_cat/indices</span><br><span class="line">/_cat/indices/&#123;index&#125;</span><br><span class="line">/_cat/segments</span><br><span class="line">/_cat/segments/&#123;index&#125;</span><br><span class="line">/_cat/count</span><br><span class="line">/_cat/count/&#123;index&#125;</span><br><span class="line">/_cat/recovery</span><br><span class="line">/_cat/recovery/&#123;index&#125;</span><br><span class="line">/_cat/health</span><br><span class="line">/_cat/pending_tasks</span><br><span class="line">/_cat/aliases</span><br><span class="line">/_cat/aliases/&#123;<span class="built_in">alias</span>&#125;</span><br><span class="line">/_cat/thread_pool</span><br><span class="line">/_cat/thread_pool/&#123;thread_pools&#125;</span><br><span class="line">/_cat/plugins</span><br><span class="line">/_cat/fielddata</span><br><span class="line">/_cat/fielddata/&#123;fields&#125;</span><br><span class="line">/_cat/nodeattrs</span><br><span class="line">/_cat/repositories</span><br><span class="line">/_cat/snapshots/&#123;repository&#125;</span><br><span class="line">/_cat/templates</span><br></pre></td></tr></table></figure></li><li><p>查看集群名称等信息</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ curl http://127.0.0.1:9200</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"name"</span> : <span class="string">"elk_elasticsearch_data_2"</span>,</span><br><span class="line">  <span class="string">"cluster_name"</span> : <span class="string">"elk_elasticsearch"</span>,</span><br><span class="line">  <span class="string">"cluster_uuid"</span> : <span class="string">"T47wQwa6TT-6MHJVFM40Tw"</span>,</span><br><span class="line">  <span class="string">"version"</span> : &#123;</span><br><span class="line">    <span class="string">"number"</span> : <span class="string">"6.4.0"</span>,</span><br><span class="line">    <span class="string">"build_flavor"</span> : <span class="string">"default"</span>,</span><br><span class="line">    <span class="string">"build_type"</span> : <span class="string">"rpm"</span>,</span><br><span class="line">    <span class="string">"build_hash"</span> : <span class="string">"595516e"</span>,</span><br><span class="line">    <span class="string">"build_date"</span> : <span class="string">"2018-08-17T23:18:47.308994Z"</span>,</span><br><span class="line">    <span class="string">"build_snapshot"</span> : <span class="literal">false</span>,</span><br><span class="line">    <span class="string">"lucene_version"</span> : <span class="string">"7.4.0"</span>,</span><br><span class="line">    <span class="string">"minimum_wire_compatibility_version"</span> : <span class="string">"5.6.0"</span>,</span><br><span class="line">    <span class="string">"minimum_index_compatibility_version"</span> : <span class="string">"5.0.0"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"tagline"</span> : <span class="string">"You Know, for Search"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>查看集群节点</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ curl http://127.0.0.1:9200/_cat/nodes?v</span><br><span class="line">ip            heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name</span><br><span class="line">172.21.16.198           29          85   0    0.10    0.04     0.05 mdi       -      elk_elasticsearch_data_2</span><br><span class="line">172.21.16.187           48          85   0    0.00    0.01     0.05 mdi       *      elk_elasticsearch_master</span><br><span class="line">172.21.16.206           25          86   0    0.08    0.03     0.05 mdi       -      elk_elasticsearch_data_3</span><br></pre></td></tr></table></figure></li><li><p>验证集群磁盘分配情况</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ curl http://127.0.0.1:9200/_cat/allocation?v</span><br><span class="line">shards disk.indices disk.used disk.avail disk.total disk.percent host          ip            node</span><br><span class="line">    98          1gb     3.6gb     96.3gb     99.9gb            3 172.21.16.198 172.21.16.198 elk_elasticsearch_data_2</span><br><span class="line">    99      887.1mb     4.5gb     95.4gb     99.9gb            4 172.21.16.187 172.21.16.187 elk_elasticsearch_master</span><br><span class="line">    99        957mb     3.5gb     96.4gb     99.9gb            3 172.21.16.206 172.21.16.206 elk_elasticsearch_data_3</span><br></pre></td></tr></table></figure></li><li><p>验证集群健康状况</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ curl http://127.0.0.1:9200/_cat/health?v </span><br><span class="line">epoch      timestamp cluster           status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent</span><br><span class="line">1571648406 17:00:06  elk_elasticsearch green           3         3    296 148    0    0        0             0                  -                100.0%</span><br><span class="line"></span><br><span class="line">$</span><br></pre></td></tr></table></figure></li><li><p>查看每个数据节点上被fielddata所使用的堆内存大小。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ curl http://127.0.0.1:9200/_cat/fielddata?v</span><br><span class="line">id                     host          ip            node                     field                    size</span><br><span class="line">VNcRqM30T3axzVjiPkDTmA 172.21.16.187 172.21.16.187 elk_elasticsearch_master event.resultCode.keyword 352b</span><br><span class="line">VNcRqM30T3axzVjiPkDTmA 172.21.16.187 172.21.16.187 elk_elasticsearch_master <span class="built_in">type</span>                     720b</span><br><span class="line">HNc5BrMWQcummBeAskQc4A 172.21.16.206 172.21.16.206 elk_elasticsearch_data_3 event.resultCode.keyword 704b</span><br><span class="line">z3zUA8KxTH6B7C8CmVRUIQ 172.21.16.198 172.21.16.198 elk_elasticsearch_data_2 <span class="built_in">type</span>                     720b</span><br><span class="line">z3zUA8KxTH6B7C8CmVRUIQ 172.21.16.198 172.21.16.198 elk_elasticsearch_data_2 event.resultCode.keyword 704b</span><br></pre></td></tr></table></figure></li></ul><!-- rebuild by neat -->]]></content>
      <categories>
        <category>elasticserch</category>
      </categories>
      <tags>
        <tag>elasticserch</tag>
      </tags>
  </entry>
  <entry>
    <title>nexus配置ldap</title>
    <url>/2019/10/15/nexus%E9%85%8D%E7%BD%AEldap/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><h3 id="配置nexus"><a href="#配置nexus" class="headerlink" title="配置nexus"></a>配置nexus</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;登录nexus在设置页，点击ldap，</p><a id="more"></a><p><img src="https://img.xxlaila.cn/1571131890608.jpg" alt="img"><br>参数介绍:</p><ul><li>Name: 随便写</li><li>LDAP server address: 支持ldaps和ldap,而端口则取决于配置。 如果没有特殊配置，ldap默认端口是389</li><li>Search base: 只需要填DC即可，比如DC=example,DC=com。 其它内容，比如CN、OU等，不需要填写</li><li>Authentication method有以下选项:<ul><li>Simple Authentication</li><li>Anonymous Authentication</li><li>DIGEST-MD5</li><li>CRAM-MD5<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通常选择Simple Authentication即可。Username or DN、Password里填写账户、密码，而 Connection rules无需修改。填写完毕后，点击【Verify connection】按钮，可以验证信息。 如果成功，即可保存。</li></ul></li></ul><h4 id="Choose-Users-and-Groups"><a href="#Choose-Users-and-Groups" class="headerlink" title="Choose Users and Groups"></a>Choose Users and Groups</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这项故名思义就是配置用户和组的，在最开头的Configuration template中，有四种模板可选：</p><ul><li>Active Directory</li><li>Generic Ldap Server</li><li>Posix with Dynamic Groups</li><li>Posix with Static Groups</li></ul><p>这里选择<code>Generic Ldap Server</code></p><ul><li>Base DN 在LDAP中找到用户的基本位置。这是相对于搜索基础的（例如ou = people）。</li><li>User subtree通常需要勾选。 如果把LDAP的Tree比作目录的话，勾选以后相当于递归查找子目录。</li><li>User filter通过过滤规则，减少搜索信息，用于提升性能。 仅仅只是提升性能，所以，如果不懂它特殊的匹配规则，也可以不填。</li><li>之前选择了<code>Generic Ldap Server</code>模版后，User ID attribute默认为uid，Real name attribute默认为cn、Email attribute默认为mail、Password attribute为空。</li><li>Map LDAP groups as roles如果不勾选，就不会同步用户组信息。 如果勾选，则可以选择Group type和Group member of attribute。 若无必要，保持默认即可，默认是勾选的。<br><img src="https://img.xxlaila.cn/1571133103461.jpg" alt="img"></li><li>填写完成后，通过【Verify user mapping】可以验证查询结果<br><img src="https://img.xxlaila.cn/1571133221971.jpg" alt="img"><br>点击创建<br><img src="https://img.xxlaila.cn/1571133286829.jpg" alt="img"></li></ul><h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;新起一个窗口利用ldap里面的账号进行登录，可以登录，没有问题，但是登录之后用户没有任何权限，这对于研发来说又是一个不可接受的事情。接下来配置权限</p><h5 id="禁止匿名访问"><a href="#禁止匿名访问" class="headerlink" title="禁止匿名访问"></a>禁止匿名访问</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在实际应用中，是不允许匿名用户不可以登录就能访问的，这样我们ldap就没有任何意义了<br><img src="https://img.xxlaila.cn/1571133691247.jpg" alt="img"></p><ul><li>禁止匿名用户<br><img src="https://img.xxlaila.cn/1571133811908.jpg" alt="img"></li></ul><h5 id="创建角色"><a href="#创建角色" class="headerlink" title="创建角色"></a>创建角色</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在Security——&gt;Roles——&gt;Create role，这里创建角色有两种。一种是nexus relos本地角色，一种是External roles mapping外部映射的形式。为了满足我们ldap账户登录进来有浏览库的权限，研发又可以上传第三方依赖库的权限，但是不能删除和私下增加库Repositories。所以这里我们需要单独建立一个本地的relos，然后在映射外部的ldap到这个本地的roles，这样ldap账户登录进来就能实现日常的基本操作。</p><ul><li><p>创建nexus relos本地角色<br><img src="https://img.xxlaila.cn/1571296771150.jpg" alt="img"><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;创建完成后，我们需要对他赋予权限，对用户进行权限控制，没有权限控制，就没办法达成我们上面的目标。下面是我赋予的权限，可以结合实际需求来进行赋予。</p></li><li><p>权限介绍:</p><ul><li>ng-component-upload: 有上传的权限，比如java依赖的一些第三方库，研发可以自己进行上传</li><li>ng-repository-admin-<em>-</em>-browse: 浏览所有的repository</li><li>ng-repository-admin-<em>-</em>-read: 可以所有读取repository的配置信息</li><li>ng-repository-view-maven2-maven-central-browse: 具有浏览maven-central内容</li><li>ng-repository-view-maven2-maven-central-read: 读取maven-central内容，在maven编译的时候具有下载的权限，(后面不一一介绍)</li><li>ng-repository-view-maven2-maven-public-browse</li><li>ng-repository-view-maven2-maven-public-read</li><li>ng-repository-view-maven2-maven-releases-browse</li><li>ng-repository-view-maven2-maven-releases-read</li><li>ng-repository-view-maven2-maven-snapshots-browse</li><li>ng-repository-view-maven2-maven-snapshots-read</li><li>ng-repository-view-npm-npm-kxl-all-browse: 以下是自己做的npm代理缓存，可以参考之前的<a href="https://xxlaila.github.io/2019/08/23/nexus3搭建npm私服/" target="_blank" rel="noopener">nexus3搭建npm私服</a></li><li>ng-repository-view-npm-npm-kxl-all-read</li><li>ng-repository-view-npm-npm-external-browse</li><li>ng-repository-view-npm-npm-external-read</li><li>ng-repository-view-npm-npm-internal-browse</li><li>ng-repository-view-npm-npm-internal-read</li><li>ng-search-read: 让用户具有所有权限，没有此权限，研发查找一个包，估计会死</li></ul></li><li><p>创建是External roles mapping外部映射<br><img src="https://img.xxlaila.cn/1571134166780.jpg" alt="img"><br><img src="https://img.xxlaila.cn/1571297568491.jpg" alt="img"><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在进行Roles ID 这栏目，需要填写的是Users，这个Users会在ldap上同步Users的一个用户组。根据自己的ldap账户组设置来进行填写。下图是ldap的组设置<br><img src="https://img.xxlaila.cn/1571298567078.jpg" alt="img"></p></li></ul><p><strong>注</strong>: 其实在这里我们也可以进行Privileges的权限赋予，但是我选择的是先创建一个本地的nexus relos。然后我们在Roles栏关联之前创建的<code>Developer</code>，完成以后通过ldap账户登录进行测试</p><h5 id="测试-1"><a href="#测试-1" class="headerlink" title="测试"></a>测试</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里主要是从四个方面来测试ldap账户。分别是: 登录默认的权限、浏览所有库的权限、Browse的浏览、Browse库的上传</p><ul><li><p>登录默认的权限<br><img src="https://img.xxlaila.cn/1571297962563.jpg" alt="img"></p></li><li><p>浏览所有库的权限<br><img src="https://img.xxlaila.cn/1571298121188.jpg" alt="img"></p></li><li><p>Browse的浏<br><img src="https://img.xxlaila.cn/1571298018356.jpg" alt="img"><br><img src="https://img.xxlaila.cn/1571298167348.jpg" alt="img"></p></li><li><p>Browse库的上传<br><img src="https://img.xxlaila.cn/1571298224331.jpg" alt="img"><br><img src="https://img.xxlaila.cn/1571298260091.jpg" alt="img"></p></li></ul><!-- rebuild by neat -->]]></content>
      <categories>
        <category>nexus</category>
      </categories>
      <tags>
        <tag>ldap</tag>
      </tags>
  </entry>
  <entry>
    <title>jenkins配置备份</title>
    <url>/2019/10/15/jenkins%E9%85%8D%E7%BD%AE%E5%A4%87%E4%BB%BD/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><h3 id="jenkins-备份"><a href="#jenkins-备份" class="headerlink" title="jenkins 备份"></a>jenkins 备份</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当jenkins在用起来的时候，我们难保他不会出故障，但是出了故障我们怎么做到快速的恢复呢，这时备份就显得尤为重要了。但jenkins本身不提供备份的功能，<a id="more"></a> 所以这里就需要借助外力。备份可以多样化，一种是我们直接到jenkins的目录下面手动备份jenkins目录。一种是我们就jenkins自带的插件<code>thinBackup</code>和<code>Periodic Backup</code>进行备份恢复，下面进行分别介绍</p><h3 id="thinBackup备份"><a href="#thinBackup备份" class="headerlink" title="thinBackup备份"></a>thinBackup备份</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;登录jenkins——&gt;系统管理——&gt;插件管理<br><img src="https://img.xxlaila.cn/1571101180571.jpg" alt="img"><br>安装完成之后重启jenkins服务，登录jenkins在系统管理界面可以看到<br><img src="https://img.xxlaila.cn/1571101557754.jpg" alt="img"></p><h4 id="配置ThinBackup"><a href="#配置ThinBackup" class="headerlink" title="配置ThinBackup"></a>配置ThinBackup</h4><ul><li>点击ThinBackup<br><img src="https://img.xxlaila.cn/1571101640273.jpg" alt="img"><br>可以看到有三个选项:</li><li>Backup Now: 手动立即备份</li><li>Restore: 恢复备份</li><li>Settings: 备份参数的设置</li></ul><h5 id="Settings"><a href="#Settings" class="headerlink" title="Settings"></a>Settings</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;下面是我的备份参数，可以根据自己的需要自己设定备份参数，设置好友save即可，<code>Backup schedule for full backups</code>意思是周一到周五每天凌晨两点进行备份<br><img src="https://img.xxlaila.cn/1571102057919.jpg" alt="img"></p><h5 id="Restore"><a href="#Restore" class="headerlink" title="Restore"></a>Restore</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;备份文件是以日期+时间节点组成的文件名，我们恢复什么时间段的，点击进行恢复，<br><img src="https://img.xxlaila.cn/1571102188007.jpg" alt="img"></p><h3 id="Periodic-Backup"><a href="#Periodic-Backup" class="headerlink" title="Periodic Backup"></a>Periodic Backup</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;备份除了上面提到的插件还有一个插件是<code>Periodic Backup</code>，安装<code>Periodic Backup</code>不阐述，安装完成后可以在系统管理菜单下面有一个<code>Periodic Backup Manager</code>菜单<br><img src="https://img.xxlaila.cn/1571709136813.jpg" alt="img"><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;打开<code>Periodic Backup Manager</code>，第一次打开是没有任何东西的，需要我们自己去建立一个规则，点击<code>Configure</code><br><img src="https://img.xxlaila.cn/1571709270639.jpg" alt="img"></p><p>配置项很简单:</p><ul><li>Temporary Directory: 临时目录</li><li>Backup schedule (cron): 进行备份cron的表达式，填写完成后点击<code>Validate cron syntax</code>进行验证</li><li>Maximum backups in location: 最大位置备份，保留多少个备份文件</li><li>Store no older than (days): 保留的时间</li><li>File Management Strategy: 备份策略<ul><li>ConfigOnly: 只备份配置文件</li><li>FullBackup: 进行全量备份，可以通过Excludes list中填入Ant风格表达式，排除不希望备份的文件，多个表达式使用分号分隔</li></ul></li><li>Storage Strategy: 存储策略，就是是否需要进行压缩存储</li><li>Backup Location: 备份的位置，都是本地目录<br><img src="https://img.xxlaila.cn/1571709879768.jpg" alt="img"></li></ul><!-- rebuild by neat -->]]></content>
      <categories>
        <category>jenkins</category>
      </categories>
      <tags>
        <tag>jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title>jenkins配置ldap</title>
    <url>/2019/10/14/jenkins%E9%85%8D%E7%BD%AEldap/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;公司研发人员和测试人员，还有运维人员有时候登录jenkins去查看一些job的状态或者是其他的东西，虽然有企业微信的通知，但是感觉还是不能满足，<a id="more"></a> 比如job错误了，企业微信虽然吧错误发给了研发人员，但是研发还是要登录jenkins上去看，就感觉要舒服一点，测试上做的一些自动化测试，有时候失败了他们也会去看或者是去建立一些自动化的job。之前建立了公共的账号，开发和测试人员都去登录，但是有时候他们误操作了，导致一些其他的东西失败或者错误，虽然做了权限控制，但是他们还是死不承认，所以这里介入ldap。谁动的就知道了，这样就不怕了。</p><h3 id="开始配置"><a href="#开始配置" class="headerlink" title="开始配置"></a>开始配置</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;登录jenkins——&gt;系统管理——&gt;全局安全配置<br><img src="https://img.xxlaila.cn/1571025388007.jpg" alt="img"><br>访问控制——&gt;LDAP<br><img src="https://img.xxlaila.cn/1571027524602.jpg" alt="img"><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;配置完成以后我们需要测试一下连接是否正常，点击<code>Test LDAP setttings</code>，输入在ldap的其中一个账户来进行验证，没问题的结果如下:<br><img src="https://img.xxlaila.cn/1571027696951.jpg" alt="img"><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;配置完成并测试通过后就可以用LDAP直接登录了<br><strong>注</strong>: 启用了LDAP登录后将无法再用之前的登录方式（本地认证将无法在使用）登录，登录进来的任何一个账号都是管理员，都是管理着肯定来说不安全，权限配置请下看</p><p><a href="https://wiki.jenkins.io/display/JENKINS/LDAP+Plugin" target="_blank" rel="noopener">官方参考</a></p><h3 id="配置ldap的账户权限"><a href="#配置ldap的账户权限" class="headerlink" title="配置ldap的账户权限"></a>配置ldap的账户权限</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上面我们虽然吧ldap配置完成，但是我们需要对用户进行权限的配置，不可能每个人登录都能对我们jenkins进行无限制的操作，这不符合我们之前的意图。安装<code>Role-based Authorization Strategy</code>插件</p><ul><li>在系统管理——&gt;全局安全配置,可以看到下面选项，每项介<a href="https://xxlaila.github.io/2019/08/09/jenkins%E7%94%A8%E6%88%B7%E6%9D%83%E9%99%90%E9%85%8D%E7%BD%AE/" target="_blank" rel="noopener">参考</a><br><img src="https://img.xxlaila.cn/1571034253089.jpg" alt="img"></li></ul><p>保存以后，返回系统管理界面就可以看到多处一个<code>Manage and Assign Roles</code><br><img src="https://img.xxlaila.cn/1571034433352.jpg" alt="img"><br>点击进去</p><p><img src="https://img.xxlaila.cn/1571034507945.jpg" alt="img"></p><ul><li><strong>Manage Roles</strong>: 角色分为Global和Project，可创建角色分组和添加项目。</li><li><strong>Assign Roles</strong>: 增加具体的用户，分配到角色组，指定项目权限。</li></ul><p><a href="https://xxlaila.github.io/2019/08/09/jenkins%E7%94%A8%E6%88%B7%E6%9D%83%E9%99%90%E9%85%8D%E7%BD%AE/" target="_blank" rel="noopener">权限设置</a></p><ul><li>下面我的配置，和之前的大同小异<br><img src="https://img.xxlaila.cn/1571038684383.jpg" alt="img"></li></ul><p><strong>注</strong>: 这里有一个小问题，这样配置以后，新用户登录进来以后就会提示没有权限，<code>Access Denied,xxxx没有全部/Read权限</code>，这是因为在打开jenkins后，没有创建用户前，先不要勾选系统设置中启用安全选项，如果勾选了，就会出现无法进入jenkins的现象<br><img src="https://img.xxlaila.cn/1571037187865.jpg" alt="img"></p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在网上看到有这种的解决办法，有几种方案，一个是修改confing.xml的文件，修改config.xml文件的三种方式感觉都不太切合实际的业务；下面是我做的两种办法，推荐使用第二种</p><ul><li><p>Role-Based Strategy<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在<code>Assign Roles</code>吧用户添加进来，然后勾选权限，<br>系统管理——&gt;Manage and Assign Roles——&gt;Assign Roles<br><img src="https://img.xxlaila.cn/1571037604678.jpg" alt="img"><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;但是这有一个弊端，就是每次新来一个用户就得去添加一次用户权限，虽然满足了业务需求，但是不科学</p></li><li><p>项目矩阵授权策略<br><img src="https://img.xxlaila.cn/1571041499340.jpg" alt="img"><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这是一个全局的配置，特定组只能按照最小的权限授权，额外的权限可以在具体的项目权限矩阵里面在添加。 默认只有<code>Anonymous Users</code>和<code>Authenticated Users</code>，管理员组是需要添加的<code>admin</code></p></li><li><p>Anonymous Users: 匿名用户，显然不能</p></li><li><p>Authenticated Users: 认证用户，就是只要是认证的账号都可以拥有的权限</p></li><li><p>admin: 就是拥有所有的权限了，这个组一般只能运维人员和部门老大加入。</p></li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;加入admin组以后，他会自动去同步ldap的组用户，如果用户在ldap是admin组，那么在这里就会是管理员权限，如果用户是普通组，那么就是<code>Authenticated Users</code>组赋予的权限，使用这种方式只要用户是ldap里面的，就可以登录查看。这样就满足了业务场景需求</p><h3 id="日志记录"><a href="#日志记录" class="headerlink" title="日志记录"></a>日志记录</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;记录用户日志需要单独的安装<code>Audit Trail</code>插件，该插件在Jenkins主配置页面中添加了一个配置部分，可以在此处配置日志位置和设置（文件大小和循环日志文件的数量），以及用于记录请求的URI模式。默认选项选择效果显着的大多数操作，例如创建/配置/删除作业和视图或永久删除/保存/开始构建。日志将按照配置写入磁盘，最近的条目也可以在“管理/系统日志”部分中查看。<br><img src="https://img.xxlaila.cn/1572057054289.jpg" alt="img"><br><a href="https://plugins.jenkins.io/audit-trail" target="_blank" rel="noopener">Audit Trail官方参考</a></p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里配置以后还不能记录job的日志，需要对job进行记录需要另外的安装<a href="https://wiki.jenkins.io/display/JENKINS/JobConfigHistory+Plugin" target="_blank" rel="noopener">Job Configuration History插件</a>，根据官方的介绍，可用于查看所有作业配置历史记录或仅查看已删除的作业或所有类型的配置历史记录条目。同时，如果配置了安全策略，还可以查看哪个用户进行了哪些更改。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;安装完成后，我们在job里面可以看到一个<code>Job Config History</code>的菜单。最开始没有没有任何记录，只有当构建job或者修改过job以后才会有记录<br><img src="https://img.xxlaila.cn/1572057782047.jpg" alt="img"><br><img src="https://img.xxlaila.cn/1572057958019.jpg" alt="img"></p><ul><li>点击Show Diffs 可以看到我们具体修改了什么东西<br><img src="https://img.xxlaila.cn/1572058118436.jpg" alt="img"></li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当我们安装好这个插件以后，也测试可以使用，但是我们不能让所有的job日志记录保存历史过久，如果job过多，记录过多，这会对我们的磁盘空间来说，肯定是一个压力，所以这里我们就需要进行配置，保存多少次的记录，而且还可以设置排除的文件。<br><img src="https://img.xxlaila.cn/1572058857084.jpg" alt="img"></p><p><a href="https://wiki.jenkins.io/display/JENKINS/JobConfigHistory+Plugin" target="_blank" rel="noopener">Job Configuration History官方</a></p><!-- rebuild by neat -->]]></content>
      <categories>
        <category>jenkins</category>
      </categories>
      <tags>
        <tag>ldap</tag>
      </tags>
  </entry>
  <entry>
    <title>java应用部署</title>
    <url>/2019/10/12/java%E5%BA%94%E7%94%A8%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <div class="hbe-input-container">
  <input type="password" id="hbePass" placeholder="Welcome to my blog, enter password to read." />
    <label for="hbePass">Welcome to my blog, enter password to read.</label>
    <div class="bottom-line"></div>
  </div>
  <script id="hbeData" type="hbeData" data-hmacdigest="762879c2105bb4dfd8ef1efdb11920e401fd3c30ae1ae476d4c978e8e0697330">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</script>
</div>
<script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>harbor 使用</title>
    <url>/2019/10/10/harbor-%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><h4 id="days-2019-10-10"><a href="#days-2019-10-10" class="headerlink" title="days(2019-10-10)"></a>days(2019-10-10)</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;前面文章介绍了harbor的部署，今天第一次学习入门使用。</p><a id="more"></a><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;服务器安装docker以后，我们怎么吧镜像push到我们的私有仓库，和怎么吧镜像pull到本地，首先在服务器上装备docker环境</p><h5 id="连接harbor"><a href="#连接harbor" class="headerlink" title="连接harbor"></a>连接harbor</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># docker login reg.xxlaila.cn</span></span><br><span class="line">Username: admin</span><br><span class="line">Password: </span><br><span class="line">Error response from daemon: Get https://172.21.16.90/v1/users/: dial tcp reg.xxlaila.cn:443: connect: connection refused</span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里第一次连接报错，Docker自从1.3.X之后docker registry交互默认使用的是HTTPS，但是我们搭建私有镜像默认使用的是HTTP服务，所以与私有镜像交时出现以上错误。</p><h5 id="解决问题"><a href="#解决问题" class="headerlink" title="解决问题"></a>解决问题</h5><ul><li><p>方法一: 修改或添加配置</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat /etc/docker/daemon.json </span></span><br><span class="line">&#123;</span><br><span class="line"><span class="string">"insecure-registries"</span> : [<span class="string">"reg.xxlaila.cn"</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>重新启动docker，并重新登录</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># systemctl restart docker</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#  docker login reg.xxlaila.cn</span></span><br><span class="line">Username: admin</span><br><span class="line">Password: </span><br><span class="line">Login Succeeded</span><br></pre></td></tr></table></figure></li><li><p>方法二：修改启动文件</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># vim /usr/lib/systemd/system/docker.service  </span></span><br><span class="line">ExecStart=/usr/bin/dockerd --insecure-registry reg.xxlaila.cn <span class="variable">$DOCKER_NETWORK_OPTIONS</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># systemctl daemon-reload</span></span><br><span class="line"><span class="comment"># systemctl restart docker</span></span><br></pre></td></tr></table></figure></li></ul><h5 id="Harbor上创建新项目供上传使用"><a href="#Harbor上创建新项目供上传使用" class="headerlink" title="Harbor上创建新项目供上传使用"></a>Harbor上创建新项目供上传使用</h5><p><img src="https://img.xxlaila.cn/1570697850857.jpg" alt="img"></p><h5 id="Docker服务器给镜像打标签"><a href="#Docker服务器给镜像打标签" class="headerlink" title="Docker服务器给镜像打标签"></a>Docker服务器给镜像打标签</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># docker images</span></span><br><span class="line">REPOSITORY                     TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">docker.io/xxlaila/kxl-eureka   v2                  eb8cf7e3f24f        7 months ago        474 MB</span><br><span class="line"></span><br><span class="line"><span class="comment"># docker tag docker.io/xxlaila/kxl-eureka:v2 reg.xxlaila.cn/kxl/kxl-eureka:v2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># docker images</span></span><br><span class="line">REPOSITORY                      TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">docker.io/xxlaila/kxl-eureka    v2                  eb8cf7e3f24f        7 months ago        474 MB</span><br><span class="line">reg.xxlaila.cn/kxl/kxl-eureka   v2                  eb8cf7e3f24f        7 months ago        474 MB</span><br></pre></td></tr></table></figure><h5 id="上传镜像"><a href="#上传镜像" class="headerlink" title="上传镜像"></a>上传镜像</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># docker push reg.xxlaila.cn/kxl/kxl-eureka:v2</span></span><br><span class="line">The push refers to a repository [reg.xxlaila.cn/kxl/kxl-eureka]</span><br><span class="line">f6026bf67b63: Pushed </span><br><span class="line">1489a4b0f1dd: Pushed </span><br><span class="line">2af6e035aa36: Pushed </span><br><span class="line">472cfce4528e: Pushed </span><br><span class="line">071d8bd76517: Pushed </span><br><span class="line">v2: digest: sha256:20d3bc74fdcb2fc4cdfc9066f742c828898c728f7e3f2114498ebe2848b71653 size: 1368</span><br></pre></td></tr></table></figure><p><img src="https://img.xxlaila.cn/1570698233987.jpg" alt="img"></p><h5 id="下载镜像"><a href="#下载镜像" class="headerlink" title="下载镜像"></a>下载镜像</h5><ul><li><p>删除本地镜像</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># docker rmi reg.xxlaila.cn/kxl/kxl-eureka:v2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># docker rmi docker.io/xxlaila/kxl-eureka:v2</span></span><br></pre></td></tr></table></figure></li><li><p>下载harbor上的镜像</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># docker pull reg.xxlaila.cn/kxl/kxl-eureka:v2</span></span><br><span class="line">Trying to pull repository reg.xxlaila.cn/kxl/kxl-eureka ... </span><br><span class="line">v2: Pulling from reg.xxlaila.cn/kxl/kxl-eureka</span><br><span class="line">a02a4930cb5d: Pull complete </span><br><span class="line">6ea3dcbee0db: Extracting [==================================================&gt;]  81.4 MB/81.4 MB</span><br><span class="line">6ea3dcbee0db: Pull complete </span><br><span class="line">c423a7a79cc1: Pull complete </span><br><span class="line">7418081934c1: Pull complete </span><br><span class="line">f89b73853622: Pull complete </span><br><span class="line">Digest: sha256:20d3bc74fdcb2fc4cdfc9066f742c828898c728f7e3f2114498ebe2848b71653</span><br><span class="line">Status: Downloaded newer image <span class="keyword">for</span> reg.xxlaila.cn/kxl/kxl-eureka:v2</span><br></pre></td></tr></table></figure></li></ul><p><img src="https://img.xxlaila.cn/1570698681073.jpg" alt="img"></p><h4 id="days-2019-10-12"><a href="#days-2019-10-12" class="headerlink" title="days(2019-10-12)"></a>days(2019-10-12)</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于公司需求，开发人员比较多，又不想研发用一个账号，也不想给研发一个个的开账号，位置harbor支持了ldap。有了这么一个东西，我们就能很好的为研发创建账号支持研发随时查看docker的镜像。</p><h4 id="配置harbor-ldap"><a href="#配置harbor-ldap" class="headerlink" title="配置harbor ldap"></a>配置harbor ldap</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;新版本的harbor很多东西都可以直接在界面配置，不需要去修改文件，省去了大量的工作，web界面配置更加方便快捷，登录harbor平台，点击配置管理——&gt;修改认证模式，认证模式支持很多类型，这里选择ldap。<br><img src="https://img.xxlaila.cn/1571019665079.jpg" alt="img"><br><strong>注</strong>: 在密码这栏填写需要填写管理员的密码，普通用户的密码是不行的，即使是在管理员的用户也是不行的。<br>点击测试ldap，提示连接成功后保存<br><img src="https://img.xxlaila.cn/1571019741060.jpg" alt="img"></p><h4 id="配置邮箱"><a href="#配置邮箱" class="headerlink" title="配置邮箱"></a>配置邮箱</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在配置ldap页面旁边有一个邮箱配置，邮件服务器用于向请求重设密码的用户发送响应。<br><img src="https://img.xxlaila.cn/1570873497729.jpg" alt="img"><br>点击测试，测试没问题之后点击保存。</p><h4 id="测试ladp连接"><a href="#测试ladp连接" class="headerlink" title="测试ladp连接"></a>测试ladp连接</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;新打开一个一个浏览器窗口，利用ladp账户进行登录。<br><img src="https://img.xxlaila.cn/1571019859287.jpg" alt="img"><br><strong>注释</strong>: 新版本的在登录界面没有什么选择ldap登录，直接使用ldap账号登录就ok</p><h4 id="将项目角色分配给LDAP-AD组"><a href="#将项目角色分配给LDAP-AD组" class="headerlink" title="将项目角色分配给LDAP / AD组"></a>将项目角色分配给LDAP / AD组</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;修改之前的ldap配置，增加组的配置<br><img src="https://img.xxlaila.cn/1571023069387.jpg" alt="img"><br>在项目-&gt;成员-&gt; +组中。<br><img src="https://img.xxlaila.cn/1571023177214.jpg" alt="img"><br><img src="https://img.xxlaila.cn/1571023223796.jpg" alt="img"></p><h4 id="设置ldap账户的权限"><a href="#设置ldap账户的权限" class="headerlink" title="设置ldap账户的权限"></a>设置ldap账户的权限</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当ldap配置以后，ldap账户登录没有管理员权限，我们管理harbor还的使用<code>harbor</code>的admin账户登录，这样无疑对运维人员维护带来了不便利。当ldap用户登录，harbor就会记录该用户，我们设置运维用户为超级管理员，这样就实现了一个账号登录，维护的时候也不用账号切换<br><img src="https://img.xxlaila.cn/1571023451423.jpg" alt="img"></p><!-- rebuild by neat -->]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>harbor</tag>
      </tags>
  </entry>
  <entry>
    <title>HPA认识</title>
    <url>/2019/10/09/hpa/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><h3 id="Pod-自动扩缩容"><a href="#Pod-自动扩缩容" class="headerlink" title="Pod 自动扩缩容"></a>Pod 自动扩缩容</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Kubernetes提供了这样一个资源对象: <code>Horizontal Pod Autoscaling</code> Pod水平自动伸缩），简称HPA。HAP通过监控分析RC或者Deployment控制的所有Pod的负载变化情况来确定是否需要调整Pod的副本数量，这是HPA最基本的原理。</p><a id="more"></a><p><img src="https://img.xxlaila.cn/1570605234009.jpg" alt="img"><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;HPA在kubernetes集群中被设计成一个Kubernetes API资源和控制器，可以通过kubectl autoscale命令来创建一个HPA资源对象，HPA Controller默认15s轮询一次（可通过kube-controller-manager的标志–horizontal-pod-autoscaler-sync-period进行设置），查询指定的资源（RC或者Deployment）中Pod的资源使用率，并且与创建时设定的值和指标做对比，从而实现自动伸缩的功能。<br><a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/" target="_blank" rel="noopener">详细介绍</a></p><h3 id="Pod水平自动伸缩练习"><a href="#Pod水平自动伸缩练习" class="headerlink" title="Pod水平自动伸缩练习"></a>Pod水平自动伸缩练习</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于Horizontal Pod Autoscaler使用此API收集指标，因此需要在群集中部署metrics-server监视以通过资源指标API提供指标,</p><h4 id="运行php-apache服务器"><a href="#运行php-apache服务器" class="headerlink" title="运行php-apache服务器"></a>运行php-apache服务器</h4><p>首先，我们将开始运行该映像的部署，并将其服务公开</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl run php-apache --image=0layfolk0/hpa-example --requests=cpu=200m --limits=cpu=500m --expose --port=80</span><br><span class="line">kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed <span class="keyword">in</span> a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.</span><br><span class="line">service/php-apache created</span><br><span class="line">deployment.apps/php-apache created</span><br></pre></td></tr></table></figure><h4 id="创建水平Pod自动缩放器"><a href="#创建水平Pod自动缩放器" class="headerlink" title="创建水平Pod自动缩放器"></a>创建水平Pod自动缩放器</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当服务运行以后。我们将使用kubectl autoscale创建自动 缩放器。以下命令将创建一个水平Pod自动缩放器，该缩放器将维护由我们在这些说明的第一步中创建的php-apache部署控制的Pod的1至10个副本。粗略地说，HPA将（通过部署）增加或减少副本数，以将所有Pod的平均CPU利用率维持在50％（因为每个pod通过kubectl运行请求200毫核，这意味着平均CPU利用率为100毫-核心）。<a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#algorithm-details" target="_blank" rel="noopener">算法更多信息</a></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10</span><br><span class="line">horizontalpodautoscaler.autoscaling/php-apache autoscaled</span><br></pre></td></tr></table></figure><p>我们可以通过运行以下命令检查自动定标器的当前状态:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl get hpa</span><br><span class="line">NAME         REFERENCE               TARGETS   MINPODS   MAXPODS   REPLICAS   AGE</span><br><span class="line">php-apache   Deployment/php-apache   0%/50%    1         10        1          14s</span><br></pre></td></tr></table></figure><p><strong>注释</strong>: 由于我们没有向服务器发送任何请求，因此当前CPU消耗为0％（“ CURRENT”列显示了由相应部署控制的所有Pod的平均值）。</p><h4 id="增加压力测试"><a href="#增加压力测试" class="headerlink" title="增加压力测试"></a>增加压力测试</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;现在我们要对<code>php-apache</code>做压力测试来观看自动缩放如何对增加的负载做出反应，我们将启动一个容器，并将无限循环的查询发送到php-apache服务。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl run -i --tty load-generator --image=busybox /bin/sh</span><br><span class="line">kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed <span class="keyword">in</span> a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.</span><br><span class="line">If you don<span class="string">'t see a command prompt, try pressing enter.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">/ # while true; do wget -q -O- http://php-apache.default.svc.cluster.local; done</span></span><br><span class="line"><span class="string">OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!OK!O</span></span><br></pre></td></tr></table></figure><p>在一分钟左右的时间内，我们应该通过执行以下命令来看到更高的CPU负载：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl get hpa</span><br><span class="line">NAME         REFERENCE               TARGETS    MINPODS   MAXPODS   REPLICAS   AGE</span><br><span class="line">php-apache   Deployment/php-apache   250%/50%   1         10        1          9m12s</span><br><span class="line"></span><br><span class="line">$ kubectl get deployment php-apache</span><br><span class="line">NAME         READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">php-apache   3/5     5            3           88m</span><br></pre></td></tr></table></figure><p>这里由于网络问题和pull 镜像太慢了，我就直接结束了测试</p><h4 id="停止压力测试"><a href="#停止压力测试" class="headerlink" title="停止压力测试"></a>停止压力测试</h4><p>我们在<code>busybox</code>容器的终端里面执行<code>&lt;Ctrl&gt; + C</code>来结束压力测试，然后我们在观察结果</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$  kubectl get hpa</span><br><span class="line">NAME         REFERENCE               TARGETS   MINPODS   MAXPODS   REPLICAS   AGE</span><br><span class="line">php-apache   Deployment/php-apache   91%/50%   1         10        5          10m</span><br><span class="line"></span><br><span class="line">$ kubectl get deployment php-apache</span><br><span class="line">NAME         READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">php-apache   2/2     2            2           99m</span><br></pre></td></tr></table></figure><p><a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/" target="_blank" rel="noopener">自动缩放多个指标和自定义指标</a></p><h3 id="nginx-测试"><a href="#nginx-测试" class="headerlink" title="nginx 测试"></a>nginx 测试</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;利用之前<a href="https://xxlaila.github.io/2019/10/09/Deployment%E4%BD%BF%E7%94%A8/" target="_blank" rel="noopener">Deployment</a>里面的nginx做测试，我们只需要吧之前的yaml文件稍作修改即可</p><h4 id="修改nginx-deployment-yaml"><a href="#修改nginx-deployment-yaml" class="headerlink" title="修改nginx-deployment.yaml"></a>修改nginx-deployment.yaml</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat &gt; nginx-deployment.yaml &lt;&lt;EOF</span><br><span class="line">apiVersion: apps/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-deploy</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: nginx-deploy</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  minReadySeconds: 5</span><br><span class="line">  revisionHistoryLimit: 10</span><br><span class="line">  strategy:</span><br><span class="line">    <span class="built_in">type</span>: RollingUpdate</span><br><span class="line">    rollingUpdate:</span><br><span class="line">      maxSurge: 1</span><br><span class="line">      maxUnavailable: 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx-deploy</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx-deploy</span><br><span class="line">        image: nginx:1.13.3</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">        resources:</span><br><span class="line">          requests:</span><br><span class="line">            memory: <span class="string">"200Mi"</span></span><br><span class="line">            cpu: <span class="string">"200m"</span></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h4 id="新建立nginx-deploy-hpa-yaml"><a href="#新建立nginx-deploy-hpa-yaml" class="headerlink" title="新建立nginx-deploy-hpa.yaml"></a>新建立nginx-deploy-hpa.yaml</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat &gt; nginx-deploy-hpa.yaml &lt;&lt;EOF</span><br><span class="line">apiVersion: autoscaling/v1</span><br><span class="line">kind: HorizontalPodAutoscaler</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-deploy</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  maxReplicas: 5</span><br><span class="line">  minReplicas: 1</span><br><span class="line">  scaleTargetRef:</span><br><span class="line">    apiVersion: extensions/v1beta1</span><br><span class="line">    kind: Deployment</span><br><span class="line">    name: nginx-deploy</span><br><span class="line">  targetCPUUtilizationPercentage: 10</span><br><span class="line">status:</span><br><span class="line">  currentCPUUtilizationPercentage: 8</span><br><span class="line">  currentReplicas: 1</span><br><span class="line">  desiredReplicas: 0</span><br></pre></td></tr></table></figure><ul><li><p>执行创建</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl apply -f nginx-deployment.yaml</span><br><span class="line">$ kubectl apply -f kubectl apply -f nginx-deploy-hpa.yaml</span><br></pre></td></tr></table></figure></li><li><p>查看验证</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl get hpa</span><br><span class="line">NAME           REFERENCE                 TARGETS   MINPODS   MAXPODS   REPLICAS   AGE</span><br><span class="line">nginx-deploy   Deployment/nginx-deploy   0%/10%    1         5         2          45s</span><br><span class="line"></span><br><span class="line">$ kubectl get rs</span><br><span class="line">NAME                        DESIRED   CURRENT   READY   AGE</span><br><span class="line">load-generator-7fbcc7489f   1         1         1       8m28s</span><br><span class="line">nginx-deploy-d494b9564      2         2         2       13m</span><br></pre></td></tr></table></figure></li><li><p>执行压力测试</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl run -i --tty load-generator --image=busybox /bin/sh</span><br><span class="line">kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed <span class="keyword">in</span> a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.</span><br><span class="line">If you don<span class="string">'t see a command prompt, try pressing enter.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">/ # while true; do wget -q -O- http://172.30.224.5:80; done</span></span><br></pre></td></tr></table></figure></li><li><p>查看效果</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl get hpa</span><br><span class="line">NAME           REFERENCE                 TARGETS   MINPODS   MAXPODS   REPLICAS   AGE</span><br><span class="line">nginx-deploy   Deployment/nginx-deploy   28%/10%   1         5         4          4m48s</span><br><span class="line"></span><br><span class="line">$ kubectl get rs</span><br><span class="line">NAME                        DESIRED   CURRENT   READY   AGE</span><br><span class="line">load-generator-7fbcc7489f   1         1         1       12m</span><br><span class="line">nginx-deploy-d494b9564      5         5         5       18m</span><br><span class="line"></span><br><span class="line">$ kubectl get hpa</span><br><span class="line">NAME           REFERENCE                 TARGETS   MINPODS   MAXPODS   REPLICAS   AGE</span><br><span class="line">nginx-deploy   Deployment/nginx-deploy   16%/10%   1         5         5          5m39s</span><br></pre></td></tr></table></figure></li><li><p>结束压测<br>等待一会查看结果</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl get hpa</span><br><span class="line">NAME           REFERENCE                 TARGETS   MINPODS   MAXPODS   REPLICAS   AGE</span><br><span class="line">nginx-deploy   Deployment/nginx-deploy   0%/10%    1         5         1          12m</span><br><span class="line"></span><br><span class="line">$ kubectl get rs</span><br><span class="line">NAME                        DESIRED   CURRENT   READY   AGE</span><br><span class="line">load-generator-7fbcc7489f   1         1         1       19m</span><br><span class="line">nginx-deploy-d494b9564      1         1         1       25m</span><br></pre></td></tr></table></figure></li></ul><!-- rebuild by neat -->]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>hpa</tag>
      </tags>
  </entry>
  <entry>
    <title>Deployment使用</title>
    <url>/2019/10/09/Deployment%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><h3 id="Deployment和rc的对比"><a href="#Deployment和rc的对比" class="headerlink" title="Deployment和rc的对比"></a>Deployment和rc的对比</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;首先RC是Kubernetes的一个核心概念，当我们把应用部署到集群之后，需要保证应用能够持续稳定的运行，RC就是这个保证的关键，主要功能如:</p><ul><li>确保Pod数量: 它会确保Kubernetes中有指定数量的Pod在运行，如果少于指定数量的Pod，RC就会创建新的，反之这会删除多余的，保证Pod的副本数量不变。</li><li>确保Pod健康: 当Pod不健康，比如运行出错了，总之无法提供正常服务时，RC也会杀死不健康的Pod，重新创建新的。</li><li>弹性伸缩: 在业务高峰或者低峰的时候，可以用过RC来动态的调整Pod数量来提供资源的利用率，当然我们也提到过如果使用HPA这种资源对象的话可以做到自动伸缩。</li><li>滚动升级: 滚动升级是一种平滑的升级方式，通过逐步替换的策略，保证整体系统的稳定性</li></ul><a id="more"></a><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Deployment同样也是Kubernetes系统的一个核心概念，主要职责和RC一样的都是保证Pod的数量和健康，二者大部分功能都是完全一致的，我们可以看成是一个升级版的RC控制器，Deployment具备的新特性</p><ul><li>RC的全部功能: Deployment具备上面描述的RC的全部功能</li><li>事件和状态查看: 可以查看Deployment的升级详细进度和状态</li><li>回滚: 当升级Pod的时候如果出现问题，可以使用回滚操作回滚到之前的任一版本</li><li>版本记录: 每一次对Deployment的操作，都能够保存下来，这也是保证可以回滚到任一版本的基础</li><li>暂停和启动: 对于每一次升级都能够随时暂停和启动</li></ul><p><strong>对比</strong>: Deployment作为新一代的RC，在功能上更为丰富，同时官方也是推荐使用Deployment来管理Pod，比如一些官方组件kube-dns、kube-proxy也都是使用的Deployment来管理的，所以最好使用Deployment来管理Pod。</p><h3 id="Deployment-介绍"><a href="#Deployment-介绍" class="headerlink" title="Deployment 介绍"></a>Deployment 介绍</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Deployment拥有多个Replica Set，而一个Replica Set拥有一个或多个Pod。一个Deployment控制多个rs主要是为了支持回滚机制，每当Deployment操作时，Kubernetes会重新生成一个Replica Set并保留，以后有需要的话就可以回滚至之前的状态。</p><p><strong>实例</strong>: 创建一个Deployment，它创建了一个Replica Set来启动3个nginx pod，yaml文件如下:</p><ul><li><p>nginx-deployment.yaml</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat &gt; nginx-deployment.yaml &lt;&lt;EOF</span><br><span class="line">apiVersion: apps/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-deploy</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: nginx-demo</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.7.9</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></li><li><p>执行创建</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl create -f nginx-deployment.yaml</span><br><span class="line">deployment.apps/nginx-deploy created</span><br></pre></td></tr></table></figure></li><li><p>执行一下命令查看刚刚创建的Deployment</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl get deployments</span><br><span class="line">NAME           READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">nginx-deploy   0/3     3            0           12s</span><br><span class="line"></span><br><span class="line"><span class="comment"># 再次执行上面命令</span></span><br><span class="line">$ kubectl get deployments</span><br><span class="line">NAME           READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">nginx-deploy   1/3     3            1           35s</span><br></pre></td></tr></table></figure></li><li><p>可以看到Deployment已经创建了1个Replica Set了，执行下面的命令查看rs和pod</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl get rs</span><br><span class="line">NAME                     DESIRED   CURRENT   READY   AGE</span><br><span class="line">nginx-deploy-6dd86d77d   3         3         2       70s</span><br><span class="line"></span><br><span class="line"><span class="comment"># </span></span><br><span class="line">$ kubectl get pod --show-labels</span><br><span class="line">NAME                           READY   STATUS              RESTARTS   AGE   LABELS</span><br><span class="line">nginx-deploy-6dd86d77d-9n9vf   1/1     Running             0          99s   app=nginx,pod-template-hash=6dd86d77d</span><br><span class="line">nginx-deploy-6dd86d77d-bhrsk   0/1     ContainerCreating   0          99s   app=nginx,pod-template-hash=6dd86d77d</span><br><span class="line">nginx-deploy-6dd86d77d-jdnrh   1/1     Running             0          99s   app=nginx,pod-template-hash=6dd86d77d</span><br></pre></td></tr></table></figure></li></ul><p>上面的Deployment的yaml文件中的replicas:3将会保证我们始终有3个POD在运行。</p><h3 id="滚动升级"><a href="#滚动升级" class="headerlink" title="滚动升级"></a>滚动升级</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;修改之前使用的nginx-deployment.yaml文件中的nginx镜像修改为nginx:1.13.3，然后在spec下面添加滚动升级策略：</p><ul><li><p>nginx-deploments.yml</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">apiVersion: apps/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-deploy</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: nginx-demo</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  minReadySeconds: 5</span><br><span class="line">  strategy:</span><br><span class="line">    <span class="built_in">type</span>: RollingUpdate</span><br><span class="line">    rollingUpdate:</span><br><span class="line">      maxSurge: 1</span><br><span class="line">      maxUnavailable: 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.13.3</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br></pre></td></tr></table></figure></li><li><p>minReadySeconds:</p><ul><li>滚动升级时5s后认为该pod就绪</li><li>如果没有设置该值，Kubernetes会假设该容器启动起来后就提供服务了</li><li>如果没有设置该值，在某些极端情况下可能会造成服务不正常运行</li></ul></li><li><p>rollingUpdate:</p><ul><li>于replicas为3,则整个升级,pod个数在2-4个之间</li></ul></li><li><p>maxSurge:</p><ul><li>升级过程中最多可以比原先设置多出的POD数量</li><li>例如：maxSurage=1，replicas=3,则表示Kubernetes会先启动1一个新的Pod后才删掉一个旧的POD，整个升级过程中最多会有3+1个POD。</li></ul></li><li><p>maxUnavaible:</p><ul><li>升级过程中最多有多少个POD处于无法提供服务的状态</li><li>当maxSurge不为0时，该值也不能为0</li><li>例如：maxUnavaible=1，则表示Kubernetes整个升级过程中最多会有1个POD处于无法服务的状态。</li></ul></li><li><p>执行创建</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl apply -f nginx-deployment.yaml</span><br><span class="line">deployment.apps/nginx-deploy configured</span><br></pre></td></tr></table></figure></li><li><p>查看状态</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用rollout命令</span></span><br><span class="line">$ kubectl rollout status deployment/nginx-deploy</span><br><span class="line">Waiting <span class="keyword">for</span> deployment <span class="string">"nginx-deploy"</span> rollout to finish: 2 out of 3 new replicas have been updated...</span><br><span class="line"></span><br><span class="line"><span class="comment"># 暂停升级</span></span><br><span class="line">$ kubectl rollout pause deployment deployment/nginx-deploy</span><br><span class="line"></span><br><span class="line"><span class="comment"># 继续升级</span></span><br><span class="line">$ kubectl rollout resume deployment deployment/nginx-deploy</span><br></pre></td></tr></table></figure></li></ul><p>升级结束后，继续查看rs的状态</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl get rs</span><br><span class="line">NAME                      DESIRED   CURRENT   READY   AGE</span><br><span class="line">nginx-deploy-6dd86d77d    0         0         0       21m</span><br><span class="line">nginx-deploy-799d666985   3         3         3       10m</span><br></pre></td></tr></table></figure><p>根据AGE我们可以看到离我们最近的当前状态是：3，和我们的yaml文件是一致的，证明升级成功了。用describe命令可以查看升级的全部信息:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl describe deploy nginx-deploy</span><br><span class="line">Name:                   nginx-deploy</span><br><span class="line">Namespace:              default</span><br><span class="line">CreationTimestamp:      Wed, 09 Oct 2019 10:12:56 +0800</span><br><span class="line">Labels:                 k8s-app=nginx-demo</span><br><span class="line">Annotations:            deployment.kubernetes.io/revision: 2</span><br><span class="line">                        kubectl.kubernetes.io/last-applied-configuration:</span><br><span class="line">                          &#123;<span class="string">"apiVersion"</span>:<span class="string">"apps/v1beta1"</span>,<span class="string">"kind"</span>:<span class="string">"Deployment"</span>,<span class="string">"metadata"</span>:&#123;<span class="string">"annotations"</span>:&#123;&#125;,<span class="string">"labels"</span>:&#123;<span class="string">"k8s-app"</span>:<span class="string">"nginx-demo"</span>&#125;,<span class="string">"name"</span>:<span class="string">"nginx-deploy"</span>,<span class="string">"nam...</span></span><br><span class="line"><span class="string">Selector:               app=nginx</span></span><br><span class="line"><span class="string">Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable</span></span><br><span class="line"><span class="string">StrategyType:           RollingUpdate</span></span><br><span class="line"><span class="string">MinReadySeconds:        5</span></span><br><span class="line"><span class="string">RollingUpdateStrategy:  1 max unavailable, 1 max surge</span></span><br><span class="line"><span class="string">Pod Template:</span></span><br><span class="line"><span class="string">  Labels:  app=nginx</span></span><br><span class="line"><span class="string">  Containers:</span></span><br><span class="line"><span class="string">   nginx:</span></span><br><span class="line"><span class="string">    Image:        nginx:1.13.3</span></span><br><span class="line"><span class="string">    Port:         80/TCP</span></span><br><span class="line"><span class="string">    Host Port:    0/TCP</span></span><br><span class="line"><span class="string">    Environment:  &lt;none&gt;</span></span><br><span class="line"><span class="string">    Mounts:       &lt;none&gt;</span></span><br><span class="line"><span class="string">  Volumes:        &lt;none&gt;</span></span><br><span class="line"><span class="string">Conditions:</span></span><br><span class="line"><span class="string">  Type           Status  Reason</span></span><br><span class="line"><span class="string">  ----           ------  ------</span></span><br><span class="line"><span class="string">  Available      True    MinimumReplicasAvailable</span></span><br><span class="line"><span class="string">  Progressing    True    NewReplicaSetAvailable</span></span><br><span class="line"><span class="string">OldReplicaSets:  &lt;none&gt;</span></span><br><span class="line"><span class="string">NewReplicaSet:   nginx-deploy-799d666985 (3/3 replicas created)</span></span><br><span class="line"><span class="string">Events:</span></span><br><span class="line"><span class="string">  Type    Reason             Age   From                   Message</span></span><br><span class="line"><span class="string">  ----    ------             ----  ----                   -------</span></span><br><span class="line"><span class="string">  Normal  ScalingReplicaSet  23m   deployment-controller  Scaled up replica set nginx-deploy-6dd86d77d to 3</span></span><br><span class="line"><span class="string">  Normal  ScalingReplicaSet  12m   deployment-controller  Scaled up replica set nginx-deploy-799d666985 to 1</span></span><br><span class="line"><span class="string">  Normal  ScalingReplicaSet  12m   deployment-controller  Scaled down replica set nginx-deploy-6dd86d77d to 2</span></span><br><span class="line"><span class="string">  Normal  ScalingReplicaSet  12m   deployment-controller  Scaled up replica set nginx-deploy-799d666985 to 2</span></span><br><span class="line"><span class="string">  Normal  ScalingReplicaSet  11m   deployment-controller  Scaled down replica set nginx-deploy-6dd86d77d to 1</span></span><br><span class="line"><span class="string">  Normal  ScalingReplicaSet  11m   deployment-controller  Scaled up replica set nginx-deploy-799d666985 to 3</span></span><br><span class="line"><span class="string">  Normal  ScalingReplicaSet  10m   deployment-controller  Scaled down replica set nginx-deploy-6dd86d77d to 0</span></span><br></pre></td></tr></table></figure><h3 id="回滚Deployment"><a href="#回滚Deployment" class="headerlink" title="回滚Deployment"></a>回滚Deployment</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;前面已经滚动平滑的升级Deployment，但是如果升级后的POD出了问题该怎么办？我们能够想到的最好最快的方式当然是回退到上一次能够提供正常工作的版本，Deployment就为我们提供了回滚机制。</p><ul><li>首先，查看Deployment的升级历史:<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl rollout <span class="built_in">history</span> deployment nginx-deploy</span><br><span class="line">deployment.extensions/nginx-deploy </span><br><span class="line">REVISION  CHANGE-CAUSE</span><br><span class="line">1         &lt;none&gt;</span><br><span class="line">2         &lt;none&gt;</span><br></pre></td></tr></table></figure></li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从上面的结果可以看出在执行Deployment升级的时候最好带上record参数，便于我们查看历史版本信息。<code>kubectl apply --filename=nginx-deployment.yaml --record=true</code><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;默认情况下，所有通过kubectl xxxx –record都会被kubernetes记录到etcd进行持久化，这无疑会占用资源，最重要的是，时间久了，当你kubectl get rs时，会有成百上千的垃圾RS返回，这对于运维来说维护很不便利，<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当我们在上生产时，我们最好通过设置Deployment的.spec.revisionHistoryLimit来限制最大保留的revision number，比如15个版本，回滚的时候一般只会回滚到最近的几个版本就足够了。其实rollout history中记录的revision都和ReplicaSets一一对应。如果手动delete某个ReplicaSet，对应的rollout history就会被删除，也就是还说你无法回滚到这个revison。rollout history和ReplicaSet的对应关系，可以在kubectl describe rs $RSNAME返回的revision字段中得到，这里的revision就对应着rollout history返回的revison。</p><ul><li><p>yaml例子</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat nginx-deployment.yaml </span><br><span class="line">apiVersion: apps/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-deploy</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: nginx-demo</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  minReadySeconds: 5</span><br><span class="line">  revisionHistoryLimit: 10</span><br><span class="line">  strategy:</span><br><span class="line">    <span class="built_in">type</span>: RollingUpdate</span><br><span class="line">    rollingUpdate:</span><br><span class="line">      maxSurge: 1</span><br><span class="line">      maxUnavailable: 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.13.3</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br></pre></td></tr></table></figure></li><li><p>可以使用下面的命令查看单个revison的信息</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl rollout <span class="built_in">history</span> deployment nginx-deploy --revision=2</span><br><span class="line">deployment.extensions/nginx-deploy with revision <span class="comment">#2</span></span><br><span class="line">Pod Template:</span><br><span class="line">  Labels:	app=nginx</span><br><span class="line">	pod-template-hash=799d666985</span><br><span class="line">  Annotations:	kubernetes.io/change-cause: kubectl apply --filename=nginx-deployment.yaml --record=<span class="literal">true</span></span><br><span class="line">  Containers:</span><br><span class="line">   nginx:</span><br><span class="line">    Image:	nginx:1.13.3</span><br><span class="line">    Port:	80/TCP</span><br><span class="line">    Host Port:	0/TCP</span><br><span class="line">    Environment:	&lt;none&gt;</span><br><span class="line">    Mounts:	&lt;none&gt;</span><br><span class="line">  Volumes:	&lt;none&gt;</span><br></pre></td></tr></table></figure></li><li><p>直接回退到当前版本的前一个版本：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl rollout undo deployment nginx-deploy</span><br><span class="line">deployment.extensions/nginx-deploy rolled back</span><br><span class="line"></span><br><span class="line"><span class="comment"># 也可以用revision回退到指定的版本</span></span><br><span class="line">$ kubectl rollout undo deployment nginx-deploy --to-revision=1</span><br><span class="line">deployment.extensions/nginx-deploy rolled back</span><br></pre></td></tr></table></figure></li><li><p>查看Deployment现在的状态</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl get deployments</span><br><span class="line">NAME           READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">nginx-deploy   2/3     3            2           56m</span><br><span class="line"></span><br><span class="line">$ kubectl get rs</span><br><span class="line">NAME                      DESIRED   CURRENT   READY   AGE</span><br><span class="line">nginx-deploy-6dd86d77d    1         1         1       56m</span><br><span class="line">nginx-deploy-799d666985   3         3         1       46m</span><br><span class="line"></span><br><span class="line">$ kubectl rollout status deployment/nginx-deploy</span><br><span class="line">Waiting <span class="keyword">for</span> deployment <span class="string">"nginx-deploy"</span> rollout to finish: 2 of 3 updated replicas are available...</span><br><span class="line">Waiting <span class="keyword">for</span> deployment <span class="string">"nginx-deploy"</span> rollout to finish: 2 of 3 updated replicas are available...</span><br><span class="line">deployment <span class="string">"nginx-deploy"</span> successfully rolled out</span><br><span class="line"></span><br><span class="line"><span class="comment"># 完成后查看</span></span><br><span class="line">$ kubectl get rs</span><br><span class="line">NAME                      DESIRED   CURRENT   READY   AGE</span><br><span class="line">nginx-deploy-6dd86d77d    0         0         0       57m</span><br><span class="line">nginx-deploy-799d666985   3         3         3       47m</span><br></pre></td></tr></table></figure></li></ul><p><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/" target="_blank" rel="noopener">官方参考</a></p><!-- rebuild by neat -->]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>Deployment</tag>
      </tags>
  </entry>
  <entry>
    <title>harbor私有仓库部署</title>
    <url>/2019/09/30/harbor%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:23 GMT+0800 (China Standard Time) --><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Harbor是一个用于存储和分发Docker镜像的企业级Registry服务器，通过添加一些企业必需的功能特性，例如安全、标识和管理等，扩展了开源Docker Distribution。作为一个企业级私有Registry服务器，Harbor提供了更好的性能和安全。提升用户使用Registry构建和运行环境传输镜像的效率。Harbor支持安装在多个Registry节点的镜像资源复制，镜像全部保存在私有Registry中， 确保数据和知识产权在公司内部网络中管控。另外，Harbor也提供了高级的安全特性，诸如用户管理，访问控制和活动审计等。</p><a id="more"></a><h3 id="部署环境准备"><a href="#部署环境准备" class="headerlink" title="部署环境准备"></a>部署环境准备</h3><h4 id="服务器配置"><a href="#服务器配置" class="headerlink" title="服务器配置"></a>服务器配置</h4><table><thead><tr><th>系统</th><th>配置</th><th>ip</th></tr></thead><tbody><tr><td>centos 7.4</td><td>4/8G/200G</td><td>172.21.16.90</td></tr></tbody></table><h4 id="下载所需文件"><a href="#下载所需文件" class="headerlink" title="下载所需文件"></a>下载所需文件</h4><h5 id="docker-compose-下载"><a href="#docker-compose-下载" class="headerlink" title="docker-compose 下载"></a>docker-compose 下载</h5><p>docker compose <a href="https://github.com/docker/compose/releases" target="_blank" rel="noopener">发布页面</a>下载最新的 docker-compose 二进制文件</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># wget https://github.com/docker/compose/releases/download/1.24.1/docker-compose-Linux-x86_64</span></span><br><span class="line"><span class="comment"># mv ~/docker-compose-Linux-x86_64 /usr/bin/docker-compose </span></span><br><span class="line"><span class="comment"># chmod a+x  /ur/bin/docker-compose</span></span><br></pre></td></tr></table></figure><ul><li>官方的安装<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># curl -L https://github.com/docker/compose/releases/download/1.24.1/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose</span></span><br><span class="line"><span class="comment"># chmod +x /usr/local/bin/docker-compose</span></span><br></pre></td></tr></table></figure></li></ul><h5 id="harbor-下载"><a href="#harbor-下载" class="headerlink" title="harbor 下载"></a>harbor 下载</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;harbor 安装方式有两种，一种是在线安装，一种是离线安装，这里由于网络不好，使用的是离线安装，harbor<a href="https://github.com/goharbor/harbor/releases" target="_blank" rel="noopener">发布页面</a>下载最新的 harbor 离线安装包</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># wget https://storage.googleapis.com/harbor-releases/release-1.9.0/harbor-offline-installer-v1.9.0.tgz</span></span><br><span class="line"><span class="comment"># tar -zxvf harbor-offline-installer-v1.9.0.tgz</span></span><br><span class="line"><span class="comment">#</span></span><br></pre></td></tr></table></figure><h3 id="开始安装"><a href="#开始安装" class="headerlink" title="开始安装"></a>开始安装</h3><h4 id="docker-安装"><a href="#docker-安装" class="headerlink" title="docker 安装"></a>docker 安装</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># yum-config-manager   --add-repo   https://download.docker.com/linux/centos/docker-ce.repo</span></span><br><span class="line"><span class="comment"># sudo yum -y install docker-ce-18.09.6-3.el7.x86_64</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># cat /etc/sysctl.d/k8s.conf &lt;&lt;EOF</span></span><br><span class="line">net.bridge.bridge-nf-call-ip6tables: 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables: 1</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># sysctl -p /etc/sysctl.d/k8s.conf</span></span><br><span class="line"><span class="comment"># systemctl  start docker</span></span><br></pre></td></tr></table></figure><p><strong>注意</strong>: 不添加<code>/etc/sysctl.d/k8s.conf</code> 启动docker会提示<code>WARNING: bridge-nf-call-iptables is disabled WARNING: bridge-nf-call-ip6tables is disabled</code></p><h4 id="导入-docker-images"><a href="#导入-docker-images" class="headerlink" title="导入 docker images"></a>导入 docker images</h4><p>导入离线安装包中harbor相关的 docker images：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cd harbor</span></span><br><span class="line"><span class="comment"># docker load -i harbor.v1.9.0.tar.gz </span></span><br><span class="line"><span class="comment"># docker images</span></span><br><span class="line">REPOSITORY                      TAG                        IMAGE ID            CREATED             SIZE</span><br><span class="line">goharbor/chartmuseum-photon     v0.9.0-v1.9.0              00c12627cbd7        2 weeks ago         131MB</span><br><span class="line">goharbor/harbor-migrator        v1.9.0                     75d4de5e0f16        2 weeks ago         362MB</span><br><span class="line">goharbor/redis-photon           v1.9.0                     3249afaa9965        2 weeks ago         109MB</span><br><span class="line">goharbor/clair-photon           v2.0.9-v1.9.0              e54ad567c58f        2 weeks ago         165MB</span><br><span class="line">goharbor/notary-server-photon   v0.6.1-v1.9.0              2cdecba59f38        2 weeks ago         138MB</span><br><span class="line">goharbor/notary-signer-photon   v0.6.1-v1.9.0              973378593def        2 weeks ago         135MB</span><br><span class="line">goharbor/harbor-registryctl     v1.9.0                     30a01bf0f4df        2 weeks ago         99.6MB</span><br><span class="line">goharbor/registry-photon        v2.7.1-patch-2819-v1.9.0   32571099a9fe        2 weeks ago         82.3MB</span><br><span class="line">goharbor/nginx-photon           v1.9.0                     f933d62f9952        2 weeks ago         43.9MB</span><br><span class="line">goharbor/harbor-log             v1.9.0                     28e27d511335        2 weeks ago         82.6MB</span><br><span class="line">goharbor/harbor-jobservice      v1.9.0                     f3cd0b181a89        2 weeks ago         141MB</span><br><span class="line">goharbor/harbor-core            v1.9.0                     f2814ed8aadd        2 weeks ago         155MB</span><br><span class="line">goharbor/harbor-portal          v1.9.0                     0778d4c5d27e        2 weeks ago         51.3MB</span><br><span class="line">goharbor/harbor-db              v1.9.0                     a809e14d2d49        2 weeks ago         147MB</span><br><span class="line">goharbor/prepare                v1.9.0                     aa594772c1e8        2 weeks ago         147MB</span><br></pre></td></tr></table></figure><h4 id="修改-harbor-yml-文件"><a href="#修改-harbor-yml-文件" class="headerlink" title="修改 harbor.yml 文件"></a>修改 harbor.yml 文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># vim harbor.yml</span></span><br><span class="line">hostname: reg.xxlaila.cn</span><br><span class="line"></span><br><span class="line"><span class="comment"># email configure</span></span><br><span class="line">email_server: smtp.exmail.qq.com</span><br><span class="line">email_server_port: 465</span><br><span class="line">email_username: admin@xxlaila.cn</span><br><span class="line">email_password: 123</span><br><span class="line">email_from: admin&lt;admin@xxlaila.cn&gt;</span><br><span class="line">email_ssl: <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># User registration is prohibited</span></span><br><span class="line">self_registration: off</span><br><span class="line"></span><br><span class="line"><span class="comment"># LDAP authentication configuration item</span></span><br><span class="line"><span class="comment">#ldap_url: ldaps://ldap.xxlaila.cn</span></span><br><span class="line"><span class="comment">#ldap_searchdn: uid=username,ou=people,dc=xxlaila,dc=com</span></span><br><span class="line"><span class="comment">#ldap_search_pwd: password</span></span><br><span class="line"><span class="comment">#ldap_basedn: ou=people,dc=xxlaila,dc=com</span></span><br><span class="line"><span class="comment">#ldap_filter: (objectClass=person)</span></span><br><span class="line"><span class="comment">#ldap_uid: uid </span></span><br><span class="line"><span class="comment">#ldap_scope: 3 </span></span><br><span class="line"><span class="comment">#ldap_timeout: 5</span></span><br></pre></td></tr></table></figure><p><strong>注</strong>: 新版本的邮箱、ldap现在都不需要在配置文件里面来添加配置了，直接通过web界面来进行配置即可，这里我只是添加进来，保留，😁😁😁</p><h4 id="加载和启动-harbor-镜像"><a href="#加载和启动-harbor-镜像" class="headerlink" title="加载和启动 harbor 镜像"></a>加载和启动 harbor 镜像</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># mkdir /data</span></span><br><span class="line"><span class="comment"># chmod 777 /var/run/docker.sock /data</span></span><br><span class="line"><span class="comment"># ./install.sh </span></span><br><span class="line"></span><br><span class="line">[Step 0]: checking installation environment ...</span><br><span class="line"></span><br><span class="line">Note: docker version: 19.03.2</span><br><span class="line"></span><br><span class="line">Note: docker-compose version: 1.24.1</span><br><span class="line"></span><br><span class="line">[Step 1]: loading Harbor images ...</span><br><span class="line">Loaded image: goharbor/harbor-portal:v1.9.0</span><br><span class="line">Loaded image: goharbor/harbor-core:v1.9.0</span><br><span class="line">Loaded image: goharbor/nginx-photon:v1.9.0</span><br><span class="line">Loaded image: goharbor/notary-signer-photon:v0.6.1-v1.9.0</span><br><span class="line">Loaded image: goharbor/registry-photon:v2.7.1-patch-2819-v1.9.0</span><br><span class="line">Loaded image: goharbor/harbor-migrator:v1.9.0</span><br><span class="line">Loaded image: goharbor/chartmuseum-photon:v0.9.0-v1.9.0</span><br><span class="line">Loaded image: goharbor/prepare:v1.9.0</span><br><span class="line">Loaded image: goharbor/harbor-log:v1.9.0</span><br><span class="line">Loaded image: goharbor/harbor-db:v1.9.0</span><br><span class="line">Loaded image: goharbor/clair-photon:v2.0.9-v1.9.0</span><br><span class="line">Loaded image: goharbor/harbor-jobservice:v1.9.0</span><br><span class="line">Loaded image: goharbor/harbor-registryctl:v1.9.0</span><br><span class="line">Loaded image: goharbor/redis-photon:v1.9.0</span><br><span class="line">Loaded image: goharbor/notary-server-photon:v0.6.1-v1.9.0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[Step 2]: preparing environment ...</span><br><span class="line">prepare base dir is <span class="built_in">set</span> to /opt/harbor</span><br><span class="line">Clearing the configuration file: /config/<span class="built_in">log</span>/logrotate.conf</span><br><span class="line">Clearing the configuration file: /config/<span class="built_in">log</span>/rsyslog_docker.conf</span><br><span class="line">Generated configuration file: /config/<span class="built_in">log</span>/logrotate.conf</span><br><span class="line">Generated configuration file: /config/<span class="built_in">log</span>/rsyslog_docker.conf</span><br><span class="line">Generated configuration file: /config/nginx/nginx.conf</span><br><span class="line">Generated configuration file: /config/core/env</span><br><span class="line">Generated configuration file: /config/core/app.conf</span><br><span class="line">Generated configuration file: /config/registry/config.yml</span><br><span class="line">Generated configuration file: /config/registryctl/env</span><br><span class="line">Generated configuration file: /config/db/env</span><br><span class="line">Generated configuration file: /config/jobservice/env</span><br><span class="line">Generated configuration file: /config/jobservice/config.yml</span><br><span class="line">Generated and saved secret to file: /secret/keys/secretkey</span><br><span class="line">Generated certificate, key file: /secret/core/private_key.pem, cert file: /secret/registry/root.crt</span><br><span class="line">Generated configuration file: /compose_location/docker-compose.yml</span><br><span class="line">Clean up the input dir</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[Step 3]: starting Harbor ...</span><br><span class="line">Creating network <span class="string">"harbor_harbor"</span> with the default driver</span><br><span class="line">Creating harbor-log ... <span class="keyword">done</span></span><br><span class="line">Creating registryctl   ... <span class="keyword">done</span></span><br><span class="line">Creating redis         ... <span class="keyword">done</span></span><br><span class="line">Creating harbor-portal ... <span class="keyword">done</span></span><br><span class="line">Creating harbor-db     ... <span class="keyword">done</span></span><br><span class="line">Creating registry      ... <span class="keyword">done</span></span><br><span class="line">Creating harbor-core   ... <span class="keyword">done</span></span><br><span class="line">Creating nginx             ... <span class="keyword">done</span></span><br><span class="line">Creating harbor-jobservice ... <span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">✔ ----Harbor has been installed and started successfully.----</span><br><span class="line"></span><br><span class="line">Now you should be able to visit the admin portal at http://reg.xxlaila.cn. </span><br><span class="line">For more details, please visit https://github.com/goharbor/harbor .</span><br></pre></td></tr></table></figure><h4 id="访问管理界面"><a href="#访问管理界面" class="headerlink" title="访问管理界面"></a>访问管理界面</h4><p>确认所有组件都工作正常：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># docker-compose  ps</span></span><br><span class="line">      Name                     Command                       State                     Ports          </span><br><span class="line">------------------------------------------------------------------------------------------------------</span><br><span class="line">harbor-core         /harbor/harbor_core              Up (healthy)                                     </span><br><span class="line">harbor-db           /docker-entrypoint.sh            Up (healthy)            5432/tcp                 </span><br><span class="line">harbor-jobservice   /harbor/harbor_jobservice  ...   Up (health: starting)                            </span><br><span class="line">harbor-log          /bin/sh -c /usr/<span class="built_in">local</span>/bin/ ...   Up (healthy)            127.0.0.1:1514-&gt;10514/tcp</span><br><span class="line">harbor-portal       nginx -g daemon off;             Up (healthy)            8080/tcp                 </span><br><span class="line">nginx               nginx -g daemon off;             Up (healthy)            0.0.0.0:80-&gt;8080/tcp     </span><br><span class="line">redis               redis-server /etc/redis.conf     Up (healthy)            6379/tcp                 </span><br><span class="line">registry            /entrypoint.sh /etc/regist ...   Up (healthy)            5000/tcp                 </span><br><span class="line">registryctl         /harbor/start.sh                 Up (healthy)</span><br></pre></td></tr></table></figure><h5 id="harbor-组建介绍"><a href="#harbor-组建介绍" class="headerlink" title="harbor 组建介绍"></a>harbor 组建介绍</h5><ul><li>harbor-core: Harbor的核心功能，主要提供以下服务：<ul><li>UI：提供图形化界面，帮助用户管理registry上的镜像（image）, 并对用户进行授权。</li><li>webhook：为了及时获取registry 上image状态变化的情况， 在Registry上配置webhook，把状态变化传递给UI模块。</li><li>token 服务：负责根据用户权限给每个docker push/pull命令签发token. Docker 客户端向Regiøstry服务发起的请求,如果不包含token，会被重定向到这里，获得token后再重新向Registry进行请求。</li></ul></li><li>harbor-db: 为core services提供数据库服务，负责储存用户权限、审计日志、Docker image分组信息等数据。</li><li>harbor-jobservice: harbor-jobservice 是harbor的job管理模块，job在harbor里面主要是为了镜像仓库之前同步使用的;</li><li>harbor-log: 为了帮助监控Harbor运行，负责收集其他组件的log，供日后进行分析。</li><li>nginx: nginx负责流量转发和安全验证，对外提供的流量都是从nginx中转，所以开放https的443端口，它将流量分发到后端的ui和正在docker镜像存储的docker registry。</li><li>redis: 存储缓存信息</li><li>registry: 负责储存Docker镜像，并处理docker push/pull 命令。由于我们要对用户进行访问控制，即不同用户对Docker image有不同的读写权限，Registry会指向一个token服务，强制用户的每次docker pull/push请求都要携带一个合法的token, Registry会通过公钥对token 进行解密验证。</li><li>registryctl: 是harbor的管理员配置harbor的一些常用配置和高级配置</li></ul><p>在浏览器访问<a href="http://reg.xxlaila.cn，" target="_blank" rel="noopener">http://reg.xxlaila.cn，</a> 用账号 admin 和 harbor.yml 配置文件中的默认密码 Harbor12345 登陆系统<br><img src="https://img.xxlaila.cn/8095d05-b9b7-4bdc-b0fc-7810db649e23.png" alt="img"><br><img src="https://img.xxlaila.cn/4bfab8be-e5de-4165-9268-fa591c5f12f8.png" alt="img"></p><h4 id="harbor-运行时产生的文件、目录"><a href="#harbor-运行时产生的文件、目录" class="headerlink" title="harbor 运行时产生的文件、目录"></a>harbor 运行时产生的文件、目录</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;harbor 将日志打印到 /var/log/harbor 的相关目录下，传统的docker logs XXX 或 docker-compose logs XXX 看不到容器的日志。只有使用常用系统命令来进行日志的查看</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># # 日志目录</span></span><br><span class="line"><span class="comment"># ls /var/log/harbor</span></span><br><span class="line">core.log  jobservice.log  portal.log  postgresql.log  proxy.log  redis.log  registryctl.log  registry.log</span><br><span class="line"></span><br><span class="line"><span class="comment"># # 数据目录，包括数据库、镜像仓库</span></span><br><span class="line"><span class="comment"># ls /data/</span></span><br><span class="line">ca_download  database  job_logs  psc  redis  registry  secret</span><br></pre></td></tr></table></figure><h4 id="其它操作"><a href="#其它操作" class="headerlink" title="其它操作"></a>其它操作</h4><p>下列操作的工作目录均为解压离线安装文件后生成的 harbor 目录。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># # 停止 harbor</span></span><br><span class="line"><span class="comment"># docker-compose down -v</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # 启动 harbor</span></span><br><span class="line"><span class="comment"># docker-compose up -d</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # 更修改的配置更新到 docker-compose.yml 文件</span></span><br><span class="line"><span class="comment"># ./prepare</span></span><br><span class="line">prepare base dir is <span class="built_in">set</span> to /opt/harbor</span><br><span class="line">Clearing the configuration file: /config/<span class="built_in">log</span>/logrotate.conf</span><br><span class="line">Clearing the configuration file: /config/<span class="built_in">log</span>/rsyslog_docker.conf</span><br><span class="line">Clearing the configuration file: /config/nginx/nginx.conf</span><br><span class="line">Clearing the configuration file: /config/core/env</span><br><span class="line">Clearing the configuration file: /config/core/app.conf</span><br><span class="line">Clearing the configuration file: /config/registry/config.yml</span><br><span class="line">Clearing the configuration file: /config/registry/root.crt</span><br><span class="line">Clearing the configuration file: /config/registryctl/env</span><br><span class="line">Clearing the configuration file: /config/registryctl/config.yml</span><br><span class="line">Clearing the configuration file: /config/db/env</span><br><span class="line">Clearing the configuration file: /config/jobservice/env</span><br><span class="line">Clearing the configuration file: /config/jobservice/config.yml</span><br><span class="line">Generated configuration file: /config/<span class="built_in">log</span>/logrotate.conf</span><br><span class="line">Generated configuration file: /config/<span class="built_in">log</span>/rsyslog_docker.conf</span><br><span class="line">Generated configuration file: /config/nginx/nginx.conf</span><br><span class="line">Generated configuration file: /config/core/env</span><br><span class="line">Generated configuration file: /config/core/app.conf</span><br><span class="line">Generated configuration file: /config/registry/config.yml</span><br><span class="line">Generated configuration file: /config/registryctl/env</span><br><span class="line">Generated configuration file: /config/db/env</span><br><span class="line">Generated configuration file: /config/jobservice/env</span><br><span class="line">Generated configuration file: /config/jobservice/config.yml</span><br><span class="line">loaded secret from file: /secret/keys/secretkey</span><br><span class="line">Generated configuration file: /compose_location/docker-compose.yml</span><br><span class="line">Clean up the input dir</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>harbor</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s pod健康检测</title>
    <url>/2019/09/27/k8s-pod%E5%81%A5%E5%BA%B7%E6%A3%80%E6%B5%8B/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:23 GMT+0800 (China Standard Time) --><h3 id="Pod健康检测机制"><a href="#Pod健康检测机制" class="headerlink" title="Pod健康检测机制"></a>Pod健康检测机制</h3><p>对于Pod的健康状态检测，kubernetes提供了两类探针(Probe)来执行对Pod的健康状态检测:</p><ul><li><strong>LivenessProbe探针</strong>:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;用于判断容器是否存活，即Pod是否为running状态，如果LivenessProbe探针探测到容器不健康，则kubelet将kill掉容器，并根据容器的重启策略是否重启，如果一个容器不包含LivenessProbe探针，则Kubelet认为容器的LivenessProbe探针的返回值永远成功.</li></ul><a id="more"></a><ul><li><strong>ReadinessProbe探针</strong>:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;用于判断容器是否启动完成，即容器的Ready是否为True，可以接收请求，如果ReadinessProbe探测失败，则容器的Ready将为False，控制器将此Pod的Endpoint从对应的service的Endpoint列表中移除，从此不再将任何请求调度此Pod上，直到下次探测成功。</li></ul><!--more--><p>每类探针都支持三种探测方法:</p><ul><li><strong>exec</strong>: 通过执行命令来检查服务是否正常，针对复杂检测或无HTTP接口的服务，命令返回值为0则表示容器健康。</li><li><strong>httpGet</strong>: 通过发送http请求检查服务是否正常，返回200-399状态码则表明容器健康。</li><li><strong>tcpSocket</strong>: 通过容器的IP和Port执行TCP检查，如果能够建立TCP连接，则表明容器健康。</li></ul><p>探针探测的结果有以下三者之一:</p><ul><li><strong>Success</strong>: Container通过了检查</li><li><strong>Failure</strong>: Container未通过检查</li><li><strong>Unknown</strong>: 未能执行检查，因此不采取任何措施</li></ul><h3 id="LivenessProbe探针配置"><a href="#LivenessProbe探针配置" class="headerlink" title="LivenessProbe探针配置"></a>LivenessProbe探针配置</h3><h4 id="例一：通过exec方式做健康探测"><a href="#例一：通过exec方式做健康探测" class="headerlink" title="例一：通过exec方式做健康探测"></a>例一：通过exec方式做健康探测</h4><ul><li>exec-liveness.yaml<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat &gt; <span class="built_in">exec</span>-liveness.yaml &lt;&lt;EOF</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    <span class="built_in">test</span>: liveness</span><br><span class="line">  name: liveness-exec</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: liveness</span><br><span class="line">    image: busybox</span><br><span class="line">    args:</span><br><span class="line">    - /bin/sh</span><br><span class="line">    - -c</span><br><span class="line">    - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600</span><br><span class="line">    livenessProbe:</span><br><span class="line">      <span class="built_in">exec</span>:</span><br><span class="line">        <span class="built_in">command</span>:</span><br><span class="line">        - cat</span><br><span class="line">        - /tmp/healthy</span><br><span class="line">      initialDelaySeconds: 5</span><br><span class="line">      periodSeconds: 5</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在该配置文件中，对容器执行livenessProbe检查，periodSeconds字段指定kubelet每5s执行一次检查，检查的命令为cat /tmp/healthy，initialDelaySeconds字段告诉kubelet应该在执行第一次检查之前等待5秒，如果命令执行成功，则返回0，那么kubelet就认为容器是健康的，如果为非0，则Kubelet会Kill掉容器并根据重启策略来决定是否需要重启。</p><ul><li>当容器启动时，它会执行以下命令<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/bin/sh -c <span class="string">"touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600"</span></span><br></pre></td></tr></table></figure></li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于容器的前30秒，有一个/tmp/healthy文件。因此，在前30秒内，该命令cat /tmp/healthy返回成功代码。30秒后，cat /tmp/healthy返回失败代码。</p><ul><li><p>创建Pod</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$  kubectl create -f  <span class="built_in">exec</span>-liveness.yaml </span><br><span class="line">pod/liveness-exec created</span><br></pre></td></tr></table></figure></li><li><p>在30秒内，查看Pod事件</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl describe pod liveness-exec</span><br><span class="line">…………</span><br><span class="line">QoS Class:       BestEffort</span><br><span class="line">Node-Selectors:  &lt;none&gt;</span><br><span class="line">Tolerations:     node.kubernetes.io/not-ready:NoExecute <span class="keyword">for</span> 360s</span><br><span class="line">                 node.kubernetes.io/unreachable:NoExecute <span class="keyword">for</span> 360s</span><br><span class="line">Events:</span><br><span class="line">  Type    Reason     Age   From                   Message</span><br><span class="line">  ----    ------     ----  ----                   -------</span><br><span class="line">  Normal  Scheduled  23s   default-scheduler      Successfully assigned default/liveness-exec to 172.21.17.34</span><br><span class="line">  Normal  Pulling    20s   kubelet, 172.21.17.34  Pulling image <span class="string">"busybox"</span></span><br><span class="line">  Normal  Pulled     2s    kubelet, 172.21.17.34  Successfully pulled image <span class="string">"busybox"</span></span><br><span class="line">  Normal  Created    2s    kubelet, 172.21.17.34  Created container liveness</span><br><span class="line">  Normal  Started    1s    kubelet, 172.21.17.34  Started container liveness</span><br></pre></td></tr></table></figure></li><li><p>35秒后，再次查看Pod事件</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl describe pod liveness-exec</span><br><span class="line">…………</span><br><span class="line">Tolerations:     node.kubernetes.io/not-ready:NoExecute <span class="keyword">for</span> 360s</span><br><span class="line">                 node.kubernetes.io/unreachable:NoExecute <span class="keyword">for</span> 360s</span><br><span class="line">Events:</span><br><span class="line">  Type     Reason     Age              From                   Message</span><br><span class="line">  ----     ------     ----             ----                   -------</span><br><span class="line">  Normal   Scheduled  58s              default-scheduler      Successfully assigned default/liveness-exec to 172.21.17.34</span><br><span class="line">  Normal   Pulling    55s              kubelet, 172.21.17.34  Pulling image <span class="string">"busybox"</span></span><br><span class="line">  Normal   Pulled     37s              kubelet, 172.21.17.34  Successfully pulled image <span class="string">"busybox"</span></span><br><span class="line">  Normal   Created    37s              kubelet, 172.21.17.34  Created container liveness</span><br><span class="line">  Normal   Started    36s              kubelet, 172.21.17.34  Started container liveness</span><br><span class="line">  Warning  Unhealthy  0s (x2 over 5s)  kubelet, 172.21.17.34  Liveness probe failed: cat: can<span class="string">'t open '</span>/tmp/healthy<span class="string">': No such file or directory</span></span><br></pre></td></tr></table></figure></li><li><p>再等30秒，确认Container已重新启动</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl get pod liveness-exec</span><br><span class="line">NAME            READY   STATUS    RESTARTS   AGE</span><br><span class="line">liveness-exec   1/1     Running   1          115s</span><br><span class="line"></span><br><span class="line">$ kubectl describe pod liveness-exec</span><br><span class="line">………………</span><br><span class="line">QoS Class:       BestEffort</span><br><span class="line">Node-Selectors:  &lt;none&gt;</span><br><span class="line">Tolerations:     node.kubernetes.io/not-ready:NoExecute <span class="keyword">for</span> 360s</span><br><span class="line">                 node.kubernetes.io/unreachable:NoExecute <span class="keyword">for</span> 360s</span><br><span class="line">Events:</span><br><span class="line">  Type     Reason     Age                 From                   Message</span><br><span class="line">  ----     ------     ----                ----                   -------</span><br><span class="line">  Normal   Scheduled  2m7s                default-scheduler      Successfully assigned default/liveness-exec to 172.21.17.34</span><br><span class="line">  Warning  Unhealthy  64s (x3 over 74s)   kubelet, 172.21.17.34  Liveness probe failed: cat: can<span class="string">'t open '</span>/tmp/healthy<span class="string">': No such file or directory</span></span><br><span class="line"><span class="string">  Normal   Killing    64s                 kubelet, 172.21.17.34  Container liveness failed liveness probe, will be restarted</span></span><br><span class="line"><span class="string">  Normal   Pulling    34s (x2 over 2m4s)  kubelet, 172.21.17.34  Pulling image "busybox"</span></span><br><span class="line"><span class="string">  Normal   Pulled     25s (x2 over 106s)  kubelet, 172.21.17.34  Successfully pulled image "busybox"</span></span><br><span class="line"><span class="string">  Normal   Created    25s (x2 over 106s)  kubelet, 172.21.17.34  Created container liveness</span></span><br><span class="line"><span class="string">  Normal   Started    25s (x2 over 105s)  kubelet, 172.21.17.34  Started container liveness</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="例二-通过HTTP方式做健康探测"><a href="#例二-通过HTTP方式做健康探测" class="headerlink" title="例二: 通过HTTP方式做健康探测"></a>例二: 通过HTTP方式做健康探测</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat &gt; http-liveness.yaml &lt;&lt;EOF</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    <span class="built_in">test</span>: liveness</span><br><span class="line">  name: liveness-http</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: liveness</span><br><span class="line">    image: carlziess/liveness</span><br><span class="line">    args:</span><br><span class="line">    - /server</span><br><span class="line">    livenessProbe:</span><br><span class="line">      httpGet:</span><br><span class="line">        path: /healthz</span><br><span class="line">        port: 8080</span><br><span class="line">        httpHeaders:</span><br><span class="line">        - name: X-Custom-Header</span><br><span class="line">          value: Awesome</span><br><span class="line">      initialDelaySeconds: 3</span><br><span class="line">      periodSeconds: 3</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;创建一个Pod，其中periodSeconds字段指定kubelet每3秒执行一次探测，initialDelaySeconds字段告诉kubelet延迟等待3秒，探测方式为向容器中运行的服务发送HTTP GET请求，请求8080端口下的/healthz, 任何大于或等于200且小于400的代码表示成功。任何其他代码表示失败。</p><ul><li><p>创建pod</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl apply -f http-liveness.yaml </span><br><span class="line">pod/liveness-http created</span><br></pre></td></tr></table></figure></li><li><p>检查验证</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl describe pod liveness-http</span><br><span class="line">………………</span><br><span class="line">Node-Selectors:  &lt;none&gt;</span><br><span class="line">Tolerations:     node.kubernetes.io/not-ready:NoExecute <span class="keyword">for</span> 360s</span><br><span class="line">                 node.kubernetes.io/unreachable:NoExecute <span class="keyword">for</span> 360s</span><br><span class="line">Events:</span><br><span class="line">  Type     Reason     Age                   From                   Message</span><br><span class="line">  ----     ------     ----                  ----                   -------</span><br><span class="line">  Normal   Scheduled  2m59s                 default-scheduler      Successfully assigned default/liveness-http to 172.21.17.34</span><br><span class="line">  Normal   Pulled     119s (x3 over 2m46s)  kubelet, 172.21.17.34  Successfully pulled image <span class="string">"carlziess/liveness"</span></span><br><span class="line">  Normal   Created    119s (x3 over 2m46s)  kubelet, 172.21.17.34  Created container liveness</span><br><span class="line">  Normal   Started    118s (x3 over 2m45s)  kubelet, 172.21.17.34  Started container liveness</span><br><span class="line"></span><br><span class="line">$ kubectl get pod</span><br><span class="line">NAME            READY   STATUS    RESTARTS   AGE</span><br><span class="line">liveness-http   1/1     Running   0          26s</span><br></pre></td></tr></table></figure></li><li><p><strong>httpGet</strong>探测方式有如下可选的控制字段</p><ul><li>host: 要连接的主机名，默认为Pod IP，可以在http request head中设置host头部。</li><li>scheme: 用于连接host的协议，默认为HTTP。</li><li>path: http服务器上的访问URL</li><li>httpHeaders: 自定义HTTP请求headers，HTTP允许重复headers</li><li>port: 容器上要访问端口号或名称</li></ul></li></ul><h4 id="例三-通过TCP方式做健康探测"><a href="#例三-通过TCP方式做健康探测" class="headerlink" title="例三: 通过TCP方式做健康探测"></a>例三: 通过TCP方式做健康探测</h4><p>Kubelet将尝试在指定的端口上打开容器上的套接字，如果能建立连接，则表明容器健康。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat &gt; tcp-liveness-readiness.yaml &lt;&lt;EOF</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: goproxy</span><br><span class="line">  labels:</span><br><span class="line">    app: goproxy</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: goproxy</span><br><span class="line">    image: goproxy/goproxy</span><br><span class="line">    ports:</span><br><span class="line">    - containerPort: 8080</span><br><span class="line">    readinessProbe:</span><br><span class="line">      tcpSocket:</span><br><span class="line">        port: 8080</span><br><span class="line">      initialDelaySeconds: 5</span><br><span class="line">      periodSeconds: 10</span><br><span class="line">    livenessProbe:</span><br><span class="line">      tcpSocket:</span><br><span class="line">        port: 8080</span><br><span class="line">      initialDelaySeconds: 15</span><br><span class="line">      periodSeconds: 20</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TCP检查方式和HTTP检查方式非常相似，示例中两种探针都使用了，在容器启动5秒后，kubelet将发送第一个readinessProbe探针，这将连接到容器的8080端口，如果探测成功，则该Pod将被标识为ready，10秒后，kubelet将进行第二次连接。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;除此，配置还包含了livenessProbe探针，在容器启动15秒后，kubelet将发送第一个livenessProbe探针，仍然尝试连接容器的8080端口，如果连接失败则重启容器。</p><ul><li><p>执行创建</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl apply -f tcp-liveness-readiness.yaml</span><br><span class="line">pod/goproxy created</span><br></pre></td></tr></table></figure></li><li><p>15秒后，查看Pod事件以验证活动探测</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl describe pod goproxy</span><br><span class="line">………………</span><br><span class="line">Node-Selectors:  &lt;none&gt;</span><br><span class="line">Tolerations:     node.kubernetes.io/not-ready:NoExecute <span class="keyword">for</span> 360s</span><br><span class="line">                 node.kubernetes.io/unreachable:NoExecute <span class="keyword">for</span> 360s</span><br><span class="line">Events:</span><br><span class="line">  Type    Reason     Age   From                    Message</span><br><span class="line">  ----    ------     ----  ----                    -------</span><br><span class="line">  Normal  Scheduled  26s   default-scheduler       Successfully assigned default/goproxy to 172.21.16.231</span><br><span class="line">  Normal  Pulling    22s   kubelet, 172.21.16.231  Pulling image <span class="string">"goproxy/goproxy"</span></span><br></pre></td></tr></table></figure></li></ul><p>当容器有多个端口时，通常会给每个端口命名，所以在使用探针探测时，也可以直接写自定义的端口名称</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ports:</span><br><span class="line">- name: liveness-port</span><br><span class="line">  containerPort: 8080</span><br><span class="line">  hostPort: 8080</span><br><span class="line">livenessProbe:</span><br><span class="line">  httpGet:</span><br><span class="line">    path: /healthz</span><br><span class="line">    port: liveness-port</span><br></pre></td></tr></table></figure><h3 id="ReadinessProbe探针配置"><a href="#ReadinessProbe探针配置" class="headerlink" title="ReadinessProbe探针配置"></a>ReadinessProbe探针配置</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ReadinessProbe探针的使用场景livenessProbe稍有不同，有的时候应用程序可能暂时无法接受请求，比如Pod已经Running了，但是容器内应用程序尚未启动成功，在这种情况下，如果没有ReadinessProbe，则Kubernetes认为它可以处理请求了，然而此时，我们知道程序还没启动成功是不能接收用户请求的，所以不希望kubernetes把请求调度给它，则使用ReadinessProbe探针。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ReadinessProbe和livenessProbe可以使用相同探测方式，只是对Pod的处置方式不同，ReadinessProbe是将Pod IP:Port从对应的EndPoint列表中删除，而livenessProbe则Kill容器并根据Pod的重启策略来决定作出对应的措施。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;探针探测容器是否已准备就绪，如果未准备就绪则kubernetes不会将流量转发给此Pod。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ReadinessProbe探针与livenessProbe一样也支持exec、httpGet、TCP的探测方式，配置方式相同，只不过是将livenessProbe字段修改为ReadinessProbe。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">readinessProbe:</span><br><span class="line">  <span class="built_in">exec</span>:</span><br><span class="line">    <span class="built_in">command</span>:</span><br><span class="line">    - cat</span><br><span class="line">    - /tmp/healthy</span><br><span class="line">  initialDelaySeconds: 5</span><br><span class="line">  periodSeconds: 5</span><br></pre></td></tr></table></figure><p>ReadinessProbe探针的HTTP、TCP的探测方式也与livenessProbe的基本一致。</p><h4 id="例四-ReadinessProbe示例"><a href="#例四-ReadinessProbe示例" class="headerlink" title="例四: ReadinessProbe示例"></a>例四: ReadinessProbe示例</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;加入ReadinessProbe探针和一个没有ReadinessProbe探针的示例，该示例中，创建了一个deploy，名为JavaApp，启动的容器运行一个java应用程序，程序监听端口为9093。</p><ul><li><p>没有ReadinessProbe</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat &gt; k8s.yaml &lt;&lt; EOF</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: biz-gateway</span><br><span class="line">  labels:</span><br><span class="line">    app: biz-gateway</span><br><span class="line">  namespace:</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - port: 9093</span><br><span class="line">    name: biz-gateway</span><br><span class="line">  selector:</span><br><span class="line">    app: biz-gateway</span><br><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: biz-gateway</span><br><span class="line">  namespace:</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: biz-gateway</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: biz-gateway</span><br><span class="line">        image: docker.io/xxlaila/biz-gateway:dev-08c8a4e</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 9093</span><br><span class="line">        env:</span><br><span class="line">        - name: RUN_ENV</span><br><span class="line">          value: dev</span><br><span class="line">        - name: CONFIG_API_SERVER</span><br><span class="line">          value: http://api.conf.xxlaila.cn</span><br><span class="line">        - name: RUN_CLUSTER</span><br><span class="line">          value: default</span><br><span class="line">        - name: RUN_MODE</span><br><span class="line">          value: AUTO</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></li><li><p>执行创建</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl apply -f k8s.yaml </span><br><span class="line">service/biz-gateway created</span><br><span class="line">deployment.extensions/biz-gateway created</span><br></pre></td></tr></table></figure></li><li><p>刚创建后，等一会后，查看Pod状态，记着要给image留下pull的时间</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl get pods  |grep <span class="string">"biz-gateway"</span></span><br><span class="line">NAME                          READY   STATUS    RESTARTS   AGE</span><br><span class="line">biz-gateway-95f6b677f-rnz22   1/1     Running   0          2m8s</span><br></pre></td></tr></table></figure></li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;可以看到，整个过程Pod用了2m8s，自身状态已Running，其READ字段，1/1 表示1个容器状态已准备就绪了，此时，对于kubernetes而言，已经可以接收请求了,而实际上服务还无法访问，因为JAVA程序还尚启动起来，2m8ss后方可正常访问，所以针对此类程序，必须配置ReadinessProbe。</p><ul><li>加入readinessProbe<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat &gt; k8s.yaml &lt;&lt; EOF</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: biz-gateway</span><br><span class="line">  labels:</span><br><span class="line">    app: biz-gateway</span><br><span class="line">  namespace:</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - protocol: TCP</span><br><span class="line">    port: 9093</span><br><span class="line">    name: biz-gateway</span><br><span class="line">  selector:</span><br><span class="line">    app: biz-gateway</span><br><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: biz-gateway</span><br><span class="line">  namespace:</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: biz-gateway</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: biz-gateway</span><br><span class="line">        image: docker.io/xxlaila/biz-gateway:dev-08c8a4e</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 9093</span><br><span class="line">        readinessProbe:</span><br><span class="line">          tcpSocket:</span><br><span class="line">            port: 9093</span><br><span class="line">          initialDelaySeconds: 140</span><br><span class="line">          periodSeconds: 10</span><br><span class="line">        env:</span><br><span class="line">        - name: RUN_ENV</span><br><span class="line">          value: dev</span><br><span class="line">        - name: CONFIG_API_SERVER</span><br><span class="line">          value: http://api.conf.xxlaila.cn</span><br><span class="line">        - name: RUN_CLUSTER</span><br><span class="line">          value: default</span><br><span class="line">        - name: RUN_MODE</span><br><span class="line">          value: AUTO</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在该配置文件中，ReadinessProbe探针的探测方式为tcpSocket，因为程序监听在9093端口，所以这里探测为对9093建立连接,这里第一次探测时间是在Pod Runing后140秒后，间隔10秒后执行第二次探测。</p><ul><li><p>创建</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl apply -f ./</span><br><span class="line">service/biz-gateway created</span><br><span class="line">deployment.extensions/biz-gateway created</span><br></pre></td></tr></table></figure></li><li><p>查看验证</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建后等待了60s</span></span><br><span class="line">$ kubectl get pod -o wide</span><br><span class="line">NAME                          READY   STATUS    RESTARTS   AGE   IP            NODE           NOMINATED NODE   READINESS GATES</span><br><span class="line">biz-gateway-f69cc8678-qs8s7   0/1     Running   0          60s   172.30.56.6   172.21.17.40   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 继续等待一会</span></span><br><span class="line">$ kubectl get pod -o wide</span><br><span class="line">NAME                          READY   STATUS    RESTARTS   AGE     IP            NODE           NOMINATED NODE   READINESS GATES</span><br><span class="line">biz-gateway-f69cc8678-qs8s7   1/1     Running   0          2m36s   172.30.56.6   172.21.17.40   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure></li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;可以看到在2m36秒后，pod启动ok，在第一次查看的时候，Pod虽然已处于Runnig状态，但是由于第一次探测时间未到，所以READY字段为0/1，即容器的状态为未准备就绪，在未准备就绪的情况下，其Pod对应的Service下的Endpoint也为空，所以才不会有任何请求被调度进来。</p><ul><li>查看Endpoint<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 第一次执行</span></span><br><span class="line">$ kubectl get endpoints</span><br><span class="line">NAME          ENDPOINTS                                                AGE</span><br><span class="line">biz-gateway                                                            57s</span><br><span class="line">kubernetes    172.21.16.110:6443,172.21.17.30:6443,172.21.17.31:6443   13d</span><br><span class="line"></span><br><span class="line">在2m36s后在次执行</span><br><span class="line">$ kubectl get endpoints</span><br><span class="line">NAME          ENDPOINTS                                                AGE</span><br><span class="line">biz-gateway   172.30.56.6:9093                                         2m41s</span><br><span class="line">kubernetes    172.21.16.110:6443,172.21.17.30:6443,172.21.17.31:6443   13d</span><br></pre></td></tr></table></figure></li></ul><h3 id="配置探针-Probe-相关属性"><a href="#配置探针-Probe-相关属性" class="headerlink" title="配置探针(Probe)相关属性"></a>配置探针(Probe)相关属性</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;探针(Probe)有许多可选字段，可以用来更加精确的控制Liveness和Readiness两种探针的行为(Probe)：</p><ul><li>initialDelaySeconds：Pod启动后延迟多久才进行检查，单位：秒</li><li>periodSeconds：检查的间隔时间，默认为10，单位：秒。</li><li>timeoutSeconds：探测的超时时间，默认为1，单位：秒。</li><li>successThreshold：探测失败后认为成功的最小连接成功次数，默认为1，在Liveness探针中必须为1，最小值为1。</li><li>failureThreshold：探测失败的重试次数，重试一定次数后将认为失败，在readiness探针中，Pod会被标记为未就绪，默认为3，最小值为1。</li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>之前错误参考排查介绍</strong>: 在之前安装jenkins的时候，创建pod就一值处于<code>running</code>,但是过一会，界面就报错，错误如下图<br><img src="https://img.xxlaila.cn/15008WechatIMG.png" alt="img"><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;然后查看pod日志和系统系统，都没有任何问题，pod日志如下，然后就问了朋友，就说有可能是pod的健康检测机制，最后就修改了pod的健康检测机制，jenkins服务器部署ok。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl <span class="built_in">log</span> $(kubectl get pods -n kube-ops | awk <span class="string">'&#123;print $1&#125;'</span> | grep jenkins) -n kube-ops</span><br><span class="line"><span class="built_in">log</span> is DEPRECATED and will be removed <span class="keyword">in</span> a future version. Use logs instead.</span><br><span class="line">VM settings:</span><br><span class="line">    Max. Heap Size: 3.00G</span><br><span class="line">    Ergonomics Machine Class: server</span><br><span class="line">    Using VM: OpenJDK 64-Bit Server VM</span><br><span class="line"></span><br><span class="line">Running from: /usr/share/jenkins/jenkins.war</span><br><span class="line">webroot: EnvVars.masterEnvVars.get(<span class="string">"JENKINS_HOME"</span>)</span><br><span class="line">2019-09-27 03:02:24.133+0000 [id=1] INFO org.eclipse.jetty.util.log.Log<span class="comment">#initialized: Logging initialized @429ms to org.eclipse.jetty.util.log.JavaUtilLog</span></span><br><span class="line">2019-09-27 03:02:24.247+0000 [id=1] INFO winstone.Logger<span class="comment">#logInternal: Beginning extraction from war file</span></span><br></pre></td></tr></table></figure><p><strong>后续</strong>: 虽然健康检测可以取消，不加入，但是当我们在上生产环境的时候还是要加上，正如例四介绍的那样。如果我们在生产环境错故障自愈、轮询发布等。都需要这个东西，加入再升级的时候，服务器都还没起来，k8s就吧流量给调度过来，升级下一个pod，外部用户访问就会报错，那就是很尴尬</p><!-- rebuild by neat -->]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>pod</tag>
      </tags>
  </entry>
  <entry>
    <title>EFK</title>
    <url>/2019/09/25/EFK/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><h3 id="初始化配置文件准备"><a href="#初始化配置文件准备" class="headerlink" title="初始化配置文件准备"></a>初始化配置文件准备</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;将下载的 kubernetes-server-linux-amd64.tar.gz 解压后，再解压其中的 kubernetes-src.tar.gz 文件。<code>kubernetes/cluster/addons/fluentd-elasticsearch</code>这是文件所在的路径</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;es 数据默认的存储在docker里面，在用的是node节点的空间，而node节点我们不可能都准备很大的空间，那样很浪费资源，所以这里我们需要准备外部的nfs存储空间，然后通过<a href="https://xxlaila.github.io/2019/09/24/%E5%88%A9%E7%94%A8NFS%E5%8A%A8%E6%80%81%E6%8F%90%E4%BE%9BKubernetes%E5%90%8E%E7%AB%AF%E5%AD%98%E5%82%A8%E5%8D%B7/" target="_blank" rel="noopener">pv</a>的模式进行挂载，数据存储到nfs服务器上，这样保障了es收集数据的可用性。</p><a id="more"></a><h3 id="创建存储介质"><a href="#创建存储介质" class="headerlink" title="创建存储介质"></a>创建存储介质</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat &gt; pvc.yaml &lt;&lt;EOF</span><br><span class="line">---</span><br><span class="line">apiVersion: storage.k8s.io/v1</span><br><span class="line">kind: StorageClass</span><br><span class="line">metadata:</span><br><span class="line">  name: es-nfs-data</span><br><span class="line">provisioner: fuseim.pri/ifs</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">$ kubectl apply -f pvc.yaml</span><br></pre></td></tr></table></figure><h3 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h3><ul><li><p>es-statefulset.yaml</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># RBAC authn and authz</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: elasticsearch-logging</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: elasticsearch-logging</span><br><span class="line">    kubernetes.io/cluster-service: <span class="string">"true"</span></span><br><span class="line">    addonmanager.kubernetes.io/mode: Reconcile</span><br><span class="line">---</span><br><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: elasticsearch-logging</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: elasticsearch-logging</span><br><span class="line">    kubernetes.io/cluster-service: <span class="string">"true"</span></span><br><span class="line">    addonmanager.kubernetes.io/mode: Reconcile</span><br><span class="line">rules:</span><br><span class="line">- apiGroups:</span><br><span class="line">  - <span class="string">""</span></span><br><span class="line">  resources:</span><br><span class="line">  - <span class="string">"services"</span></span><br><span class="line">  - <span class="string">"namespaces"</span></span><br><span class="line">  - <span class="string">"endpoints"</span></span><br><span class="line">  verbs:</span><br><span class="line">  - <span class="string">"get"</span></span><br><span class="line">---</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  name: elasticsearch-logging</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: elasticsearch-logging</span><br><span class="line">    kubernetes.io/cluster-service: <span class="string">"true"</span></span><br><span class="line">    addonmanager.kubernetes.io/mode: Reconcile</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: elasticsearch-logging</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  apiGroup: <span class="string">""</span></span><br><span class="line">roleRef:</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: elasticsearch-logging</span><br><span class="line">  apiGroup: <span class="string">""</span></span><br><span class="line">---</span><br><span class="line"><span class="comment"># Elasticsearch deployment itself</span></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: StatefulSet</span><br><span class="line">metadata:</span><br><span class="line">  name: elasticsearch-logging</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: elasticsearch-logging</span><br><span class="line">    version: v6.6.1</span><br><span class="line">    kubernetes.io/cluster-service: <span class="string">"true"</span></span><br><span class="line">    addonmanager.kubernetes.io/mode: Reconcile</span><br><span class="line">spec:</span><br><span class="line">  serviceName: elasticsearch-logging</span><br><span class="line">  replicas: 2</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      k8s-app: elasticsearch-logging</span><br><span class="line">      version: v6.6.1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        k8s-app: elasticsearch-logging</span><br><span class="line">        version: v6.6.1</span><br><span class="line">        kubernetes.io/cluster-service: <span class="string">"true"</span></span><br><span class="line">    spec:</span><br><span class="line">      serviceAccountName: elasticsearch-logging</span><br><span class="line">      containers:</span><br><span class="line">      - image: elasticsearch:6.6.1</span><br><span class="line">        name: elasticsearch-logging</span><br><span class="line">        resources:</span><br><span class="line">          <span class="comment"># need more cpu upon initialization, therefore burstable class</span></span><br><span class="line">          limits:</span><br><span class="line">            cpu: 1000m</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 100m</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 9200</span><br><span class="line">          name: db</span><br><span class="line">          protocol: TCP</span><br><span class="line">        - containerPort: 9300</span><br><span class="line">          name: transport</span><br><span class="line">          protocol: TCP</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: elasticsearch-logging</span><br><span class="line">          mountPath: /data</span><br><span class="line">        env:</span><br><span class="line">        - name: <span class="string">"NAMESPACE"</span></span><br><span class="line">          valueFrom:</span><br><span class="line">            fieldRef:</span><br><span class="line">              fieldPath: metadata.namespace</span><br><span class="line">      <span class="comment"># Elasticsearch requires vm.max_map_count to be at least 262144.</span></span><br><span class="line">      <span class="comment"># If your OS already sets up this number to a higher value, feel free</span></span><br><span class="line">      <span class="comment"># to remove this init container.</span></span><br><span class="line">      initContainers:</span><br><span class="line">      - image: alpine:3.6</span><br><span class="line">        <span class="built_in">command</span>: [<span class="string">"/sbin/sysctl"</span>, <span class="string">"-w"</span>, <span class="string">"vm.max_map_count=262144"</span>]</span><br><span class="line">        name: elasticsearch-logging-init</span><br><span class="line">        securityContext:</span><br><span class="line">          privileged: <span class="literal">true</span></span><br><span class="line">  volumeClaimTemplates:</span><br><span class="line">  - metadata:</span><br><span class="line">      name: elasticsearch-logging</span><br><span class="line">    spec:</span><br><span class="line">      accessModes: [ <span class="string">"ReadWriteMany"</span> ]</span><br><span class="line">      storageClassName: <span class="string">"es-nfs-data"</span></span><br><span class="line">      resources:</span><br><span class="line">        requests:</span><br><span class="line">          storage: 30Gi</span><br></pre></td></tr></table></figure></li><li><p>fluentd-es-ds.yaml</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: fluentd-es</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: fluentd-es</span><br><span class="line">    kubernetes.io/cluster-service: <span class="string">"true"</span></span><br><span class="line">    addonmanager.kubernetes.io/mode: Reconcile</span><br><span class="line">---</span><br><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: fluentd-es</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: fluentd-es</span><br><span class="line">    kubernetes.io/cluster-service: <span class="string">"true"</span></span><br><span class="line">    addonmanager.kubernetes.io/mode: Reconcile</span><br><span class="line">rules:</span><br><span class="line">- apiGroups:</span><br><span class="line">  - <span class="string">""</span></span><br><span class="line">  resources:</span><br><span class="line">  - <span class="string">"namespaces"</span></span><br><span class="line">  - <span class="string">"pods"</span></span><br><span class="line">  verbs:</span><br><span class="line">  - <span class="string">"get"</span></span><br><span class="line">  - <span class="string">"watch"</span></span><br><span class="line">  - <span class="string">"list"</span></span><br><span class="line">---</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: fluentd-es</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: fluentd-es</span><br><span class="line">    kubernetes.io/cluster-service: <span class="string">"true"</span></span><br><span class="line">    addonmanager.kubernetes.io/mode: Reconcile</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: fluentd-es</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  apiGroup: <span class="string">""</span></span><br><span class="line">roleRef:</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: fluentd-es</span><br><span class="line">  apiGroup: <span class="string">""</span></span><br><span class="line">---</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: DaemonSet</span><br><span class="line">metadata:</span><br><span class="line">  name: fluentd-es-v2.4.0</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: fluentd-es</span><br><span class="line">    version: v2.4.0</span><br><span class="line">    kubernetes.io/cluster-service: <span class="string">"true"</span></span><br><span class="line">    addonmanager.kubernetes.io/mode: Reconcile</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      k8s-app: fluentd-es</span><br><span class="line">      version: v2.4.0</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        k8s-app: fluentd-es</span><br><span class="line">        kubernetes.io/cluster-service: <span class="string">"true"</span></span><br><span class="line">        version: v2.4.0</span><br><span class="line">      <span class="comment"># This annotation ensures that fluentd does not get evicted if the node</span></span><br><span class="line">      <span class="comment"># supports critical pod annotation based priority scheme.</span></span><br><span class="line">      <span class="comment"># Note that this does not guarantee admission on the nodes (#40573).</span></span><br><span class="line">      annotations:</span><br><span class="line">        scheduler.alpha.kubernetes.io/critical-pod: <span class="string">''</span></span><br><span class="line">        seccomp.security.alpha.kubernetes.io/pod: <span class="string">'docker/default'</span></span><br><span class="line">    spec:</span><br><span class="line">      priorityClassName: system-node-critical</span><br><span class="line">      serviceAccountName: fluentd-es</span><br><span class="line">      containers:</span><br><span class="line">      - name: fluentd-es</span><br><span class="line">        image: docker.io/xxlaila/fluentd-elasticsearch:v2.4.0</span><br><span class="line">        env:</span><br><span class="line">        - name: FLUENTD_ARGS</span><br><span class="line">          value: --no-supervisor -q</span><br><span class="line">        resources:</span><br><span class="line">          limits:</span><br><span class="line">            memory: 500Mi</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 100m</span><br><span class="line">            memory: 200Mi</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: varlog</span><br><span class="line">          mountPath: /var/<span class="built_in">log</span></span><br><span class="line">        - name: varlibdockercontainers</span><br><span class="line">          mountPath: /var/lib/docker/containers</span><br><span class="line">          readOnly: <span class="literal">true</span></span><br><span class="line">        - name: config-volume</span><br><span class="line">          mountPath: /etc/fluent/config.d</span><br><span class="line">      terminationGracePeriodSeconds: 30</span><br><span class="line">      volumes:</span><br><span class="line">      - name: varlog</span><br><span class="line">        hostPath:</span><br><span class="line">          path: /var/<span class="built_in">log</span></span><br><span class="line">      - name: varlibdockercontainers</span><br><span class="line">        hostPath:</span><br><span class="line">          path: /var/lib/docker/containers</span><br><span class="line">      - name: config-volume</span><br><span class="line">        configMap:</span><br><span class="line">          name: fluentd-es-config-v0.2.0</span><br></pre></td></tr></table></figure></li><li><p>kibana-deployment.yaml<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;注释里面的两行配置,不注释的话，打开kibana的时候会提示<code>kibana {&quot;statusCode&quot;:404,&quot;error&quot;:&quot;Not Found&quot;,&quot;message&quot;:&quot;Not Found&quot;}</code>,参考<a href="https://github.com/kubernetes-sigs/kubespray/issues/3322" target="_blank" rel="noopener">解决方案</a>,注释配置如下:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">- name: SERVER_BASEPATH</span><br><span class="line">  value: /api/v1/namespaces/kube-system/services/kibana-logging/proxy</span><br></pre></td></tr></table></figure></li></ul><h4 id="执行创建"><a href="#执行创建" class="headerlink" title="执行创建"></a>执行创建</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl apply -f ./</span><br></pre></td></tr></table></figure><h4 id="查看创建"><a href="#查看创建" class="headerlink" title="查看创建"></a>查看创建</h4><ul><li><p>查看pod</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl get pods -n kube-system |egrep <span class="string">"kibana|elasticsearch|fluentd"</span></span><br><span class="line">elasticsearch-logging-0                       1/1     Running   0          65m</span><br><span class="line">elasticsearch-logging-1                       1/1     Running   0          61m</span><br><span class="line">fluentd-es-v2.4.0-4fp28                       1/1     Running   0          30m</span><br><span class="line">fluentd-es-v2.4.0-b7k67                       1/1     Running   0          30m</span><br><span class="line">fluentd-es-v2.4.0-f8jzp                       1/1     Running   0          30m</span><br><span class="line">fluentd-es-v2.4.0-shwzm                       1/1     Running   0          30m</span><br><span class="line">fluentd-es-v2.4.0-ww8r8                       1/1     Running   0          30m</span><br><span class="line">kibana-logging-57b55f58bc-xh5lp               1/1     Running   0          6m35s</span><br></pre></td></tr></table></figure></li><li><p>查看service</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl get svc -n kube-system |egrep <span class="string">"kibana|elasticsearch"</span></span><br><span class="line">elasticsearch-logging     ClusterIP   10.254.30.110    &lt;none&gt;        9200/TCP                 9s</span><br><span class="line">kibana-logging            ClusterIP   10.254.188.5     &lt;none&gt;        5601/TCP                 16h</span><br></pre></td></tr></table></figure></li><li><p>查看pv，pvc</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$  kubectl get pv,pvc -n kube-system</span><br><span class="line">NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                                                       STORAGECLASS   REASON   AGE</span><br><span class="line">persistentvolume/pvc-65fdd14e-dffc-11e9-bc90-fa163e5af833   30Gi       RWX            Delete           Bound    kube-system/elasticsearch-logging-elasticsearch-logging-0   es-nfs-data             21m</span><br><span class="line">persistentvolume/pvc-fe818f55-dffc-11e9-bc90-fa163e5af833   30Gi       RWX            Delete           Bound    kube-system/elasticsearch-logging-elasticsearch-logging-1   es-nfs-data             16m</span><br><span class="line"></span><br><span class="line">NAME                                                                  STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">persistentvolumeclaim/elasticsearch-logging-elasticsearch-logging-0   Bound    pvc-65fdd14e-dffc-11e9-bc90-fa163e5af833   30Gi       RWX            es-nfs-data    21m</span><br><span class="line">persistentvolumeclaim/elasticsearch-logging-elasticsearch-logging-1   Bound    pvc-fe818f55-dffc-11e9-bc90-fa163e5af833   30Gi       RWX            es-nfs-data    17m</span><br></pre></td></tr></table></figure></li></ul><h3 id="创建web访问"><a href="#创建web访问" class="headerlink" title="创建web访问"></a>创建web访问</h3><ul><li><p>kibana-Ingress.yaml</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat &gt; kibana-Ingress.yaml &lt;&lt;EOF</span><br><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: kibana-web-ui</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  annotations:</span><br><span class="line">    kubernetes.io/ingress.class: traefik</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: kibana.xxlaila.cn</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: /</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: kibana-logging</span><br><span class="line">          servicePort: 5601</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></li><li><p>es-Ingress.yaml</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat &gt; es-Ingress &lt;&lt;EOF</span><br><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: es-web-ui</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  annotations:</span><br><span class="line">    kubernetes.io/ingress.class: traefik</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: es.xxlaila.cn</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: /</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: elasticsearch-logging</span><br><span class="line">          servicePort: 9200</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></li><li><p>执行创建</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl apply -f es-Ingress.yaml kibana-Ingress.yaml</span><br></pre></td></tr></table></figure></li><li><p>在浏览器访问es<br><img src="https://img.xxlaila.cn/1569462606884.jpg" alt="img"></p></li><li><p>浏览器访问kibana<br><img src="https://img.xxlaila.cn/1569464839630.jpg" alt="img"><br>建立索引，默认的索引是根据天来自动创建在es里面，这里我是在kibana里面是根据月来却分的<br><img src="https://img.xxlaila.cn/1569464950776.jpg" alt="img"></p></li></ul><!-- rebuild by neat -->]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>efk</tag>
      </tags>
  </entry>
  <entry>
    <title>网络状态监控</title>
    <url>/2019/09/25/%E7%BD%91%E7%BB%9C%E7%8A%B6%E6%80%81%E7%9B%91%E6%8E%A7/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:23 GMT+0800 (China Standard Time) --><h3 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;监控IDC机房网络质量情况，本地区到其他地区，其他地区到本节点，或者各省市时间网络、运营商网络状态，监视网络性能，包括常规的 ping，用 fping、echoping、tracert 监视 www 服务器性能，监视 dns 查询性能，监视 ssh 性能等。底层也是 rrdtool 做支持，特点是画的图非常漂亮，网络丢包和延迟用颜色和阴影来表示。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Smokeping。最新版本的 Smokeping 支持多个节点的检测结果从一个图上画出来</p><a id="more"></a><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><h4 id="安装yum源"><a href="#安装yum源" class="headerlink" title="安装yum源"></a>安装yum源</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># rpmforge-release-0.5.3-1.el6.rf.x86_64.rpm               </span></span><br><span class="line"><span class="comment"># rpm –Uvh http://mirrors.neusoft.edu.cn/epel/6/i386/epel-release-6-8.noarch.rpm</span></span><br></pre></td></tr></table></figure><h4 id="安装依赖包"><a href="#安装依赖包" class="headerlink" title="安装依赖包"></a>安装依赖包</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># yum –y install perl perl-Net-Telnet perl-Net-DNS perl-LDAP perl-libwww-perl perl-RadiusPerl perl-IO-Socket-SSL perl-Socket6 perl-CGI-SpeedyCGI perl-FCGI perl-CGI-SpeedCGI perl-Time-HiRes perl-ExtUtils-MakeMaker perl-RRD-Simple rrdtool rrdtool-perl curl fping echo</span></span><br><span class="line">ping  httpd httpd-devel gcc make  wget libxml2-devel libpng-devel glib pango pango-devel freetype freetype-devel fontconfig cairo cairo-devel libart_lgpl libart_lgpl-devel mod_fastcgi</span><br></pre></td></tr></table></figure><h3 id="安装smokeping"><a href="#安装smokeping" class="headerlink" title="安装smokeping"></a>安装smokeping</h3><h4 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># wget http://oss.oetiker.ch/smokeping/pub/smokeping-2.6.11.tar.gz 这里下载的最新版</span></span><br></pre></td></tr></table></figure><h4 id="安装FCGI"><a href="#安装FCGI" class="headerlink" title="安装FCGI"></a>安装FCGI</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># tar zxf CGI-4.33.tar.gz</span></span><br><span class="line"><span class="comment"># cd CGI-4.33</span></span><br><span class="line"><span class="comment"># perl Makefile.PL</span></span><br><span class="line"><span class="comment"># make &amp;&amp; make install</span></span><br></pre></td></tr></table></figure><h4 id="安装Config-Grammar"><a href="#安装Config-Grammar" class="headerlink" title="安装Config-Grammar"></a>安装Config-Grammar</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># tar zxf Config-Grammar-1.10.tar.gz</span></span><br><span class="line"><span class="comment"># cd Config-Grammar-1.10</span></span><br><span class="line"><span class="comment"># perl Makefile.PL</span></span><br><span class="line"><span class="comment"># make &amp;&amp; make install</span></span><br></pre></td></tr></table></figure><h4 id="安装ExtUtils-MakeMaker"><a href="#安装ExtUtils-MakeMaker" class="headerlink" title="安装ExtUtils-MakeMaker"></a>安装ExtUtils-MakeMaker</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># tar zxf ExtUtils-MakeMaker-7.24.tar.gz</span></span><br><span class="line"><span class="comment"># cd ExtUtils-MakeMaker</span></span><br><span class="line"><span class="comment"># perl Makefile.PL</span></span><br><span class="line"><span class="comment"># make &amp;&amp; make install</span></span><br></pre></td></tr></table></figure><h4 id="安装Simple"><a href="#安装Simple" class="headerlink" title="安装Simple"></a>安装Simple</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># tar zxf Test-Simple-1.302056.tar.gz</span></span><br><span class="line"><span class="comment"># cd Test-Simple-1.302056</span></span><br><span class="line"><span class="comment"># perl Makefile.PL</span></span><br><span class="line"><span class="comment"># make &amp;&amp; make install</span></span><br><span class="line">`</span><br></pre></td></tr></table></figure><h4 id="安装Net-OpenSSH"><a href="#安装Net-OpenSSH" class="headerlink" title="安装Net-OpenSSH"></a>安装Net-OpenSSH</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># tar zxf Net-OpenSSH-0.73.tar.gz</span></span><br><span class="line"><span class="comment"># cd Net-OpenSSH-0.73</span></span><br><span class="line"><span class="comment"># perl Makefile.PL</span></span><br><span class="line"><span class="comment"># make &amp;&amp; make install</span></span><br></pre></td></tr></table></figure><h4 id="安装Net-SNMP"><a href="#安装Net-SNMP" class="headerlink" title="安装Net-SNMP"></a>安装Net-SNMP</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># tar Net-SNMP-v6.0.1.tar.gz</span></span><br><span class="line"><span class="comment"># cd Net-SNMP-v6.0.1</span></span><br><span class="line"><span class="comment"># perl Makefile.PL</span></span><br><span class="line"><span class="comment"># make &amp;&amp; make install</span></span><br></pre></td></tr></table></figure><h4 id="安装perl-ldap"><a href="#安装perl-ldap" class="headerlink" title="安装perl-ldap"></a>安装perl-ldap</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># tar zxf perl-ldap-0.65.tar.gz</span></span><br><span class="line"><span class="comment"># cd perl-ldap-0.65</span></span><br><span class="line"><span class="comment"># ./install-nomake</span></span><br></pre></td></tr></table></figure><h4 id="安装Net-DNS"><a href="#安装Net-DNS" class="headerlink" title="安装Net-DNS"></a>安装Net-DNS</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># tar zxf Net-DNS-1.06.tar.gz</span></span><br><span class="line"><span class="comment"># cd Net-DNS-1.06</span></span><br><span class="line"><span class="comment"># perl Makefile.PL</span></span><br><span class="line"><span class="comment"># make &amp;&amp; make install</span></span><br></pre></td></tr></table></figure><h4 id="安装IO-Tty"><a href="#安装IO-Tty" class="headerlink" title="安装IO-Tty"></a>安装IO-Tty</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># tar IO-Tty-1.12.tar.gz</span></span><br><span class="line"><span class="comment"># cd IO-Tty-1.12</span></span><br><span class="line"><span class="comment"># perl Makefile.PL</span></span><br><span class="line"><span class="comment"># make &amp;&amp; make install</span></span><br></pre></td></tr></table></figure><h4 id="安装libwww-perl"><a href="#安装libwww-perl" class="headerlink" title="安装libwww-perl"></a>安装libwww-perl</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># tar zxf libwww-perl-6.15.tar.gz</span></span><br><span class="line"><span class="comment"># cd libwww-perl-6.15</span></span><br><span class="line"><span class="comment"># perl Makefile.PL</span></span><br><span class="line"><span class="comment"># make &amp;&amp; make install</span></span><br></pre></td></tr></table></figure><h4 id="安装smokeping-1"><a href="#安装smokeping-1" class="headerlink" title="安装smokeping"></a>安装smokeping</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># tar zxf smokeping-2.6.11.tar.gz</span></span><br><span class="line"><span class="comment"># cd smokeping-2.6.11</span></span><br><span class="line"><span class="comment"># ./configure --prefix=/usr/local/smokeping</span></span><br><span class="line"><span class="comment"># /usr/bin/gmake install</span></span><br></pre></td></tr></table></figure><p>上面是手动安装，针对网络不能翻墙。也可以采取smokeping一键安装的方式进行安装</p><h3 id="smokeping一键安装"><a href="#smokeping一键安装" class="headerlink" title="smokeping一键安装"></a>smokeping一键安装</h3><h4 id="安装smokeping-2"><a href="#安装smokeping-2" class="headerlink" title="安装smokeping"></a>安装smokeping</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># tar zxf smokeping-2.6.11.tar.gz</span></span><br><span class="line"><span class="comment"># cd smokeping-2.6.11</span></span><br><span class="line"><span class="comment"># ./setup/build-perl-modules.sh /usr/local/smokeping/thirdparty</span></span><br><span class="line"><span class="comment"># ./configure --prefix=/usr/local/smokeping</span></span><br><span class="line"><span class="comment"># /usr/bin/gmake install</span></span><br></pre></td></tr></table></figure><h3 id="配置smkeping"><a href="#配置smkeping" class="headerlink" title="配置smkeping"></a>配置smkeping</h3><h4 id="创建cache、data、var目录"><a href="#创建cache、data、var目录" class="headerlink" title="创建cache、data、var目录"></a>创建cache、data、var目录</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cd /usr/local/smokeping/</span></span><br><span class="line"><span class="comment"># mkdir &#123;cache,data,var&#125;</span></span><br></pre></td></tr></table></figure><h4 id="创建日志文件"><a href="#创建日志文件" class="headerlink" title="创建日志文件"></a>创建日志文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># touch /var/log/smokeping.log</span></span><br></pre></td></tr></table></figure><h4 id="赋权限"><a href="#赋权限" class="headerlink" title="赋权限"></a>赋权限</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># chown apache:apache cache/ data/ var/</span></span><br><span class="line"><span class="comment"># chown  apache:apache /var/log/smokeping.log</span></span><br><span class="line"><span class="comment"># chmod 755 cache/ data/ var/    #这里也要赋权限，会影响图片无法加载</span></span><br></pre></td></tr></table></figure><h4 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># /usr/local/smokeping/htdocs</span></span><br><span class="line"><span class="comment"># cp -arp smokeping.fcgi.dist smokeping.fcgi</span></span><br><span class="line"><span class="comment"># cd ../etc/</span></span><br><span class="line"><span class="comment"># cp -arp config.dist config</span></span><br><span class="line"><span class="comment"># chmod 600 /usr/local/smokeping/etc/smokeping_secrets.dist</span></span><br><span class="line"><span class="comment"># vim config</span></span><br><span class="line">*** General ***</span><br><span class="line">owner    = Peter Random</span><br><span class="line">contact  = some@address.nowhere</span><br><span class="line">mailhost = my.mail.host</span><br><span class="line">sendmail = /usr/sbin/sendmail</span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> do not put the Image Cache below cgi-bin</span></span><br><span class="line"><span class="comment"># since all files under cgi-bin will be executed ... this is not</span></span><br><span class="line"><span class="comment"># good for images.</span></span><br><span class="line">imgcache = /usr/<span class="built_in">local</span>/smokeping/cache</span><br><span class="line">imgurl   = http://172.16.1.100/cache                                                      <span class="comment">#这里如果不配置正确，会影响后面出图，这里一个坑</span></span><br><span class="line">datadir  = /usr/<span class="built_in">local</span>/smokeping/data</span><br><span class="line">piddir  = /usr/<span class="built_in">local</span>/smokeping/var</span><br><span class="line">cgiurl   = http://172.16.1.100/smokeping/smokeping.cgi</span><br><span class="line"><span class="comment">#cgiurl   = http://some.url/smokeping.cgi</span></span><br><span class="line">smokemail = /usr/<span class="built_in">local</span>/smokeping/etc/smokemail.dist</span><br><span class="line">tmail = /usr/<span class="built_in">local</span>/smokeping/etc/tmail.dist</span><br><span class="line"><span class="comment"># specify this to get syslog logging</span></span><br><span class="line">syslogfacility = local0</span><br><span class="line"><span class="comment"># each probe is now run in its own process</span></span><br><span class="line"><span class="comment"># disable this to revert to the old behaviour</span></span><br><span class="line"><span class="comment"># concurrentprobes = no</span></span><br><span class="line">*** Alerts ***</span><br><span class="line">to = alertee@address.somewhere</span><br><span class="line">from = smokealert@company.xy</span><br><span class="line">+someloss</span><br><span class="line"><span class="built_in">type</span> = loss</span><br><span class="line"><span class="comment"># in percent</span></span><br><span class="line">pattern = &gt;0%,*12*,&gt;0%,*12*,&gt;0%</span><br><span class="line">comment = loss 3 <span class="built_in">times</span>  <span class="keyword">in</span> a row</span><br><span class="line">*** Database ***</span><br><span class="line">step     = 60                                              <span class="comment">#检测时间，默认300</span></span><br><span class="line">pings    = 20</span><br></pre></td></tr></table></figure><p>配置文件上述修改带有注视部分，其他参数参考官方，而且都能看懂。后面有很多配置不全部贴出来</p><h3 id="配置apache"><a href="#配置apache" class="headerlink" title="配置apache"></a>配置apache</h3><h4 id="配置httpd-conf"><a href="#配置httpd-conf" class="headerlink" title="配置httpd.conf"></a>配置httpd.conf</h4><p>在DocumentRoot “/var/www/html”这行增加如下内容</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># vim /etc/httpd/conf/httpd.conf</span></span><br><span class="line">Alias /cache <span class="string">"/usr/local/smokeping/cache"</span></span><br><span class="line">Alias /cropper <span class="string">"/usr/local/smokeping/htdocs/cropper"</span></span><br><span class="line">Alias /smokeping <span class="string">"/usr/local/smokeping/htdocs/smokeping.fcgi"</span></span><br><span class="line">&lt;Directory <span class="string">"/usr/local/smokeping"</span>&gt;</span><br><span class="line">        AllowOverride None</span><br><span class="line">        Options All</span><br><span class="line">        AddHandler cgi-script .fcgi .cgi</span><br><span class="line">        Order allow,deny</span><br><span class="line">        Allow from all</span><br><span class="line">        AuthName <span class="string">"Smokeping"</span></span><br><span class="line">        AuthType Basic</span><br><span class="line">        AuthUserFile /usr/<span class="built_in">local</span>/smokeping/htdocs/htpasswd</span><br><span class="line">        Require valid-user</span><br><span class="line">        DirectoryIndex smokeping.fcgi</span><br><span class="line">&lt;/Directory&gt;</span><br></pre></td></tr></table></figure><h4 id="apache登录认证"><a href="#apache登录认证" class="headerlink" title="apache登录认证"></a>apache登录认证</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># /usr/local/smokeping/htdocs</span></span><br><span class="line"><span class="comment"># htpasswd -c /usr/local/smokeping/htdocs/htpasswd admin                   #回车设置admin账户的密码</span></span><br></pre></td></tr></table></figure><h4 id="安装网页支持的中文字体"><a href="#安装网页支持的中文字体" class="headerlink" title="安装网页支持的中文字体"></a>安装网页支持的中文字体</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># yum -y install wqy-zenhei-fonts.noarch</span></span><br></pre></td></tr></table></figure><h4 id="smokeping开机脚本"><a href="#smokeping开机脚本" class="headerlink" title="smokeping开机脚本"></a>smokeping开机脚本</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># vim /etc/init.d/smokeping</span></span><br><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">PIDFILE=/usr/<span class="built_in">local</span>/smokeping/var/smokeping.pid</span><br><span class="line">SMOKEPING=/usr/<span class="built_in">local</span>/smokeping/bin/smokeping</span><br><span class="line">ERROR=0</span><br><span class="line">RUNNING=0</span><br><span class="line">ARGV=<span class="string">"<span class="variable">$@</span>"</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="string">"x<span class="variable">$ARGV</span>"</span> = <span class="string">"x"</span> ] ; <span class="keyword">then</span></span><br><span class="line">ARGS=<span class="built_in">help</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">for</span> ARG <span class="keyword">in</span> <span class="variable">$@</span> <span class="variable">$ARGS</span></span><br><span class="line"><span class="keyword">do</span></span><br><span class="line"><span class="keyword">if</span> [ -f <span class="variable">$PIDFILE</span> ] ; <span class="keyword">then</span></span><br><span class="line">PID=`cat <span class="variable">$PIDFILE</span>`</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">kill</span> -0 <span class="variable">$PID</span> 2&gt;/dev/null ; <span class="keyword">then</span></span><br><span class="line"><span class="comment"># smokeping is running</span></span><br><span class="line">RUNNING=1</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="comment"># smokeping not running but PID file exists =&gt; delete PID file</span></span><br><span class="line">rm -f <span class="variable">$PIDFILE</span></span><br><span class="line">RUNNING=0</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="comment"># smokeping (no pid file) not running</span></span><br><span class="line">RUNNING=0</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">case</span> <span class="variable">$ARG</span> <span class="keyword">in</span></span><br><span class="line">start)</span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$RUNNING</span> -eq 0 ] ; <span class="keyword">then</span></span><br><span class="line"><span class="keyword">if</span> <span class="variable">$SMOKEPING</span> &gt; /dev/null; <span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"<span class="variable">$0</span> <span class="variable">$ARG</span>: smokeping started"</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"<span class="variable">$0</span> <span class="variable">$ARG</span>: smokeping could not be started"</span></span><br><span class="line">ERROR=1</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"<span class="variable">$0</span> <span class="variable">$ARG</span>: smokeping is running with PID <span class="variable">$PID</span>"</span></span><br><span class="line">ERROR=2</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line">;;</span><br><span class="line">stop)</span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$RUNNING</span> -eq 1 ] ; <span class="keyword">then</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">kill</span> <span class="variable">$PID</span> ; <span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"<span class="variable">$0</span> <span class="variable">$ARG</span>: smokeping (<span class="variable">$PID</span>) stopped"</span></span><br><span class="line">rm <span class="variable">$PIDFILE</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"<span class="variable">$0</span> <span class="variable">$ARG</span>: smokeping could not be stopped"</span></span><br><span class="line">ERROR=3</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"<span class="variable">$0</span> <span class="variable">$ARG</span>: smokeping not running"</span></span><br><span class="line">ERROR=4</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line">;;</span><br><span class="line">restart)</span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$RUNNING</span> -eq 1 ] ; <span class="keyword">then</span></span><br><span class="line"><span class="keyword">if</span> <span class="variable">$SMOKEPING</span> --restart &gt; /dev/null; <span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"<span class="variable">$0</span> <span class="variable">$ARG</span>: smokeping restarted"</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"<span class="variable">$0</span> <span class="variable">$ARG</span>: smokeping could not be started"</span></span><br><span class="line">ERROR=5</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="variable">$0</span> start</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line">;;</span><br><span class="line">strace_debug)</span><br><span class="line">rm -f /tmp/strace_smokeping</span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$RUNNING</span> -eq 1 ] ; <span class="keyword">then</span></span><br><span class="line"><span class="keyword">if</span> strace -o/tmp/strace_smokeping <span class="variable">$SMOKEPING</span> --restart &gt;/dev/null; <span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"<span class="variable">$0</span> <span class="variable">$ARG</span>: smokeping restarted with strace debug in /tmp/strace_smokeping"</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"<span class="variable">$0</span> <span class="variable">$ARG</span>: smokeping strace debug could not be started"</span></span><br><span class="line">ERROR=6</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="keyword">if</span> strace -o/tmp/strace_smokeping <span class="variable">$SMOKEPING</span> &gt;/dev/null; <span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"<span class="variable">$0</span> <span class="variable">$ARG</span>: smokeping started with strace debug in /tmp/strace_smokeping"</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"<span class="variable">$0</span> <span class="variable">$ARG</span>: smokeping strace debug could not be started"</span></span><br><span class="line">ERROR=7</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line">;;</span><br><span class="line">status)</span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$RUNNING</span> -eq 1 ] ; <span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"<span class="variable">$0</span> <span class="variable">$ARG</span>: smokeping is running with PID (<span class="variable">$PID</span>)"</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"<span class="variable">$0</span> <span class="variable">$ARG</span>: smokeping is not running"</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line">;;</span><br><span class="line">*)</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"usage: <span class="variable">$0</span> (start|stop|restart|status|strace_debug|help)"</span></span><br><span class="line">cat</span><br><span class="line">start - start smokeping</span><br><span class="line">stop - stop smokeping</span><br><span class="line">restart - restart smokeping <span class="keyword">if</span> running or start <span class="keyword">if</span> not running</span><br><span class="line">status - show status <span class="keyword">if</span> smokeping is running or not</span><br><span class="line"><span class="built_in">help</span> - this screen</span><br><span class="line">EOF</span><br><span class="line">;;</span><br><span class="line"><span class="keyword">esac</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"><span class="built_in">exit</span> <span class="variable">$ERROR</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># chmod +x /etc/init.d/smokeping</span></span><br></pre></td></tr></table></figure><h4 id="启动服务"><a href="#启动服务" class="headerlink" title="启动服务"></a>启动服务</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># service httpd start</span></span><br><span class="line"><span class="comment"># /etc/init.d/smokeping start</span></span><br></pre></td></tr></table></figure><p>打开浏览器测试http://{ip}/smokeping 会提示输入用户和密码<br><img src="https://img.xxlaila.cn/74D2C8DE-129F-4219-87C5-D6A771D19484.png" alt="img"><br><img src="https://img.xxlaila.cn/91D9FA70-65B1-4752-8F15-68A158E72A49.png" alt="img"></p><h4 id="配置文件添加"><a href="#配置文件添加" class="headerlink" title="配置文件添加"></a>配置文件添加</h4><p>配置文件添介绍，在配置文件里面+表示一级++表示二级+++三级<br>本次添加的内容</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">+ Other</span><br><span class="line">menu = 其他网络监控</span><br><span class="line">title = 其他所有网络监控列表</span><br><span class="line">++ dianxin</span><br><span class="line">menu = 电信网络监控</span><br><span class="line">title = 电信网络监控列表</span><br><span class="line">host = /Other/dianxin/dianxin-hlj /Other/dianxin/dianxin-gd /Other/dianxin/dianxin-gs /Other/dianxin/dianxin-sh /Other/dianxin/dianxin-sc /Other/dianxin/dianxin-cq /Other/dianxin/dianxin-gz /Other/dianxin/dianxin-ln /Other/dianxin/dianxin-zj /Other/dianxin/dianxin-sd /Other/dianxin/dianxin-hib /Other/dianxin/dianxin-ah /Other/dianxin/dianxin-hb /Other/dianxin/dianxin-jl /Other/dianxin/dianxin-jx</span><br><span class="line">+++ dianxin-hlj</span><br><span class="line">menu = 黑龙江电信</span><br><span class="line">title = 黑龙江电信</span><br><span class="line">alerts = someloss</span><br><span class="line"><span class="comment">#slaves = boomer slave2</span></span><br><span class="line">host = 219.150.32.132</span><br><span class="line">+++ dianxin-gd</span><br><span class="line">menu = 广东电信</span><br><span class="line">title = 广东电信</span><br><span class="line">alerts = someloss</span><br><span class="line"><span class="comment">#slaves = boomer slave2</span></span><br><span class="line">host = 202.96.134.133</span><br><span class="line">+++ dianxin-gs</span><br><span class="line">menu = 甘肃电信</span><br><span class="line">title = 甘肃电信</span><br><span class="line">alerts = someloss</span><br><span class="line"><span class="comment">#slaves = boomer slave2</span></span><br><span class="line">host = 202.100.64.68</span><br><span class="line">+++ dianxin-sh</span><br><span class="line">menu = 上海电信</span><br><span class="line">title = 上海电信</span><br><span class="line">alerts = someloss</span><br><span class="line"><span class="comment">#slaves = boomer slave2</span></span><br><span class="line">host = 202.96.209.5</span><br><span class="line">+++ dianxin-sc</span><br><span class="line">menu = 四川电信</span><br><span class="line">title = 四川电信</span><br><span class="line">alerts = someloss</span><br><span class="line">host = 218.6.145.111</span><br><span class="line">+++ dianxin-cq</span><br><span class="line">menu = 重庆电信</span><br><span class="line">title = 重庆电信</span><br><span class="line">alerts = someloss</span><br><span class="line">host = 61.128.128.68</span><br><span class="line">+++ dianxin-gz</span><br><span class="line">menu = 贵州电信</span><br><span class="line">title = 贵州电信</span><br><span class="line">alerts = someloss</span><br><span class="line">host = 202.98.192.68</span><br><span class="line">+++ dianxin-ln</span><br><span class="line">menu = 辽宁电信</span><br><span class="line">title = 辽宁电信</span><br><span class="line">alerts = someloss</span><br><span class="line">host = 219.149.6.99</span><br><span class="line">+++ dianxin-zj</span><br><span class="line">menu = 浙江电信</span><br><span class="line">title = 浙江电信</span><br><span class="line">alerts = someloss</span><br><span class="line">host = 202.96.96.68</span><br><span class="line">+++ dianxin-sd</span><br><span class="line">menu = 山东电信</span><br><span class="line">title = 山东电信</span><br><span class="line">alerts = someloss</span><br><span class="line">host = 222.173.95.53</span><br><span class="line">+++ dianxin-hib</span><br><span class="line">menu = 湖北电信</span><br><span class="line">title = 湖北电信</span><br><span class="line">alerts = someloss</span><br><span class="line">host = 202.103.0.68</span><br><span class="line">+++ dianxin-ah</span><br><span class="line">menu = 安徽电信</span><br><span class="line">title = 安徽电信</span><br><span class="line">alerts = someloss</span><br><span class="line">host = 220.178.75.134</span><br><span class="line">+++ dianxin-hb</span><br><span class="line">menu = 河北电信</span><br><span class="line">title = 河北电信</span><br><span class="line">alerts = someloss</span><br><span class="line">host = 202.99.160.68</span><br><span class="line">+++ dianxin-jl</span><br><span class="line">menu = 吉林电信</span><br><span class="line">title = 吉林电信</span><br><span class="line">alerts = someloss</span><br><span class="line">host =  219.149.194.55</span><br><span class="line">+++ dianxin-jx</span><br><span class="line">menu = 江西电信</span><br><span class="line">title = 江西电信</span><br><span class="line">alerts = someloss</span><br><span class="line">host = 202.101.224.68</span><br><span class="line"><span class="comment">#+++ dianxin-multi</span></span><br><span class="line"><span class="comment">#menu = 多个电信网络监控列表</span></span><br><span class="line"><span class="comment">#title = 多个电信网络监控列表</span></span><br><span class="line"><span class="comment">#alerts = someloss</span></span><br><span class="line"><span class="comment">#slaves = boomer slave2</span></span><br><span class="line"><span class="comment">#host = /Other/dianxin/dianxin-hlj /Other/dianxin/dianxin-gd /Other/dianxin/dianxin-gs /Other/dianxin/dianxin-sh</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">++ liantong</span><br><span class="line">menu = 联通网络监控</span><br><span class="line">title = 联通网络监控列表</span><br><span class="line">host = /Other/liantong/liantong-hlj /Other/liantong/liantong-gd /Other/liantong/liantong-gs /Other/liantong/liantong-sh /Other/liantong/liantong-sc /Other/liantong/liantong-cq /Other/liantong/liantong-gz /Other/liantong/liantong-ln /Other/liantong/liantong-zj /Other/liantong/liantong-sd /Other/liantong/liantong-hib /Other/liantong/liantong-ah /Other/liantong/liantong-hb /Other/liantong/liantong-jl /Other/liantong/liantong-jx</span><br><span class="line">+++ liantong-hlj</span><br><span class="line">menu = 黑龙江联通</span><br><span class="line">title = 黑龙江联通</span><br><span class="line">alerts = someloss</span><br><span class="line"><span class="comment">#slaves = boomer slave2</span></span><br><span class="line">host = 202.97.224.68</span><br><span class="line">+++ liantong-gd</span><br><span class="line">menu = 广东联通</span><br><span class="line">title = 广东联通</span><br><span class="line">alerts = someloss</span><br><span class="line"><span class="comment">#slaves = boomer slave2</span></span><br><span class="line">host = 221.4.66.66</span><br><span class="line">+++ liantong-gs</span><br><span class="line">menu = 甘肃联通</span><br><span class="line">title = 甘肃联通</span><br><span class="line">alerts = someloss</span><br><span class="line"><span class="comment">#slaves = boomer slave2</span></span><br><span class="line">host = 221.7.34.10</span><br><span class="line">+++ liantong-sh</span><br><span class="line">menu = 上海联通</span><br><span class="line">title = 上海联通</span><br><span class="line">alerts = someloss</span><br><span class="line"><span class="comment">#slaves = boomer slave2</span></span><br><span class="line">host = 210.22.70.3</span><br><span class="line">+++ liantong-sc</span><br><span class="line">menu = 四川联通</span><br><span class="line">title = 四川联通</span><br><span class="line">alerts = someloss</span><br><span class="line">host = 119.6.6.6</span><br><span class="line">+++ liantong-cq</span><br><span class="line">menu = 重庆联通</span><br><span class="line">title = 重庆联通</span><br><span class="line">alerts = someloss</span><br><span class="line">host = 221.7.92.98</span><br><span class="line">+++ liantong-gz</span><br><span class="line">menu = 贵州联通</span><br><span class="line">title = 贵州联通</span><br><span class="line">alerts = someloss</span><br><span class="line">host = 221.13.30.242</span><br><span class="line">+++ liantong-ln</span><br><span class="line">menu = 辽宁联通</span><br><span class="line">title = 辽宁联通</span><br><span class="line">alerts = someloss</span><br><span class="line">host = 124.161.97.234</span><br><span class="line">+++ liantong-zj</span><br><span class="line">menu = 浙江联通</span><br><span class="line">title = 浙江联通</span><br><span class="line">alerts = someloss</span><br><span class="line">host = 221.12.33.227</span><br><span class="line">+++ liantong-sd</span><br><span class="line">menu = 山东联通</span><br><span class="line">title = 山东联通</span><br><span class="line">alerts = someloss</span><br><span class="line">host = 202.102.152.3</span><br><span class="line">+++ liantong-hib</span><br><span class="line">menu = 湖北联通</span><br><span class="line">title = 湖北联通</span><br><span class="line">alerts = someloss</span><br><span class="line">host = 218.104.111.114</span><br><span class="line">+++ liantong-ah</span><br><span class="line">menu = 安徽联通</span><br><span class="line">title = 安徽联通</span><br><span class="line">alerts = someloss</span><br><span class="line">host = 211.91.88.129</span><br><span class="line">+++ liantong-hb</span><br><span class="line">menu = 河北联通</span><br><span class="line">title = 河北联通</span><br><span class="line">alerts = someloss</span><br><span class="line">host = 202.99.160.68</span><br><span class="line">+++ liantong-jl</span><br><span class="line">menu = 吉林联通</span><br><span class="line">title = 吉林联通</span><br><span class="line">alerts = someloss</span><br><span class="line">host = 202.98.5.6</span><br><span class="line">+++ liantong-jx</span><br><span class="line">menu = 江西联通</span><br><span class="line">title = 江西联通</span><br><span class="line">alerts = someloss</span><br><span class="line">host = 220.248.192.12</span><br><span class="line"><span class="comment">#+++ liantong-multi</span></span><br><span class="line"><span class="comment">#menu = 多个联通网络监控列表</span></span><br><span class="line"><span class="comment">#title = 多个联通网络监控列表</span></span><br><span class="line"><span class="comment">#alerts = someloss</span></span><br><span class="line"><span class="comment">#slaves = boomer slave2</span></span><br><span class="line"><span class="comment">#host = /Other/liantong/liantong-hlj /Other/liantong/liantong-gd /Other/liantong/liantong-gs /Other/liantong/liantong-sh</span></span><br><span class="line">++ yidong</span><br><span class="line">menu = 移动网络监控</span><br><span class="line">title = 移动网络监控列表</span><br><span class="line">host = /Other/yidong/yidong-hlj /Other/yidong/yidong-gd /Other/yidong/yidong-gs /Other/yidong/yidong-sh /Other/yidong/yidong-sc /Other/yidong/yidong-cq /Other/yidong/yidong-gz /Other/yidong/yidong-ln /Other/yidong/yidong-zj /Other/yidong/yidong-sd /Other/yidong/yidong-hib /Other/yidong/yidong-ah /Other/yidong/yidong-hb</span><br><span class="line">+++ yidong-hlj</span><br><span class="line">menu = 黑龙江移动</span><br><span class="line">title = 黑龙江移动</span><br><span class="line">alerts = someloss</span><br><span class="line"><span class="comment">#slaves = boomer slave2</span></span><br><span class="line">host = 211.137.241.34</span><br><span class="line">+++ yidong-gd</span><br><span class="line">menu = 广东移动</span><br><span class="line">title = 广东移动</span><br><span class="line">alerts = someloss</span><br><span class="line"><span class="comment">#slaves = boomer slave2</span></span><br><span class="line">host = 211.137.241.34</span><br><span class="line">+++ yidong-gs</span><br><span class="line">menu = 甘肃移动</span><br><span class="line">title = 甘肃移动</span><br><span class="line">alerts = someloss</span><br><span class="line"><span class="comment">#slaves = boomer slave2</span></span><br><span class="line">host = 218.203.160.194</span><br><span class="line">+++ yidong-sh</span><br><span class="line">menu = 上海移动</span><br><span class="line">title = 上海移动</span><br><span class="line">alerts = someloss</span><br><span class="line"><span class="comment">#slaves = boomer slave2</span></span><br><span class="line">host = 117.131.0.22</span><br><span class="line">+++ yidong-sc</span><br><span class="line">menu = 四川移动</span><br><span class="line">title = 四川移动</span><br><span class="line">alerts = someloss</span><br><span class="line">host = 211.137.96.205</span><br><span class="line">+++ yidong-cq</span><br><span class="line">menu = 重庆移动</span><br><span class="line">title = 重庆移动</span><br><span class="line">alerts = someloss</span><br><span class="line">host = 218.201.4.3</span><br><span class="line">+++ yidong-gz</span><br><span class="line">menu = 贵州移动</span><br><span class="line">title = 贵州移动</span><br><span class="line">alerts = someloss</span><br><span class="line">host = 211.139.1.3</span><br><span class="line">+++ yidong-ln</span><br><span class="line">menu = 辽宁移动</span><br><span class="line">title = 辽宁移动</span><br><span class="line">alerts = someloss</span><br><span class="line">host = 218.59.181.182</span><br><span class="line">+++ yidong-zj</span><br><span class="line">menu = 浙江移动</span><br><span class="line">title = 浙江移动</span><br><span class="line">alerts = someloss</span><br><span class="line">host = 211.140.10.2</span><br><span class="line">+++ yidong-sd</span><br><span class="line">menu = 山东移动</span><br><span class="line">title = 山东移动</span><br><span class="line">alerts = someloss</span><br><span class="line">host = 211.137.191.26</span><br><span class="line">+++ yidong-hib</span><br><span class="line">menu = 湖北移动</span><br><span class="line">title = 湖北移动</span><br><span class="line">alerts = someloss</span><br><span class="line">host = 211.137.76.68</span><br><span class="line">+++ yidong-ah</span><br><span class="line">menu = 安徽移动</span><br><span class="line">title = 安徽移动</span><br><span class="line">alerts = someloss</span><br><span class="line">host = 211.138.180.2</span><br><span class="line">+++ yidong-hb</span><br><span class="line">menu = 河北移动</span><br><span class="line">title = 河北移动</span><br><span class="line">alerts = someloss</span><br><span class="line">host = 211.98.2.4</span><br><span class="line"><span class="comment">#+++ yidong-multi</span></span><br><span class="line"><span class="comment">#menu = 多个移动网络监控列表</span></span><br><span class="line"><span class="comment">#title = 多个移动网络监控列表</span></span><br><span class="line"><span class="comment">#alerts = someloss</span></span><br><span class="line"><span class="comment">#slaves = boomer slave2</span></span><br><span class="line"><span class="comment">#host = /Other/yidong/yidong-hlj /Other/yidong/yidong-gd /Other/yidong/yidong-gs /Other/yidong/yidong-sh</span></span><br><span class="line">++ jiaoyu</span><br><span class="line">menu = 教育网络监控</span><br><span class="line">title = 教育网络监控列表</span><br><span class="line">host = /Other/jiaoyu/jiaoyu-qh /Other/jiaoyu/jiaoyu-sh /Other/jiaoyu/jiaoyu-wh /Other/jiaoyu/jiaoyu-hn</span><br><span class="line">+++ jiaoyu-qh</span><br><span class="line">menu = 清华大学</span><br><span class="line">title = 清华大学</span><br><span class="line">alerts = someloss</span><br><span class="line"><span class="comment">#slaves = boomer slave2</span></span><br><span class="line">host = 166.111.8.28</span><br><span class="line">+++ jiaoyu-sh</span><br><span class="line">menu = 上海交大</span><br><span class="line">title = 上海交大</span><br><span class="line">alerts = someloss</span><br><span class="line"><span class="comment">#slaves = boomer slave2</span></span><br><span class="line">host = 202.112.26.34</span><br><span class="line">+++ jiaoyu-wh</span><br><span class="line">menu = 武汉科技大学</span><br><span class="line">title = 武汉科技大学</span><br><span class="line">alerts = someloss</span><br><span class="line"><span class="comment">#slaves = boomer slave2</span></span><br><span class="line">host = 202.114.240.6</span><br><span class="line">+++ jiaoyu-hn</span><br><span class="line">menu = 华南农业大学</span><br><span class="line">title = 华南农业大学</span><br><span class="line">alerts = someloss</span><br><span class="line"><span class="comment">#slaves = boomer slave2</span></span><br><span class="line">host = 202.116.160.33</span><br><span class="line"><span class="comment">#+++ jiaoyu-multi</span></span><br><span class="line"><span class="comment">#menu = 多个教育网络监控列表</span></span><br><span class="line"><span class="comment">#title = 多个教育网络监控列表</span></span><br><span class="line"><span class="comment">#alerts = someloss</span></span><br><span class="line"><span class="comment">#slaves = boomer slave2</span></span><br><span class="line"><span class="comment">#host = /Other/jiaoyu/jiaoyu-qh /Other/jiaoyu/jiaoyu-sh /Other/jiaoyu/jiaoyu-wh /Other/jiaoyu/jiaoyu-hn</span></span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      <categories>
        <category>监控</category>
      </categories>
      <tags>
        <tag>Smokeping</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux 常用命令学习</title>
    <url>/2019/09/25/Linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><h4 id="查找文件使用命令"><a href="#查找文件使用命令" class="headerlink" title="查找文件使用命令"></a>查找文件使用命令</h4><ul><li><p>查找目录下面大小超过5M的文件</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ find /home/ -size +5M</span><br></pre></td></tr></table></figure></li><li><p>查找目录下100天之前修改过的文件</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ find /home/ -mtime +100</span><br></pre></td></tr></table></figure></li><li><p>查找目录下60天未被访问过的文件</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ find /home/ \! atime -60</span><br></pre></td></tr></table></figure></li></ul><a id="more"></a><ul><li>查找目录下面文件“core“，如果发现无需提示直接删除。<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ find / -name core -<span class="built_in">exec</span> rm &#123;&#125; \</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查找排除某一个文件然后进行删除</span></span><br><span class="line">$ find / -<span class="built_in">type</span> f ! -name <span class="string">"test"</span> -<span class="built_in">exec</span> rm &#123;&#125; \;</span><br><span class="line">$ find ./ -mtime +3 -name <span class="string">"*.log"</span> -<span class="built_in">exec</span> rm -rf &#123;&#125; \;</span><br><span class="line">$ find /tmp -mtime +30 -<span class="built_in">type</span> f -name <span class="string">"*.sh[ab]"</span> -<span class="built_in">exec</span> rm -f &#123;&#125; \;</span><br></pre></td></tr></table></figure></li></ul><p>在一个目录中保留最近30天的文件，30天前的文件自动删除</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ find /tmp -mtime +30 -<span class="built_in">type</span> f -name <span class="string">"*.sh[ab]"</span> -<span class="built_in">exec</span> rm -f &#123;&#125; \;</span><br></pre></td></tr></table></figure><ul><li>/tmp –设置查找的目录；</li><li>-mtime +30 –设置时间为30天前；</li><li>-type f –设置查找的类型为文件；</li><li>-name *.sh[ab] –设置文件名称中包含sha或者shb；</li><li>-exec rm -f –查找完毕后执行删除操作；</li><li><strong>提示</strong>：将此命令写入crontab后即可自动完成查找并删除的工作</li></ul><ul><li>显示目录文件的文件名和它们的拥有者<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ll | awk <span class="string">'&#123;print $3,"owns",$9&#125;'</span></span><br></pre></td></tr></table></figure></li></ul><p>显示你的系统上PCI总线和附加设备的信息。指定-v，-vv或-vvv来获取越来越详细的输出</p><ul><li><p>lspci 安装</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ sudo yum whatprovides */lspci</span><br><span class="line">pciutils-3.5.1-2.el7.x86_64 : PCI bus related utilities</span><br><span class="line">Repo        : base</span><br><span class="line">Matched from:</span><br><span class="line">Filename    : /usr/sbin/lspci</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">pciutils-3.5.1-3.el7.x86_64 : PCI bus related utilities</span><br><span class="line">Repo        : base</span><br><span class="line">Matched from:</span><br><span class="line">Filename    : /usr/sbin/lspci</span><br><span class="line"></span><br><span class="line">$ sudo yum install pciutils</span><br><span class="line"></span><br><span class="line">lspci 更多[详细使用](https://blog.csdn.net/styshoo/article/details/51281437)</span><br><span class="line"></span><br><span class="line">$ lspci -vvvvv</span><br></pre></td></tr></table></figure></li><li><p>查看当前的Linux服务器的运行级别</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ who -r</span><br><span class="line">$ who -b </span><br><span class="line"><span class="comment"># 查看系统最后一次启动的时间</span></span><br><span class="line"></span><br><span class="line">$ last reboot</span><br><span class="line"><span class="comment"># 查看系统历史启动的时间</span></span><br></pre></td></tr></table></figure></li><li><p>查看系统运行了多长时间</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat /proc/uptime| awk -F. <span class="string">'&#123;run_days=$1 / 86400;run_hour=($1 % 86400)/3600;run_minute=($1 % 3600)/60;run_second=$1 % 60;printf("系统已运行：%d天%d时%d分%d秒",run_days,run_hour,run_minute,run_second)&#125;'</span></span><br><span class="line">$ w</span><br><span class="line">$ uptime</span><br></pre></td></tr></table></figure></li><li><p>查看系统启动的日期</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ date -d <span class="string">"<span class="variable">$(awk -F. '&#123;print $1&#125;' /proc/uptime)</span> second ago"</span> +<span class="string">"%Y-%m-%d %H:%M:%S"</span></span><br></pre></td></tr></table></figure></li><li><p>查找目录下文件内容没有包括“nginx”、“msgType”字符串的文件</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ grep -r -l -v <span class="string">"nginx"</span> /data/</span><br><span class="line">$ grep -r  -v <span class="string">"msgType"</span> /data/</span><br></pre></td></tr></table></figure></li><li><p>查找目录下文件内容包括”nginx”字符串的文件。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ grep -r <span class="string">"nginx"</span> /data/                                             会把<span class="string">"nginx"</span>字符串所在这行的内容显示出来</span><br><span class="line">$ grep -o “nginx” /data/</span><br><span class="line">$ grep -r -l <span class="string">"nginx"</span> /data/                                          不显示<span class="string">"nginx"</span>字符串所在行，是显示文件</span><br></pre></td></tr></table></figure></li><li><p>cat使用</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat sentry.conf.py |grep -v <span class="string">"^#"</span>          查看配置文件不包括注释内容</span><br><span class="line">$ cat -b `find /var/<span class="built_in">log</span>/httpd/ -cmin -60 -<span class="built_in">print</span> |sed <span class="string">"1d"</span>`\ |awk <span class="string">'&#123;print $2&#125;'</span>|sort |uniq -c |sort -n -k 1 -r |head -n 1               统计当前目录下日志文件里面I平访问量最多的一个IP</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看某一个时间段的IP地址访问排名前10</span></span><br><span class="line">$ cat nginx_access.log|grep <span class="string">'+0800'</span>|awk <span class="string">'&#123;split($1,array,"[");if(array[2]&gt;="25/Jul/2017:14:17:30" &amp;&amp; array[2]&lt;="25/Jul/2017:20:17:30")&#123;print $0&#125;&#125;'</span>|awk -F<span class="string">"^`"</span> &amp;&amp; <span class="string">"-"</span> &amp;&amp; <span class="string">"^`"</span> <span class="string">'&#123;print $1&#125;'</span>|sort|uniq -c|sort -n -k 1 -r|head -n 10</span><br><span class="line"></span><br><span class="line"><span class="comment"># 统计当前日志ip访问前10</span></span><br><span class="line">$ cat nginx_access.log |awk -F<span class="string">"^"</span> <span class="string">'&#123;print $1&#125;'</span>|sort|uniq -c|sort -n -k 1 -r|head -n 10</span><br></pre></td></tr></table></figure></li><li><p>获取IP地址通用</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ifconfig |sed -n 2p |awk <span class="string">'&#123;print $1$2&#125;'</span>|sed <span class="string">'s/^.*[^0-9]\([0-9]\&#123;1,3\&#125;\)\.\([0-9]\&#123;1,3\&#125;\)\.\([0-9]\&#123;1,3\&#125;\)\.\([0-9]\&#123;1,3\&#125;\)$/\1\.\2\.\3\.\4/g'</span></span><br></pre></td></tr></table></figure></li><li><p>curl使用</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 监控网页的响应时间</span></span><br><span class="line">$ curl -o /dev/null -s -w <span class="string">"time_connect: %&#123;time_connect&#125;\ntime_starttransfer: %&#123;time_starttransfer&#125;\ntime_total: %&#123;time_total&#125;\n"</span> <span class="string">"http://www.baidu.com"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 监控站点可用性</span></span><br><span class="line">$ curl -o /dev/null -s -w %&#123;http_code&#125; <span class="string">"http://www.baidu.com"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 开启gzip请求</span></span><br><span class="line">$ curl -I http://www.sina.com.cn/ -H Accept-Encoding:gzip,defalte</span><br></pre></td></tr></table></figure></li><li><p>每10秒显示一次复制的大小</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ watch -n 10 du -sh /root</span><br></pre></td></tr></table></figure></li><li><p>统计目录(包括子目录)下面文件个数</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ find ./ -<span class="built_in">type</span> f | wc -l</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用find命令查找当前目录下是文件类型的文件，然后用wc来计数</span></span><br><span class="line">$ ls -lR|grep <span class="string">"^-"</span>|wc -l</span><br><span class="line"><span class="comment"># ls命令加R参数，列出下级子目录，使用grep命令过滤以“-”开头的，如果是目录就改成“^d”，后面用wc计数。</span></span><br><span class="line"></span><br><span class="line">$ find ./ -name <span class="string">"*.*"</span> |xargs cat|grep -v ^$|wc -l</span><br><span class="line">$ find . \( ! -name <span class="string">'*.png'</span> ! -name <span class="string">'*.gif'</span> ! -name <span class="string">'*.jpg'</span> ! -name <span class="string">'*.swf'</span> \) -<span class="built_in">type</span> f |wc -l</span><br><span class="line"></span><br><span class="line"><span class="comment"># 统计目录下所有文件的的行数，去掉空行</span></span><br><span class="line">$ find ./ -name <span class="string">"*.*"</span> |xargs cat|wc -l   </span><br><span class="line">$ find . \( ! -name <span class="string">'*.png'</span> ! -name <span class="string">'*.gif'</span> ! -name <span class="string">'*.jpg'</span> ! -name <span class="string">'*.swf'</span> \) -<span class="built_in">type</span> f |xargs cat|wc -l</span><br></pre></td></tr></table></figure></li><li><p>查看系统tcp连接中各个状态的连接数</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># netstat -an | awk '/^tcp/ &#123;++S[$NF]&#125; END &#123;for(a in S) print a, S[a]&#125;'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出每个IP的连接数，以及总的各个状态的连接数</span></span><br><span class="line">$ netstat -n | awk <span class="string">'/^tcp/ &#123;n=split($(NF-1),array,":");if(n&lt;=2)++S[array[(1)]];else++S[array[(4)]];++s[$NF];++N&#125; END &#123;for(a in S)&#123;printf("%-20s %s\n", a, S[a]);++I&#125;printf("%-20s %s\n","TOTAL_IP",I);for(a in s) printf("%-20s %s\n",a, s[a]);printf("%-20s %s\n","TOTAL_LINK",N);&#125;'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 统计当前tcp/ip链接数排名前10的IP</span></span><br><span class="line">$ netstat -n|awk <span class="string">'/^tcp/ &#123;print $5&#125;'</span>|awk -F<span class="string">':'</span> <span class="string">'&#123;print $1&#125;'</span>|sort|uniq -c|sort -n -k 1 -r|head -n 10</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用grep统计当前文件里面所有的IP地址</span></span><br><span class="line">$ grep -E -o <span class="string">"(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)"</span> nginx_access.log</span><br></pre></td></tr></table></figure></li></ul><p>查看系统当前进程打开的文件句柄数，按照最大的进行排序</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ lsof -n | awk <span class="string">'&#123;print $2&#125;'</span> | sort | uniq -c | sort -nr | more</span><br></pre></td></tr></table></figure><ul><li>ping命令显示时间以及日期<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ping www.sina.com.cn -i 3 | awk <span class="string">'&#123; print $0"\t" strftime("%Y-%m-%d %H:%M:%S",systime()) &#125; '</span> &gt; /opt/sina.log &amp;</span><br></pre></td></tr></table></figure></li></ul><!-- rebuild by neat -->]]></content>
      <categories>
        <category>Centos</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>iptables</title>
    <url>/2019/09/25/iptables/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><h3 id="Iptables"><a href="#Iptables" class="headerlink" title="Iptables"></a>Iptables</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Iptalbes 是用来设置、维护和检查Linux内核的IP包过滤规则的。可以定义不同的表，每个表都包含几个内部的链，也能包含用户定义的链。每个链都是一个规则列表，对对应的包进行匹配：每条规则指定应当如何处理与之相匹配的包。这被称作’target’（目标），也可以跳向同一个表内的用户定义的链。</p><a id="more"></a><h4 id="iptables限制IP访问特定端口"><a href="#iptables限制IP访问特定端口" class="headerlink" title="iptables限制IP访问特定端口"></a>iptables限制IP访问特定端口</h4><ul><li><p>允许某个IP （192.168.6.100）的机器进行SSH连接：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ iptables -A INPUT -s 192.168.6.100 -p tcp --dport 22 -j ACCEPT</span><br><span class="line">$ iptables -L -n</span><br><span class="line">Chain INPUT (policy ACCEPT)</span><br><span class="line">target prot opt <span class="built_in">source</span> destination</span><br><span class="line">ACCEPT tcp -- 192.168.6.100 0.0.0.0/0 tcp dpt:22</span><br></pre></td></tr></table></figure></li><li><p>允许某一段的IP 访问SSH</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ iptables -A INPUT -s 192.168.6.0/24 -p tcp --dport 22 -j ACCEPT</span><br><span class="line">$ iptables -L -n</span><br><span class="line">Chain INPUT (policy ACCEPT)</span><br><span class="line">target prot opt <span class="built_in">source</span> destination</span><br><span class="line">ACCEPT tcp -- 192.168.6.0/24 0.0.0.0/0 tcp dpt:22</span><br></pre></td></tr></table></figure></li><li><p>限制某一IP 访问SSH</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ iptables -A INPUT -p tcp -s ! 192.168.6.100 --dport 22 -j ACCEPT --注意！号有个空格</span><br><span class="line">$ iptables -L -n</span><br><span class="line">Chain INPUT (policy ACCEPT)</span><br><span class="line">target prot opt <span class="built_in">source</span> destination</span><br><span class="line">ACCEPT tcp -- 192.168.6.0/24 0.0.0.0/0 tcp dpt:22</span><br></pre></td></tr></table></figure></li></ul><h3 id="配置一个NAT表放火墙"><a href="#配置一个NAT表放火墙" class="headerlink" title="配置一个NAT表放火墙"></a>配置一个NAT表放火墙</h3><ul><li><p>防止外网用内网IP欺骗</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ iptables -t nat -A PREROUTING -i eth0 -s 10.0.0.0/8 -j DROP</span><br><span class="line">$ iptables -t nat -A PREROUTING -i eth0 -s 172.16.0.0/12 -j DROP</span><br><span class="line">$ iptables -t nat -A PREROUTING -i eth0 -s 192.168.0.0/16 -j DROP</span><br></pre></td></tr></table></figure></li><li><p>禁止与211.101.46.253的所有连接</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ iptables -t nat -A PREROUTING -d 211.101.46.253 -j DROP</span><br></pre></td></tr></table></figure></li><li><p>禁用FTP(21)端口</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ iptables -t nat -A PREROUTING -p tcp --dport 21 -j DROP</span><br><span class="line"><span class="comment"># 这样写范围太大了,我们可以更精确的定义.</span></span><br><span class="line">$ iptables -t nat -A PREROUTING -p tcp --dport 21 -d 211.101.46.253 -j DROP</span><br><span class="line"><span class="comment"># 这样只禁用211.101.46.253地址的FTP连接,其他连接还可以.如web(80端口)连接.</span></span><br></pre></td></tr></table></figure></li><li><p>iptables白名单</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ iptables -A INPUT -s 0.0.0.0/0 -p tcp --dport 80 -j DROP</span><br><span class="line"><span class="comment"># 拒绝所有IP链接80端口</span></span><br><span class="line"></span><br><span class="line">$ iptables -A INPUT -s 58.17.245.222 -p tcp --dport 80 -j ACCEPT</span><br><span class="line"><span class="comment"># 允许指定IP访问80端口</span></span><br></pre></td></tr></table></figure></li><li><p>允许所有已经建立的和相关的连接</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT</span><br><span class="line">$ iptables -A OUTPUT -m state --state ESTABLISHED,RELATED -j ACCEPT</span><br></pre></td></tr></table></figure></li><li><p>drop非法连接</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ iptables -A INPUT -m state --state INVALID -j DROP</span><br><span class="line">$ iptables -A OUTPUT -m state --state INVALID -j DROP</span><br><span class="line">$ iptables -A FORWARD -m state --state INVALID -j DROP</span><br></pre></td></tr></table></figure></li></ul><h3 id="端口映射"><a href="#端口映射" class="headerlink" title="端口映射"></a>端口映射</h3><ul><li>这里使用的是FTP服务(36542)<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ iptables -t nat -A PREROUTING -p tcp --dport 36542 -j DNAT --to 192.168.50.2:36542</span><br><span class="line">$ iptables -t nat -A POSTROUTING -p tcp --dport 36542 -j MASQUERADE</span><br><span class="line"><span class="comment"># 因为FTP使用了两个端口21和20，21只是用于连接，20是执行命令的。20没办法修改，这里使用了被动模式连接。</span></span><br><span class="line"></span><br><span class="line">$ iptables -t nat -I PREROUTING -p tcp --dport 60000:65000 -j DNAT --to 192.168.50.2</span><br><span class="line"><span class="comment"># 被动连接端口60000-65000全部转发给50.2</span></span><br><span class="line"></span><br><span class="line">$ iptables -t nat -I POSTROUTING -p tcp --dport 60000:65000 -j MASQUERADE</span><br><span class="line"><span class="comment"># 需要开放60000:65000端口，</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="端口转发"><a href="#端口转发" class="headerlink" title="端口转发"></a>端口转发</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;公司有一台服务器连接外网，其他的服务器都不能上外网，我们可以通过这个外网服务器用作网关服务器，做端口转发，连接到内网服务器</p><ul><li><p>这里使用数据库的3306映射到外网的的36544</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ iptables -t nat -A PREROUTING  -m tcp -p tcp --dport 36544 -j DNAT --to-destination 172.16.1.11:3306</span><br><span class="line">$ iptables -t nat -A POSTROUTING -m tcp -p tcp --dport 3306 -d 172.16.1.11 -j SNAT --to-source 172.16.1.1</span><br></pre></td></tr></table></figure></li><li><p>添加连续端口</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ iptables -A INPUT -p tcp --dport 60000:65000 -j ACCEPT</span><br><span class="line"><span class="comment"># 冒号表示添加一个连续的端口</span></span><br><span class="line"></span><br><span class="line">$ iptables -A INPUT -p tcp -m multiport –dport 21:25,135:139 -j DROP</span><br><span class="line"><span class="comment">#使用multiport参数配置不连续端口和多个端口</span></span><br></pre></td></tr></table></figure></li><li><p>代理上网<br>内网机子无法上网，通过一台可以上网的电脑，在可以访问外网的server上iptables让其一个网段内的机子访问外网，这里是阿里云环境来做的，开启IP转发功能</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ sed -i <span class="string">'s/net.ipv4.ip_forward = 0/net.ipv4.ip_forward = 1/g'</span> /etc/sysctl.conf</span><br><span class="line">$ iptables -t nat -I POSTROUTING -s 172.16.3.0/24 -j SNAT --to-source 172.16.3.2</span><br></pre></td></tr></table></figure></li></ul><h4 id="操作iptables的nat规则"><a href="#操作iptables的nat规则" class="headerlink" title="操作iptables的nat规则"></a>操作iptables的nat规则</h4><ul><li><p>查看规则</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ iptables -nvL -t nat</span><br><span class="line">$ iptables -t nat -L -n --line-numbers</span><br></pre></td></tr></table></figure></li><li><p>删除规则</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ iptables -t nat -D POSTROUTING 1</span><br><span class="line"></span><br><span class="line"><span class="comment"># iptables的规则号</span></span><br><span class="line">$ iptables -nL --line-number</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改/替换规则</span></span><br><span class="line">$ iptbales -R INPUT &#123;1&#125; -j ACCEPT</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除规则</span></span><br><span class="line">$ iptables -D INPUT &#123;1&#125;</span><br></pre></td></tr></table></figure></li><li><p>iptales端口通过一张网卡出去</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ iptables -A INPUT -i eth0 -p tcp -s 192.168.100.0/24 --dport 22 -m state --state NEW,ESTABLESHED -j ACCEPT</span><br><span class="line">$ iptables -A OUTPUT -o eth0 -p tcp --sport 22 -m state --state ESTABLISHED -j ACCEPT</span><br></pre></td></tr></table></figure></li><li><p>本机端口，映射到本机端口</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ iptables -t nat -A PREROUTING -i eth0 -p tcp --dport 25 -j REDIRECT --to-port 2525</span><br><span class="line">$ iptables -t nat -I PREROUTING --src 0/0 --dst 192.168.1.5 -p tcp --dport 80 -j REDIRECT --to-ports 8123</span><br><span class="line">$ iptables -t nat -I OUTPUT --src 0/0 --dst 192.168.1.5 -p tcp --dport 80 -j REDIRECT --to-ports 8123</span><br></pre></td></tr></table></figure></li><li><p>保存防火墙</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ sudo /usr/libexec/iptables/iptables.init save</span><br></pre></td></tr></table></figure></li><li><p>奇葩需求，开放ssh端口指定的IP地址访问，其他端口太多不想添加能对外访问</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat /etc/sysconfig/iptables</span><br><span class="line"></span><br><span class="line"><span class="comment"># sample configuration for iptables service</span></span><br><span class="line"><span class="comment"># you can edit this manually or use system-config-firewall</span></span><br><span class="line"><span class="comment"># please do not ask us to add additional ports/services to this default configuration</span></span><br><span class="line">*filter</span><br><span class="line">:INPUT ACCEPT [0:0]</span><br><span class="line">:FORWARD ACCEPT [0:0]</span><br><span class="line">:OUTPUT ACCEPT [0:0]</span><br><span class="line">-A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT</span><br><span class="line">-A INPUT -s 192.168.10.1/32 -p tcp -m tcp --dport 22 -j ACCEPT</span><br><span class="line">-A INPUT -p tcp -m tcp --dport 22 -j  REJECT --reject-with icmp-port-unreachable</span><br><span class="line"><span class="comment">#-A INPUT -j REJECT --reject-with icmp-host-prohibited</span></span><br><span class="line"><span class="comment">#-A FORWARD -j REJECT --reject-with icmp-host-prohibited</span></span><br><span class="line">COMMIT</span><br></pre></td></tr></table></figure></li></ul><!-- rebuild by neat -->]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>iptables</tag>
      </tags>
  </entry>
  <entry>
    <title>交换机做端口聚合</title>
    <url>/2019/09/25/%E4%BA%A4%E6%8D%A2%E6%9C%BA%E5%81%9A%E7%AB%AF%E5%8F%A3%E8%81%9A%E5%90%88/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><p><strong>应用场景</strong>：h3c s5500 (Switch A)。huawei s5720S-SI-AC（Switch B）</p><p>Switch A 作为上行交换机，Switch B作为下行交换机</p><p><strong>组网</strong>：两个交换机的id、vlan号这里使用的是相同</p><a id="more"></a><p><img src="https://img.xxlaila.cn/2846sjdhausiy84yhks.png" alt="img"></p><h3 id="Switch-A配置"><a href="#Switch-A配置" class="headerlink" title="Switch A配置"></a>Switch A配置</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[Switch A-SW]vlan 50</span><br><span class="line">[Switch A-SW-vlan50]quit</span><br><span class="line">[Switch A-SW]interface Bridge-Aggregation 50</span><br><span class="line">[Switch A-SW-Bridge-Aggregation50]port access vlan 50</span><br><span class="line">[Switch A-SW]interface GigabitEthernet 1/0/19</span><br><span class="line">[Switch A-SW-GigabitEthernet1/0/19]port link-aggregation group 50</span><br><span class="line">[Switch A-SW-GigabitEthernet1/0/19] port access vlan 50</span><br><span class="line">[Switch A-SW]interface GigabitEthernet 1/0/20</span><br><span class="line">[Switch A-SW-GigabitEthernet1/0/20]port link-aggregation group 50</span><br><span class="line">[Switch A-SW-GigabitEthernet1/0/20]port access vlan 50</span><br><span class="line">[Switch A-SW]link-aggregation load-sharing mode <span class="built_in">source</span>-mac destination-mac</span><br></pre></td></tr></table></figure><ul><li>查看端口聚合<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[Switch A-SW]dis link-aggregation verbose</span><br></pre></td></tr></table></figure></li></ul><h3 id="Switch-B配置"><a href="#Switch-B配置" class="headerlink" title="Switch B配置"></a>Switch B配置</h3><h4 id="1、创建eth-trunk接口并加入成员"><a href="#1、创建eth-trunk接口并加入成员" class="headerlink" title="1、创建eth-trunk接口并加入成员"></a>1、创建eth-trunk接口并加入成员</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[Switch B] interface eth-trunk 50</span><br><span class="line">[Switch B-Eth-Trunk1] trunkport gigabitethernet 0/0/1 to 0/0/3</span><br><span class="line">[Switch B-Eth-Trunk1] quit</span><br></pre></td></tr></table></figure><h4 id="2、创建vlan并吧串行加入vlan"><a href="#2、创建vlan并吧串行加入vlan" class="headerlink" title="2、创建vlan并吧串行加入vlan"></a>2、创建vlan并吧串行加入vlan</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[Switch B] vlan batch 50</span><br><span class="line">[Switch B] interface eth-trunk 50</span><br><span class="line">[Switch B-Eth-Trunk1] port link-type trunk</span><br><span class="line">[Switch B-Eth-Trunk1] port trunk allow-pass vlan 50</span><br><span class="line">[Switch B-Eth-Trunk1] quit</span><br></pre></td></tr></table></figure><h4 id="3、配置eth-trunk的负载分担方式"><a href="#3、配置eth-trunk的负载分担方式" class="headerlink" title="3、配置eth-trunk的负载分担方式"></a>3、配置eth-trunk的负载分担方式</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[Switch B] interface eth-trunk 1</span><br><span class="line">[Switch B-Eth-Trunk1] load-balance src-dst-mac</span><br><span class="line">[Switch B-Eth-Trunk1] quit</span><br></pre></td></tr></table></figure><h3 id="Switch-A配置地址段"><a href="#Switch-A配置地址段" class="headerlink" title="Switch A配置地址段"></a>Switch A配置地址段</h3><p>在vlan里面起一个网络，但不启用dhcp服务</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[Switch A-SW]int vlan 50</span><br><span class="line">[Switch A-SW-Vlan-interface50]ip ad 172.21.16.1 20</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      <categories>
        <category>网络设备</category>
      </categories>
      <tags>
        <tag>交换机</tag>
      </tags>
  </entry>
  <entry>
    <title>pv pvc</title>
    <url>/2019/09/25/pv-pvc/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><h3 id="1、介绍"><a href="#1、介绍" class="headerlink" title="1、介绍"></a>1、介绍</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PersistentVolume（pv）和PersistentVolumeClaim（pvc）是k8s提供的两种API资源，用于抽象存储细节。管理员关注于如何通过pv提供存储功能而无需关注用户如何使用，同样的用户只需要挂载pvc到容器中而不需要关注存储卷采用何种技术实现。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pvc和pv的关系与pod和node关系类似，前者消耗后者的资源。pvc可以向pv申请指定大小的存储资源并设置访问模式,这就可以通过Provision -&gt; Claim 的方式，来对存储资源进行控制。</p><a id="more"></a><h3 id="2、生命周期"><a href="#2、生命周期" class="headerlink" title="2、生命周期"></a>2、生命周期</h3><p>pv和pvc遵循以下生命周期:</p><ul><li>供应准备。通过集群外的存储系统或者云平台来提供存储持久化支持。<ul><li><strong>静态提供</strong>: 管理员手动创建多个PV，供PVC使用。</li><li><strong>动态提供</strong>: 动态创建PVC特定的PV，并绑定。</li></ul></li><li>绑定。用户创建pvc并指定需要的资源和访问模式。在找到可用pv之前，pvc会保持未绑定状态。</li><li>使用。用户可在pod中像volume一样使用pvc。</li><li>释放。用户删除pvc来回收存储资源，pv将变成“released”状态。由于还保留着之前的数据，这些数据需要根据不同的策略来处理，否则这些存储资源无法被其他pvc使用。</li><li>回收(Reclaiming)。pv可以设置三种回收策略：保留（Retain），回收（Recycle）和删除（Delete）。</li></ul><ul><li><strong>保留策略</strong>: 允许人工处理保留的数据。</li><li><strong>删除策略</strong>: 将删除pv和外部关联的存储资源，需要插件支持。</li><li><strong>回收策略</strong>: 将执行清除操作，之后可以被新的pvc使用，需要插件支持。</li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;目前只有NFS和HostPath类型卷支持回收策略，AWS EBS,GCE PD,Azure Disk和Cinder支持删除(Delete)策略。</p><h4 id="2-1、Provisioning"><a href="#2-1、Provisioning" class="headerlink" title="2.1、Provisioning"></a>2.1、Provisioning</h4><p>两种方式提供的PV资源供给：</p><ul><li><p>static:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过集群管理者创建多个PV，为集群“使用者”提供存储能力而隐藏真实存储的细节。并且存在于kubenretes api中，可被直接使用。</p></li><li><p>dynamic:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;动态卷供给是kubernetes独有的功能，这一功能允许按需创建存储建。在此之前，集群管理员需要事先在集群外由存储提供者或者云提供商创建<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;存储卷，成功之后再创建PersistentVolume对象，才能够在kubernetes中使用。动态卷供给能让集群管理员不必进行预先创建存储卷，而是随着用户需求进行创建。在1.5版本提高了动态卷的弹性和可用性。</p></li></ul><h3 id="PV类型"><a href="#PV类型" class="headerlink" title="PV类型"></a>PV类型</h3><p>pv支持以下类型:</p><ul><li>GCEPersistentDisk</li><li>AWSElasticBlockStore</li><li>NFS</li><li>iSCSI</li><li>RBD (Ceph Block Device)</li><li>Glusterfs</li><li>AzureFile</li><li>AzureDisk</li><li>CephFS</li><li>cinder</li><li>FC</li><li>FlexVolume</li><li>Flocker</li><li>PhotonPersistentDisk</li><li>Quobyte</li><li>VsphereVolume</li><li>HostPath (single node testing only – local storage is not supported in any way and WILL NOT WORK in a multi-node cluster)</li></ul><h4 id="3-1、PV属性"><a href="#3-1、PV属性" class="headerlink" title="3.1、PV属性"></a>3.1、PV属性</h4><ul><li>访问模式,与pv的语义相同。在请求资源时使用特定模式。</li><li>资源,申请的存储资源数额。</li></ul><h4 id="3-2、PV卷阶段状态"><a href="#3-2、PV卷阶段状态" class="headerlink" title="3.2、PV卷阶段状态"></a>3.2、PV卷阶段状态</h4><ul><li>Available – 资源尚未被claim使用</li><li>Bound – 卷已经被绑定到claim了</li><li>Released – claim被删除，卷处于释放状态，但未被集群回收。</li><li>Failed – 卷自动回收失败</li></ul><!-- rebuild by neat -->]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>pv, pvc</tag>
      </tags>
  </entry>
  <entry>
    <title>利用NFS动态提供Kubernetes后端存储卷</title>
    <url>/2019/09/24/%E5%88%A9%E7%94%A8NFS%E5%8A%A8%E6%80%81%E6%8F%90%E4%BE%9BKubernetes%E5%90%8E%E7%AB%AF%E5%AD%98%E5%82%A8%E5%8D%B7/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:23 GMT+0800 (China Standard Time) --><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nfs-client-provisioner是一个automatic provisioner，使用NFS作为存储，自动创建PV和对应的PVC，本身不提供NFS存储，需要外部先有一套NFS存储服务。</p><ul><li>PV以 ${namespace}-${pvcName}-${pvName}的命名格式提供（在NFS服务器上）</li><li>PV回收的时候以 archieved-${namespace}-${pvcName}-${pvName} 的命名格式（在NFS服务器上）</li></ul><p><a href="https://github.com/kubernetes-incubator/external-storage/tree/master/nfs-client" target="_blank" rel="noopener">官方访问地址</a></p><a id="more"></a><h3 id="1、权限体系构建"><a href="#1、权限体系构建" class="headerlink" title="1、权限体系构建"></a>1、权限体系构建</h3><h4 id="1-1、创建serviceaccount"><a href="#1-1、创建serviceaccount" class="headerlink" title="1.1、创建serviceaccount"></a>1.1、创建serviceaccount</h4><p>ServiceAccount也是一种账号, 供运行在pod中的进程使用, 为pod中的进程提供必要的身份证明</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat &gt; serviceaccount.yaml &lt;&lt;EOF</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs-client-provisioner</span><br><span class="line">  namespace: kube-ops</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h4 id="1-2、创建role"><a href="#1-2、创建role" class="headerlink" title="1.2、创建role"></a>1.2、创建role</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat &gt;clusterrole.yaml&lt;&lt;EOF</span><br><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs-client-provisioner-runner</span><br><span class="line">  namespace: kube-ops</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups: [<span class="string">""</span>]</span><br><span class="line">    resources: [<span class="string">"persistentvolumes"</span>]</span><br><span class="line">    verbs: [<span class="string">"get"</span>, <span class="string">"list"</span>, <span class="string">"watch"</span>, <span class="string">"create"</span>, <span class="string">"delete"</span>]</span><br><span class="line">  - apiGroups: [<span class="string">""</span>]</span><br><span class="line">    resources: [<span class="string">"persistentvolumeclaims"</span>]</span><br><span class="line">    verbs: [<span class="string">"get"</span>, <span class="string">"list"</span>, <span class="string">"watch"</span>, <span class="string">"update"</span>]</span><br><span class="line">  - apiGroups: [<span class="string">"storage.k8s.io"</span>]</span><br><span class="line">    resources: [<span class="string">"storageclasses"</span>]</span><br><span class="line">    verbs: [<span class="string">"get"</span>, <span class="string">"list"</span>, <span class="string">"watch"</span>]</span><br><span class="line">  - apiGroups: [<span class="string">""</span>]</span><br><span class="line">    resources: [<span class="string">"events"</span>]</span><br><span class="line">    verbs: [<span class="string">"watch"</span>, <span class="string">"create"</span>, <span class="string">"update"</span>, <span class="string">"patch"</span>]</span><br><span class="line">  - apiGroups: [<span class="string">""</span>]</span><br><span class="line">    resources: [<span class="string">"services"</span>, <span class="string">"endpoints"</span>]</span><br><span class="line">    verbs: [<span class="string">"get"</span>, <span class="string">"create"</span>,<span class="string">"list"</span>, <span class="string">"watch"</span>,<span class="string">"update"</span>]</span><br><span class="line">  - apiGroups: [<span class="string">"extensions"</span>]</span><br><span class="line">    resources: [<span class="string">"podsecuritypolicies"</span>]</span><br><span class="line">    resourceNames: [<span class="string">"nfs-client-provisioner"</span>]</span><br><span class="line">    verbs: [<span class="string">"use"</span>]</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h4 id="1-3、账户和角色绑定"><a href="#1-3、账户和角色绑定" class="headerlink" title="1.3、账户和角色绑定"></a>1.3、账户和角色绑定</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat &gt;clusterrolebinding.yaml &lt;&lt;EOF</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  namespace: kube-ops</span><br><span class="line">  name: run-nfs-client-provisioner</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: nfs-client-provisioner</span><br><span class="line">    namespace: kube-ops</span><br><span class="line">roleRef:</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: nfs-client-provisioner-runner</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h4 id="1-4、执行创建"><a href="#1-4、执行创建" class="headerlink" title="1.4、执行创建"></a>1.4、执行创建</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl create -f serviceaccount.yaml -f clusterrole.yaml -f clusterrolebinding.yaml</span><br><span class="line">serviceaccount/nfs-client-provisioner created</span><br><span class="line">clusterrole.rbac.authorization.k8s.io/nfs-client-provisioner-runner created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/run-nfs-client-provisioner created</span><br></pre></td></tr></table></figure><h3 id="2、安装部署"><a href="#2、安装部署" class="headerlink" title="2、安装部署"></a>2、安装部署</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;下载deployment.yaml文件,需要修改NFS服务器所在的IP地址（10.10.10.60），以及NFS服务器共享的路径（/ifs/kubernetes），两处都需要修改为你实际的NFS服务器和共享目录</p><h4 id="2-1、部署存储供应卷"><a href="#2-1、部署存储供应卷" class="headerlink" title="2.1、部署存储供应卷"></a>2.1、部署存储供应卷</h4><p>根据PVC的请求, 动态创建PV存储.</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat &gt; deployment.yaml &lt;&lt;EOF</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  namespace: kube-ops</span><br><span class="line">  name: nfs-client-provisioner</span><br><span class="line">---</span><br><span class="line">kind: Deployment</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  namespace: kube-ops</span><br><span class="line">  name: nfs-client-provisioner</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  strategy:</span><br><span class="line">    <span class="built_in">type</span>: Recreate</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nfs-client-provisioner</span><br><span class="line">    spec:</span><br><span class="line">      serviceAccountName: nfs-client-provisioner</span><br><span class="line">      containers:</span><br><span class="line">        - name: nfs-client-provisioner</span><br><span class="line">          image: quay.io/external_storage/nfs-client-provisioner:latest</span><br><span class="line">          volumeMounts:</span><br><span class="line">            - name: nfs-client-root</span><br><span class="line">              mountPath: /persistentvolumes</span><br><span class="line">          env:</span><br><span class="line">            - name: PROVISIONER_NAME</span><br><span class="line">              value: fuseim.pri/ifs</span><br><span class="line">            - name: NFS_SERVER</span><br><span class="line">              value: 172.21.17.39</span><br><span class="line">            - name: NFS_PATH</span><br><span class="line">              value: /opt</span><br><span class="line">      volumes:</span><br><span class="line">        - name: nfs-client-root</span><br><span class="line">          nfs:</span><br><span class="line">            server: /opt</span><br><span class="line">            path: 172.21.17.39</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li>修改StorageClass文件并部署class.yaml</li></ul><p>此处可以不修改，或者修改provisioner的名字，需要与上面的deployment的PROVISIONER_NAME名字一致</p><h4 id="2-2、创建storageclass"><a href="#2-2、创建storageclass" class="headerlink" title="2.2、创建storageclass"></a>2.2、创建storageclass</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat &gt; class.yaml &lt;&lt;EOF</span><br><span class="line">apiVersion: storage.k8s.io/v1</span><br><span class="line">kind: StorageClass</span><br><span class="line">metadata:</span><br><span class="line">  name: managed-nfs-storage</span><br><span class="line">provisioner: fuseim.pri/ifs <span class="comment"># or choose another name, must match deployment's env PROVISIONER_NAME'</span></span><br><span class="line">parameters:</span><br><span class="line">  archiveOnDelete: <span class="string">"false"</span></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h4 id="2-3、执行创建"><a href="#2-3、执行创建" class="headerlink" title="2.3、执行创建"></a>2.3、执行创建</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl apply -f deployment.yaml </span><br><span class="line">serviceaccount/nfs-client-provisioner created</span><br><span class="line">deployment.extensions/nfs-client-provisioner created</span><br><span class="line"></span><br><span class="line">$ kubectl apply -f class.yaml </span><br><span class="line">storageclass.storage.k8s.io/managed-nfs-storage created</span><br></pre></td></tr></table></figure><h5 id="2-3-1、查看StorageClass"><a href="#2-3-1、查看StorageClass" class="headerlink" title="2.3.1、查看StorageClass"></a>2.3.1、查看StorageClass</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl get storageclass</span><br><span class="line">NAME                  PROVISIONER      AGE</span><br><span class="line">managed-nfs-storage   fuseim.pri/ifs   18s</span><br></pre></td></tr></table></figure><h5 id="2-3-2、设置默认后端存储"><a href="#2-3-2、设置默认后端存储" class="headerlink" title="2.3.2、设置默认后端存储"></a>2.3.2、设置默认后端存储</h5><p>设置这个default名字的SC为Kubernetes的默认存储后端</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl patch storageclass managed-nfs-storage -p <span class="string">'&#123;"metadata": &#123;"annotations":&#123;"storageclass.kubernetes.io/is-default-class":"true"&#125;&#125;&#125;'</span></span><br><span class="line">storageclass.storage.k8s.io/managed-nfs-storage patched</span><br></pre></td></tr></table></figure><ul><li>storage.yaml (和上面一样)<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat &gt; storage.yaml &lt;&lt;EOF</span><br><span class="line">apiVersion: storage.k8s.io/v1</span><br><span class="line">kind: StorageClass</span><br><span class="line">metadata:</span><br><span class="line">  name: managed-nfs-storage</span><br><span class="line">  annotations:</span><br><span class="line">    storageclass.kubernetes.io/is-default-class: <span class="string">"true"</span></span><br><span class="line">provisioner: fuseim.pri/ifs</span><br><span class="line">parameters:</span><br><span class="line">  archiveOnDelete: <span class="string">"false"</span></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></li></ul><h4 id="2-3-3、查看验证"><a href="#2-3-3、查看验证" class="headerlink" title="2.3.3、查看验证"></a>2.3.3、查看验证</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl get all -n kube-ops</span><br><span class="line">NAME                                          READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod/nfs-client-provisioner-77f678858b-8d2d6   1/1     Running   0          26m</span><br><span class="line"></span><br><span class="line">NAME                                     READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps/nfs-client-provisioner   1/1     1            1           29m</span><br><span class="line"></span><br><span class="line">NAME                                                DESIRED   CURRENT   READY   AGE</span><br><span class="line">replicaset.apps/nfs-client-provisioner-77f678858b   1         1         1       26m</span><br></pre></td></tr></table></figure><h3 id="3、验证测试"><a href="#3、验证测试" class="headerlink" title="3、验证测试"></a>3、验证测试</h3><h4 id="3-1、创建一个测试存储"><a href="#3-1、创建一个测试存储" class="headerlink" title="3.1、创建一个测试存储"></a>3.1、创建一个测试存储</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat &gt; <span class="built_in">test</span>-claim.yaml &lt;&lt;EOF</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: <span class="built_in">test</span>-claim</span><br><span class="line">  namespace: kube-ops</span><br><span class="line">  annotations:</span><br><span class="line">    volume.beta.kubernetes.io/storage-class: <span class="string">"managed-nfs-storage"</span></span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteMany</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 1Mi</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h4 id="3-2、启动测试POD"><a href="#3-2、启动测试POD" class="headerlink" title="3.2、启动测试POD"></a>3.2、启动测试POD</h4><p>POD文件如下，作用就是在test-claim的PV里touch一个SUCCESS文件</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat  &gt; <span class="built_in">test</span>-pod.yaml &lt;&lt;EOF</span><br><span class="line">kind: Pod</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: <span class="built_in">test</span>-pod</span><br><span class="line">  namespace: kube-ops</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: <span class="built_in">test</span>-pod</span><br><span class="line">    image: docker.io/busybox:1.24</span><br><span class="line">    <span class="built_in">command</span>:</span><br><span class="line">      - <span class="string">"/bin/sh"</span></span><br><span class="line">    args:</span><br><span class="line">      - <span class="string">"-c"</span></span><br><span class="line">      - <span class="string">"touch /mnt/SUCCESS &amp;&amp; exit 0 || exit 1"</span></span><br><span class="line">    volumeMounts:</span><br><span class="line">      - name: nfs-pvc</span><br><span class="line">        mountPath: <span class="string">"/mnt"</span></span><br><span class="line">  restartPolicy: <span class="string">"Never"</span></span><br><span class="line">  volumes:</span><br><span class="line">    - name: nfs-pvc</span><br><span class="line">      persistentVolumeClaim:</span><br><span class="line">        claimName: <span class="built_in">test</span>-claim</span><br></pre></td></tr></table></figure><h4 id="3-3、执行创建"><a href="#3-3、执行创建" class="headerlink" title="3.3、执行创建"></a>3.3、执行创建</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl apply -f ./</span><br><span class="line">persistentvolumeclaim/<span class="built_in">test</span>-claim created</span><br><span class="line">pod/<span class="built_in">test</span>-pod created</span><br></pre></td></tr></table></figure><h4 id="3-4、查看验证"><a href="#3-4、查看验证" class="headerlink" title="3.4、查看验证"></a>3.4、查看验证</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl get pod,pv -n kube-ops</span><br><span class="line">NAME                                          READY   STATUS      RESTARTS   AGE</span><br><span class="line">pod/nfs-client-provisioner-77f678858b-8d2d6   1/1     Running     0          3h26m</span><br><span class="line">pod/<span class="built_in">test</span>-pod                                  0/1     Completed   0          172m</span><br><span class="line"></span><br><span class="line">NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                 STORAGECLASS          REASON   AGE</span><br><span class="line">persistentvolume/pvc-2f0057b0-df35-11e9-ad62-fa163e53d4c8   1Mi        RWX            Retain           Bound    kube-ops/<span class="built_in">test</span>-claim   managed-nfs-storage            172m</span><br></pre></td></tr></table></figure><ul><li>登录nfs服务器查看是否成功的创建目录<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ls /opt/</span><br><span class="line">kube-ops-test-claim-pvc-2f0057b0-df35-11e9-ad62-fa163e53d4c8</span><br></pre></td></tr></table></figure></li></ul><h4 id="3-5、更改PersistentVolumes-中的一个回收策略"><a href="#3-5、更改PersistentVolumes-中的一个回收策略" class="headerlink" title="3.5、更改PersistentVolumes 中的一个回收策略"></a>3.5、更改PersistentVolumes 中的一个回收策略</h4><ul><li><p>查看集群中PersistentVolumes</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl get pv -n kube-ops</span><br><span class="line">NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                 STORAGECLASS          REASON   AGE</span><br><span class="line">pvc-2f0057b0-df35-11e9-ad62-fa163e53d4c8   1Mi        RWX            Delete           Bound    kube-ops/<span class="built_in">test</span>-claim   managed-nfs-storage            3m6s</span><br></pre></td></tr></table></figure></li><li><p>更改PersistentVolumes</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl patch pv pvc-2f0057b0-df35-11e9-ad62-fa163e53d4c8  -p <span class="string">'&#123;"spec":&#123;"persistentVolumeReclaimPolicy":"Retain"&#125;&#125;'</span></span><br><span class="line">persistentvolume/pvc-2f0057b0-df35-11e9-ad62-fa163e53d4c8 patched</span><br><span class="line"></span><br><span class="line">$ kubectl get pv -n kube-ops</span><br><span class="line">NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                 STORAGECLASS          REASON   AGE</span><br><span class="line">pvc-2f0057b0-df35-11e9-ad62-fa163e53d4c8   1Mi        RWX            Retain           Bound    kube-ops/<span class="built_in">test</span>-claim   managed-nfs-storage            3m54s</span><br></pre></td></tr></table></figure></li></ul><!-- rebuild by neat -->]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>pvc,pv</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s v1.14 prometheus</title>
    <url>/2019/09/20/k8s-v1-14-prometheus/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><h1 id="Prometheus、Grafana-部署"><a href="#Prometheus、Grafana-部署" class="headerlink" title="Prometheus、Grafana 部署"></a>Prometheus、Grafana 部署</h1><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Grafana是一个开源的度量分析与可视化套件。经常被用作基础设施的时间序列数据和应用程序分析的可视化，我们这里用它来做Kubernetes集群监控数据的可视化。</p><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;截至当前，prometheus、grafana均采用最新的镜像包，在在第一次部署的时候grafana报了一个错误<code>mkdir: cannot create directory &#39;/var/lib/grafana/plugins&#39;: No such file or directory</code>,这是因为Grafana启动使用的用户和用户组都是472，造成对外挂存储没有权限。<a href="https://grafana.com/docs/installation/docker/#migration-from-a-previous-version-of-the-docker-container-to-5-1-or-later" target="_blank" rel="noopener">参考官方</a></p><a id="more"></a><h3 id="开始部署"><a href="#开始部署" class="headerlink" title="开始部署"></a>开始部署</h3><p>新建yaml文件</p><ul><li>monitor-namespace.yaml<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat  monitor-namespace.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Namespace</span><br><span class="line">metadata:</span><br><span class="line">  name: monitoring</span><br></pre></td></tr></table></figure></li></ul><p>其他的文件均采用以前历史的，然后稍加修改，其他<a href="https://github.com/xxlaila/kubernetes-yaml.git" target="_blank" rel="noopener">yaml</a>文件,移除<code>grafana-ingress.yaml</code>、<code>prometheus-ingress.yaml</code></p><h3 id="文件修改"><a href="#文件修改" class="headerlink" title="文件修改"></a>文件修改</h3><ul><li><p>grafana-deploy.yaml</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: grafana-core</span><br><span class="line">  namespace: monitoring</span><br><span class="line">  labels:</span><br><span class="line">    app: grafana</span><br><span class="line">    component: core</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: grafana</span><br><span class="line">        component: core</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - image: grafana/grafana:latest</span><br><span class="line">        name: grafana-core</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        <span class="comment"># env:</span></span><br><span class="line">        resources:</span><br><span class="line">          <span class="comment"># keep request = limit to keep this container in guaranteed class</span></span><br><span class="line">          limits:</span><br><span class="line">            cpu: 100m</span><br><span class="line">            memory: 100Mi</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 100m</span><br><span class="line">            memory: 100Mi</span><br><span class="line">        env:</span><br><span class="line">          <span class="comment"># The following env variables set up basic auth twith the default admin user and admin password.</span></span><br><span class="line">          - name: GF_AUTH_BASIC_ENABLED</span><br><span class="line">            value: <span class="string">"true"</span></span><br><span class="line">          - name: GF_AUTH_ANONYMOUS_ENABLED</span><br><span class="line">            value: <span class="string">"false"</span></span><br><span class="line">          <span class="comment"># - name: GF_AUTH_ANONYMOUS_ORG_ROLE</span></span><br><span class="line">          <span class="comment">#   value: Admin</span></span><br><span class="line">          <span class="comment"># does not really work, because of template variables in exported dashboards:</span></span><br><span class="line">          <span class="comment"># - name: GF_DASHBOARDS_JSON_ENABLED</span></span><br><span class="line">          <span class="comment">#   value: "true"</span></span><br><span class="line">        readinessProbe:</span><br><span class="line">          httpGet:</span><br><span class="line">            path: /login</span><br><span class="line">            port: 3000</span><br><span class="line">          <span class="comment"># initialDelaySeconds: 30</span></span><br><span class="line">          <span class="comment"># timeoutSeconds: 1</span></span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: grafana-persistent-storage</span><br><span class="line">          mountPath: /var/lib/grafana</span><br><span class="line">      volumes:</span><br><span class="line">      - name: grafana-persistent-storage</span><br><span class="line">        emptyDir: &#123;&#125;</span><br></pre></td></tr></table></figure></li><li><p>prometheus-deploy.yaml</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">spec:</span><br><span class="line">      containers:</span><br><span class="line">      - image: prom/prometheus:latest</span><br><span class="line">        name: prometheus</span><br><span class="line">        <span class="built_in">command</span>:</span><br><span class="line">        - <span class="string">"/bin/prometheus"</span></span><br></pre></td></tr></table></figure></li><li><p>prometheus-svc.yaml</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">kind: Service</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: prometheus</span><br><span class="line">  name: prometheus</span><br><span class="line">  namespace: monitoring</span><br><span class="line">spec:</span><br><span class="line">  <span class="comment">#type: NodePort</span></span><br><span class="line">  ports:</span><br><span class="line">  - port: 9090</span><br><span class="line">    targetPort: 9090</span><br><span class="line">    <span class="comment">#nodePort: 30005</span></span><br><span class="line">  selector:</span><br><span class="line">    app: prometheus</span><br></pre></td></tr></table></figure></li><li><p>grafana-svc.yaml</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat grafana-svc.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: grafana</span><br><span class="line">  namespace: monitoring</span><br><span class="line">  labels:</span><br><span class="line">    app: grafana</span><br><span class="line">    component: core</span><br><span class="line">spec:</span><br><span class="line">  <span class="comment">#type: NodePort</span></span><br><span class="line">  ports:</span><br><span class="line">    - port: 3000</span><br><span class="line">  selector:</span><br><span class="line">    app: grafana</span><br><span class="line">    component: core</span><br></pre></td></tr></table></figure></li></ul><h3 id="执行创建"><a href="#执行创建" class="headerlink" title="执行创建"></a>执行创建</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl apply -f ./</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 多执行几次</span></span><br></pre></td></tr></table></figure><h3 id="检查部署"><a href="#检查部署" class="headerlink" title="检查部署"></a>检查部署</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get pod,svc,deploy -n monitoring</span></span><br><span class="line">NAME                                READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod/grafana-core-7b5989cf9d-snbk5   1/1     Running   0          2m31s</span><br><span class="line">pod/node-exporter-dddv7             1/1     Running   0          12m</span><br><span class="line">pod/node-exporter-fhfp6             1/1     Running   0          12m</span><br><span class="line">pod/node-exporter-m46bf             1/1     Running   0          12m</span><br><span class="line">pod/node-exporter-xkrzp             1/1     Running   0          12m</span><br><span class="line">pod/node-exporter-zfcxh             1/1     Running   0          12m</span><br><span class="line">pod/prometheus-67bcf457db-999ns     1/1     Running   0          12m</span><br><span class="line"></span><br><span class="line">NAME                    TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">service/grafana         ClusterIP   10.254.95.151    &lt;none&gt;        3000/TCP         12m</span><br><span class="line">service/node-exporter   ClusterIP   10.254.114.12    &lt;none&gt;        9100/TCP         12m</span><br><span class="line">service/prometheus      ClusterIP   10.254.104.216   &lt;none&gt;        9090/TCP         12m</span><br><span class="line"></span><br><span class="line">NAME                                 READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.extensions/grafana-core   1/1     1            1           12m</span><br><span class="line">deployment.extensions/prometheus     1/1     1            1           12m</span><br></pre></td></tr></table></figure><h3 id="创建Ingress"><a href="#创建Ingress" class="headerlink" title="创建Ingress"></a>创建Ingress</h3><h4 id="prometheus-Ingress"><a href="#prometheus-Ingress" class="headerlink" title="prometheus Ingress"></a>prometheus Ingress</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat prometheus-Ingress.yaml </span></span><br><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: prometheus-web-ui</span><br><span class="line">  namespace: monitoring</span><br><span class="line">  annotations:</span><br><span class="line">    kubernetes.io/ingress.class: traefik</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: prometheus.xxlaila.cn</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: /</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: prometheus</span><br><span class="line">          servicePort: 9090</span><br></pre></td></tr></table></figure><h4 id="grafana-Ingress"><a href="#grafana-Ingress" class="headerlink" title="grafana Ingress"></a>grafana Ingress</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat grafana-Ingress.yaml </span></span><br><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: grafana-web-ui</span><br><span class="line">  namespace: monitoring</span><br><span class="line">  annotations:</span><br><span class="line">    kubernetes.io/ingress.class: traefik</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: grafana.xxlaila.cn</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: /</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: grafana</span><br><span class="line">          servicePort: 3000</span><br></pre></td></tr></table></figure><h4 id="执行创建-1"><a href="#执行创建-1" class="headerlink" title="执行创建"></a>执行创建</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl apply -f prometheus-Ingress.yaml </span></span><br><span class="line">ingress.extensions/prometheus-web-ui created</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f grafana-Ingress.yaml </span></span><br><span class="line">ingress.extensions/grafana-web-ui created</span><br></pre></td></tr></table></figure><p>在浏览器输入prometheus.xxlaila.cn访问prometheus，输入grafana.xxlaila.cn访问grafana。</p><h3 id="访问prometheus"><a href="#访问prometheus" class="headerlink" title="访问prometheus"></a>访问prometheus</h3><p><img src="https://img.xxlaila.cn/1569218750254.jpg" alt="img"></p><h3 id="配置grafana"><a href="#配置grafana" class="headerlink" title="配置grafana"></a>配置grafana</h3><p><img src="https://img.xxlaila.cn/1568968344227.jpg" alt="img"></p><p>到grafana的官方下载对应的模版文件导入，就可以出图啦<br><img src="https://img.xxlaila.cn/1568968420655.jpg" alt="img"></p><p><a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/prometheus" target="_blank" rel="noopener">后续利用pvc</a></p><!-- rebuild by neat -->]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>k8s v1.14,prometheus</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s v1.14 weave-scope</title>
    <url>/2019/09/20/k8s-v1-14-weave-scope/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><h3 id="前沿"><a href="#前沿" class="headerlink" title="前沿"></a>前沿</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Kubernetes 集群并部署容器化应用只是第一步。一旦集群运行起来，我们需要确保一起正常，所有必要组件就位并各司其职，有足够的资源满足应用的需求。Kubernetes 是一个复杂系统，运维团队需要有一套工具帮助他们获知集群的实时状态，并为故障排查提供及时和准确的数据支持。</p><h3 id="weave-scope-介绍"><a href="#weave-scope-介绍" class="headerlink" title="weave scope 介绍"></a>weave scope 介绍</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Weave Scope是Docker和Kubernetes的可视化和监控工具。它提供了一个自上而下的应用程序以及整个基础架构视图，并允许您在部署到云提供商时实时诊断分布式容器化应用程序的任何问题。</p><a id="more"></a><h3 id="功能介绍"><a href="#功能介绍" class="headerlink" title="功能介绍"></a>功能介绍</h3><ul><li>pod拓扑映射</li><li>图形或表格模式</li><li>灵活过滤</li><li>强大的搜索功能</li><li>实时应用和容器指标</li><li>排除故障并管理容器</li><li>使用Plugin API生成自定义指标</li></ul><p><a href="https://www.weave.works/docs/scope/latest/features/" target="_blank" rel="noopener">介绍参考</a></p><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>在 K8s 集群中安装 Scope 的方法很简单，使用下面的命令</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl apply -f "https://cloud.weave.works/k8s/scope.yaml?k8s-version=$(kubectl version | base64 | tr -d '\n')"</span></span><br><span class="line">namespace/weave created</span><br><span class="line">serviceaccount/weave-scope created</span><br><span class="line">clusterrole.rbac.authorization.k8s.io/weave-scope created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/weave-scope created</span><br><span class="line">deployment.apps/weave-scope-app created</span><br><span class="line">service/weave-scope-app created</span><br><span class="line">deployment.apps/weave-scope-cluster-agent created</span><br><span class="line">daemonset.extensions/weave-scope-agent created</span><br></pre></td></tr></table></figure><h3 id="检查"><a href="#检查" class="headerlink" title="检查"></a>检查</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get pod,svc,deploy -n weave</span></span><br><span class="line">NAME                                             READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod/weave-scope-agent-2t4m5                      1/1     Running   0          15m</span><br><span class="line">pod/weave-scope-agent-6tfp5                      1/1     Running   0          15m</span><br><span class="line">pod/weave-scope-agent-fxj5f                      1/1     Running   0          15m</span><br><span class="line">pod/weave-scope-agent-gkxc6                      1/1     Running   0          15m</span><br><span class="line">pod/weave-scope-agent-qnbbv                      1/1     Running   0          15m</span><br><span class="line">pod/weave-scope-app-b99fb9585-wld6n              1/1     Running   0          15m</span><br><span class="line">pod/weave-scope-cluster-agent-77bc946585-8fcjj   1/1     Running   0          15m</span><br><span class="line"></span><br><span class="line">NAME                      TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">service/weave-scope-app   ClusterIP   10.254.184.106   &lt;none&gt;        80/TCP    15m</span><br><span class="line"></span><br><span class="line">NAME                                              READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.extensions/weave-scope-app             1/1     1            1           15m</span><br><span class="line">deployment.extensions/weave-scope-cluster-agent   1/1     1            1           15m</span><br></pre></td></tr></table></figure><h3 id="创建weave-scope-ingress"><a href="#创建weave-scope-ingress" class="headerlink" title="创建weave-scope ingress"></a>创建weave-scope ingress</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat weave-scope.yaml </span></span><br><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: weave-web-ui</span><br><span class="line">  namespace: weave</span><br><span class="line">  annotations:</span><br><span class="line">    kubernetes.io/ingress.class: traefik</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: weave-scope.xxlaila.cn</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: /</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: weave-scope-app</span><br><span class="line">          servicePort: 80</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f weave-scope.yaml </span></span><br><span class="line">ingress.extensions/weave-web-ui created</span><br></pre></td></tr></table></figure><p>在浏览输入<code>weave-scope.xxlaila.cn</code>即可访问<br><img src="https://img.xxlaila.cn/1568958836846.jpg" alt="img"></p><h4 id="拓扑结构"><a href="#拓扑结构" class="headerlink" title="拓扑结构"></a>拓扑结构</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Scope 会自动构建应用和集群的逻辑拓扑。比如点击顶部 Pods，会显示所有 Pod 以及 Pod 之间的依赖关系<br><img src="https://img.xxlaila.cn/1568958666089.jpg" alt="img"><br>点击 Hosts，会显示各个节点之间的关系，可以在 Scope 中查看资源的 CPU 和内存使用情况。<br><img src="https://img.xxlaila.cn/1568958913275.jpg" alt="img"></p><h3 id="在线操作"><a href="#在线操作" class="headerlink" title="在线操作"></a>在线操作</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Scope 还提供了便捷的在线操作功能，比如选中某个 Host，点击 &gt;_按钮可以直接在浏览器中打开节点的命令行终端：<br><img src="https://img.xxlaila.cn/1568959004395.jpg" alt="img"></p><ul><li><p>点击 Deployment 的 + 可以执行新增一个pod实列<br><img src="https://img.xxlaila.cn/1568959269040.jpg" alt="img"></p></li><li><p>查看pod的日志<br><img src="https://img.xxlaila.cn/1568959359334.jpg" alt="img"></p></li><li><p>attach、restart、stop 容器，以及直接在 Scope 中排查问题<br><img src="https://img.xxlaila.cn/1568959467442.jpg" alt="img"></p></li></ul><p>更多功呢个请<a href="https://www.weave.works/docs/scope/latest/plugins/" target="_blank" rel="noopener">参考官方</a>,或者实操</p><!-- rebuild by neat -->]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>k8s v1.14, weave-scope</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s v1.14 traefik部署</title>
    <url>/2019/09/20/k8s-v1-14-traefik%E9%83%A8%E7%BD%B2/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;traefix 前篇是可以使用，这里k8s v1.14 之前的拿来用不上，然后折腾了一下，参考官方的折腾起来了</p><h3 id="基于角色的访问控制配置（仅限Kubernetes-1-6-）"><a href="#基于角色的访问控制配置（仅限Kubernetes-1-6-）" class="headerlink" title="基于角色的访问控制配置（仅限Kubernetes 1.6+）"></a>基于角色的访问控制配置（仅限Kubernetes 1.6+）</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Kubernetes在1.6+中引入了基于角色的访问控制（RBAC），以允许对Kubernetes资源和API进行细粒度控制。群集配置了RBAC，则需要授权Traefik使用Kubernetes API。有两种方法可以设置适当的权限：通过特定于命名空间的RoleBindings或单个全局ClusterRoleBinding。</p><a id="more"></a><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;每个命名空间的RoleBinding可以限制授予权限，只有Traefik正在监视的名称空间才能使用，从而遵循最小权限原则。如果Traefik不应该监视所有名称空间，并且名称空间集不会动态更改，那么这是首选方法。否则，必须使用单个ClusterRoleBinding。</p><p><a href="https://xxlaila.github.io/2019/09/05/traefik-ingress%E4%BD%BF%E7%94%A8/" target="_blank" rel="noopener">traefik学习</a><br><a href="https://docs.traefik.io/v1.7/user-guide/kubernetes/" target="_blank" rel="noopener">traefik官方</a></p><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><p>下载trarfix代码，然后切换到v1.7的分支</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># git clone https://github.com/containous/traefik.git</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># git branch --all</span></span><br><span class="line">* master</span><br><span class="line">  remotes/origin/HEAD -&gt; origin/master</span><br><span class="line">  remotes/origin/add-plugin-support</span><br><span class="line">  remotes/origin/gh-pages</span><br><span class="line">  remotes/origin/master</span><br><span class="line">  remotes/origin/v1.0</span><br><span class="line">  remotes/origin/v1.1</span><br><span class="line">  remotes/origin/v1.2</span><br><span class="line">  remotes/origin/v1.3</span><br><span class="line">  remotes/origin/v1.4</span><br><span class="line">  remotes/origin/v1.5</span><br><span class="line">  remotes/origin/v1.6</span><br><span class="line">  remotes/origin/v1.7</span><br><span class="line">  remotes/origin/v2.0</span><br><span class="line"></span><br><span class="line"><span class="comment"># git checkout v1.7</span></span><br><span class="line">Branch <span class="string">'v1.7'</span> <span class="built_in">set</span> up to track remote branch <span class="string">'v1.7'</span> from <span class="string">'origin'</span>.</span><br><span class="line">Switched to a new branch <span class="string">'v1.7'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># /root/traefik/examples/k8s</span></span><br></pre></td></tr></table></figure><h3 id="安装部署"><a href="#安装部署" class="headerlink" title="安装部署"></a>安装部署</h3><h4 id="使用ClusterRoleBinding"><a href="#使用ClusterRoleBinding" class="headerlink" title="使用ClusterRoleBinding"></a>使用ClusterRoleBinding</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl apply -f traefik-rbac.yaml </span></span><br><span class="line">clusterrole.rbac.authorization.k8s.io/traefik-ingress-controller created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/traefik-ingress-controller created</span><br></pre></td></tr></table></figure><p>对于命名空间限制，每个监视命名空间需要一个RoleBinding以及Traefik kubernetes.namespaces参数的相应配置。</p><h4 id="使用Deployments部署或部署DaemonSet"><a href="#使用Deployments部署或部署DaemonSet" class="headerlink" title="使用Deployments部署或部署DaemonSet"></a>使用Deployments部署或部署DaemonSet</h4><p>可以将Traefik与Deployment或DaemonSet对象一起使用，而这两个选项各有利弊：</p><ul><li>使用部署时，可伸缩性可以更好，因为在使用DaemonSet时您将拥有每个节点的Single-Pod模型，而在使用部署时，可能需要更少的基于环境的副本。</li><li>当节点加入群集时，DaemonSet会自动扩展到新节点，而部署窗格仅在需要时在新节点上进行调度。</li><li>DaemonSets确保只有一个pod副本在任何单个节点上运行。如果要确保两个pod不在同一节点上，则部署需要关联设置</li><li>可以使用该NET_BIND_SERVICE功能运行DaemonSet ，这将允许它绑定到每个主机上的端口80/443 / etc。这将允许绕过kube-proxy，并减少流量跳跃。请注意，这违反了Kubernetes最佳实践指南，并提出了调度/扩展问题的可能性。尽管存在潜在问题，但这仍然是大多数入口控制器的选择。</li></ul><h4 id="Deployments部署"><a href="#Deployments部署" class="headerlink" title="Deployments部署"></a>Deployments部署</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl  apply -f  traefik-deployment.yaml</span></span><br><span class="line">serviceaccount/traefik-ingress-controller created</span><br><span class="line">deployment.extensions/traefik-ingress-controller created</span><br><span class="line">service/traefik-ingress-service created</span><br></pre></td></tr></table></figure><h4 id="DaemonSets-部署-可选"><a href="#DaemonSets-部署-可选" class="headerlink" title="DaemonSets 部署(可选)"></a>DaemonSets 部署(可选)</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl apply -f traefik-ds.yaml</span></span><br></pre></td></tr></table></figure><ul><li>Deployments和DaemonSets之间存在一些显着差异:<ul><li>部署具有更容易的向上和向下扩展可能性。它可以实现完整的pod生命周期，并支持Kubernetes 1.2的滚动更新。运行部署至少需要一个Pod。</li><li>DaemonSet会自动扩展到满足特定选择器的所有节点，并保证一次填充一个节点。Kubernetes 1.7也完全支持滚动更新，适用于DaemonSets</li></ul></li></ul><h3 id="检查部署"><a href="#检查部署" class="headerlink" title="检查部署"></a>检查部署</h3><ul><li><p>查看pod</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl --namespace=kube-system get pods</span></span><br><span class="line">NAME                                         READY   STATUS    RESTARTS   AGE</span><br><span class="line">coredns-5579b8778b-xw8m9                     1/1     Running   2          3d21h</span><br><span class="line">kubernetes-dashboard-65dfbf6f4f-hcgbb        1/1     Running   0          2d16h</span><br><span class="line">metrics-server-94ff5d4cc-b97l5               1/1     Running   1          3d</span><br><span class="line">tiller-deploy-5cbcf75545-rbzld               1/1     Running   0          17h</span><br><span class="line">traefik-ingress-controller-c595665d6-cm7kh   1/1     Running   0          3m20s</span><br></pre></td></tr></table></figure></li><li><p>查看services</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get services --namespace=kube-system</span></span><br><span class="line">NAME                      TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                       AGE</span><br><span class="line">kube-dns                  ClusterIP   10.254.0.2       &lt;none&gt;        53/UDP,53/TCP,9153/TCP        3d21h</span><br><span class="line">kubernetes-dashboard      NodePort    10.254.214.153   &lt;none&gt;        443:32533/TCP                 3d21h</span><br><span class="line">metrics-server            ClusterIP   10.254.61.132    &lt;none&gt;        443/TCP                       3d</span><br><span class="line">tiller-deploy             ClusterIP   10.254.207.227   &lt;none&gt;        44134/TCP                     17h</span><br><span class="line">traefik-ingress-service   NodePort    10.254.246.158   &lt;none&gt;        80:32146/TCP,8080:30455/TCP   3m53s</span><br></pre></td></tr></table></figure></li></ul><p>这里使用的是nodeport模式进行部署的，可以看到端口为32146，这里访问会返回<code>404 page not found</code>,那是因为我们还没有给Traefik任何配置。</p><h3 id="创建一个服务和一个将公开Traefik-Web-UI的Ingres"><a href="#创建一个服务和一个将公开Traefik-Web-UI的Ingres" class="headerlink" title="创建一个服务和一个将公开Traefik Web UI的Ingres"></a>创建一个服务和一个将公开Traefik Web UI的Ingres</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl apply -f ui.yaml </span></span><br><span class="line">service/traefik-web-ui created</span><br><span class="line">ingress.extensions/traefik-web-ui created</span><br></pre></td></tr></table></figure><p>在/etc/hosts 文件设置一个路由条目<code>traefik-ui.minikube</code></p><p>在浏览器进行访问可以看到Traefik Web UI</p><!-- rebuild by neat -->]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>k8s v1.14, traefik</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s v1.14 metrics-server</title>
    <url>/2019/09/17/k8s-v1-14-metrics-server/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:23 GMT+0800 (China Standard Time) --><p>metrics-server这里不详细介绍，可以参考<a href="https://xxlaila.github.io/2019/09/04/metrics-server安装季/" target="_blank" rel="noopener">metrics-server安装季</a></p><h3 id="安装metrics-server"><a href="#安装metrics-server" class="headerlink" title="安装metrics-server"></a>安装metrics-server</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里安装和之前的<strong>metrics-server安装季</strong>稍微有点不一样，之前集群安装没有使用https证书，后面去各种生成的证书和踩坑，这里是在安装的时候一开始就使用了https全证书,所有稍微有一点区别，这里只列出有区别的地方，其他的完全可以参考<a href="https://xxlaila.github.io/2019/09/04/metrics-server安装季/" target="_blank" rel="noopener">metrics-server安装季</a>，这里https证书<strong>不需要</strong>重新生成；</p><a id="more"></a><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;配置文件也不需要添加，在v1.14安装的时候就已经吧配置文件添加进去了，所以这里配置文件也不需要增加</p><h3 id="文件的修改"><a href="#文件的修改" class="headerlink" title="文件的修改"></a>文件的修改</h3><ul><li><p>修改 metrics-server-deployment.yaml</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat metrics-server-deployment.yaml</span></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: metrics-server</span><br><span class="line">  namespace: kube-system</span><br><span class="line">---</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: metrics-server</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: metrics-server</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      k8s-app: metrics-server</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: metrics-server</span><br><span class="line">      labels:</span><br><span class="line">        k8s-app: metrics-server</span><br><span class="line">    spec:</span><br><span class="line">      serviceAccountName: metrics-server</span><br><span class="line">      hostNetwork: <span class="literal">true</span> 这个还是需要增加</span><br><span class="line">      volumes:</span><br><span class="line">      <span class="comment"># mount in tmp so we can safely use from-scratch images and/or read-only containers</span></span><br><span class="line">      - name: tmp-dir</span><br><span class="line">        emptyDir: &#123;&#125;</span><br><span class="line">      containers:</span><br><span class="line">      - name: metrics-server</span><br><span class="line">        image: mirrorgooglecontainers/metrics-server-amd64:v0.3.4</span><br><span class="line">        imagePullPolicy: Always</span><br><span class="line">        args:  <span class="comment"># 这里不一样</span></span><br><span class="line">        - --metric-resolution=30s</span><br><span class="line">        - --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: tmp-dir</span><br><span class="line">          mountPath: /tmp</span><br></pre></td></tr></table></figure></li><li><p>–metric-resolution=30s：从 kubelet 采集数据的周期；</p></li><li><p>–kubelet-preferred-address-types：优先使用 InternalIP 来访问 kubelet，这样可以避免节点名称没有 DNS 解析记录时，通过节点名称调用节点 kubelet API 失败的情况（未配置时默认的情况）；</p></li><li><p><strong>hostNetwork: true:</strong> 这个不增加的会提示：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Error from server (ServiceUnavailable): the server is currently unable to handle the request</span><br></pre></td></tr></table></figure></li><li><p>修改 resource-reader.yaml</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat resource-reader.yaml </span></span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRole</span><br><span class="line">metadata:</span><br><span class="line">  name: system:metrics-server</span><br><span class="line">rules:</span><br><span class="line">- apiGroups:</span><br><span class="line">  - <span class="string">""</span></span><br><span class="line">  resources:</span><br><span class="line">  - pods</span><br><span class="line">  - nodes</span><br><span class="line">  - nodes/stats</span><br><span class="line">  - namespaces</span><br><span class="line">  verbs:</span><br><span class="line">  - get</span><br><span class="line">  - list</span><br><span class="line">  - watch</span><br><span class="line">- apiGroups: <span class="comment"># 增加</span></span><br><span class="line">  - <span class="string">"extensions"</span></span><br><span class="line">  resources:</span><br><span class="line">  - deployments</span><br><span class="line">  verbs:</span><br><span class="line">  - get</span><br><span class="line">  - list</span><br><span class="line">  - update</span><br><span class="line">  - watch</span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: system:metrics-server</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: system:metrics-server</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: metrics-server</span><br><span class="line">  namespace: kube-system</span><br></pre></td></tr></table></figure></li></ul><h3 id="执行创建"><a href="#执行创建" class="headerlink" title="执行创建"></a>执行创建</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl apply -f ./</span></span><br></pre></td></tr></table></figure><h3 id="查看运行情况"><a href="#查看运行情况" class="headerlink" title="查看运行情况"></a>查看运行情况</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl -n kube-system get pods -l k8s-app=metrics-server</span></span><br><span class="line">NAME                             READY   STATUS    RESTARTS   AGE</span><br><span class="line">metrics-server-94ff5d4cc-b97l5   1/1     Running   0          21m</span><br><span class="line"></span><br><span class="line"><span class="comment">#  kubectl get svc -n kube-system  metrics-server</span></span><br><span class="line">NAME             TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">metrics-server   ClusterIP   10.254.61.132   &lt;none&gt;        443/TCP   27m</span><br></pre></td></tr></table></figure><h3 id="获取v1beta1-metrics-k8s-io并验证"><a href="#获取v1beta1-metrics-k8s-io并验证" class="headerlink" title="获取v1beta1.metrics.k8s.io并验证"></a>获取v1beta1.metrics.k8s.io并验证</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get apiservice</span></span><br><span class="line">NAME                                   SERVICE                      AVAILABLE   AGE</span><br><span class="line">v1.                                    Local                        True        23h</span><br><span class="line">v1.apps                                Local                        True        23h</span><br><span class="line">v1.authentication.k8s.io               Local                        True        23h</span><br><span class="line">v1.authorization.k8s.io                Local                        True        23h</span><br><span class="line">v1.autoscaling                         Local                        True        23h</span><br><span class="line">v1.batch                               Local                        True        23h</span><br><span class="line">v1.coordination.k8s.io                 Local                        True        23h</span><br><span class="line">v1.networking.k8s.io                   Local                        True        23h</span><br><span class="line">v1.rbac.authorization.k8s.io           Local                        True        23h</span><br><span class="line">v1.scheduling.k8s.io                   Local                        True        23h</span><br><span class="line">v1.storage.k8s.io                      Local                        True        23h</span><br><span class="line">v1alpha1.auditregistration.k8s.io      Local                        True        23h</span><br><span class="line">v1alpha1.node.k8s.io                   Local                        True        23h</span><br><span class="line">v1alpha1.rbac.authorization.k8s.io     Local                        True        23h</span><br><span class="line">v1alpha1.scheduling.k8s.io             Local                        True        23h</span><br><span class="line">v1alpha1.settings.k8s.io               Local                        True        23h</span><br><span class="line">v1alpha1.storage.k8s.io                Local                        True        23h</span><br><span class="line">v1beta1.admissionregistration.k8s.io   Local                        True        23h</span><br><span class="line">v1beta1.apiextensions.k8s.io           Local                        True        23h</span><br><span class="line">v1beta1.apps                           Local                        True        23h</span><br><span class="line">v1beta1.authentication.k8s.io          Local                        True        23h</span><br><span class="line">v1beta1.authorization.k8s.io           Local                        True        23h</span><br><span class="line">v1beta1.batch                          Local                        True        23h</span><br><span class="line">v1beta1.certificates.k8s.io            Local                        True        23h</span><br><span class="line">v1beta1.coordination.k8s.io            Local                        True        23h</span><br><span class="line">v1beta1.events.k8s.io                  Local                        True        23h</span><br><span class="line">v1beta1.extensions                     Local                        True        23h</span><br><span class="line">v1beta1.metrics.k8s.io                 kube-system/metrics-server   True        27m</span><br><span class="line">v1beta1.networking.k8s.io              Local                        True        23h</span><br><span class="line">v1beta1.node.k8s.io                    Local                        True        23h</span><br><span class="line">v1beta1.policy                         Local                        True        23h</span><br><span class="line">v1beta1.rbac.authorization.k8s.io      Local                        True        23h</span><br><span class="line">v1beta1.scheduling.k8s.io              Local                        True        23h</span><br><span class="line">v1beta1.storage.k8s.io                 Local                        True        23h</span><br><span class="line">v1beta2.apps                           Local                        True        23h</span><br><span class="line">v2alpha1.batch                         Local                        True        23h</span><br><span class="line">v2beta1.autoscaling                    Local                        True        23h</span><br><span class="line">v2beta2.autoscaling                    Local                        True        23h</span><br></pre></td></tr></table></figure><h3 id="metrics-server-的命令行参数"><a href="#metrics-server-的命令行参数" class="headerlink" title="metrics-server 的命令行参数"></a>metrics-server 的命令行参数</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl exec --namespace kube-system -it metrics-server-94ff5d4cc-b97l5 -- /metrics-server --help</span></span><br><span class="line">Launch metrics-server</span><br><span class="line"></span><br><span class="line">Usage:</span><br><span class="line">   [flags]</span><br><span class="line"></span><br><span class="line">Flags:</span><br><span class="line">      --alsologtostderr                                         <span class="built_in">log</span> to standard error as well as files</span><br><span class="line">      --authentication-kubeconfig string                        kubeconfig file pointing at the <span class="string">'core'</span> kubernetes server with enough rights to create tokenaccessreviews.authentication.k8s.io.</span><br><span class="line">      --authentication-skip-lookup                              If <span class="literal">false</span>, the authentication-kubeconfig will be used to lookup missing authentication configuration from the cluster.</span><br><span class="line">      --authentication-token-webhook-cache-ttl duration         The duration to cache responses from the webhook token authenticator. (default 10s)</span><br><span class="line">      --authentication-tolerate-lookup-failure                  If <span class="literal">true</span>, failures to look up missing authentication configuration from the cluster are not considered fatal. Note that this can result <span class="keyword">in</span> authentication that treats all requests as anonymous.</span><br><span class="line">      --authorization-always-allow-paths strings                A list of HTTP paths to skip during authorization, i.e. these are authorized without contacting the <span class="string">'core'</span> kubernetes server.</span><br><span class="line">      --authorization-kubeconfig string                         kubeconfig file pointing at the <span class="string">'core'</span> kubernetes server with enough rights to create subjectaccessreviews.authorization.k8s.io.</span><br><span class="line">      --authorization-webhook-cache-authorized-ttl duration     The duration to cache <span class="string">'authorized'</span> responses from the webhook authorizer. (default 10s)</span><br><span class="line">      --authorization-webhook-cache-unauthorized-ttl duration   The duration to cache <span class="string">'unauthorized'</span> responses from the webhook authorizer. (default 10s)</span><br><span class="line">      --<span class="built_in">bind</span>-address ip                                         The IP address on <span class="built_in">which</span> to listen <span class="keyword">for</span> the --secure-port port. The associated interface(s) must be reachable by the rest of the cluster, and by CLI/web clients. If blank, all interfaces will be used (0.0.0.0 <span class="keyword">for</span> all IPv4 interfaces and :: <span class="keyword">for</span> all IPv6 interfaces). (default 0.0.0.0)</span><br><span class="line">      --cert-dir string                                         The directory <span class="built_in">where</span> the TLS certs are located. If --tls-cert-file and --tls-private-key-file are provided, this flag will be ignored. (default <span class="string">"apiserver.local.config/certificates"</span>)</span><br><span class="line">      --client-ca-file string                                   If <span class="built_in">set</span>, any request presenting a client certificate signed by one of the authorities <span class="keyword">in</span> the client-ca-file is authenticated with an identity corresponding to the CommonName of the client certificate.</span><br><span class="line">      --contention-profiling                                    Enable lock contention profiling, <span class="keyword">if</span> profiling is enabled</span><br><span class="line">  -h, --<span class="built_in">help</span>                                                    <span class="built_in">help</span> <span class="keyword">for</span> this <span class="built_in">command</span></span><br><span class="line">      --http2-max-streams-per-connection int                    The <span class="built_in">limit</span> that the server gives to clients <span class="keyword">for</span> the maximum number of streams <span class="keyword">in</span> an HTTP/2 connection. Zero means to use golang<span class="string">'s default.</span></span><br><span class="line"><span class="string">      --kubeconfig string                                       The path to the kubeconfig used to connect to the Kubernetes API server and the Kubelets (defaults to in-cluster config)</span></span><br><span class="line"><span class="string">      --kubelet-certificate-authority string                    Path to the CA to use to validate the Kubelet'</span>s serving certificates.</span><br><span class="line">      --kubelet-insecure-tls                                    Do not verify CA of serving certificates presented by Kubelets.  For testing purposes only.</span><br><span class="line">      --kubelet-port int                                        The port to use to connect to Kubelets. (default 10250)</span><br><span class="line">      --kubelet-preferred-address-types strings                 The priority of node address types to use when determining <span class="built_in">which</span> address to use to connect to a particular node (default [Hostname,InternalDNS,InternalIP,ExternalDNS,ExternalIP])</span><br><span class="line">      --<span class="built_in">log</span>-flush-frequency duration                            Maximum number of seconds between <span class="built_in">log</span> flushes (default 5s)</span><br><span class="line">      --log_backtrace_at traceLocation                          when logging hits line file:N, emit a stack trace (default :0)</span><br><span class="line">      --log_dir string                                          If non-empty, write <span class="built_in">log</span> files <span class="keyword">in</span> this directory</span><br><span class="line">      --log_file string                                         If non-empty, use this <span class="built_in">log</span> file</span><br><span class="line">      --logtostderr                                             <span class="built_in">log</span> to standard error instead of files (default <span class="literal">true</span>)</span><br><span class="line">      --metric-resolution duration                              The resolution at <span class="built_in">which</span> metrics-server will retain metrics. (default 1m0s)</span><br><span class="line">      --profiling                                               Enable profiling via web interface host:port/debug/pprof/ (default <span class="literal">true</span>)</span><br><span class="line">      --requestheader-allowed-names strings                     List of client certificate common names to allow to provide usernames <span class="keyword">in</span> headers specified by --requestheader-username-headers. If empty, any client certificate validated by the authorities <span class="keyword">in</span> --requestheader-client-ca-file is allowed.</span><br><span class="line">      --requestheader-client-ca-file string                     Root certificate bundle to use to verify client certificates on incoming requests before trusting usernames <span class="keyword">in</span> headers specified by --requestheader-username-headers. WARNING: generally <span class="keyword">do</span> not depend on authorization being already <span class="keyword">done</span> <span class="keyword">for</span> incoming requests.</span><br><span class="line">      --requestheader-extra-headers-prefix strings              List of request header prefixes to inspect. X-Remote-Extra- is suggested. (default [x-remote-extra-])</span><br><span class="line">      --requestheader-group-headers strings                     List of request headers to inspect <span class="keyword">for</span> groups. X-Remote-Group is suggested. (default [x-remote-group])</span><br><span class="line">      --requestheader-username-headers strings                  List of request headers to inspect <span class="keyword">for</span> usernames. X-Remote-User is common. (default [x-remote-user])</span><br><span class="line">      --secure-port int                                         The port on <span class="built_in">which</span> to serve HTTPS with authentication and authorization.If 0, don<span class="string">'t serve HTTPS at all. (default 443)</span></span><br><span class="line"><span class="string">      --skip_headers                                            If true, avoid header prefixes in the log messages</span></span><br><span class="line"><span class="string">      --stderrthreshold severity                                logs at or above this threshold go to stderr</span></span><br><span class="line"><span class="string">      --tls-cert-file string                                    File containing the default x509 Certificate for HTTPS. (CA cert, if any, concatenated after server cert). If HTTPS serving is enabled, and --tls-cert-file and --tls-private-key-file are not provided, a self-signed certificate and key are generated for the public address and saved to the directory specified by --cert-dir.</span></span><br><span class="line"><span class="string">      --tls-cipher-suites strings                               Comma-separated list of cipher suites for the server. If omitted, the default Go cipher suites will be use.  Possible values: TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_RC4_128_SHA,TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_RC4_128_SHA,TLS_RSA_WITH_3DES_EDE_CBC_SHA,TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_RSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_256_CBC_SHA,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_RC4_128_SHA</span></span><br><span class="line"><span class="string">      --tls-min-version string                                  Minimum TLS version supported. Possible values: VersionTLS10, VersionTLS11, VersionTLS12</span></span><br><span class="line"><span class="string">      --tls-private-key-file string                             File containing the default x509 private key matching --tls-cert-file.</span></span><br><span class="line"><span class="string">      --tls-sni-cert-key namedCertKey                           A pair of x509 certificate and private key file paths, optionally suffixed with a list of domain patterns which are fully qualified domain names, possibly with prefixed wildcard segments. If no domain patterns are provided, the names of the certificate are extracted. Non-wildcard matches trump over wildcard matches, explicit domain patterns trump over extracted names. For multiple key/certificate pairs, use the --tls-sni-cert-key multiple times. Examples: "example.crt,example.key" or "foo.crt,foo.key:*.foo.com,foo.com". (default [])</span></span><br><span class="line"><span class="string">  -v, --v Level                                                 number for the log level verbosity</span></span><br><span class="line"><span class="string">      --vmodule moduleSpec                                      comma-separated list of pattern=N settings for file-filtered logging</span></span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>metrics-server</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s v1.14 dashboard</title>
    <url>/2019/09/16/k8s-v1-14-dashboard/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:23 GMT+0800 (China Standard Time) --><p>kuberntes 自带插件的 manifests yaml 文件使用 gcr.io 的 docker registry，国内被墙，需要手动替换为其它 registry 地址</p><h3 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h3><p>将下载的 kubernetes-server-linux-amd64.tar.gz 解压后，再解压其中的 kubernetes-src.tar.gz 文件。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cd kubernetes</span></span><br><span class="line"><span class="comment"># tar -xzvf kubernetes-src.tar.gz</span></span><br></pre></td></tr></table></figure><p>dashboard 对应的目录是：cluster/addons/dashboard：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cd cluster/addons/dashboard</span></span><br></pre></td></tr></table></figure><p>修改 service 定义，指定端口类型为 NodePort，这样外界可以通过地址 NodeIP:NodePort 访问 dashboard；</p><a id="more"></a><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat dashboard-service.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: kubernetes-dashboard</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: kubernetes-dashboard</span><br><span class="line">    kubernetes.io/cluster-service: <span class="string">"true"</span></span><br><span class="line">    addonmanager.kubernetes.io/mode: Reconcile</span><br><span class="line">spec:</span><br><span class="line">  <span class="built_in">type</span>: NodePort <span class="comment"># 增加这一行</span></span><br><span class="line">  selector:</span><br><span class="line">    k8s-app: kubernetes-dashboard</span><br><span class="line">  ports:</span><br><span class="line">  - port: 443</span><br><span class="line">    targetPort: 8443</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat dashboard-controller.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: kubernetes-dashboard</span><br><span class="line">    addonmanager.kubernetes.io/mode: Reconcile</span><br><span class="line">  name: kubernetes-dashboard</span><br><span class="line">  namespace: kube-system</span><br><span class="line">---</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: kubernetes-dashboard</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: kubernetes-dashboard</span><br><span class="line">    kubernetes.io/cluster-service: <span class="string">"true"</span></span><br><span class="line">    addonmanager.kubernetes.io/mode: Reconcile</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      k8s-app: kubernetes-dashboard</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        k8s-app: kubernetes-dashboard</span><br><span class="line">      annotations:</span><br><span class="line">        scheduler.alpha.kubernetes.io/critical-pod: <span class="string">''</span></span><br><span class="line">        seccomp.security.alpha.kubernetes.io/pod: <span class="string">'docker/default'</span></span><br><span class="line">    spec:</span><br><span class="line">      priorityClassName: system-cluster-critical</span><br><span class="line">      containers:</span><br><span class="line">      - name: kubernetes-dashboard</span><br><span class="line">        image: docker.io/xxlaila/kubernetes-dashboard-amd64:v1.10.0  <span class="comment">#修改这一行</span></span><br><span class="line">        resources:</span><br><span class="line">          limits:</span><br><span class="line">            cpu: 100m</span><br><span class="line">            memory: 300Mi</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 50m</span><br><span class="line">            memory: 100Mi</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 8443</span><br><span class="line">          protocol: TCP</span><br></pre></td></tr></table></figure><h3 id="执行所有定义文件"><a href="#执行所有定义文件" class="headerlink" title="执行所有定义文件"></a>执行所有定义文件</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ls *.yaml</span></span><br><span class="line">dashboard-configmap.yaml  dashboard-controller.yaml  dashboard-rbac.yaml  dashboard-secret.yaml  dashboard-service.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl apply -f  .</span></span><br></pre></td></tr></table></figure><h3 id="查看分配的-NodePort"><a href="#查看分配的-NodePort" class="headerlink" title="查看分配的 NodePort"></a>查看分配的 NodePort</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get deployment kubernetes-dashboard  -n kube-system</span></span><br><span class="line">NAME                   READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">kubernetes-dashboard   1/1     1            1           5h10m</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl --namespace kube-system get pods -o wide</span></span><br><span class="line">NAME                                    READY   STATUS    RESTARTS   AGE     IP             NODE            NOMINATED NODE   READINESS GATES</span><br><span class="line">coredns-5579b8778b-xw8m9                1/1     Running   1          5h15m   172.30.232.3   172.21.16.204   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kubernetes-dashboard-6cc78dfc99-hb4l5   1/1     Running   0          5h10m   172.30.176.3   172.21.16.240   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl get services kubernetes-dashboard -n kube-system</span></span><br><span class="line">NAME                   TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)         AGE</span><br><span class="line">kubernetes-dashboard   NodePort   10.254.214.153   &lt;none&gt;        443:32533/TCP   5h10m</span><br></pre></td></tr></table></figure><ul><li>NodePort 32533 映射到 dashboard pod 443 端口；</li></ul><h3 id="查看-dashboard-支持的命令行参数"><a href="#查看-dashboard-支持的命令行参数" class="headerlink" title="查看 dashboard 支持的命令行参数"></a>查看 dashboard 支持的命令行参数</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl exec --namespace kube-system -it kubernetes-dashboard-6cc78dfc99-hb4l5  -- /dashboard --help</span></span><br><span class="line">2019/09/16 09:51:33 Starting overwatch</span><br><span class="line">Usage of /dashboard:</span><br><span class="line">      --alsologtostderr                  <span class="built_in">log</span> to standard error as well as files</span><br><span class="line">      --api-log-level string             Level of API request logging. Should be one of <span class="string">'INFO|NONE|DEBUG'</span>. Default: <span class="string">'INFO'</span>. (default <span class="string">"INFO"</span>)</span><br><span class="line">      --apiserver-host string            The address of the Kubernetes Apiserver to connect to <span class="keyword">in</span> the format of protocol://address:port, e.g., http://localhost:8080. If not specified, the assumption is that the binary runs inside a Kubernetes cluster and <span class="built_in">local</span> discovery is attempted.</span><br><span class="line">      --authentication-mode strings      Enables authentication options that will be reflected on login screen. Supported values: token, basic. Default: token.Note that basic option should only be used <span class="keyword">if</span> apiserver has <span class="string">'--authorization-mode=ABAC'</span> and <span class="string">'--basic-auth-file'</span> flags <span class="built_in">set</span>. (default [token])</span><br><span class="line">      --auto-generate-certificates       When <span class="built_in">set</span> to <span class="literal">true</span>, Dashboard will automatically generate certificates used to serve HTTPS. Default: <span class="literal">false</span>.</span><br><span class="line">      --<span class="built_in">bind</span>-address ip                  The IP address on <span class="built_in">which</span> to serve the --secure-port (<span class="built_in">set</span> to 0.0.0.0 <span class="keyword">for</span> all interfaces). (default 0.0.0.0)</span><br><span class="line">      --default-cert-dir string          Directory path containing <span class="string">'--tls-cert-file'</span> and <span class="string">'--tls-key-file'</span> files. Used also when auto-generating certificates flag is <span class="built_in">set</span>. (default <span class="string">"/certs"</span>)</span><br><span class="line">      --<span class="built_in">disable</span>-settings-authorizer      When enabled, Dashboard settings page will not require user to be logged <span class="keyword">in</span> and authorized to access settings page.</span><br><span class="line">      --<span class="built_in">disable</span>-skip                     When enabled, the skip button on the login page will not be shown. Default: <span class="literal">false</span>.</span><br><span class="line">      --<span class="built_in">enable</span>-insecure-login            When enabled, Dashboard login view will also be shown when Dashboard is not served over HTTPS. Default: <span class="literal">false</span>.</span><br><span class="line">      --heapster-host string             The address of the Heapster Apiserver to connect to <span class="keyword">in</span> the format of protocol://address:port, e.g., http://localhost:8082. If not specified, the assumption is that the binary runs inside a Kubernetes cluster and service proxy will be used.</span><br><span class="line">      --insecure-bind-address ip         The IP address on <span class="built_in">which</span> to serve the --port (<span class="built_in">set</span> to 0.0.0.0 <span class="keyword">for</span> all interfaces). (default 127.0.0.1)</span><br><span class="line">      --insecure-port int                The port to listen to <span class="keyword">for</span> incoming HTTP requests. (default 9090)</span><br><span class="line">      --kubeconfig string                Path to kubeconfig file with authorization and master location information.</span><br><span class="line">      --log_backtrace_at traceLocation   when logging hits line file:N, emit a stack trace (default :0)</span><br><span class="line">      --log_dir string                   If non-empty, write <span class="built_in">log</span> files <span class="keyword">in</span> this directory</span><br><span class="line">      --logtostderr                      <span class="built_in">log</span> to standard error instead of files</span><br><span class="line">      --metric-client-check-period int   Time <span class="keyword">in</span> seconds that defines how often configured metric client health check should be run. Default: 30 seconds. (default 30)</span><br><span class="line">      --port int                         The secure port to listen to <span class="keyword">for</span> incoming HTTPS requests. (default 8443)</span><br><span class="line">      --stderrthreshold severity         logs at or above this threshold go to stderr (default 2)</span><br><span class="line">      --system-banner string             When non-empty displays message to Dashboard users. Accepts simple HTML tags. Default: <span class="string">''</span>.</span><br><span class="line">      --system-banner-severity string    Severity of system banner. Should be one of <span class="string">'INFO|WARNING|ERROR'</span>. Default: <span class="string">'INFO'</span>. (default <span class="string">"INFO"</span>)</span><br><span class="line">      --tls-cert-file string             File containing the default x509 Certificate <span class="keyword">for</span> HTTPS.</span><br><span class="line">      --tls-key-file string              File containing the default x509 private key matching --tls-cert-file.</span><br><span class="line">      --token-ttl int                    Expiration time (<span class="keyword">in</span> seconds) of JWE tokens generated by dashboard. Default: 15 min. 0 - never expires (default 900)</span><br><span class="line">  -v, --v Level                          <span class="built_in">log</span> level <span class="keyword">for</span> V logs</span><br><span class="line">      --vmodule moduleSpec               comma-separated list of pattern=N settings <span class="keyword">for</span> file-filtered logging</span><br><span class="line">pflag: <span class="built_in">help</span> requested</span><br><span class="line"><span class="built_in">command</span> terminated with <span class="built_in">exit</span> code 2</span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dashboard 的 –authentication-mode 支持 token、basic，默认为 token。如果使用 basic，则 kube-apiserver 必须配置 –authorization-mode=ABAC 和 –basic-auth-file 参数</p><h3 id="访问-dashboard"><a href="#访问-dashboard" class="headerlink" title="访问 dashboard"></a>访问 dashboard</h3><p>使用https协议，在浏览器输入任意node的ip加端口即可访问<br><img src="https://img.xxlaila.cn/1568961339763.jpg" alt="img"></p><h3 id="创建登录-Dashboard-的-token-和-kubeconfig-配置文件"><a href="#创建登录-Dashboard-的-token-和-kubeconfig-配置文件" class="headerlink" title="创建登录 Dashboard 的 token 和 kubeconfig 配置文件"></a>创建登录 Dashboard 的 token 和 kubeconfig 配置文件</h3><p>dashboard 默认只支持 token 认证（不支持 client 证书认证），所以如果使用 Kubeconfig 文件，需要将 token 写入到该文件。</p><h4 id="创建登录-token"><a href="#创建登录-token" class="headerlink" title="创建登录 token"></a>创建登录 token</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl create sa dashboard-admin -n kube-system</span></span><br><span class="line"><span class="comment"># kubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin</span></span><br><span class="line"><span class="comment"># ADMIN_SECRET=$(kubectl get secrets -n kube-system | grep dashboard-admin | awk '&#123;print $1&#125;')</span></span><br><span class="line"><span class="comment"># DASHBOARD_LOGIN_TOKEN=$(kubectl describe secret -n kube-system $&#123;ADMIN_SECRET&#125; | grep -E '^token' | awk '&#123;print $2&#125;')</span></span><br><span class="line"><span class="comment"># echo $&#123;DASHBOARD_LOGIN_TOKEN&#125;</span></span><br></pre></td></tr></table></figure><p>使用输出的 token 登录 Dashboard。</p><h3 id="创建使用-token-的-KubeConfig-文件"><a href="#创建使用-token-的-KubeConfig-文件" class="headerlink" title="创建使用 token 的 KubeConfig 文件"></a>创建使用 token 的 KubeConfig 文件</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 设置集群参数</span></span><br><span class="line">kubectl config <span class="built_in">set</span>-cluster kubernetes \</span><br><span class="line">  --certificate-authority=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --embed-certs=<span class="literal">true</span> \</span><br><span class="line">  --server=<span class="variable">$&#123;KUBE_APISERVER&#125;</span> \</span><br><span class="line">  --kubeconfig=dashboard.kubeconfig</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置客户端认证参数，使用上面创建的 Token</span></span><br><span class="line">kubectl config <span class="built_in">set</span>-credentials dashboard_user \</span><br><span class="line">  --token=<span class="variable">$&#123;DASHBOARD_LOGIN_TOKEN&#125;</span> \</span><br><span class="line">  --kubeconfig=dashboard.kubeconfig</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置上下文参数</span></span><br><span class="line">kubectl config <span class="built_in">set</span>-context default \</span><br><span class="line">  --cluster=kubernetes \</span><br><span class="line">  --user=dashboard_user \</span><br><span class="line">  --kubeconfig=dashboard.kubeconfig</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置默认上下文</span></span><br><span class="line">kubectl config use-context default --kubeconfig=dashboard.kubeconfig</span><br></pre></td></tr></table></figure><p>如图:<br><img src="https://img.xxlaila.cn/1568961447890.jpg" alt="img"></p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;用生成的 dashboard.kubeconfig 登录 Dashboard。由于k8s 默认的Dashboard 15分钟后就会弹出，又要重新登录和获取token麻烦，可以参考之前的<a href="https://xxlaila.github.io/2019/08/29/k8s配置Dashboard/" target="_blank" rel="noopener">k8s配置Dashboard</a></p><!-- rebuild by neat -->]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>dashboard</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s v1.14 dns插件</title>
    <url>/2019/09/16/k8s-v1-14-dns%E6%8F%92%E4%BB%B6/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><h3 id="部署-coredns-插件"><a href="#部署-coredns-插件" class="headerlink" title="部署 coredns 插件"></a>部署 coredns 插件</h3><p><strong>注意:</strong></p><ul><li>kuberntes 自带插件的 manifests yaml 文件使用 gcr.io 的 docker registry，国内被墙，需要手动替换为其它 registry 地址;</li></ul><h3 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h3><p>将下载的 kubernetes-server-linux-amd64.tar.gz 解压后，再解压其中的 kubernetes-src.tar.gz 文件。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cd kubernetes</span></span><br><span class="line"><span class="comment"># tar -xzvf kubernetes-src.tar.gz</span></span><br></pre></td></tr></table></figure><a id="more"></a><ul><li>coredns 目录是 cluster/addons/dns<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cd cluster/addons/dns/coredns</span></span><br><span class="line"><span class="comment"># cp coredns.yaml.base coredns.yaml</span></span><br><span class="line"><span class="comment"># sed -i -e "s/__PILLAR__DNS__DOMAIN__/cluster.local/" -e "s/__PILLAR__DNS__SERVER__/10.254.0.2/" coredns.yaml</span></span><br><span class="line"><span class="comment"># sed -i "s/k8s.gcr.io/coredns/" coredns.yaml</span></span><br><span class="line"><span class="comment"># kubectl create -f coredns.yaml</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="检查-coredns-功能"><a href="#检查-coredns-功能" class="headerlink" title="检查 coredns 功能"></a>检查 coredns 功能</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get all -n kube-system</span></span><br><span class="line">NAME                                        READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod/coredns-5579b8778b-xw8m9                1/1     Running   1          5h7m</span><br><span class="line"></span><br><span class="line">NAME                           TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                  AGE</span><br><span class="line">service/kube-dns               ClusterIP   10.254.0.2       &lt;none&gt;        53/UDP,53/TCP,9153/TCP   5h7m</span><br><span class="line"></span><br><span class="line">NAME                                   READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps/coredns                1/1     1            1           5h7m</span><br><span class="line"></span><br><span class="line">NAME                                              DESIRED   CURRENT   READY   AGE</span><br><span class="line">replicaset.apps/coredns-5579b8778b                1         1         1       5h7m</span><br></pre></td></tr></table></figure><h3 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get pods</span></span><br><span class="line">NAME             READY   STATUS    RESTARTS   AGE</span><br><span class="line">nginx-ds-9fb46   1/1     Running   0          5h14m</span><br><span class="line">nginx-ds-bgfzt   1/1     Running   0          5h14m</span><br><span class="line">nginx-ds-t22wj   1/1     Running   0          5h14m</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl -it exec nginx-ds-9fb46 bash</span></span><br><span class="line">root@nginx-ds-9fb46:/<span class="comment"># cat /etc/resolv.conf</span></span><br><span class="line">nameserver 10.254.0.2</span><br><span class="line">search default.svc.cluster.local svc.cluster.local cluster.local openstacklocal novalocal</span><br><span class="line">options ndots:5</span><br><span class="line"></span><br><span class="line">root@nginx-ds-9fb46:/<span class="comment"># ping www.baidu.com</span></span><br><span class="line">PING www.wshifen.com (104.193.88.77): 48 data bytes</span><br><span class="line">56 bytes from 104.193.88.77: icmp_seq=0 ttl=45 time=191.953 ms</span><br><span class="line">56 bytes from 104.193.88.77: icmp_seq=1 ttl=45 time=191.680 ms</span><br><span class="line">^C--- www.wshifen.com ping statistics ---</span><br><span class="line">2 packets transmitted, 2 packets received, 0% packet loss</span><br><span class="line">round-trip min/avg/max/stddev = 191.680/191.817/191.953/0.137 ms</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@nginx-ds-9fb46:/<span class="comment"># ping kube-dns.kube-system.svc</span></span><br><span class="line">PING kube-dns.kube-system.svc.cluster.local (10.254.0.2): 48 data bytes</span><br><span class="line">56 bytes from 10.254.0.2: icmp_seq=0 ttl=64 time=0.120 ms</span><br><span class="line">56 bytes from 10.254.0.2: icmp_seq=1 ttl=64 time=0.116 ms</span><br><span class="line">^C--- kube-dns.kube-system.svc.cluster.local ping statistics ---</span><br><span class="line">2 packets transmitted, 2 packets received, 0% packet loss</span><br><span class="line">round-trip min/avg/max/stddev = 0.116/0.118/0.120/0.000 ms</span><br><span class="line"></span><br><span class="line">root@nginx-ds-9fb46:/<span class="comment"># ping kube-dns.kube-system.svc.cluster.local</span></span><br><span class="line">PING kube-dns.kube-system.svc.cluster.local (10.254.0.2): 48 data bytes</span><br><span class="line">56 bytes from 10.254.0.2: icmp_seq=0 ttl=64 time=0.079 ms</span><br><span class="line">56 bytes from 10.254.0.2: icmp_seq=1 ttl=64 time=0.152 ms</span><br><span class="line">^C--- kube-dns.kube-system.svc.cluster.local ping statistics ---</span><br><span class="line">2 packets transmitted, 2 packets received, 0% packet loss</span><br><span class="line">round-trip min/avg/max/stddev = 0.079/0.115/0.152/0.037 ms</span><br><span class="line"></span><br><span class="line">root@nginx-ds-9fb46:/<span class="comment"># ping kube-dns.kube-system.svc.cluster.local.</span></span><br><span class="line">PING kube-dns.kube-system.svc.cluster.local (10.254.0.2): 48 data bytes</span><br><span class="line">56 bytes from 10.254.0.2: icmp_seq=0 ttl=64 time=0.080 ms</span><br><span class="line">^C--- kube-dns.kube-system.svc.cluster.local ping statistics ---</span><br><span class="line">1 packets transmitted, 1 packets received, 0% packet loss</span><br><span class="line">round-trip min/avg/max/stddev = 0.080/0.080/0.080/0.000 ms</span><br><span class="line">`</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>v1.14 coredns</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s v1.14集群验证</title>
    <url>/2019/09/16/k8s-v1-14%E9%9B%86%E7%BE%A4%E9%AA%8C%E8%AF%81/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><h3 id="验证集群功能"><a href="#验证集群功能" class="headerlink" title="验证集群功能"></a>验证集群功能</h3><h3 id="检查节点状态"><a href="#检查节点状态" class="headerlink" title="检查节点状态"></a>检查节点状态</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get nodes</span></span><br><span class="line">NAME            STATUS   ROLES    AGE     VERSION</span><br><span class="line">172.21.16.204   Ready    &lt;none&gt;   5h50m   v1.14.6</span><br><span class="line">172.21.16.240   Ready    &lt;none&gt;   5h48m   v1.14.6</span><br><span class="line">172.21.16.87    Ready    &lt;none&gt;   5h45m   v1.14.6</span><br></pre></td></tr></table></figure><p>都为 Ready 时正常。</p><a id="more"></a><h3 id="创建测试文件"><a href="#创建测试文件" class="headerlink" title="创建测试文件"></a>创建测试文件</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat nginx-ds.yml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ds</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx-ds</span><br><span class="line">spec:</span><br><span class="line">  <span class="built_in">type</span>: NodePort</span><br><span class="line">  selector:</span><br><span class="line">    app: nginx-ds</span><br><span class="line">  ports:</span><br><span class="line">  - name: http</span><br><span class="line">    port: 80</span><br><span class="line">    targetPort: 80</span><br><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: DaemonSet</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ds</span><br><span class="line">  labels:</span><br><span class="line">    addonmanager.kubernetes.io/mode: Reconcile</span><br><span class="line">spec:</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx-ds</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: my-nginx</span><br><span class="line">        image: nginx:1.7.9</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl create -f nginx-ds.yml</span></span><br></pre></td></tr></table></figure><h3 id="检查各节点的-Pod-IP-连通性"><a href="#检查各节点的-Pod-IP-连通性" class="headerlink" title="检查各节点的 Pod IP 连通性"></a>检查各节点的 Pod IP 连通性</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get pods  -o wide|grep nginx-ds</span></span><br><span class="line">nginx-ds-9fb46   1/1     Running   0          5h2m   172.30.232.2   172.21.16.204   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">nginx-ds-bgfzt   1/1     Running   0          5h2m   172.30.128.2   172.21.16.87    &lt;none&gt;           &lt;none&gt;</span><br><span class="line">nginx-ds-t22wj   1/1     Running   0          5h2m   172.30.176.2   172.21.16.240   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><h3 id="检查服务-IP-和端口可达性"><a href="#检查服务-IP-和端口可达性" class="headerlink" title="检查服务 IP 和端口可达性"></a>检查服务 IP 和端口可达性</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get svc |grep nginx-ds</span></span><br><span class="line">nginx-ds     NodePort    10.254.232.104   &lt;none&gt;        80:30349/TCP   5h2m</span><br></pre></td></tr></table></figure><p>在浏览器在30349进行访问可以看到neinx的欢迎界面</p><!-- rebuild by neat -->]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>v1.14</tag>
      </tags>
  </entry>
  <entry>
    <title>kubernetes-v1.14 node安装</title>
    <url>/2019/09/16/kubernetes-v1-14-node%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:23 GMT+0800 (China Standard Time) --><h3 id="1、安装docker"><a href="#1、安装docker" class="headerlink" title="1、安装docker"></a>1、安装docker</h3><h4 id="1-1、增加docker-源"><a href="#1-1、增加docker-源" class="headerlink" title="1.1、增加docker 源"></a>1.1、增加docker 源</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum-config-manager \</span><br><span class="line">  --add-repo \</span><br><span class="line">  https://download.docker.com/linux/centos/docker-ce.repo</span><br></pre></td></tr></table></figure><h4 id="1-2、安装docker"><a href="#1-2、安装docker" class="headerlink" title="1.2、安装docker"></a>1.2、安装docker</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#  yum -y install docker-ce</span></span><br></pre></td></tr></table></figure><a id="more"></a><h4 id="1-3、修改docker-systemd-unit-文件"><a href="#1-3、修改docker-systemd-unit-文件" class="headerlink" title="1.3、修改docker systemd unit 文件"></a>1.3、修改docker systemd unit 文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat /usr/lib/systemd/system/docker.service |egrep -Ev "^$|^#"</span></span><br><span class="line">[Unit]</span><br><span class="line">Description=Docker Application Container Engine</span><br><span class="line">Documentation=https://docs.docker.com</span><br><span class="line">BindsTo=containerd.service</span><br><span class="line">After=network-online.target firewalld.service containerd.service</span><br><span class="line">Wants=network-online.target</span><br><span class="line">Requires=docker.socket</span><br><span class="line">[Service]</span><br><span class="line">Type=notify</span><br><span class="line">EnvironmentFile=-/run/flannel/docker</span><br><span class="line">ExecStart=/usr/bin/dockerd <span class="variable">$DOCKER_NETWORK_OPTIONS</span></span><br><span class="line">ExecReload=/bin/<span class="built_in">kill</span> -s HUP <span class="variable">$MAINPID</span></span><br><span class="line">TimeoutSec=0</span><br><span class="line">RestartSec=2</span><br><span class="line">Restart=always</span><br><span class="line">StartLimitBurst=3</span><br><span class="line">StartLimitInterval=60s</span><br><span class="line">LimitNOFILE=infinity</span><br><span class="line">LimitNPROC=infinity</span><br><span class="line">LimitCORE=infinity</span><br><span class="line">TasksMax=infinity</span><br><span class="line">Delegate=yes</span><br><span class="line">KillMode=process</span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><ul><li>dockerd 运行时会调用其它 docker 命令，如 docker-proxy，所以需要将 docker 命令所在的目录加到 PATH 环境变量中；</li><li>flanneld 启动时将网络配置写入 /run/flannel/docker 文件中，dockerd 启动前读取该文件中的环境变量 DOCKER_NETWORK_OPTIONS ，然后设置 docker0 网桥网段；</li><li>如果指定了多个 EnvironmentFile 选项，则必须将 /run/flannel/docker 放在最后(确保 docker0 使用 flanneld 生成的 bip 参数)；</li><li>docker 需要以 root 用于运行；</li></ul><h4 id="1-4、启动-docker-服务"><a href="#1-4、启动-docker-服务" class="headerlink" title="1.4、启动 docker 服务"></a>1.4、启动 docker 服务</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># systemctl daemon-reload &amp;&amp; systemctl enable docker &amp;&amp; systemctl restart docker &amp;&amp; systemctl status docker</span></span><br></pre></td></tr></table></figure><h4 id="1-5、检查-docker0-网桥"><a href="#1-5、检查-docker0-网桥" class="headerlink" title="1.5、检查 docker0 网桥"></a>1.5、检查 docker0 网桥</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># /usr/sbin/ip addr show flannel.1 &amp;&amp; /usr/sbin/ip addr show docker0</span></span><br><span class="line">3: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN </span><br><span class="line">    link/ether 8a:be:12:b9:ab:b8 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.30.128.0/32 scope global flannel.1</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">4: docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP </span><br><span class="line">    link/ether 02:42:eb:ec:ae:94 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.30.128.1/21 brd 172.30.135.255 scope global docker0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure><h4 id="1-5、查看-docker-的状态信息"><a href="#1-5、查看-docker-的状态信息" class="headerlink" title="1.5、查看 docker 的状态信息"></a>1.5、查看 docker 的状态信息</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ps -elfH|grep docker</span></span><br><span class="line">0 S root      1436   975  0  80   0 - 28167 -      10:54 pts/0    00:00:00                 grep --color=auto docker</span><br><span class="line">4 S root      1265     1  1  80   0 - 122095 futex_ 10:54 ?       00:00:00   /usr/bin/dockerd --bip=172.30.112.1/21 --ip-masq=<span class="literal">false</span> --mtu=1450</span><br><span class="line"></span><br><span class="line"><span class="comment"># docker info</span></span><br><span class="line">vClient:</span><br><span class="line"> Debug Mode: <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">Server:</span><br><span class="line"> Containers: 0</span><br><span class="line">  Running: 0</span><br><span class="line">  Paused: 0</span><br><span class="line">  Stopped: 0</span><br><span class="line"> Images: 0</span><br><span class="line"> Server Version: 18.09.6</span><br><span class="line"> Storage Driver: overlay2</span><br><span class="line">  Backing Filesystem: xfs</span><br><span class="line">  Supports d_type: <span class="literal">true</span></span><br><span class="line">  Native Overlay Diff: <span class="literal">true</span></span><br><span class="line"> Logging Driver: json-file</span><br><span class="line"> Cgroup Driver: cgroupfs</span><br><span class="line"> Plugins:</span><br><span class="line">  Volume: <span class="built_in">local</span></span><br><span class="line">  Network: bridge host macvlan null overlay</span><br><span class="line">  Log: awslogs fluentd gcplogs gelf journald json-file <span class="built_in">local</span> logentries splunk syslog</span><br><span class="line"> Swarm: inactive</span><br><span class="line"> Runtimes: runc</span><br><span class="line"> Default Runtime: runc</span><br><span class="line"> Init Binary: docker-init</span><br><span class="line"> containerd version: 894b81a4b802e4eb2a91d1ce216b8817763c29fb</span><br><span class="line"> runc version: 425e105d5a03fabd737a126ad93d62a9eeede87f</span><br><span class="line"> init version: fec3683</span><br><span class="line"> Security Options:</span><br><span class="line">  seccomp</span><br><span class="line">   Profile: default</span><br><span class="line"> Kernel Version: 4.4.193-1.el7.elrepo.x86_64</span><br><span class="line"> Operating System: CentOS Linux 7 (Core)</span><br><span class="line"> OSType: linux</span><br><span class="line"> Architecture: x86_64</span><br><span class="line"> CPUs: 4</span><br><span class="line"> Total Memory: 7.796GiB</span><br><span class="line"> Name: k8s-node-2.kxl</span><br><span class="line"> ID: GJEA:U6PT:NMHM:KWD2:DOIJ:U6XW:6N3U:4QZN:F5PT:CQXH:MZKU:VATL</span><br><span class="line"> Docker Root Dir: /var/lib/docker</span><br><span class="line"> Debug Mode: <span class="literal">false</span></span><br><span class="line"> Registry: https://index.docker.io/v1/</span><br><span class="line"> Labels:</span><br><span class="line"> Experimental: <span class="literal">false</span></span><br><span class="line"> Insecure Registries:</span><br><span class="line">  127.0.0.0/8</span><br><span class="line"> Live Restore Enabled: <span class="literal">false</span></span><br><span class="line"> Product License: Community Engine</span><br></pre></td></tr></table></figure><h3 id="2、部署-kubelet-组件"><a href="#2、部署-kubelet-组件" class="headerlink" title="2、部署 kubelet 组件"></a>2、部署 kubelet 组件</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kubelet 运行在每个 worker 节点上，接收 kube-apiserver 发送的请求，管理 Pod 容器，执行交互式命令，如 exec、run、logs 等。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kubelet 启动时自动向 kube-apiserver 注册节点信息，内置的 cadvisor 统计和监控节点的资源使用情况。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为确保安全，部署时关闭了 kubelet 的非安全 http 端口，对请求进行认证和授权，拒绝未授权的访问(如 apiserver、heapster 的请求)。</p><h4 id="2-1、创建-kubelet-bootstrap-kubeconfig-文件"><a href="#2-1、创建-kubelet-bootstrap-kubeconfig-文件" class="headerlink" title="2.1、创建 kubelet bootstrap kubeconfig 文件"></a>2.1、创建 kubelet bootstrap kubeconfig 文件</h4><p>NODE_NAMES 里面的值是node的主机名</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># NODE_NAMES=(node-01 node-02 node-03)</span></span><br><span class="line"><span class="keyword">for</span> node_name <span class="keyword">in</span> <span class="variable">$&#123;NODE_NAMES[@]&#125;</span></span><br><span class="line">  <span class="keyword">do</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"&gt;&gt;&gt; <span class="variable">$&#123;node_name&#125;</span>"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建 token</span></span><br><span class="line">    <span class="built_in">export</span> BOOTSTRAP_TOKEN=$(kubeadm token create \</span><br><span class="line">      --description kubelet-bootstrap-token \</span><br><span class="line">      --groups system:bootstrappers:<span class="variable">$&#123;node_name&#125;</span> \</span><br><span class="line">      --kubeconfig ~/.kube/config)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置集群参数</span></span><br><span class="line">    kubectl config <span class="built_in">set</span>-cluster kubernetes \</span><br><span class="line">      --certificate-authority=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">      --embed-certs=<span class="literal">true</span> \</span><br><span class="line">      --server=<span class="variable">$&#123;KUBE_APISERVER&#125;</span> \</span><br><span class="line">      --kubeconfig=kubelet-bootstrap-<span class="variable">$&#123;node_name&#125;</span>.kubeconfig</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置客户端认证参数</span></span><br><span class="line">    kubectl config <span class="built_in">set</span>-credentials kubelet-bootstrap \</span><br><span class="line">      --token=<span class="variable">$&#123;BOOTSTRAP_TOKEN&#125;</span> \</span><br><span class="line">      --kubeconfig=kubelet-bootstrap-<span class="variable">$&#123;node_name&#125;</span>.kubeconfig</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置上下文参数</span></span><br><span class="line">    kubectl config <span class="built_in">set</span>-context default \</span><br><span class="line">      --cluster=kubernetes \</span><br><span class="line">      --user=kubelet-bootstrap \</span><br><span class="line">      --kubeconfig=kubelet-bootstrap-<span class="variable">$&#123;node_name&#125;</span>.kubeconfig</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置默认上下文</span></span><br><span class="line">    kubectl config use-context default --kubeconfig=kubelet-bootstrap-<span class="variable">$&#123;node_name&#125;</span>.kubeconfig</span><br><span class="line">  <span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># for node_name in $&#123;NODE_NAMES[@]&#125;</span></span><br><span class="line">  <span class="keyword">do</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"&gt;&gt;&gt; <span class="variable">$&#123;node_name&#125;</span>"</span></span><br><span class="line">    scp kubelet-bootstrap-<span class="variable">$&#123;node_name&#125;</span>.kubeconfig root@<span class="variable">$&#123;node_name&#125;</span>:/etc/kubernetes/kubelet-bootstrap.kubeconfig</span><br><span class="line">  <span class="keyword">done</span></span><br></pre></td></tr></table></figure><ul><li><p>向 kubeconfig 写入的是 token，bootstrap 结束后 kube-controller-manager 为 kubelet 创建 client 和 server 证书；</p></li><li><p>查看 kubeadm 为各节点创建的 token:<br>master 节点查看</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubeadm token list --kubeconfig ~/.kube/config</span></span><br><span class="line">TOKEN                     TTL       EXPIRES                     USAGES                   DESCRIPTION               EXTRA GROUPS</span><br><span class="line">016e9x.306t91l832suzg8i   19h       2019-09-17T11:29:43+08:00   authentication,signing   kubelet-bootstrap-token   system:bootstrappers:node-03</span><br><span class="line">4l4tcx.juy6qs9rmrnfpbig   19h       2019-09-17T11:29:43+08:00   authentication,signing   kubelet-bootstrap-token   system:bootstrappers:node-01</span><br><span class="line">64pk36.vbhvbmtojpskyclt   19h       2019-09-17T11:29:43+08:00   authentication,signing   kubelet-bootstrap-token   system:bootstrappers:node-02</span><br></pre></td></tr></table></figure><ul><li>token 有效期为 1 天，超期后将不能再被用来 boostrap kubelet，且会被 kube-controller-manager 的 tokencleaner 清理；</li><li>kube-apiserver 接收 kubelet 的 bootstrap token 后，将请求的 user 设置为 system:bootstrap:<token>，group 设置为 system:bootstrappers，后续将为这个 group 设置 ClusterRoleBinding；</token></li></ul></li><li><p>查看各 token 关联的 Secret：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get secrets  -n kube-system|grep bootstrap-token</span></span><br><span class="line">bootstrap-token-016e9x                           bootstrap.kubernetes.io/token         7      4h25m</span><br><span class="line">bootstrap-token-4l4tcx                           bootstrap.kubernetes.io/token         7      4h25m</span><br><span class="line">bootstrap-token-64pk36                           bootstrap.kubernetes.io/token         7      4h25m</span><br></pre></td></tr></table></figure></li></ul><h4 id="2-2、创建和分发-kubelet-参数配置文件"><a href="#2-2、创建和分发-kubelet-参数配置文件" class="headerlink" title="2.2、创建和分发 kubelet 参数配置文件"></a>2.2、创建和分发 kubelet 参数配置文件</h4><p>从 v1.10 开始，部分 kubelet 参数需在配置文件中配置，kubelet –help 会提示：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">DEPRECATED: This parameter should be <span class="built_in">set</span> via the config file specified by the Kubelet<span class="string">'s --config flag</span></span><br></pre></td></tr></table></figure><ul><li><p>创建 kubelet 参数配置文件模板</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat /etc/kubernetes/kubelet-config.yaml</span></span><br><span class="line">kind: KubeletConfiguration</span><br><span class="line">apiVersion: kubelet.config.k8s.io/v1beta1</span><br><span class="line">address: <span class="string">"0.0.0.0"</span></span><br><span class="line">staticPodPath: <span class="string">""</span></span><br><span class="line">syncFrequency: 1m</span><br><span class="line">fileCheckFrequency: 20s</span><br><span class="line">httpCheckFrequency: 20s</span><br><span class="line">staticPodURL: <span class="string">""</span></span><br><span class="line">port: 10250</span><br><span class="line">readOnlyPort: 0</span><br><span class="line">rotateCertificates: <span class="literal">true</span></span><br><span class="line">serverTLSBootstrap: <span class="literal">true</span></span><br><span class="line">authentication:</span><br><span class="line">  anonymous:</span><br><span class="line">    enabled: <span class="literal">false</span></span><br><span class="line">  webhook:</span><br><span class="line">    enabled: <span class="literal">true</span></span><br><span class="line">  x509:</span><br><span class="line">    clientCAFile: <span class="string">"/etc/kubernetes/ssl/ca.pem"</span></span><br><span class="line">authorization:</span><br><span class="line">  mode: Webhook</span><br><span class="line">registryPullQPS: 0</span><br><span class="line">registryBurst: 20</span><br><span class="line">eventRecordQPS: 0</span><br><span class="line">eventBurst: 20</span><br><span class="line">enableDebuggingHandlers: <span class="literal">true</span></span><br><span class="line">enableContentionProfiling: <span class="literal">true</span></span><br><span class="line">healthzPort: 10248</span><br><span class="line">healthzBindAddress: <span class="string">"172.21.16.87"</span> <span class="comment"># node节点ip</span></span><br><span class="line">clusterDomain: <span class="string">"cluster.local"</span></span><br><span class="line">clusterDNS:</span><br><span class="line">  - <span class="string">"10.254.0.2"</span></span><br><span class="line">nodeStatusUpdateFrequency: 10s</span><br><span class="line">nodeStatusReportFrequency: 1m</span><br><span class="line">imageMinimumGCAge: 2m</span><br><span class="line">imageGCHighThresholdPercent: 85</span><br><span class="line">imageGCLowThresholdPercent: 80</span><br><span class="line">volumeStatsAggPeriod: 1m</span><br><span class="line">kubeletCgroups: <span class="string">""</span></span><br><span class="line">systemCgroups: <span class="string">""</span></span><br><span class="line">cgroupRoot: <span class="string">""</span></span><br><span class="line">cgroupsPerQOS: <span class="literal">true</span></span><br><span class="line">cgroupDriver: cgroupfs</span><br><span class="line">runtimeRequestTimeout: 10m</span><br><span class="line">hairpinMode: promiscuous-bridge</span><br><span class="line">maxPods: 100</span><br><span class="line">podCIDR: <span class="string">"172.30.0.0/16"</span></span><br><span class="line">podPidsLimit: -1</span><br><span class="line">resolvConf: /etc/resolv.conf</span><br><span class="line">maxOpenFiles: 1000000</span><br><span class="line">kubeAPIQPS: 1000</span><br><span class="line">kubeAPIBurst: 2000</span><br><span class="line">serializeImagePulls: <span class="literal">false</span></span><br><span class="line">evictionHard:</span><br><span class="line">  memory.available:  <span class="string">"100Mi"</span></span><br><span class="line">nodefs.available:  <span class="string">"10%"</span></span><br><span class="line">nodefs.inodesFree: <span class="string">"5%"</span></span><br><span class="line">imagefs.available: <span class="string">"15%"</span></span><br><span class="line">evictionSoft: &#123;&#125;</span><br><span class="line">enableControllerAttachDetach: <span class="literal">true</span></span><br><span class="line">failSwapOn: <span class="literal">true</span></span><br><span class="line">containerLogMaxSize: 20Mi</span><br><span class="line">containerLogMaxFiles: 10</span><br><span class="line">systemReserved: &#123;&#125;</span><br><span class="line">kubeReserved: &#123;&#125;</span><br><span class="line">systemReservedCgroup: <span class="string">""</span></span><br><span class="line">kubeReservedCgroup: <span class="string">""</span></span><br><span class="line">enforceNodeAllocatable: [<span class="string">"pods"</span>]</span><br></pre></td></tr></table></figure><ul><li>address：kubelet 安全端口（https，10250）监听的地址，不能为 127.0.0.1，否则 kube-apiserver、heapster 等不能调用 kubelet 的 API；</li><li>readOnlyPort=0：关闭只读端口(默认 10255)，等效为未指定；</li><li>authentication.anonymous.enabled：设置为 false，不允许匿名�访问 10250 端口；</li><li>authentication.x509.clientCAFile：指定签名客户端证书的 CA 证书，开启 HTTP 证书认证；</li><li>authentication.webhook.enabled=true：开启 HTTPs bearer token 认证；</li><li>对于未通过 x509 证书和 webhook 认证的请求(kube-apiserver 或其他客户端)，将被拒绝，提示 Unauthorized；</li><li>authroization.mode=Webhook：kubelet 使用 SubjectAccessReview API 查询 kube-apiserver 某 user、group 是否具有操作资源的权限(RBAC)；</li><li>featureGates.RotateKubeletClientCertificate、featureGates.RotateKubeletServerCertificate：自动 rotate 证书，证书的有效期取决于 kube-controller-manager 的 –experimental-cluster-signing-duration 参数；</li><li>需要 root 账户运行；</li></ul></li></ul><h4 id="2-3、创建kubelet-systemd-unit-文件"><a href="#2-3、创建kubelet-systemd-unit-文件" class="headerlink" title="2.3、创建kubelet systemd unit 文件"></a>2.3、创建kubelet systemd unit 文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat /usr/lib/systemd/system/kubelet.service </span></span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes Kubelet</span><br><span class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</span><br><span class="line">After=docker.service</span><br><span class="line">Requires=docker.service</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">WorkingDirectory=/var/<span class="built_in">log</span>/k8s/kubelet</span><br><span class="line">ExecStart=/usr/bin/kubelet \</span><br><span class="line">  --allow-privileged=<span class="literal">true</span> \</span><br><span class="line">  --bootstrap-kubeconfig=/etc/kubernetes/kubelet-bootstrap.kubeconfig \</span><br><span class="line">  --cert-dir=/etc/kubernetes/ssl \</span><br><span class="line">  --cni-conf-dir=/etc/cni/net.d \</span><br><span class="line">  --container-runtime=docker \</span><br><span class="line">  --container-runtime-endpoint=unix:///var/run/dockershim.sock \</span><br><span class="line">  --root-dir=/var/lib/kubelet \</span><br><span class="line">  --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \</span><br><span class="line">  --config=/etc/kubernetes/kubelet-config.yaml \</span><br><span class="line">  --hostname-override=172.21.16.87 \ <span class="comment"># node 节点ip</span></span><br><span class="line">  --pod-infra-container-image=registry.cn-beijing.aliyuncs.com/images_k8s/pause-amd64:3.1 \</span><br><span class="line">  --image-pull-progress-deadline=15m \</span><br><span class="line">  --volume-plugin-dir=/var/lib/kubelet/kubelet-plugins/volume/<span class="built_in">exec</span>/ \</span><br><span class="line">  --logtostderr=<span class="literal">true</span> \</span><br><span class="line">  --v=2</span><br><span class="line">Restart=always</span><br><span class="line">RestartSec=5</span><br><span class="line">StartLimitInterval=0</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><ul><li>如果设置了 –hostname-override 选项，则 kube-proxy 也需要设置该选项，否则会出现找不到 Node 的情况；</li><li>–bootstrap-kubeconfig：指向 bootstrap kubeconfig 文件，kubelet 使用该文件中的用户名和 token 向 kube-apiserver 发送 TLS Bootstrapping 请求；</li><li>K8S approve kubelet 的 csr 请求后，在 –cert-dir 目录创建证书和私钥文件，然后写入 –kubeconfig 文件；</li><li>–pod-infra-container-image 不使用 redhat 的 pod-infrastructure:latest 镜像，它不能回收容器的僵尸；</li></ul><h4 id="2-4、Bootstrap-Token-Auth-和授予权限"><a href="#2-4、Bootstrap-Token-Auth-和授予权限" class="headerlink" title="2.4、Bootstrap Token Auth 和授予权限"></a>2.4、Bootstrap Token Auth 和授予权限</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kubelet 启动时查找 –kubeletconfig 参数对应的文件是否存在，如果不存在则使用 –bootstrap-kubeconfig 指定的 kubeconfig 文件向 kube-apiserver 发送证书签名请求 (CSR)。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kube-apiserver 收到 CSR 请求后，对其中的 Token 进行认证，认证通过后将请求的 user 设置为 system:bootstrap:<token>，group 设置为 system:bootstrappers，这一过程称为 Bootstrap Token Auth。</token></p><p>默认情况下，这个 user 和 group 没有创建 CSR 的权限，kubelet 启动失败，错误日志如下：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Sep 16 11:39:43 node-02 kubelet[20385]: E0916 11:39:43.858672   20385 reflector.go:126] k8s.io/client-go/informers/factory.go:133: Failed to list *v1beta1.RuntimeClass: Unauthorized</span><br><span class="line">Sep 16 11:39:43 node-02 kubelet[20385]: E0916 11:39:43.860429   20385 reflector.go:126] k8s.io/client-go/informers/factory.go:133: Failed to list *v1beta1.CSIDriver: Unauthorized</span><br><span class="line">Sep 16 11:39:43 node-02 kubelet[20385]: E0916 11:39:43.903098   20385 kubelet.go:2244] node <span class="string">"172.21.16.240"</span> not found</span><br><span class="line">Sep 16 11:39:43 node-02 kubelet[20385]: E0916 11:39:43.985568   20385 reflector.go:126] k8s.io/kubernetes/pkg/kubelet/kubelet.go:442: Failed to list *v1.Service: Unauthorized</span><br><span class="line">Sep 16 11:39:43 node-02 kubelet[20385]: E0916 11:39:43.986781   20385 reflector.go:126] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Unauthorized</span><br><span class="line">Sep 16 11:39:43 node-02 kubelet[20385]: E0916 11:39:43.987454   20385 reflector.go:126] k8s.io/kubernetes/pkg/kubelet/kubelet.go:451: Failed to list *v1.Node: Unauthorized</span><br></pre></td></tr></table></figure><p>解决办法是：创建一个 clusterrolebinding，将 group system:bootstrappers 和 clusterrole system:node-bootstrapper 绑定：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --group=system:bootstrappers</span></span><br></pre></td></tr></table></figure><h4 id="2-5、启动-kubelet-服务"><a href="#2-5、启动-kubelet-服务" class="headerlink" title="2.5、启动 kubelet 服务"></a>2.5、启动 kubelet 服务</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># mkdir -p /var/log/k8s/kubelet</span></span><br><span class="line"><span class="comment"># systemctl daemon-reload &amp;&amp; systemctl enable kubelet &amp;&amp; systemctl restart kubelet &amp;&amp; systemctl status kubelet</span></span><br></pre></td></tr></table></figure><ul><li>启动服务前必须先创建工作目录；</li><li>关闭 swap 分区，否则 kubelet 会启动失败；</li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kubelet 启动后使用 –bootstrap-kubeconfig 向 kube-apiserver 发送 CSR 请求，当这个 CSR 被 approve 后，kube-controller-manager 为 kubelet 创建 TLS 客户端证书、私钥和 –kubeletconfig 文件。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>注意</strong>：kube-controller-manager 需要配置 –cluster-signing-cert-file 和 –cluster-signing-key-file 参数，才会为 TLS Bootstrap 创建证书和私钥。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get csr</span></span><br><span class="line">NAME        AGE     REQUESTOR                 CONDITION</span><br><span class="line">csr-bwcbm   82s     system:bootstrap:016e9x   Pending</span><br><span class="line">csr-gqdhf   105s    system:bootstrap:64pk36   Pending</span><br><span class="line">csr-q995g   6m57s   system:bootstrap:4l4tcx   Pending</span><br><span class="line">csr-xx45v   7m33s   system:bootstrap:4l4tcx   Pending</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl get nodes</span></span><br><span class="line">No resources found.</span><br></pre></td></tr></table></figure><h4 id="2-6、自动-approve-CSR-请求"><a href="#2-6、自动-approve-CSR-请求" class="headerlink" title="2.6、自动 approve CSR 请求"></a>2.6、自动 approve CSR 请求</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat /etc/kubernetes/csr-crb.yaml</span></span><br><span class="line"><span class="comment"># Approve all CSRs for the group "system:bootstrappers"</span></span><br><span class="line"> kind: ClusterRoleBinding</span><br><span class="line"> apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line"> metadata:</span><br><span class="line">   name: auto-approve-csrs-for-group</span><br><span class="line"> subjects:</span><br><span class="line"> - kind: Group</span><br><span class="line">   name: system:bootstrappers</span><br><span class="line">   apiGroup: rbac.authorization.k8s.io</span><br><span class="line"> roleRef:</span><br><span class="line">   kind: ClusterRole</span><br><span class="line">   name: system:certificates.k8s.io:certificatesigningrequests:nodeclient</span><br><span class="line">   apiGroup: rbac.authorization.k8s.io</span><br><span class="line">---</span><br><span class="line"> <span class="comment"># To let a node of the group "system:nodes" renew its own credentials</span></span><br><span class="line"> kind: ClusterRoleBinding</span><br><span class="line"> apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line"> metadata:</span><br><span class="line">   name: node-client-cert-renewal</span><br><span class="line"> subjects:</span><br><span class="line"> - kind: Group</span><br><span class="line">   name: system:nodes</span><br><span class="line">   apiGroup: rbac.authorization.k8s.io</span><br><span class="line"> roleRef:</span><br><span class="line">   kind: ClusterRole</span><br><span class="line">   name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient</span><br><span class="line">   apiGroup: rbac.authorization.k8s.io</span><br><span class="line">---</span><br><span class="line"><span class="comment"># A ClusterRole which instructs the CSR approver to approve a node requesting a</span></span><br><span class="line"><span class="comment"># serving cert matching its client cert.</span></span><br><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: approve-node-server-renewal-csr</span><br><span class="line">rules:</span><br><span class="line">- apiGroups: [<span class="string">"certificates.k8s.io"</span>]</span><br><span class="line">  resources: [<span class="string">"certificatesigningrequests/selfnodeserver"</span>]</span><br><span class="line">  verbs: [<span class="string">"create"</span>]</span><br><span class="line">---</span><br><span class="line"> <span class="comment"># To let a node of the group "system:nodes" renew its own server credentials</span></span><br><span class="line"> kind: ClusterRoleBinding</span><br><span class="line"> apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line"> metadata:</span><br><span class="line">   name: node-server-cert-renewal</span><br><span class="line"> subjects:</span><br><span class="line"> - kind: Group</span><br><span class="line">   name: system:nodes</span><br><span class="line">   apiGroup: rbac.authorization.k8s.io</span><br><span class="line"> roleRef:</span><br><span class="line">   kind: ClusterRole</span><br><span class="line">   name: approve-node-server-renewal-csr</span><br><span class="line">   apiGroup: rbac.authorization.k8s.io</span><br></pre></td></tr></table></figure><ul><li>auto-approve-csrs-for-group：自动 approve node 的第一次 CSR； 注意第一次 CSR 时，请求的 Group 为 system:bootstrappers；</li><li>node-client-cert-renewal：自动 approve node 后续过期的 client 证书，自动生成的证书 Group 为 system:nodes;</li><li>node-server-cert-renewal：自动 approve node 后续过期的 server 证书，自动生成的证书 Group 为 system:nodes;</li></ul><h4 id="2-6、等查看-kubelet-的情况"><a href="#2-6、等查看-kubelet-的情况" class="headerlink" title="2.6、等查看 kubelet 的情况"></a>2.6、等查看 kubelet 的情况</h4><p>待一段时间(1-10 分钟)，三个节点的 CSR 都被自动 approved：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get csr</span></span><br><span class="line">NAME        AGE     REQUESTOR                   CONDITION</span><br><span class="line">csr-2t4bj   2m58s   system:node:172.21.16.240   Pending</span><br><span class="line">csr-2z2mq   4m14s   system:node:172.21.16.204   Pending</span><br><span class="line">csr-bwcbm   6m6s    system:bootstrap:016e9x     Approved,Issued</span><br><span class="line">csr-gqdhf   6m29s   system:bootstrap:64pk36     Approved,Issued</span><br><span class="line">csr-q995g   11m     system:bootstrap:4l4tcx     Approved,Issued</span><br><span class="line">csr-xx45v   12m     system:bootstrap:4l4tcx     Pending</span><br></pre></td></tr></table></figure><ul><li>所有节点均 ready：<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get nodes</span></span><br><span class="line">NAME            STATUS   ROLES    AGE     VERSION</span><br><span class="line">172.21.16.204   Ready    &lt;none&gt;   4m17s   v1.14.6</span><br><span class="line">172.21.16.240   Ready    &lt;none&gt;   3m2s    v1.14.6</span><br><span class="line">172.21.16.87    Ready    &lt;none&gt;   3s      v1.14.6</span><br></pre></td></tr></table></figure></li></ul><p>kube-controller-manager 为各 node 生成了 kubeconfig 文件和公私钥：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ls -l /etc/kubernetes/kubelet.kubeconfig</span></span><br><span class="line">-rw------- 1 root root 2311 Sep 16 11:31 /etc/kubernetes/kubelet.kubeconfig</span><br><span class="line"></span><br><span class="line"><span class="comment"># ls -l /etc/kubernetes/ssl/|grep kubelet</span></span><br><span class="line">-rw------- 1 root root 1281 Sep 16 11:43 kubelet-client-2019-09-16-11-43-20.pem</span><br><span class="line">lrwxrwxrwx 1 root root   58 Sep 16 11:43 kubelet-client-current.pem -&gt; /etc/kubernetes/ssl/kubelet-client-2019-09-16-11-43-20.pem</span><br></pre></td></tr></table></figure><p>没有自动生成 kubelet server 证书；</p><h4 id="2-8、手动-approve-server-cert-csr"><a href="#2-8、手动-approve-server-cert-csr" class="headerlink" title="2.8、手动 approve server cert csr"></a>2.8、手动 approve server cert csr</h4><p>基于<a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/#kubelet-configuratio" target="_blank" rel="noopener">安全性考虑</a>，CSR approving controllers 不会自动 approve kubelet server 证书签名请求，需要手动 approve：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get csr</span></span><br><span class="line">NAME        AGE     REQUESTOR                   CONDITION</span><br><span class="line">csr-2t4bj   3m5s    system:node:172.21.16.240   Pending</span><br><span class="line">csr-2z2mq   4m21s   system:node:172.21.16.204   Pending</span><br><span class="line">csr-bwcbm   6m13s   system:bootstrap:016e9x     Approved,Issued</span><br><span class="line">csr-gqdhf   6m36s   system:bootstrap:64pk36     Approved,Issued</span><br><span class="line">csr-gtkrt   7s      system:node:172.21.16.87    Pending</span><br><span class="line">csr-q995g   11m     system:bootstrap:4l4tcx     Approved,Issued</span><br><span class="line">csr-xx45v   12m     system:bootstrap:4l4tcx     Pending</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl certificate approve csr-2t4bj</span></span><br><span class="line">certificatesigningrequest.certificates.k8s.io/csr-2t4bj approved</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl certificate approve csr-2z2mq </span></span><br><span class="line">certificatesigningrequest.certificates.k8s.io/csr-2z2mq approved</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl certificate approve csr-gtkrt</span></span><br><span class="line">certificatesigningrequest.certificates.k8s.io/csr-gtkrt approved</span><br><span class="line"></span><br><span class="line"><span class="comment"># ls -l /etc/kubernetes/ssl/|grep kubelet</span></span><br><span class="line">-rw------- 1 root root 1281 Sep 16 11:43 kubelet-client-2019-09-16-11-43-20.pem</span><br><span class="line">lrwxrwxrwx 1 root root   58 Sep 16 11:43 kubelet-client-current.pem -&gt; /etc/kubernetes/ssl/kubelet-client-2019-09-16-11-43-20.pem</span><br><span class="line">-rw------- 1 root root 1305 Sep 16 11:44 kubelet-server-2019-09-16-11-44-12.pem</span><br><span class="line">lrwxrwxrwx 1 root root   58 Sep 16 11:44 kubelet-server-current.pem -&gt; /etc/kubernetes/ssl/kubelet-server-2019-09-16-11-44-12.pem</span><br></pre></td></tr></table></figure><h4 id="2-9、kubelet-提供的-API-接口"><a href="#2-9、kubelet-提供的-API-接口" class="headerlink" title="2.9、kubelet 提供的 API 接口"></a>2.9、kubelet 提供的 API 接口</h4><p>kubelet 启动后监听多个端口，用于接收 kube-apiserver 或其它客户端发送的请求：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># netstat -lnpt|grep kubelet</span></span><br><span class="line">tcp        0      0 127.0.0.1:43042         0.0.0.0:*               LISTEN      22726/kubelet       </span><br><span class="line">tcp        0      0 172.21.16.87:10248      0.0.0.0:*               LISTEN      22726/kubelet       </span><br><span class="line">tcp6       0      0 :::10250                :::*                    LISTEN      22726/kubelet</span><br></pre></td></tr></table></figure><ul><li>10248: healthz http 服务；</li><li>10250: https 服务，访问该端口时需要认证和授权（即使访问 /healthz 也需要）；</li><li>未开启只读端口 10255；</li><li>从 K8S v1.10 开始，去除了 –cadvisor-port 参数（默认 4194 端口），不支持访问 cAdvisor UI &amp; API。</li></ul><p>由于关闭了匿名认证，同时开启了 webhook 授权，所有访问 10250 端口 https API 的请求都需要被认证和授权。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;预定义的 ClusterRole system:kubelet-api-admin 授予访问 kubelet 所有 API 的权限(kube-apiserver 使用的 kubernetes 证书 User 授予了该权限)：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl describe clusterrole system:kubelet-api-admin</span></span><br><span class="line">Name:         system:kubelet-api-admin</span><br><span class="line">Labels:       kubernetes.io/bootstrapping=rbac-defaults</span><br><span class="line">Annotations:  rbac.authorization.kubernetes.io/autoupdate: <span class="literal">true</span></span><br><span class="line">PolicyRule:</span><br><span class="line">  Resources      Non-Resource URLs  Resource Names  Verbs</span><br><span class="line">  ---------      -----------------  --------------  -----</span><br><span class="line">  nodes/<span class="built_in">log</span>      []                 []              [*]</span><br><span class="line">  nodes/metrics  []                 []              [*]</span><br><span class="line">  nodes/proxy    []                 []              [*]</span><br><span class="line">  nodes/spec     []                 []              [*]</span><br><span class="line">  nodes/stats    []                 []              [*]</span><br><span class="line">  nodes          []                 []              [get list watch proxy]</span><br></pre></td></tr></table></figure><h4 id="2-10、kubelet-api-认证和授权"><a href="#2-10、kubelet-api-认证和授权" class="headerlink" title="2.10、kubelet api 认证和授权"></a>2.10、kubelet api 认证和授权</h4><p>kubelet 配置了如下认证参数:</p><ul><li>authentication.anonymous.enabled：设置为 false，不允许匿名访问 10250 端口；</li><li>authentication.x509.clientCAFile：指定签名客户端证书的 CA 证书，开启 HTTPs 证书认证；</li><li>authentication.webhook.enabled=true：开启 HTTPs bearer token 认证；</li></ul><p>同时配置了如下授权参数:</p><ul><li>authroization.mode=Webhook：开启 RBAC 授权</li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kubelet 收到请求后，使用 clientCAFile 对证书签名进行认证，或者查询 bearer token 是否有效。如果两者都没通过，则拒绝请求，提示 Unauthorized：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># curl -s --cacert /etc/kubernetes/ssl/ca.pem https://172.21.16.87:10250/metrics</span></span><br><span class="line">Unauthorized</span><br><span class="line"></span><br><span class="line"><span class="comment"># curl -s --cacert /etc/kubernetes/ssl/ca.pem -H "Authorization: Bearer 123456"  https://172.21.16.87:10250/metrics</span></span><br><span class="line">Unauthorized</span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过认证后，kubelet 使用 SubjectAccessReview API 向 kube-apiserver 发送请求，查询证书或 token 对应的 user、group 是否有操作资源的权限(RBAC)；</p><h4 id="2-11、证书认证和授权"><a href="#2-11、证书认证和授权" class="headerlink" title="2.11、证书认证和授权"></a>2.11、证书认证和授权</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 权限不足的证书；</span></span><br><span class="line"><span class="comment"># curl -s --cacert /etc/kubernetes/ssl/ca.pem --cert /etc/kubernetes/ssl/kube-controller-manager.pem --key /etc/kubernetes/ssl/kube-controller-manager-key.pem https://172.21.16.87:10250/metrics</span></span><br><span class="line">Forbidden (user=system:kube-controller-manager, verb=get, resource=nodes, subresource=metrics)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用部署 kubectl 命令行工具时创建的、具有最高权限的 admin 证书；</span></span><br><span class="line"><span class="comment"># curl -s --cacert /etc/kubernetes/ssl/ca.pem --cert /etc/kubernetes/ssl/admin.pem --key /etc/kubernetes/ssl/admin-key.pem https://172.21.16.87:10250/metrics</span></span><br><span class="line"><span class="comment"># HELP apiserver_audit_event_total Counter of audit events generated and sent to the audit backend.</span></span><br><span class="line"><span class="comment"># TYPE apiserver_audit_event_total counter</span></span><br><span class="line">apiserver_audit_event_total 0</span><br><span class="line"><span class="comment"># HELP apiserver_audit_requests_rejected_total Counter of apiserver requests rejected due to an error in audit logging backend.</span></span><br><span class="line"><span class="comment"># TYPE apiserver_audit_requests_rejected_total counter</span></span><br><span class="line">apiserver_audit_requests_rejected_total 0</span><br><span class="line"><span class="comment"># HELP apiserver_client_certificate_expiration_seconds Distribution of the remaining lifetime on the certificate used to authenticate a request.</span></span><br><span class="line"><span class="comment"># TYPE apiserver_client_certificate_expiration_seconds histogram</span></span><br><span class="line">apiserver_client_certificate_expiration_seconds_bucket&#123;le=<span class="string">"0"</span>&#125; 0</span><br><span class="line">apiserver_client_certificate_expiration_seconds_bucket&#123;le=<span class="string">"1800"</span>&#125; 0</span><br></pre></td></tr></table></figure><ul><li>–cacert、–cert、–key 的参数值必须是文件路径，如上面的 ./admin.pem 不能省略 ./，否则返回 401 Unauthorized；</li></ul><h4 id="2-12、bear-token-认证和授权"><a href="#2-12、bear-token-认证和授权" class="headerlink" title="2.12、bear token 认证和授权"></a>2.12、bear token 认证和授权</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;创建一个 ServiceAccount，将它和 ClusterRole system:kubelet-api-admin 绑定，从而具有调用 kubelet API 的权限：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl create sa kubelet-api-test</span></span><br><span class="line"><span class="comment"># kubectl create clusterrolebinding kubelet-api-test --clusterrole=system:kubelet-api-admin --serviceaccount=default:kubelet-api-test</span></span><br><span class="line"><span class="comment"># SECRET=$(kubectl get secrets | grep kubelet-api-test | awk '&#123;print $1&#125;')</span></span><br><span class="line"><span class="comment"># TOKEN=$(kubectl describe secret $&#123;SECRET&#125; | grep -E '^token' | awk '&#123;print $2&#125;')</span></span><br><span class="line"><span class="comment"># echo $&#123;TOKEN&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># curl -s --cacert /etc/kubernetes/ssl/ca.pem -H "Authorization: Bearer $&#123;TOKEN&#125;" https://172.21.16.87:10250/metrics|head</span></span><br></pre></td></tr></table></figure><h3 id="3、cadvisor-和-metrics"><a href="#3、cadvisor-和-metrics" class="headerlink" title="3、cadvisor 和 metrics"></a>3、cadvisor 和 metrics</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;cadvisor 是内嵌在 kubelet 二进制中的，统计所在节点各容器的资源(CPU、内存、磁盘、网卡)使用情况的服务。<br>浏览器访问 <a href="https://172.21.16.87:10250/metrics" target="_blank" rel="noopener">https://172.21.16.87:10250/metrics</a> 和 <a href="https://172.21.16.87:10250/metrics/cadvisor" target="_blank" rel="noopener">https://172.21.16.87:10250/metrics/cadvisor</a> 分别返回 kubelet 和 cadvisor 的 metrics。<br><img src="https://img.xxlaila.cn/1568624798589.jpg" alt="img"></p><p><strong>注意:</strong></p><ul><li>kubelet.config.json 设置 authentication.anonymous.enabled 为 false，不允许匿名证书访问 10250 的 https 服务；</li><li>参考<a href="https://xxlaila.github.io/2019/09/04/kubelet提供api请求接口/" target="_blank" rel="noopener">kubelet提供api请求接口</a>，创建和导入相关证书，然后访问上面的 10250 端口；</li></ul><h4 id="3-1、获取-kubelet-的配置"><a href="#3-1、获取-kubelet-的配置" class="headerlink" title="3.1、获取 kubelet 的配置"></a>3.1、获取 kubelet 的配置</h4><p>从 kube-apiserver 获取各节点 kubelet 的配置：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用部署 kubectl 命令行工具时创建的、具有最高权限的 admin 证书；</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># curl -sSL --cacert /etc/kubernetes/ssl/ca.pem --cert /etc/kubernetes/ssl/admin.pem --key /etc/kubernetes/ssl/admin-key.pem $&#123;KUBE_APISERVER&#125;/api/v1/nodes/172.21.16.87/proxy/configz | jq  '.kubeletconfig|.kind="KubeletConfiguration"|.apiVersion="kubelet.config.k8s.io/v1beta1"'</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"syncFrequency"</span>: <span class="string">"1m0s"</span>,</span><br><span class="line">  <span class="string">"fileCheckFrequency"</span>: <span class="string">"20s"</span>,</span><br><span class="line">  <span class="string">"httpCheckFrequency"</span>: <span class="string">"20s"</span>,</span><br><span class="line">  <span class="string">"address"</span>: <span class="string">"0.0.0.0"</span>,</span><br><span class="line">  <span class="string">"port"</span>: 10250,</span><br><span class="line">  <span class="string">"rotateCertificates"</span>: <span class="literal">true</span>,</span><br><span class="line">  <span class="string">"serverTLSBootstrap"</span>: <span class="literal">true</span>,</span><br><span class="line">  <span class="string">"authentication"</span>: &#123;</span><br><span class="line">    <span class="string">"x509"</span>: &#123;</span><br><span class="line">      <span class="string">"clientCAFile"</span>: <span class="string">"/etc/kubernetes/ssl/ca.pem"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"webhook"</span>: &#123;</span><br><span class="line">      <span class="string">"enabled"</span>: <span class="literal">true</span>,</span><br><span class="line">      <span class="string">"cacheTTL"</span>: <span class="string">"2m0s"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"anonymous"</span>: &#123;</span><br><span class="line">      <span class="string">"enabled"</span>: <span class="literal">false</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"authorization"</span>: &#123;</span><br><span class="line">    <span class="string">"mode"</span>: <span class="string">"Webhook"</span>,</span><br><span class="line">    <span class="string">"webhook"</span>: &#123;</span><br><span class="line">      <span class="string">"cacheAuthorizedTTL"</span>: <span class="string">"5m0s"</span>,</span><br><span class="line">      <span class="string">"cacheUnauthorizedTTL"</span>: <span class="string">"30s"</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"registryPullQPS"</span>: 0,</span><br><span class="line">  <span class="string">"registryBurst"</span>: 20,</span><br><span class="line">  <span class="string">"eventRecordQPS"</span>: 0,</span><br><span class="line">  <span class="string">"eventBurst"</span>: 20,</span><br><span class="line">  <span class="string">"enableDebuggingHandlers"</span>: <span class="literal">true</span>,</span><br><span class="line">  <span class="string">"enableContentionProfiling"</span>: <span class="literal">true</span>,</span><br><span class="line">  <span class="string">"healthzPort"</span>: 10248,</span><br><span class="line">  <span class="string">"healthzBindAddress"</span>: <span class="string">"172.21.16.87"</span>,</span><br><span class="line">  <span class="string">"oomScoreAdj"</span>: -999,</span><br><span class="line">  <span class="string">"clusterDomain"</span>: <span class="string">"cluster.local"</span>,</span><br><span class="line">  <span class="string">"clusterDNS"</span>: [</span><br><span class="line">    <span class="string">"10.254.0.2"</span></span><br><span class="line">  ],</span><br><span class="line">  <span class="string">"streamingConnectionIdleTimeout"</span>: <span class="string">"4h0m0s"</span>,</span><br><span class="line">  <span class="string">"nodeStatusUpdateFrequency"</span>: <span class="string">"10s"</span>,</span><br><span class="line">  <span class="string">"nodeStatusReportFrequency"</span>: <span class="string">"1m0s"</span>,</span><br><span class="line">  <span class="string">"nodeLeaseDurationSeconds"</span>: 40,</span><br><span class="line">  <span class="string">"imageMinimumGCAge"</span>: <span class="string">"2m0s"</span>,</span><br><span class="line">  <span class="string">"imageGCHighThresholdPercent"</span>: 85,</span><br><span class="line">  <span class="string">"imageGCLowThresholdPercent"</span>: 80,</span><br><span class="line">  <span class="string">"volumeStatsAggPeriod"</span>: <span class="string">"1m0s"</span>,</span><br><span class="line">  <span class="string">"cgroupsPerQOS"</span>: <span class="literal">true</span>,</span><br><span class="line">  <span class="string">"cgroupDriver"</span>: <span class="string">"cgroupfs"</span>,</span><br><span class="line">  <span class="string">"cpuManagerPolicy"</span>: <span class="string">"none"</span>,</span><br><span class="line">  <span class="string">"cpuManagerReconcilePeriod"</span>: <span class="string">"10s"</span>,</span><br><span class="line">  <span class="string">"runtimeRequestTimeout"</span>: <span class="string">"10m0s"</span>,</span><br><span class="line">  <span class="string">"hairpinMode"</span>: <span class="string">"promiscuous-bridge"</span>,</span><br><span class="line">  <span class="string">"maxPods"</span>: 100,</span><br><span class="line">  <span class="string">"podCIDR"</span>: <span class="string">"172.30.0.0/16"</span>,</span><br><span class="line">  <span class="string">"podPidsLimit"</span>: -1,</span><br><span class="line">  <span class="string">"resolvConf"</span>: <span class="string">"/etc/resolv.conf"</span>,</span><br><span class="line">  <span class="string">"cpuCFSQuota"</span>: <span class="literal">true</span>,</span><br><span class="line">  <span class="string">"cpuCFSQuotaPeriod"</span>: <span class="string">"100ms"</span>,</span><br><span class="line">  <span class="string">"maxOpenFiles"</span>: 1000000,</span><br><span class="line">  <span class="string">"contentType"</span>: <span class="string">"application/vnd.kubernetes.protobuf"</span>,</span><br><span class="line">  <span class="string">"kubeAPIQPS"</span>: 1000,</span><br><span class="line">  <span class="string">"kubeAPIBurst"</span>: 2000,</span><br><span class="line">  <span class="string">"serializeImagePulls"</span>: <span class="literal">false</span>,</span><br><span class="line">  <span class="string">"evictionHard"</span>: &#123;</span><br><span class="line">    <span class="string">"memory.available"</span>: <span class="string">"100Mi"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"evictionPressureTransitionPeriod"</span>: <span class="string">"5m0s"</span>,</span><br><span class="line">  <span class="string">"enableControllerAttachDetach"</span>: <span class="literal">true</span>,</span><br><span class="line">  <span class="string">"makeIPTablesUtilChains"</span>: <span class="literal">true</span>,</span><br><span class="line">  <span class="string">"iptablesMasqueradeBit"</span>: 14,</span><br><span class="line">  <span class="string">"iptablesDropBit"</span>: 15,</span><br><span class="line">  <span class="string">"failSwapOn"</span>: <span class="literal">true</span>,</span><br><span class="line">  <span class="string">"containerLogMaxSize"</span>: <span class="string">"20Mi"</span>,</span><br><span class="line">  <span class="string">"containerLogMaxFiles"</span>: 10,</span><br><span class="line">  <span class="string">"configMapAndSecretChangeDetectionStrategy"</span>: <span class="string">"Watch"</span>,</span><br><span class="line">  <span class="string">"enforceNodeAllocatable"</span>: [</span><br><span class="line">    <span class="string">"pods"</span></span><br><span class="line">  ],</span><br><span class="line">  <span class="string">"kind"</span>: <span class="string">"KubeletConfiguration"</span>,</span><br><span class="line">  <span class="string">"apiVersion"</span>: <span class="string">"kubelet.config.k8s.io/v1beta1"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="4、部署-kube-proxy-组件"><a href="#4、部署-kube-proxy-组件" class="headerlink" title="4、部署 kube-proxy 组件"></a>4、部署 kube-proxy 组件</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kube-proxy 运行在所有 worker 节点上，它监听 apiserver 中 service 和 endpoint 的变化情况，创建路由规则以提供服务 IP 和负载均衡功能。</p><h4 id="4-1、创建-kube-proxy-证书"><a href="#4-1、创建-kube-proxy-证书" class="headerlink" title="4.1、创建 kube-proxy 证书"></a>4.1、创建 kube-proxy 证书</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat &gt; kube-proxy-csr.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"CN"</span>: <span class="string">"system:kube-proxy"</span>,</span><br><span class="line">  <span class="string">"key"</span>: &#123;</span><br><span class="line">    <span class="string">"algo"</span>: <span class="string">"rsa"</span>,</span><br><span class="line">    <span class="string">"size"</span>: 2048</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"names"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"C"</span>: <span class="string">"CN"</span>,</span><br><span class="line">      <span class="string">"ST"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">      <span class="string">"L"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">      <span class="string">"O"</span>: <span class="string">"k8s"</span>,</span><br><span class="line">      <span class="string">"OU"</span>: <span class="string">"4Paradigm"</span></span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li><p>CN：指定该证书的 User 为 system:kube-proxy；</p></li><li><p>预定义的 RoleBinding system:node-proxier 将User system:kube-proxy 与 Role system:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限；</p></li><li><p>该证书只会被 kube-proxy 当做 client 证书使用，所以 hosts 字段为空；</p></li><li><p>生成证书和私钥</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes  kube-proxy-csr.json | cfssljson -bare kube-proxy</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ls kube-proxy*.pem</span></span><br><span class="line">kube-proxy-key.pem  kube-proxy.pem</span><br></pre></td></tr></table></figure></li></ul><h4 id="4-2、创建和分发-kubeconfig-文件"><a href="#4-2、创建和分发-kubeconfig-文件" class="headerlink" title="4.2、创建和分发 kubeconfig 文件"></a>4.2、创建和分发 kubeconfig 文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl config set-cluster kubernetes \</span></span><br><span class="line">  --certificate-authority=ca.pem \</span><br><span class="line">  --embed-certs=<span class="literal">true</span> \</span><br><span class="line">  --server=<span class="variable">$&#123;KUBE_APISERVER&#125;</span> \</span><br><span class="line">  --kubeconfig=kube-proxy.kubeconfig</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl config set-credentials kube-proxy \</span></span><br><span class="line">  --client-certificate=kube-proxy.pem \</span><br><span class="line">  --client-key=kube-proxy-key.pem \</span><br><span class="line">  --embed-certs=<span class="literal">true</span> \</span><br><span class="line">  --kubeconfig=kube-proxy.kubeconfig</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl config set-context default \</span></span><br><span class="line">  --cluster=kubernetes \</span><br><span class="line">  --user=kube-proxy \</span><br><span class="line">  --kubeconfig=kube-proxy.kubeconfig</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig</span></span><br></pre></td></tr></table></figure><ul><li>–embed-certs=true：将 ca.pem 和 admin.pem 证书内容嵌入到生成的 kubectl-proxy.kubeconfig 文件中(不加时，写入的是证书文件路径)</li></ul><h4 id="4-3、创建-kube-proxy-配置文件"><a href="#4-3、创建-kube-proxy-配置文件" class="headerlink" title="4.3、创建 kube-proxy 配置文件"></a>4.3、创建 kube-proxy 配置文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat /etc/kubernetes/kube-proxy-config.yaml</span></span><br><span class="line">kind: KubeProxyConfiguration</span><br><span class="line">apiVersion: kubeproxy.config.k8s.io/v1alpha1</span><br><span class="line">clientConnection:</span><br><span class="line">  burst: 200</span><br><span class="line">  kubeconfig: <span class="string">"/etc/kubernetes/kube-proxy.kubeconfig"</span></span><br><span class="line">  qps: 100</span><br><span class="line">bindAddress: 0.0.0.0</span><br><span class="line">healthzBindAddress: 172.21.16.87:10256 <span class="comment">#node</span></span><br><span class="line">metricsBindAddress: 172.21.16.87:10249 <span class="comment">#node</span></span><br><span class="line">enableProfiling: <span class="literal">true</span></span><br><span class="line">clusterCIDR: 172.30.0.0/16</span><br><span class="line">hostnameOverride: 172.21.16.87 <span class="comment">#node </span></span><br><span class="line">mode: <span class="string">"ipvs"</span></span><br><span class="line">portRange: <span class="string">""</span></span><br><span class="line">kubeProxyIPTablesConfiguration:</span><br><span class="line">  masqueradeAll: <span class="literal">false</span></span><br><span class="line">kubeProxyIPVSConfiguration:</span><br><span class="line">  scheduler: rr</span><br><span class="line">  excludeCIDRs: []</span><br></pre></td></tr></table></figure><ul><li>bindAddress: 监听地址；</li><li>clientConnection.kubeconfig: 连接 apiserver 的 kubeconfig 文件；</li><li>clusterCIDR: kube-proxy 根据 –cluster-cidr 判断集群内部和外部流量，指定 –cluster-cidr 或 –masquerade-all 选项后 kube-proxy 才会对访问 Service IP 的请求做 SNAT；</li><li>hostnameOverride: 参数值必须与 kubelet 的值一致，否则 kube-proxy 启动后会找不到该 Node，从而不会创建任何 ipvs 规则；</li><li>mode: 使用 ipvs 模式；</li></ul><h4 id="4-4、创建kube-proxy-systemd-unit-文件"><a href="#4-4、创建kube-proxy-systemd-unit-文件" class="headerlink" title="4.4、创建kube-proxy systemd unit 文件"></a>4.4、创建kube-proxy systemd unit 文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat /usr/lib/systemd/system/kube-proxy.service </span></span><br><span class="line">[Service]</span><br><span class="line">WorkingDirectory=/var/<span class="built_in">log</span>/k8s/kube-proxy</span><br><span class="line">ExecStart=/usr/bin/kube-proxy \</span><br><span class="line">  --config=/etc/kubernetes/kube-proxy-config.yaml \</span><br><span class="line">  --logtostderr=<span class="literal">true</span> \</span><br><span class="line">  --v=2</span><br><span class="line">Restart=on-failure</span><br><span class="line">RestartSec=5</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><h4 id="4-5、启动-kube-proxy-服务"><a href="#4-5、启动-kube-proxy-服务" class="headerlink" title="4.5、启动 kube-proxy 服务"></a>4.5、启动 kube-proxy 服务</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># mkdir -p /var/log/k8s/kube-proxy</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># systemctl daemon-reload &amp;&amp; systemctl enable kube-proxy &amp;&amp; systemctl restart kube-proxy &amp;&amp; systemctl status kube-proxy</span></span><br></pre></td></tr></table></figure><h4 id="4-5、检查"><a href="#4-5、检查" class="headerlink" title="4.5、检查"></a>4.5、检查</h4><ul><li><p>查看监听端口</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># netstat -lnpt|grep kube-prox</span></span><br><span class="line">tcp        0      0 172.21.16.87:10256      0.0.0.0:*               LISTEN      27423/kube-proxy    </span><br><span class="line">tcp        0      0 172.21.16.87:10249      0.0.0.0:*               LISTEN      27423/kube-proxy</span><br></pre></td></tr></table></figure></li><li><p>查看 ipvs 路由规则</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ipvsadm -ln</span></span><br><span class="line">IP Virtual Server version 1.2.1 (size=4096)</span><br><span class="line">Prot LocalAddress:Port Scheduler Flags</span><br><span class="line">  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</span><br><span class="line">TCP  10.254.0.1:443 rr</span><br><span class="line">  -&gt; 172.21.17.30:6443            Masq    1      0          0         </span><br><span class="line">  -&gt; 172.21.17.31:6443            Masq    1      0          0 </span><br><span class="line">  -&gt; 172.21.16.110:6443           Masq    1      0          0</span><br></pre></td></tr></table></figure></li></ul><!-- rebuild by neat -->]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>v1.14 node安装</tag>
      </tags>
  </entry>
  <entry>
    <title>kubernetes-v1.14安装</title>
    <url>/2019/09/11/kubernetes-v1-14%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:23 GMT+0800 (China Standard Time) --><h3 id="1、环境准备"><a href="#1、环境准备" class="headerlink" title="1、环境准备"></a>1、环境准备</h3><table><thead><tr><th>ip</th><th>type</th><th>docker</th><th>os</th><th>k8s version</th></tr></thead><tbody><tr><td>172.21.17.30</td><td>master,etcd</td><td></td><td>CentOS Linux release 7.4.1708</td><td>v1.14.6</td></tr><tr><td>172.21.17.31</td><td>master,etcd</td><td></td><td>CentOS Linux release 7.4.1708</td><td></td></tr><tr><td>172.21.16.110</td><td>master,etcd</td><td></td><td>CentOS Linux release 7.4.1708</td><td></td></tr><tr><td>172.21.16.87</td><td>node,flanneld</td><td>18.06.2-ce</td><td>CentOS Linux release 7.4.1708</td><td></td></tr><tr><td>172.21.16.240</td><td>node,flanneld,ha+kee</td><td>18.06.2-ce</td><td>CentOS Linux release 7.4.1708</td><td></td></tr><tr><td>172.21.16.204</td><td>node,flanneld,ha+kee</td><td>18.06.2-ce</td><td>CentOS Linux release 7.4.1708</td><td></td></tr><tr><td>172.21.16.45</td><td>vip</td><td></td><td>CentOS Linux release 7.4.1708</td><td></td></tr></tbody></table><h3 id="2、初始化系统"><a href="#2、初始化系统" class="headerlink" title="2、初始化系统"></a>2、初始化系统</h3><h4 id="2-1、安装依赖包"><a href="#2-1、安装依赖包" class="headerlink" title="2.1、安装依赖包"></a>2.1、安装依赖包</h4><a id="more"></a><p>每台服务器均操作,关闭防火墙,关闭selinux</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># yum install -y epel-release</span></span><br><span class="line"><span class="comment"># yum install -y conntrack ntpdate ntp ipvsadm ipset jq iptables curl sysstat libseccomp wget</span></span><br></pre></td></tr></table></figure><h4 id="2-2、关闭-swap-分区"><a href="#2-2、关闭-swap-分区" class="headerlink" title="2.2、关闭 swap 分区"></a>2.2、关闭 swap 分区</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果开启了 swap 分区，kubelet 会启动失败(可以通过将参数 –fail-swap-on 设置为 false 来忽略 swap on)，故需要在每台机器上关闭 swap 分区。同时注释 /etc/fstab 中相应的条目，防止开机自动挂载 swap 分区。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># swapoff -a</span></span><br><span class="line"><span class="comment"># sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab</span></span><br></pre></td></tr></table></figure><h4 id="2-3、加载内核模块"><a href="#2-3、加载内核模块" class="headerlink" title="2.3、加载内核模块"></a>2.3、加载内核模块</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># modprobe ip_vs_rr</span></span><br><span class="line"><span class="comment"># modprobe br_netfilter</span></span><br></pre></td></tr></table></figure><h4 id="2-4、优化内核参数"><a href="#2-4、优化内核参数" class="headerlink" title="2.4、优化内核参数"></a>2.4、优化内核参数</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat /etc/sysctl.d/kubernetes.conf </span></span><br><span class="line">net.bridge.bridge-nf-call-iptables=1</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables=1</span><br><span class="line">net.ipv4.ip_forward=1</span><br><span class="line">net.ipv4.tcp_tw_recycle=0</span><br><span class="line">vm.swappiness=0</span><br><span class="line">vm.overcommit_memory=1</span><br><span class="line">vm.panic_on_oom=0</span><br><span class="line">fs.inotify.max_user_instances=8192</span><br><span class="line">fs.inotify.max_user_watches=1048576</span><br><span class="line">fs.file-max=52706963</span><br><span class="line">fs.nr_open=52706963</span><br><span class="line">net.ipv6.conf.all.disable_ipv6=1</span><br><span class="line">net.netfilter.nf_conntrack_max=2310720</span><br><span class="line"></span><br><span class="line"><span class="comment"># sysctl -p /etc/sysctl.d/kubernetes.conf</span></span><br></pre></td></tr></table></figure><ul><li>必须关闭 tcp_tw_recycle，否则和 NAT 冲突，会导致服务不通；</li><li>关闭 IPV6，防止触发 docker BUG；</li></ul><h4 id="2-5、设置系统时区"><a href="#2-5、设置系统时区" class="headerlink" title="2.5、设置系统时区"></a>2.5、设置系统时区</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># timedatectl set-timezone Asia/Shanghai</span></span><br><span class="line"><span class="comment"># timedatectl set-local-rtc 0</span></span><br><span class="line"><span class="comment"># systemctl restart rsyslog </span></span><br><span class="line"><span class="comment"># systemctl restart crond</span></span><br></pre></td></tr></table></figure><h4 id="2-6、关闭无关的服务"><a href="#2-6、关闭无关的服务" class="headerlink" title="2.6、关闭无关的服务"></a>2.6、关闭无关的服务</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># systemctl stop postfix &amp;&amp; systemctl disable postfix</span></span><br></pre></td></tr></table></figure><h3 id="3、升级内核"><a href="#3、升级内核" class="headerlink" title="3、升级内核"></a>3、升级内核</h3><p>以下在master节点操作<br>CentOS 7.x 系统自带的 3.10.x 内核存在一些 Bugs，导致运行的 Docker、Kubernetes 不稳定，例如:</p><ul><li>1.高版本的 docker(1.13 以后) 启用了 3.10 kernel 实验支持的 kernel memory account 功能(无法关闭)，当节点压力大如频繁启动和停止容器时会导致 cgroup memory leak；</li><li>2.网络设备引用计数泄漏，会导致类似于报错：”kernel:unregister_netdevice: waiting for eth0 to become free. Usage count = 1”;<br>解决方案如下:</li><li>1.升级内核到 4.4.X 以上</li><li>2.或者，手动编译内核，disable CONFIG_MEMCG_KMEM 特性</li><li>或者，安装修复了该问题的 Docker 18.09.1 及以上的版本。但由于 kubelet 也会设置 kmem（它 vendor 了 runc），所以需要重新编译 kubelet 并指定 GOFLAGS=”-tags=nokmem”</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> --branch v1.14.1 --single-branch --depth 1 https://github.com/kubernetes/kubernetes</span><br><span class="line"><span class="built_in">cd</span> kubernetes</span><br><span class="line">KUBE_GIT_VERSION=v1.14.1 ./build/run.sh make kubelet GOFLAGS=<span class="string">"-tags=nokmem"</span></span><br></pre></td></tr></table></figure><h4 id="3-1、内核升级方法"><a href="#3-1、内核升级方法" class="headerlink" title="3.1、内核升级方法"></a>3.1、内核升级方法</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm</span></span><br><span class="line"><span class="comment"># 安装完成后检查 /boot/grub2/grub.cfg 中对应内核 menuentry 中是否包含 initrd16 配置，如果没有，再安装一次！</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># yum --enablerepo=elrepo-kernel install -y kernel-lt</span></span><br><span class="line"><span class="comment"># 设置开机从新内核启动</span></span><br><span class="line"><span class="comment"># grub2-set-default 0</span></span><br></pre></td></tr></table></figure><h4 id="3-2、安装内核源文件"><a href="#3-2、安装内核源文件" class="headerlink" title="3.2、安装内核源文件"></a>3.2、安装内核源文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># yum --enablerepo=elrepo-kernel install kernel-lt-devel-$(uname -r) kernel-lt-headers-$(uname -r)</span></span><br></pre></td></tr></table></figure><h4 id="3-3、关闭-NUMA"><a href="#3-3、关闭-NUMA" class="headerlink" title="3.3、关闭 NUMA"></a>3.3、关闭 NUMA</h4><p>在其中一台master节点操作</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cp /etc/default/grub&#123;,.bak&#125;</span></span><br><span class="line"><span class="comment"># 在 GRUB_CMDLINE_LINUX 一行添加 `numa=off` 参数，如下所示</span></span><br><span class="line"><span class="comment"># cat /etc/default/grub</span></span><br><span class="line">GRUB_TIMEOUT=1</span><br><span class="line">GRUB_DISTRIBUTOR=<span class="string">"<span class="variable">$(sed 's, release .*$,,g' /etc/system-release)</span>"</span></span><br><span class="line">GRUB_DEFAULT=saved</span><br><span class="line">GRUB_DISABLE_SUBMENU=<span class="literal">true</span></span><br><span class="line">GRUB_TERMINAL=<span class="string">"serial console"</span></span><br><span class="line">GRUB_SERIAL_COMMAND=<span class="string">"serial --speed=115200"</span></span><br><span class="line">GRUB_CMDLINE_LINUX=<span class="string">"console=tty0 crashkernel=auto console=ttyS0,115200"</span></span><br><span class="line">numa=off</span><br><span class="line">GRUB_DISABLE_RECOVERY=<span class="string">"true"</span></span><br></pre></td></tr></table></figure><ul><li>重新生成 grub2 配置文件<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cp /boot/grub2/grub.cfg&#123;,.bak&#125;</span></span><br><span class="line"><span class="comment"># grub2-mkconfig -o /boot/grub2/grub.cfg</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="4、创建CA证书和秘钥"><a href="#4、创建CA证书和秘钥" class="headerlink" title="4、创建CA证书和秘钥"></a>4、创建CA证书和秘钥</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为确保安全，kubernetes 系统各组件需要使用 x509 证书对通信进行加密和认证。CA (Certificate Authority) 是自签名的根证书，用来签名后续创建的其它证书。使用 CloudFlare 的 PKI 工具集 cfssl 创建所有证书，证书均在一台master节点进行操作，然后通过远程分发到其他的服务器上去。</p><ul><li><strong>注意</strong>: 每生成的证书均要进行分发到其他的master节点</li></ul><h4 id="4-1、安装-cfssl-工具集"><a href="#4-1、安装-cfssl-工具集" class="headerlink" title="4.1、安装 cfssl 工具集"></a>4.1、安装 cfssl 工具集</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># curl -o cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64</span></span><br><span class="line"><span class="comment"># curl -o cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64</span></span><br><span class="line"><span class="comment"># curl -o cfssl-certinfo https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64</span></span><br><span class="line"><span class="comment"># chmod +x * &amp;&amp;mv cfssl* /usr/bin/</span></span><br><span class="line"><span class="comment"># scp /usr/bin/cfssl* &#123;master-ip&#125;:/usr/bin</span></span><br></pre></td></tr></table></figure><h4 id="4-2、创建根证书-CA"><a href="#4-2、创建根证书-CA" class="headerlink" title="4.2、创建根证书 (CA)"></a>4.2、创建根证书 (CA)</h4><p>CA 证书是集群所有节点共享的，只需要创建一个 CA 证书，后续创建的所有证书都由它签名。</p><h4 id="4-3、创建配置文件"><a href="#4-3、创建配置文件" class="headerlink" title="4.3、创建配置文件"></a>4.3、创建配置文件</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CA 配置文件用于配置根证书的使用场景 (profile) 和具体参数 (usage，过期时间、服务端认证、客户端认证、加密等)，后续在签名其它证书时需要指定特定场景。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># mkdir k8s &amp;&amp; cd k8s#后面k8s生成所需要的证书均在该目录执行</span></span><br></pre></td></tr></table></figure><ul><li><p>ca-config.json</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat &gt; ca-config.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"signing"</span>: &#123;</span><br><span class="line">    <span class="string">"default"</span>: &#123;</span><br><span class="line">      <span class="string">"expiry"</span>: <span class="string">"87600h"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"profiles"</span>: &#123;</span><br><span class="line">      <span class="string">"kubernetes"</span>: &#123;</span><br><span class="line">        <span class="string">"usages"</span>: [</span><br><span class="line">            <span class="string">"signing"</span>,</span><br><span class="line">            <span class="string">"key encipherment"</span>,</span><br><span class="line">            <span class="string">"server auth"</span>,</span><br><span class="line">            <span class="string">"client auth"</span></span><br><span class="line">        ],</span><br><span class="line">        <span class="string">"expiry"</span>: <span class="string">"87600h"</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></li><li><p>signing：表示该证书可用于签名其它证书，生成的 ca.pem 证书中 CA=TRUE；</p></li><li><p>server auth：表示 client 可以用该该证书对 server 提供的证书进行验证；</p></li><li><p>client auth：表示 server 可以用该该证书对 client 提供的证书进行验证；</p></li></ul><h4 id="4-4、创建证书签名请求文件"><a href="#4-4、创建证书签名请求文件" class="headerlink" title="4.4、创建证书签名请求文件"></a>4.4、创建证书签名请求文件</h4><ul><li><p>ca-csr.json</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="string">"CN"</span>: <span class="string">"kubernetes"</span>,</span><br><span class="line">  <span class="string">"key"</span>: &#123;</span><br><span class="line">    <span class="string">"algo"</span>: <span class="string">"rsa"</span>,</span><br><span class="line">    <span class="string">"size"</span>: 2048</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"names"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"C"</span>: <span class="string">"CN"</span>,</span><br><span class="line">      <span class="string">"ST"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">      <span class="string">"L"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">      <span class="string">"O"</span>: <span class="string">"k8s"</span>,</span><br><span class="line">      <span class="string">"OU"</span>: <span class="string">"4Paradigm"</span></span><br><span class="line">    &#125;</span><br><span class="line">  ],</span><br><span class="line">  <span class="string">"ca"</span>: &#123;</span><br><span class="line">    <span class="string">"expiry"</span>: <span class="string">"876000h"</span></span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></li><li><p>CN：Common Name，kube-apiserver 从证书中提取该字段作为请求的用户名 (User Name)，浏览器使用该字段验证网站是否合法；</p></li><li><p>O：Organization，kube-apiserver 从证书中提取该字段作为请求用户所属的组 (Group)；</p></li><li><p>kube-apiserver 将提取的 User、Group 作为 RBAC 授权的用户标识；</p></li><li><p>生成 CA 证书和私钥</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cfssl gencert -initca ca-csr.json | cfssljson -bare ca</span></span><br><span class="line"><span class="comment"># ls ca*</span></span><br><span class="line"><span class="comment"># mkdir -p /etc/kubernetes/ssl &amp;&amp; cp ca*.pem ca-config.json /etc/kubernetes/ssl</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="5-部署-kubectl-命令行工具"><a href="#5-部署-kubectl-命令行工具" class="headerlink" title="5.部署 kubectl 命令行工具"></a>5.部署 kubectl 命令行工具</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kubectl 默认从 ~/.kube/config 文件读取 kube-apiserver 地址和认证信息，如果没有配置，执行 kubectl 命令时可能会出错：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get pods</span></span><br><span class="line">The connection to the server localhost:8080 was refused - did you specify the right host or port?</span><br></pre></td></tr></table></figure><ul><li><strong>注意</strong>:</li></ul><ul><li>1.如果没有特殊指明，本文档的所有操作均在 zhangjun-k8s01 节点上执行，然后远程分发文件和执行命令；</li><li>2.本文档只需要部署一次，生成的 kubeconfig 文件是通用的，可以拷贝到需要执行 kubectl 命令的机器，重命名为 ~/.kube/config；</li></ul><h4 id="5-1、下载和分发-kubectl-二进制文件"><a href="#5-1、下载和分发-kubectl-二进制文件" class="headerlink" title="5.1、下载和分发 kubectl 二进制文件"></a>5.1、下载和分发 kubectl 二进制文件</h4><p>这里吧把node和master所需要的包均给一次性分发</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># wget https://dl.k8s.io/v1.14.6/kubernetes-server-linux-amd64.tar.gz</span></span><br><span class="line"><span class="comment"># tar -xzvf kubernetes-client-linux-amd64.tar.gz</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># master 节点</span></span><br><span class="line"><span class="comment"># scp kubernetes/server/bin/&#123;apiextensions-apiserver,cloud-controller-manager,kube-apiserver,kube-controller-manager,kube-proxy,kube-scheduler,kubeadm,kubectl,kubelet,mounter&#125; &#123;master-ip&#125;:/usr/bin/</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># node 节点</span></span><br><span class="line"><span class="comment"># scp kubernetes/server/bin/&#123;kube-proxy,kubelet&#125; &#123;node-ip&#125;:/usr/bin/</span></span><br></pre></td></tr></table></figure><h4 id="5-2、创建-admin-证书和私钥"><a href="#5-2、创建-admin-证书和私钥" class="headerlink" title="5.2、创建 admin 证书和私钥"></a>5.2、创建 admin 证书和私钥</h4><p>kubectl 与 apiserver https 安全端口通信，apiserver 对提供的证书进行认证和授权。<br>kubectl 作为集群的管理工具，需要被授予最高权限，这里创建具有<strong>最高权限</strong>的 admin 证书。</p><ul><li><p>创建证书签名请求:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat &gt; admin-csr.json &lt;&lt;EOF</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"CN"</span>: <span class="string">"admin"</span>,</span><br><span class="line">  <span class="string">"hosts"</span>: [],</span><br><span class="line">  <span class="string">"key"</span>: &#123;</span><br><span class="line">    <span class="string">"algo"</span>: <span class="string">"rsa"</span>,</span><br><span class="line">    <span class="string">"size"</span>: 2048</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"names"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"C"</span>: <span class="string">"CN"</span>,</span><br><span class="line">      <span class="string">"ST"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">      <span class="string">"L"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">      <span class="string">"O"</span>: <span class="string">"system:masters"</span>,</span><br><span class="line">      <span class="string">"OU"</span>: <span class="string">"4Paradigm"</span></span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></li><li><p>O 为 system:masters，kube-apiserver 收到该证书后将请求的 Group 设置为 system:masters；</p></li><li><p>预定义的 ClusterRoleBinding cluster-admin 将 Group system:masters 与 Role cluster-admin 绑定，该 Role 授予所有 API的权限；</p></li><li><p>该证书只会被 kubectl 当做 client 证书使用，所以 hosts 字段为空；</p></li><li><p>生成证书和私钥</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json  -profile=kubernetes admin-csr.json | cfssljson -bare admin</span></span><br><span class="line"><span class="comment"># ls admin*</span></span><br><span class="line"><span class="comment"># cp admin*.pem /etc/kubernetes/ssl/</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="5-3、创建-kubeconfig-文件"><a href="#5-3、创建-kubeconfig-文件" class="headerlink" title="5.3、创建 kubeconfig 文件"></a>5.3、创建 kubeconfig 文件</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kubeconfig 为 kubectl 的配置文件，包含访问 apiserver 的所有信息，如 apiserver 地址、CA 证书和自身使用的证书；</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 设置集群API地址</span></span><br><span class="line"><span class="comment"># KUBE_APISERVER="https://172.21.16.45:8443"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置集群参数</span></span><br><span class="line">kubectl config <span class="built_in">set</span>-cluster kubernetes \</span><br><span class="line">  --certificate-authority=ca.pem \</span><br><span class="line">  --embed-certs=<span class="literal">true</span> \</span><br><span class="line">  --server=<span class="variable">$&#123;KUBE_APISERVER&#125;</span> \</span><br><span class="line">  --kubeconfig=kubectl.kubeconfig</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置客户端认证参数</span></span><br><span class="line">kubectl config <span class="built_in">set</span>-credentials admin \</span><br><span class="line">  --client-certificate=admin.pem \</span><br><span class="line">  --client-key=admin-key.pem \</span><br><span class="line">  --embed-certs=<span class="literal">true</span> \</span><br><span class="line">  --kubeconfig=kubectl.kubeconfig</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置上下文参数</span></span><br><span class="line">kubectl config <span class="built_in">set</span>-context kubernetes \</span><br><span class="line">  --cluster=kubernetes \</span><br><span class="line">  --user=admin \</span><br><span class="line">  --kubeconfig=kubectl.kubeconfig</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置默认上下文</span></span><br><span class="line">kubectl config use-context kubernetes --kubeconfig=kubectl.kubeconfig</span><br></pre></td></tr></table></figure><ul><li><strong>提示</strong>: 分发<code>kubectl.kubeconfig</code>文件，吧文件命名<code>~/.kube/config</code>;</li><li>–certificate-authority：验证 kube-apiserver 证书的根证书；</li><li>–client-certificate、–client-key：刚生成的 admin 证书和私钥，连接 kube-apiserver 时使用；</li><li>–embed-certs=true：将 ca.pem 和 admin.pem 证书内容嵌入到生成的 kubectl.kubeconfig 文件中(不加时，写入的是证书文件路径，后续拷贝 kubeconfig 到其它机器时，还需要单独拷贝证书文件，不方便。)；</li></ul><h3 id="6、部署etcd集群"><a href="#6、部署etcd集群" class="headerlink" title="6、部署etcd集群"></a>6、部署etcd集群</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;etcd 是基于 Raft 的分布式 key-value 存储系统，由 CoreOS 开发，常用于服务发现、共享配置以及并发控制（如 leader 选举、分布式锁等）。kubernetes 使用 etcd 存储所有运行数据。</p><p>三节点高可用 etcd 集群的步骤：</p><ul><li>下载和分发 etcd 二进制文件；</li><li>创建 etcd 集群各节点的 x509 证书，用于加密客户端(如 etcdctl) 与 etcd 集群、etcd 集群之间的数据流；</li><li>创建 etcd 的 systemd unit 文件，配置服务参数</li><li>检查集群工作状态;</li></ul><ul><li><strong>注意</strong>: 均在一台master<code>[etcd]</code>节点操作，其他master<code>[etcd]</code>节点通过分发</li></ul><h4 id="6-1、下载和分发-etcd-二进制文件"><a href="#6-1、下载和分发-etcd-二进制文件" class="headerlink" title="6.1、下载和分发 etcd 二进制文件"></a>6.1、下载和分发 etcd 二进制文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># mkdir etcd &amp;&amp;cd etcd</span></span><br><span class="line"><span class="comment"># https://github.com/coreos/etcd/releases/download/v3.3.13/etcd-v3.3.13-linux-amd64.tar.gz</span></span><br><span class="line"><span class="comment"># tar -xvf etcd-v3.3.13-linux-amd64.tar.gz</span></span><br><span class="line"><span class="comment"># scp etcd* &#123;master-ip&#125;:/usr/bin/</span></span><br></pre></td></tr></table></figure><h4 id="6-2、创建-etcd-证书和私钥"><a href="#6-2、创建-etcd-证书和私钥" class="headerlink" title="6.2、创建 etcd 证书和私钥"></a>6.2、创建 etcd 证书和私钥</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat &gt; etcd-csr.json &lt;&lt;EOF</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"CN"</span>: <span class="string">"etcd"</span>,</span><br><span class="line">  <span class="string">"hosts"</span>: [</span><br><span class="line">    <span class="string">"127.0.0.1"</span>,</span><br><span class="line">    <span class="string">"172.21.17.30"</span>,</span><br><span class="line">    <span class="string">"172.21.17.31"</span>,</span><br><span class="line">    <span class="string">"172.21.16.110"</span></span><br><span class="line">  ],</span><br><span class="line">  <span class="string">"key"</span>: &#123;</span><br><span class="line">    <span class="string">"algo"</span>: <span class="string">"rsa"</span>,</span><br><span class="line">    <span class="string">"size"</span>: 2048</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"names"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"C"</span>: <span class="string">"CN"</span>,</span><br><span class="line">      <span class="string">"ST"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">      <span class="string">"L"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">      <span class="string">"O"</span>: <span class="string">"k8s"</span>,</span><br><span class="line">      <span class="string">"OU"</span>: <span class="string">"4Paradigm"</span></span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li>hosts 字段指定授权使用该证书的 etcd 节点 IP 或域名列表，需要将 etcd 集群的三个节点 IP 都列在其中；</li><li>生成证书和私钥<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cfssl gencert -ca=ca.pem cfssl gencert -ca=ca-key.pem -config=ca-config.json -profile=kubernetes etcd-csr.json | cfssljson -bare etcd</span></span><br><span class="line"><span class="comment"># ls etcd*pem</span></span><br><span class="line"><span class="comment"># mkdir -p /etc/etcd/ssl &amp;&amp; cp etcd*pem /etc/etcd/ssl/</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="6-3、创建-etcd-的-systemd-unit-模板文件"><a href="#6-3、创建-etcd-的-systemd-unit-模板文件" class="headerlink" title="6.3、创建 etcd 的 systemd unit 模板文件"></a>6.3、创建 etcd 的 systemd unit 模板文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat /etc/systemd/system/etcd.service</span></span><br><span class="line">[Unit]</span><br><span class="line">Description=Etcd Server</span><br><span class="line">After=network.target</span><br><span class="line">After=network-online.target</span><br><span class="line">Wants=network-online.target</span><br><span class="line">Documentation=https://github.com/coreos</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=notify</span><br><span class="line">WorkingDirectory=/var/lib/etcd/data</span><br><span class="line">ExecStart=/usr/bin/etcd \</span><br><span class="line">  --data-dir=/var/lib/etcd/data \</span><br><span class="line">  --wal-dir=/var/lib/etcd/wal \</span><br><span class="line">  --name=etcd1 \<span class="comment">#根据节点名称进行变化</span></span><br><span class="line">  --cert-file=/etc/etcd/ssl/etcd.pem \</span><br><span class="line">  --key-file=/etc/etcd/ssl/etcd-key.pem \</span><br><span class="line">  --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --peer-cert-file=/etc/etcd/ssl/etcd.pem \</span><br><span class="line">  --peer-key-file=/etc/etcd/ssl/etcd-key.pem \</span><br><span class="line">  --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --peer-client-cert-auth \</span><br><span class="line">  --client-cert-auth \</span><br><span class="line">  --listen-peer-urls=https://172.21.17.30:2380 \</span><br><span class="line">  --initial-advertise-peer-urls=https://172.21.17.30:2380 \</span><br><span class="line">  --listen-client-urls=https://172.21.17.30:2379,http://127.0.0.1:2379 \</span><br><span class="line">  --advertise-client-urls=https://172.21.17.30:2379 \</span><br><span class="line">  --initial-cluster-token=etcd-cluster-0 \</span><br><span class="line">  --initial-cluster=etcd1=https://172.21.17.30:2380,etcd2=https://172.21.17.31:2380,etcd3=https://172.21.16.110:2380 \</span><br><span class="line">  --initial-cluster-state=new \</span><br><span class="line">  --auto-compaction-mode=periodic \</span><br><span class="line">  --auto-compaction-retention=1 \</span><br><span class="line">  --max-request-bytes=33554432 \</span><br><span class="line">  --quota-backend-bytes=6442450944 \</span><br><span class="line">  --heartbeat-interval=250 \</span><br><span class="line">  --election-timeout=2000</span><br><span class="line">Restart=on-failure</span><br><span class="line">RestartSec=5</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># mkdir -p /var/lib/etcd/&#123;data,wal&#125;</span></span><br></pre></td></tr></table></figure><ul><li>WorkingDirectory、–data-dir：指定工作目录和数据目录为 ${ETCD_DATA_DIR}，需在启动服务前创建这个目录；</li><li>–wal-dir：指定 wal 目录，为了提高性能，一般使用 SSD 或者和 –data-dir 不同的磁盘；</li><li>–name：指定节点名称，当 –initial-cluster-state 值为 new 时，–name 的参数值必须位于 –initial-cluster 列表中；</li><li>–cert-file、–key-file：etcd server 与 client 通信时使用的证书和私钥；</li><li>–trusted-ca-file：签名 client 证书的 CA 证书，用于验证 client 证书；</li><li>–peer-cert-file、–peer-key-file：etcd 与 peer 通信使用的证书和私钥；</li><li>–peer-trusted-ca-file：签名 peer 证书的 CA 证书，用于验证 peer 证书；</li></ul><h4 id="6-4、启动-etcd-服务"><a href="#6-4、启动-etcd-服务" class="headerlink" title="6.4、启动 etcd 服务"></a>6.4、启动 etcd 服务</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># systemctl daemon-reload &amp;&amp; systemctl enable etcd &amp;&amp; systemctl restart etcd &amp;&amp; systemctl status etcd</span></span><br></pre></td></tr></table></figure><h4 id="6-5、检查启动结果"><a href="#6-5、检查启动结果" class="headerlink" title="6.5、检查启动结果"></a>6.5、检查启动结果</h4><ul><li>确保状态为 active (running)，否则查看日志，确认原因：<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># journalctl -u etcd</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="6-6、验证服务状态"><a href="#6-6、验证服务状态" class="headerlink" title="6.6、验证服务状态"></a>6.6、验证服务状态</h4><p>部署完 etcd 集群后，在任一 etcd 节点上执行如下命令：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ETCDCTL_API=3 etcdctl \</span></span><br><span class="line">    --endpoints=https://172.21.17.31:2379 \</span><br><span class="line">    --cacert=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">    --cert=/etc/etcd/ssl/etcd.pem \</span><br><span class="line">    --key=/etc/etcd/ssl/etcd-key.pem endpoint health</span><br></pre></td></tr></table></figure><p>检查输出均为 healthy 时表示集群服务正常</p><h4 id="6-7、查看当前的-leader"><a href="#6-7、查看当前的-leader" class="headerlink" title="6.7、查看当前的 leader"></a>6.7、查看当前的 leader</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ETCD_ENDPOINTS="https://172.21.17.30:2379,https://172.21.17.31:2379,https://172.21.16.110:2379"</span></span><br><span class="line"><span class="comment"># ETCDCTL_API=3 etcdctl \</span></span><br><span class="line">  -w table --cacert=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --cert=/etc/etcd/ssl/etcd.pem \</span><br><span class="line">  --key=/etc/etcd/ssl/etcd-key.pem \</span><br><span class="line">  --endpoints=<span class="variable">$&#123;ETCD_ENDPOINTS&#125;</span> endpoint status </span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+-----------+------------+</span><br><span class="line">|          ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX |</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+-----------+------------+</span><br><span class="line">|  https://172.21.17.30:2379 | 5d23ebc4382fa16f |  3.3.13 |  1.2 MB |     <span class="literal">false</span> |        83 |      58127 |</span><br><span class="line">|  https://172.21.17.31:2379 |  ceaae5134701946 |  3.3.13 |  1.2 MB |     <span class="literal">false</span> |        83 |      58127 |</span><br><span class="line">| https://172.21.16.110:2379 | 575020c8e15d3a06 |  3.3.13 |  1.2 MB |      <span class="literal">true</span> |        83 |      58128 |</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+-----------+------------+</span><br></pre></td></tr></table></figure><ul><li>当前的 leader 为 172.21.16.110</li></ul><h3 id="7、部署-flannel-网络"><a href="#7、部署-flannel-网络" class="headerlink" title="7、部署 flannel 网络"></a>7、部署 flannel 网络</h3><p>flannel 网络部署在node节点，证书在master节点生成分发</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kubernetes 要求集群内各节点(包括 master 节点)能通过 Pod 网段互联互通。flannel 使用 vxlan 技术为各节点创建一个可以互通的 Pod 网络，使用的端口为 UDP 8472（需要开放该端口）<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;flanneld 第一次启动时，从 etcd 获取配置的 Pod 网段信息，为本节点分配一个未使用的地址段，然后创建 flannedl.1 网络接口（也可能是其它名称，如 flannel1 等）<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;flannel 将分配给自己的 Pod 网段信息写入 /run/flannel/docker 文件，docker 后续使用这个文件中的环境变量设置 docker0 网桥，从而从这个地址段为本节点的所有 Pod 容器分配 IP。</p><h4 id="7-1、下载和分发-flanneld-二进制文件"><a href="#7-1、下载和分发-flanneld-二进制文件" class="headerlink" title="7.1、下载和分发 flanneld 二进制文件"></a>7.1、下载和分发 flanneld 二进制文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># mkdir flannel &amp;&amp;cd flannel</span></span><br><span class="line"><span class="comment"># wget https://github.com/coreos/flannel/releases/download/v0.11.0/flannel-v0.11.0-linux-amd64.tar.gz</span></span><br><span class="line"><span class="comment"># tar -xzvf flannel-v0.11.0-linux-amd64.tar.gz -C flannel</span></span><br></pre></td></tr></table></figure><ul><li>分发flanneld 可执行文件到node节点</li></ul><h4 id="7-2、创建-flannel-证书和私钥"><a href="#7-2、创建-flannel-证书和私钥" class="headerlink" title="7.2、创建 flannel 证书和私钥"></a>7.2、创建 flannel 证书和私钥</h4><ul><li><p>flanneld-csr.json</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat &gt; flanneld-csr.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"CN"</span>: <span class="string">"flanneld"</span>,</span><br><span class="line">  <span class="string">"hosts"</span>: [],</span><br><span class="line">  <span class="string">"key"</span>: &#123;</span><br><span class="line">    <span class="string">"algo"</span>: <span class="string">"rsa"</span>,</span><br><span class="line">    <span class="string">"size"</span>: 2048</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"names"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"C"</span>: <span class="string">"CN"</span>,</span><br><span class="line">      <span class="string">"ST"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">      <span class="string">"L"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">      <span class="string">"O"</span>: <span class="string">"k8s"</span>,</span><br><span class="line">      <span class="string">"OU"</span>: <span class="string">"4Paradigm"</span></span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></li><li><p>该证书只会被 kubectl 当做 client 证书使用，所以 hosts 字段为空；</p></li><li><p>生成证书和私钥:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes flanneld-csr.json | cfssljson -bare flanneld </span></span><br><span class="line"><span class="comment"># ls flanneld*pem</span></span><br><span class="line"><span class="comment"># scp flanneld*pem &#123;node-ip&#125;:/etc/flanneld/ssl/</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="7-3、向-etcd-写入集群-Pod-网段信息"><a href="#7-3、向-etcd-写入集群-Pod-网段信息" class="headerlink" title="7.3、向 etcd 写入集群 Pod 网段信息"></a>7.3、向 etcd 写入集群 Pod 网段信息</h4><p><strong>注意</strong>：本步骤只需执行一次。在etcd集群上执行</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">etcdctl \</span><br><span class="line">  --endpoints=<span class="variable">$&#123;ETCD_ENDPOINTS&#125;</span> \</span><br><span class="line">  --ca-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --cert-file=flanneld.pem \</span><br><span class="line">  --key-file=flanneld-key.pem \</span><br><span class="line">  mk /kubernetes/network/config <span class="string">'&#123;"Network":"172.30.0.0/16", "SubnetLen": 21, "Backend": &#123;"Type": "vxlan"&#125;&#125;'</span></span><br></pre></td></tr></table></figure><ul><li>flanneld 当前版本 (v0.11.0) 不支持 etcd v3，故使用 etcd v2 API 写入配置 key 和网段数据；</li><li>写入的 Pod 网段 ${CLUSTER_CIDR} 地址段（如 /16）必须小于 SubnetLen，必须与 kube-controller-manager 的 –cluster-cidr 参数值一致；</li></ul><h4 id="7-4、创建-flanneld-的-systemd-unit-文件"><a href="#7-4、创建-flanneld-的-systemd-unit-文件" class="headerlink" title="7.4、创建 flanneld 的 systemd unit 文件"></a>7.4、创建 flanneld 的 systemd unit 文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat /usr/lib/systemd/system/flanneld.service </span></span><br><span class="line">[Unit]</span><br><span class="line">Description=Flanneld overlay address etcd agent</span><br><span class="line">After=network.target</span><br><span class="line">After=network-online.target</span><br><span class="line">Wants=network-online.target</span><br><span class="line">After=etcd.service</span><br><span class="line">Before=docker.service</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=notify</span><br><span class="line">ExecStart=/usr/bin/flanneld \</span><br><span class="line">  -etcd-cafile=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  -etcd-certfile=/etc/flanneld/ssl/flanneld.pem \</span><br><span class="line">  -etcd-keyfile=/etc/flanneld/ssl/flanneld-key.pem \</span><br><span class="line">  -etcd-endpoints=https://172.21.17.30:2379,https://172.21.17.31:2379,https://172.21.16.110:2379 \</span><br><span class="line">  -etcd-prefix=/kubernetes/network \</span><br><span class="line">  -iface=eth0 \</span><br><span class="line">  -ip-masq</span><br><span class="line">ExecStartPost=/usr/bin/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker</span><br><span class="line">Restart=always</span><br><span class="line">RestartSec=5</span><br><span class="line">StartLimitInterval=0</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">RequiredBy=docker.service</span><br></pre></td></tr></table></figure><ul><li>mk-docker-opts.sh 脚本将分配给 flanneld 的 Pod 子网段信息写入 /run/flannel/docker 文件，后续 docker 启动时使用这个文件中的环境变量配置 docker0 网桥；</li><li>flanneld 使用系统缺省路由所在的接口与其它节点通信，对于有多个网络接口（如内网和公网）的节点，可以用 -iface 参数指定通信接口;</li><li>flanneld 运行时需要 root 权限；</li><li>-ip-masq: flanneld 为访问 Pod 网络外的流量设置 SNAT 规则，同时将传递给 Docker 的变量 –ip-masq（/run/flannel/docker 文件中）设置为 false，这样 Docker 将不再创建 SNAT 规则； Docker 的 –ip-masq 为 true 时，创建的 SNAT 规则比较“暴力”：将所有本节点 Pod 发起的、访问非 docker0 接口的请求做 SNAT，这样访问其他节点 Pod 的请求来源 IP 会被设置为 flannel.1 接口的 IP，导致目的 Pod 看不到真实的来源 Pod IP。 flanneld 创建的 SNAT 规则比较温和，只对访问非 Pod 网段的请求做 SNAT。</li></ul><h4 id="7-5、启动-flanneld-服务"><a href="#7-5、启动-flanneld-服务" class="headerlink" title="7.5、启动 flanneld 服务"></a>7.5、启动 flanneld 服务</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># systemctl daemon-reload &amp;&amp; systemctl enable flanneld &amp;&amp; systemctl restart flanneld &amp;&amp; systemctl status flanneld</span></span><br></pre></td></tr></table></figure><h4 id="7-6、检查分配给各-flanneld-的-Pod-网段信息"><a href="#7-6、检查分配给各-flanneld-的-Pod-网段信息" class="headerlink" title="7.6、检查分配给各 flanneld 的 Pod 网段信息"></a>7.6、检查分配给各 flanneld 的 Pod 网段信息</h4><ul><li><p>查看集群 Pod 网段(/16)</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># etcdctl \</span></span><br><span class="line">  --endpoints=<span class="variable">$&#123;ETCD_ENDPOINTS&#125;</span> \</span><br><span class="line">  --ca-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --cert-file=/etc/flanneld/ssl/flanneld.pem \</span><br><span class="line">  --key-file=/etc/flanneld/ssl/flanneld-key.pem \</span><br><span class="line">  get /kubernetes/network/config</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line">&#123;<span class="string">"Network"</span>:<span class="string">"172.30.0.0/16"</span>, <span class="string">"SubnetLen"</span>: 21, <span class="string">"Backend"</span>: &#123;<span class="string">"Type"</span>: <span class="string">"vxlan"</span>&#125;&#125;</span><br></pre></td></tr></table></figure></li><li><p>查看已分配的 Pod 子网段列表(/24):</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># etcdctl \</span></span><br><span class="line">  --endpoints=<span class="variable">$&#123;ETCD_ENDPOINTS&#125;</span> \</span><br><span class="line">  --ca-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --cert-file=/etc/flanneld/ssl/flanneld.pem \</span><br><span class="line">  --key-file=/etc/flanneld/ssl/flanneld-key.pem \</span><br><span class="line">  ls /kubernetes/network/subnets</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">/kubernetes/network/subnets/172.30.232.0-21</span><br><span class="line">/kubernetes/network/subnets/172.30.128.0-21</span><br><span class="line">/kubernetes/network/subnets/172.30.176.0-21</span><br></pre></td></tr></table></figure></li><li><p>查看某一 Pod 网段对应的节点 IP 和 flannel 接口地址</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># etcdctl \</span></span><br><span class="line">  --endpoints=<span class="variable">$&#123;ETCD_ENDPOINTS&#125;</span> \</span><br><span class="line">  --ca-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --cert-file=/etc/flanneld/ssl/flanneld.pem \</span><br><span class="line">  --key-file=/etc/flanneld/ssl/flanneld-key.pem \</span><br><span class="line">  get /kubernetes/network/subnets/172.30.232.0-21</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">&#123;<span class="string">"PublicIP"</span>:<span class="string">"172.21.16.204"</span>,<span class="string">"BackendType"</span>:<span class="string">"vxlan"</span>,<span class="string">"BackendData"</span>:&#123;<span class="string">"VtepMAC"</span>:<span class="string">"f6:50:05:5c:9a:20"</span>&#125;&#125;</span><br></pre></td></tr></table></figure></li><li><p>172.30.232.0/21 被分配给节点172.21.16.204）；</p></li><li><p>VtepMAC 为172.21.16.204节点的 flannel.1 网卡 MAC 地址；</p></li></ul><h4 id="7-7、检查节点-flannel-网络信息"><a href="#7-7、检查节点-flannel-网络信息" class="headerlink" title="7.7、检查节点 flannel 网络信息"></a>7.7、检查节点 flannel 网络信息</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ip addr show</span></span><br></pre></td></tr></table></figure><ul><li><p>flannel.1 网卡的地址为分配的 Pod 子网段的第一个 IP（.0），且是 /32 的地址；</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ip route show |grep flannel.1</span></span><br><span class="line">172.30.128.0/21 via 172.30.128.0 dev flannel.1 onlink </span><br><span class="line">172.30.176.0/21 via 172.30.176.0 dev flannel.1 onlink</span><br></pre></td></tr></table></figure></li><li><p>到其它节点 Pod 网段请求都被转发到 flannel.1 网卡；</p></li><li><p>flanneld 根据 etcd 中子网段的信息，如/kubernetes/network/subnets/172.30.232.0-21 ，来决定进请求发送给哪个节点的互联 IP；</p></li><li><p>验证各节点能通过 Pod 网段互通</p></li></ul><h3 id="8、master节点部署"><a href="#8、master节点部署" class="headerlink" title="8、master节点部署"></a>8、master节点部署</h3><p>kubernetes master 节点运行如下组件：</p><ul><li>kube-apiserver</li><li>kube-scheduler</li><li>kube-controller-manager<br>kube-apiserver、kube-scheduler 和 kube-controller-manager 均以多实例模式运行：<br>1、kube-scheduler 和 kube-controller-manager 会自动选举产生一个 leader 实例，其它实例处于阻塞模式，当 leader 挂了后，重新选举产生新的 leader，从而保证服务可用性；<br>2、kube-apiserver 是无状态的，需要通过<a href="https://xxlaila.github.io/2019/08/10/haproxy-keepalived/" target="_blank" rel="noopener">haproxy+keepalived</a>进行代理访问，从而保证服务可用性；</li></ul><h4 id="8-1、创建-kubernetes-证书和私钥"><a href="#8-1、创建-kubernetes-证书和私钥" class="headerlink" title="8.1、创建 kubernetes 证书和私钥"></a>8.1、创建 kubernetes 证书和私钥</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat &gt; kubernetes-csr.json &lt;&lt;EOF</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"CN"</span>: <span class="string">"kubernetes"</span>,</span><br><span class="line">  <span class="string">"hosts"</span>: [</span><br><span class="line">    <span class="string">"127.0.0.1"</span>,</span><br><span class="line">    <span class="string">"172.21.17.30"</span>,</span><br><span class="line">    <span class="string">"172.21.17.31"</span>,</span><br><span class="line">    <span class="string">"172.21.16.110"</span>,</span><br><span class="line">    <span class="string">"172.21.16.45"</span>,</span><br><span class="line">    <span class="string">"10.254.0.1"</span>,</span><br><span class="line">    <span class="string">"kubernetes"</span>,</span><br><span class="line">    <span class="string">"kubernetes.default"</span>,</span><br><span class="line">    <span class="string">"kubernetes.default.svc"</span>,</span><br><span class="line">    <span class="string">"kubernetes.default.svc.cluster"</span>,</span><br><span class="line">    <span class="string">"kubernetes.default.svc.cluster.local."</span></span><br><span class="line">  ],</span><br><span class="line">  <span class="string">"key"</span>: &#123;</span><br><span class="line">    <span class="string">"algo"</span>: <span class="string">"rsa"</span>,</span><br><span class="line">    <span class="string">"size"</span>: 2048</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"names"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"C"</span>: <span class="string">"CN"</span>,</span><br><span class="line">      <span class="string">"ST"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">      <span class="string">"L"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">      <span class="string">"O"</span>: <span class="string">"k8s"</span>,</span><br><span class="line">      <span class="string">"OU"</span>: <span class="string">"4Paradigm"</span></span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li><p>hosts 字段指定授权使用该证书的 IP 和域名列表，这里列出了 master 节点 IP、kubernetes 服务的 IP 和域名,以及VIP地址；</p></li><li><p>kubernetes 服务 IP 是 apiserver 自动创建的，一般是 –service-cluster-ip-range 参数指定的网段的第一个IP,后续可以通过下面命令获取：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get svc kubernetes</span></span><br><span class="line">NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">kubernetes   ClusterIP   10.254.0.1   &lt;none&gt;        443/TCP   4h13m</span><br></pre></td></tr></table></figure></li><li><p>生成证书和私钥</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes</span></span><br><span class="line"><span class="comment"># ls kubernetes*pem</span></span><br><span class="line">kubernetes-key.pem  kubernetes.pem</span><br><span class="line"><span class="comment"># cp kubernetes*pem /etc/kubernetes/ssl/</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="8-2、创建加密配置文件"><a href="#8-2、创建加密配置文件" class="headerlink" title="8.2、创建加密配置文件"></a>8.2、创建加密配置文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64)</span></span><br><span class="line"><span class="comment"># cat &gt; encryption-config.yaml &lt;&lt;EOF</span></span><br><span class="line">kind: EncryptionConfig</span><br><span class="line">apiVersion: v1</span><br><span class="line">resources:</span><br><span class="line">  - resources:</span><br><span class="line">      - secrets</span><br><span class="line">    providers:</span><br><span class="line">      - aescbc:</span><br><span class="line">          keys:</span><br><span class="line">            - name: key1</span><br><span class="line">              secret: <span class="variable">$&#123;ENCRYPTION_KEY&#125;</span></span><br><span class="line">      - identity: &#123;&#125;</span><br><span class="line">EOF</span><br><span class="line"><span class="comment"># cp encryption-config.yaml /etc/kubernetes/</span></span><br></pre></td></tr></table></figure><h4 id="8-3、创建审计策略文件"><a href="#8-3、创建审计策略文件" class="headerlink" title="8.3、创建审计策略文件"></a>8.3、创建审计策略文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat &gt; audit-policy.yaml &lt;&lt;EOF</span></span><br><span class="line">apiVersion: audit.k8s.io/v1beta1</span><br><span class="line">kind: Policy</span><br><span class="line">rules:</span><br><span class="line">  <span class="comment"># The following requests were manually identified as high-volume and low-risk, so drop them.</span></span><br><span class="line">  - level: None</span><br><span class="line">    resources:</span><br><span class="line">      - group: <span class="string">""</span></span><br><span class="line">        resources:</span><br><span class="line">          - endpoints</span><br><span class="line">          - services</span><br><span class="line">          - services/status</span><br><span class="line">    users:</span><br><span class="line">      - <span class="string">'system:kube-proxy'</span></span><br><span class="line">    verbs:</span><br><span class="line">      - watch</span><br><span class="line"></span><br><span class="line">  - level: None</span><br><span class="line">    resources:</span><br><span class="line">      - group: <span class="string">""</span></span><br><span class="line">        resources:</span><br><span class="line">          - nodes</span><br><span class="line">          - nodes/status</span><br><span class="line">    userGroups:</span><br><span class="line">      - <span class="string">'system:nodes'</span></span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line"></span><br><span class="line">  - level: None</span><br><span class="line">    namespaces:</span><br><span class="line">      - kube-system</span><br><span class="line">    resources:</span><br><span class="line">      - group: <span class="string">""</span></span><br><span class="line">        resources:</span><br><span class="line">          - endpoints</span><br><span class="line">    users:</span><br><span class="line">      - <span class="string">'system:kube-controller-manager'</span></span><br><span class="line">      - <span class="string">'system:kube-scheduler'</span></span><br><span class="line">      - <span class="string">'system:serviceaccount:kube-system:endpoint-controller'</span></span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - update</span><br><span class="line"></span><br><span class="line">  - level: None</span><br><span class="line">    resources:</span><br><span class="line">      - group: <span class="string">""</span></span><br><span class="line">        resources:</span><br><span class="line">          - namespaces</span><br><span class="line">          - namespaces/status</span><br><span class="line">          - namespaces/finalize</span><br><span class="line">    users:</span><br><span class="line">      - <span class="string">'system:apiserver'</span></span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Don't log HPA fetching metrics.</span></span><br><span class="line">  - level: None</span><br><span class="line">    resources:</span><br><span class="line">      - group: metrics.k8s.io</span><br><span class="line">    users:</span><br><span class="line">      - <span class="string">'system:kube-controller-manager'</span></span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Don't log these read-only URLs.</span></span><br><span class="line">  - level: None</span><br><span class="line">    nonResourceURLs:</span><br><span class="line">      - <span class="string">'/healthz*'</span></span><br><span class="line">      - /version</span><br><span class="line">      - <span class="string">'/swagger*'</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Don't log events requests.</span></span><br><span class="line">  - level: None</span><br><span class="line">    resources:</span><br><span class="line">      - group: <span class="string">""</span></span><br><span class="line">        resources:</span><br><span class="line">          - events</span><br><span class="line"></span><br><span class="line">  <span class="comment"># node and pod status calls from nodes are high-volume and can be large, don't log responses for expected updates from nodes</span></span><br><span class="line">  - level: Request</span><br><span class="line">    omitStages:</span><br><span class="line">      - RequestReceived</span><br><span class="line">    resources:</span><br><span class="line">      - group: <span class="string">""</span></span><br><span class="line">        resources:</span><br><span class="line">          - nodes/status</span><br><span class="line">          - pods/status</span><br><span class="line">    users:</span><br><span class="line">      - kubelet</span><br><span class="line">      - <span class="string">'system:node-problem-detector'</span></span><br><span class="line">      - <span class="string">'system:serviceaccount:kube-system:node-problem-detector'</span></span><br><span class="line">    verbs:</span><br><span class="line">      - update</span><br><span class="line">      - patch</span><br><span class="line"></span><br><span class="line">  - level: Request</span><br><span class="line">    omitStages:</span><br><span class="line">      - RequestReceived</span><br><span class="line">    resources:</span><br><span class="line">      - group: <span class="string">""</span></span><br><span class="line">        resources:</span><br><span class="line">          - nodes/status</span><br><span class="line">          - pods/status</span><br><span class="line">    userGroups:</span><br><span class="line">      - <span class="string">'system:nodes'</span></span><br><span class="line">    verbs:</span><br><span class="line">      - update</span><br><span class="line">      - patch</span><br><span class="line"></span><br><span class="line">  <span class="comment"># deletecollection calls can be large, don't log responses for expected namespace deletions</span></span><br><span class="line">  - level: Request</span><br><span class="line">    omitStages:</span><br><span class="line">      - RequestReceived</span><br><span class="line">    users:</span><br><span class="line">      - <span class="string">'system:serviceaccount:kube-system:namespace-controller'</span></span><br><span class="line">    verbs:</span><br><span class="line">      - deletecollection</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Secrets, ConfigMaps, and TokenReviews can contain sensitive &amp; binary data,</span></span><br><span class="line">  <span class="comment"># so only log at the Metadata level.</span></span><br><span class="line">  - level: Metadata</span><br><span class="line">    omitStages:</span><br><span class="line">      - RequestReceived</span><br><span class="line">    resources:</span><br><span class="line">      - group: <span class="string">""</span></span><br><span class="line">        resources:</span><br><span class="line">          - secrets</span><br><span class="line">          - configmaps</span><br><span class="line">      - group: authentication.k8s.io</span><br><span class="line">        resources:</span><br><span class="line">          - tokenreviews</span><br><span class="line">  <span class="comment"># Get repsonses can be large; skip them.</span></span><br><span class="line">  - level: Request</span><br><span class="line">    omitStages:</span><br><span class="line">      - RequestReceived</span><br><span class="line">    resources:</span><br><span class="line">      - group: <span class="string">""</span></span><br><span class="line">      - group: admissionregistration.k8s.io</span><br><span class="line">      - group: apiextensions.k8s.io</span><br><span class="line">      - group: apiregistration.k8s.io</span><br><span class="line">      - group: apps</span><br><span class="line">      - group: authentication.k8s.io</span><br><span class="line">      - group: authorization.k8s.io</span><br><span class="line">      - group: autoscaling</span><br><span class="line">      - group: batch</span><br><span class="line">      - group: certificates.k8s.io</span><br><span class="line">      - group: extensions</span><br><span class="line">      - group: metrics.k8s.io</span><br><span class="line">      - group: networking.k8s.io</span><br><span class="line">      - group: policy</span><br><span class="line">      - group: rbac.authorization.k8s.io</span><br><span class="line">      - group: scheduling.k8s.io</span><br><span class="line">      - group: settings.k8s.io</span><br><span class="line">      - group: storage.k8s.io</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Default level for known APIs</span></span><br><span class="line">  - level: RequestResponse</span><br><span class="line">    omitStages:</span><br><span class="line">      - RequestReceived</span><br><span class="line">    resources:</span><br><span class="line">      - group: <span class="string">""</span></span><br><span class="line">      - group: admissionregistration.k8s.io</span><br><span class="line">      - group: apiextensions.k8s.io</span><br><span class="line">      - group: apiregistration.k8s.io</span><br><span class="line">      - group: apps</span><br><span class="line">      - group: authentication.k8s.io</span><br><span class="line">      - group: authorization.k8s.io</span><br><span class="line">      - group: autoscaling</span><br><span class="line">      - group: batch</span><br><span class="line">      - group: certificates.k8s.io</span><br><span class="line">      - group: extensions</span><br><span class="line">      - group: metrics.k8s.io</span><br><span class="line">      - group: networking.k8s.io</span><br><span class="line">      - group: policy</span><br><span class="line">      - group: rbac.authorization.k8s.io</span><br><span class="line">      - group: scheduling.k8s.io</span><br><span class="line">      - group: settings.k8s.io</span><br><span class="line">      - group: storage.k8s.io</span><br><span class="line">      </span><br><span class="line">  <span class="comment"># Default level for all other requests.</span></span><br><span class="line">  - level: Metadata</span><br><span class="line">    omitStages:</span><br><span class="line">      - RequestReceived</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># cp audit-policy.yaml /etc/kubernetes/</span></span><br></pre></td></tr></table></figure><h4 id="8-4、创建后续访问-metrics-server-使用的证书"><a href="#8-4、创建后续访问-metrics-server-使用的证书" class="headerlink" title="8.4、创建后续访问 metrics-server 使用的证书"></a>8.4、创建后续访问 metrics-server 使用的证书</h4><ul><li><p>创建证书签名请求:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat &gt; proxy-client-csr.json &lt;&lt;EOF</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"CN"</span>: <span class="string">"aggregator"</span>,</span><br><span class="line">  <span class="string">"hosts"</span>: [],</span><br><span class="line">  <span class="string">"key"</span>: &#123;</span><br><span class="line">    <span class="string">"algo"</span>: <span class="string">"rsa"</span>,</span><br><span class="line">    <span class="string">"size"</span>: 2048</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"names"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"C"</span>: <span class="string">"CN"</span>,</span><br><span class="line">      <span class="string">"ST"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">      <span class="string">"L"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">      <span class="string">"O"</span>: <span class="string">"k8s"</span>,</span><br><span class="line">      <span class="string">"OU"</span>: <span class="string">"4Paradigm"</span></span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></li><li><p>CN 名称需要位于 kube-apiserver 的 –requestheader-allowed-names 参数中，否则后续访问 metrics 时会提示权限不足。</p></li><li><p>生成证书和私钥</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cfssl gencert -ca=ca.pem  -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes proxy-client-csr.json | cfssljson -bare proxy-client</span></span><br><span class="line"><span class="comment"># ls proxy-client*.pem</span></span><br><span class="line">proxy-client-key.pem  proxy-client.pem</span><br><span class="line"><span class="comment"># cp proxy-client*.pem /etc/kubernetes/ssl/</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="8-5、创建-kube-apiserver-systemd-unit-模板文件"><a href="#8-5、创建-kube-apiserver-systemd-unit-模板文件" class="headerlink" title="8.5、创建 kube-apiserver systemd unit 模板文件"></a>8.5、创建 kube-apiserver systemd unit 模板文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat /usr/lib/systemd/system/kube-apiserver.service </span></span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes API Server</span><br><span class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</span><br><span class="line">After=network.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">WorkingDirectory=/var/<span class="built_in">log</span>/k8s/kube-apiserver</span><br><span class="line">ExecStart=/usr/bin/kube-apiserver \</span><br><span class="line">  --advertise-address=172.21.17.30 \<span class="comment">#master 节点的ip</span></span><br><span class="line">  --default-not-ready-toleration-seconds=360 \</span><br><span class="line">  --default-unreachable-toleration-seconds=360 \</span><br><span class="line">  --feature-gates=DynamicAuditing=<span class="literal">true</span> \</span><br><span class="line">  --max-mutating-requests-inflight=2000 \</span><br><span class="line">  --max-requests-inflight=4000 \</span><br><span class="line">  --default-watch-cache-size=200 \</span><br><span class="line">  --delete-collection-workers=2 \</span><br><span class="line">  --encryption-provider-config=/etc/kubernetes/encryption-config.yaml \</span><br><span class="line">  --etcd-cafile=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem \</span><br><span class="line">  --etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem \</span><br><span class="line">  --etcd-servers=https://172.21.17.30:2379,https://172.21.17.31:2379,https://172.21.16.110:2379 \</span><br><span class="line">  --<span class="built_in">bind</span>-address=0.0.0.0 \</span><br><span class="line">  --secure-port=6443 \</span><br><span class="line">  --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \</span><br><span class="line">  --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \</span><br><span class="line">  --insecure-port=0 \</span><br><span class="line">  --audit-dynamic-configuration \</span><br><span class="line">  --audit-log-maxage=15 \</span><br><span class="line">  --audit-log-maxbackup=3 \</span><br><span class="line">  --audit-log-maxsize=100 \</span><br><span class="line">  --audit-log-truncate-enabled \</span><br><span class="line">  --audit-log-path=/var/<span class="built_in">log</span>/k8s/kube-apiserver/audit.log \</span><br><span class="line">  --audit-policy-file=/etc/kubernetes/audit-policy.yaml \</span><br><span class="line">  --profiling \</span><br><span class="line">  --anonymous-auth=<span class="literal">false</span> \</span><br><span class="line">  --client-ca-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --<span class="built_in">enable</span>-bootstrap-token-auth \</span><br><span class="line">  --requestheader-allowed-names=<span class="string">"aggregator"</span> \</span><br><span class="line">  --requestheader-client-ca-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --requestheader-extra-headers-prefix=<span class="string">"X-Remote-Extra-"</span> \</span><br><span class="line">  --requestheader-group-headers=X-Remote-Group \</span><br><span class="line">  --requestheader-username-headers=X-Remote-User \</span><br><span class="line">  --service-account-key-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --authorization-mode=Node,RBAC \</span><br><span class="line">  --runtime-config=api/all=<span class="literal">true</span> \</span><br><span class="line">  --<span class="built_in">enable</span>-admission-plugins=NodeRestriction \</span><br><span class="line">  --allow-privileged=<span class="literal">true</span> \</span><br><span class="line">  --apiserver-count=3 \</span><br><span class="line">  --event-ttl=168h \</span><br><span class="line">  --kubelet-certificate-authority=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --kubelet-client-certificate=/etc/kubernetes/ssl/kubernetes.pem \</span><br><span class="line">  --kubelet-client-key=/etc/kubernetes/ssl/kubernetes-key.pem \</span><br><span class="line">  --kubelet-https=<span class="literal">true</span> \</span><br><span class="line">  --kubelet-timeout=10s \</span><br><span class="line">  --proxy-client-cert-file=/etc/kubernetes/ssl/proxy-client.pem \</span><br><span class="line">  --proxy-client-key-file=/etc/kubernetes/ssl/proxy-client-key.pem \</span><br><span class="line">  --service-cluster-ip-range=10.254.0.0/16 \</span><br><span class="line">  --service-node-port-range=30000-32767 \</span><br><span class="line">  --logtostderr=<span class="literal">true</span> \</span><br><span class="line">  --<span class="built_in">enable</span>-aggregator-routing=<span class="literal">true</span> \</span><br><span class="line">  --v=2</span><br><span class="line">Restart=on-failure</span><br><span class="line">RestartSec=10</span><br><span class="line">Type=notify</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># mkdir -p /var/log/k8s/kube-apiserver</span></span><br></pre></td></tr></table></figure><ul><li>–advertise-address：apiserver 对外通告的 IP（kubernetes 服务后端节点 IP）；</li><li>–default-*-toleration-seconds：设置节点异常相关的阈值；</li><li>–max-*-requests-inflight：请求相关的最大阈值；</li><li>–etcd-*：访问 etcd 的证书和 etcd 服务器地址；</li><li>–experimental-encryption-provider-config：指定用于加密 etcd 中 secret 的配置；</li><li>–bind-address： https 监听的 IP，不能为 127.0.0.1，否则外界不能访问它的安全端口 6443；</li><li>–secret-port：https 监听端口；</li><li>–insecure-port=0：关闭监听 http 非安全端口(8080)；</li><li>–tls-*-file：指定 apiserver 使用的证书、私钥和 CA 文件；</li><li>–audit-*：配置审计策略和审计日志文件相关的参数；</li><li>–client-ca-file：验证 client (kue-controller-manager、kube-scheduler、kubelet、kube-proxy 等)请求所带的证书；</li><li>–enable-bootstrap-token-auth：启用 kubelet bootstrap 的 token 认证；</li><li>–requestheader-*：kube-apiserver 的 aggregator layer 相关的配置参数，proxy-client &amp; HPA 需要使用；</li><li>–requestheader-client-ca-file：用于签名 –proxy-client-cert-file 和 –proxy-client-key-file 指定的证书；在启用了 metric aggregator 时使用；</li><li>–requestheader-allowed-names：不能为空，值为逗号分割的 –proxy-client-cert-file 证书的 CN 名称，这里设置为 “aggregator”；</li><li>–service-account-key-file：签名 ServiceAccount Token 的公钥文件，kube-controller-manager 的 –service-account-private-key-file 指定私钥文件，两者配对使用；</li><li>–runtime-config=api/all=true： 启用所有版本的 APIs，如 autoscaling/v2alpha1；</li><li>–authorization-mode=Node,RBAC、–anonymous-auth=false： 开启 Node 和 RBAC 授权模式，拒绝未授权的请求；</li><li>–enable-admission-plugins：启用一些默认关闭的 plugins；</li><li>–allow-privileged：运行执行 privileged 权限的容器；</li><li>–apiserver-count=3：指定 apiserver 实例的数量；</li><li>–event-ttl：指定 events 的保存时间；</li><li>–kubelet-<em>：如果指定，则使用 https 访问 kubelet APIs；需要为证书对应的用户(上面 kubernetes</em>.pem 证书的用户为 kubernetes) 用户定义 RBAC 规则，否则访问 kubelet API 时提示未授权；</li><li>–proxy-client-*：apiserver 访问 metrics-server 使用的证书；</li><li>–service-cluster-ip-range： 指定 Service Cluster IP 地址段；</li><li>–service-node-port-range： 指定 NodePort 的端口范围；</li><li>如果 kube-apiserver 机器没有运行 kube-proxy，则还需要添加 –enable-aggregator-routing=true 参数</li></ul><p><strong>注意</strong>:<br>1.requestheader-client-ca-file 指定的 CA 证书，必须具有 client auth and server auth；<br>2.如果 –requestheader-allowed-names 为空，或者 –proxy-client-cert-file 证书的 CN 名称不在 allowed-names 中，则后续查看 node 或 pods 的 metrics 失败，提示：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl top nodes</span></span><br><span class="line">Error from server (Forbidden): nodes.metrics.k8s.io is forbidden: User <span class="string">"aggregator"</span> cannot list resource <span class="string">"nodes"</span> <span class="keyword">in</span> API group <span class="string">"metrics.k8s.io"</span> at the cluster scope</span><br></pre></td></tr></table></figure><h4 id="8-6、启动-kube-apiserver-服务"><a href="#8-6、启动-kube-apiserver-服务" class="headerlink" title="8.6、启动 kube-apiserver 服务"></a>8.6、启动 kube-apiserver 服务</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># systemctl daemon-reload &amp;&amp; systemctl enable kube-apiserver &amp;&amp; systemctl restart kube-apiserver &amp;&amp;systemctl status kube-apiserver</span></span><br><span class="line"><span class="comment"># systemctl status kube-apiserver |grep 'Active:'</span></span><br><span class="line">   Active: active (running) since Mon 2019-09-16 14:38:31 CST; 1min 41s ago</span><br></pre></td></tr></table></figure><h4 id="8-6、打印-kube-apiserver-写入-etcd-的数据"><a href="#8-6、打印-kube-apiserver-写入-etcd-的数据" class="headerlink" title="8.6、打印 kube-apiserver 写入 etcd 的数据"></a>8.6、打印 kube-apiserver 写入 etcd 的数据</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ETCDCTL_API=3 etcdctl \</span></span><br><span class="line">    --endpoints=<span class="variable">$&#123;ETCD_ENDPOINTS&#125;</span> \</span><br><span class="line">    --cacert=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">    --cert=/etc/etcd/ssl/etcd.pem \</span><br><span class="line">    --key=/etc/etcd/ssl/etcd-key.pem \</span><br><span class="line">    get /registry/ --prefix --keys-only</span><br></pre></td></tr></table></figure><h4 id="8-9、检查集群信息"><a href="#8-9、检查集群信息" class="headerlink" title="8.9、检查集群信息"></a>8.9、检查集群信息</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl cluster-info</span></span><br><span class="line">Kubernetes master is running at https://172.21.16.45:8443</span><br><span class="line"></span><br><span class="line">To further debug and diagnose cluster problems, use <span class="string">'kubectl cluster-info dump'</span>.</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl get all --all-namespaces</span></span><br><span class="line">NAMESPACE   NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">default     service/kubernetes   ClusterIP   10.254.0.1   &lt;none&gt;        443/TCP   12m</span><br><span class="line"></span><br><span class="line"><span class="comment">#  kubectl get componentstatuses</span></span><br><span class="line">NAME                 STATUS      MESSAGE                                                                                     ERROR</span><br><span class="line">controller-manager   Unhealthy   Get http://127.0.0.1:10252/healthz: dial tcp 127.0.0.1:10252: connect: connection refused</span><br><span class="line">scheduler            Unhealthy   Get http://127.0.0.1:10251/healthz: dial tcp 127.0.0.1:10251: connect: connection refused</span><br><span class="line">etcd-0               Healthy     &#123;<span class="string">"health"</span>:<span class="string">"true"</span>&#125;</span><br><span class="line">etcd-2               Healthy     &#123;<span class="string">"health"</span>:<span class="string">"true"</span>&#125;</span><br><span class="line">etcd-1               Healthy     &#123;<span class="string">"health"</span>:<span class="string">"true"</span>&#125;</span><br></pre></td></tr></table></figure><ul><li>执行 kubectl get componentstatuses 命令时，apiserver 默认向 127.0.0.1 发送请求。当 controller-manager、scheduler 以集群模式运行时，有可能和 kube-apiserver 不在一台机器上，这时 controller-manager 或 scheduler 的状态为 Unhealthy，但实际上它们工作正常。</li></ul><h4 id="8-10、检查-kube-apiserver-监听的端口"><a href="#8-10、检查-kube-apiserver-监听的端口" class="headerlink" title="8.10、检查 kube-apiserver 监听的端口"></a>8.10、检查 kube-apiserver 监听的端口</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># netstat -lnpt|grep kube</span></span><br><span class="line">tcp6       0      0 :::6443                 :::*                    LISTEN      10845/kube-apiserve</span><br></pre></td></tr></table></figure><ul><li>6443: 接收 https 请求的安全端口，对所有请求做认证和授权；</li><li>由于关闭了非安全端口，故没有监听 8080；</li></ul><h4 id="8-11、授予-kube-apiserver-访问-kubelet-API-的权限"><a href="#8-11、授予-kube-apiserver-访问-kubelet-API-的权限" class="headerlink" title="8.11、授予 kube-apiserver 访问 kubelet API 的权限"></a>8.11、授予 kube-apiserver 访问 kubelet API 的权限</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在执行 kubectl exec、run、logs 等命令时，apiserver 会将请求转发到 kubelet 的 https 端口。这里定义 RBAC 规则，授权 apiserver 使用的证书（kubernetes.pem）用户名（CN：kuberntes）访问 kubelet API 的权限：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl create clusterrolebinding kube-apiserver:kubelet-apis --clusterrole=system:kubelet-api-admin --user kubernetes</span><br></pre></td></tr></table></figure><h3 id="9、部署高可用-kube-controller-manager"><a href="#9、部署高可用-kube-controller-manager" class="headerlink" title="9、部署高可用 kube-controller-manager"></a>9、部署高可用 kube-controller-manager</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;该集群包含 3 个节点，启动后将通过竞争选举机制产生一个 leader 节点，其它节点为阻塞状态。当 leader 节点不可用时，阻塞的节点将再次进行选举产生新的 leader 节点，从而保证服务的可用性。<br>为保证通信安全，本文档先生成 x509 证书和私钥，kube-controller-manager 在如下两种情况下使用该证书：<br>1、与 kube-apiserver 的安全端口通信;<br>2、在安全端口(https，10252) 输出 prometheus 格式的 metrics；</p><h4 id="9-1、创建-kube-controller-manager-证书和私钥"><a href="#9-1、创建-kube-controller-manager-证书和私钥" class="headerlink" title="9.1、创建 kube-controller-manager 证书和私钥"></a>9.1、创建 kube-controller-manager 证书和私钥</h4><ul><li><p>创建证书签名请求</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat &gt; kube-controller-manager-csr.json &lt;&lt;EOF</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"CN"</span>: <span class="string">"system:kube-controller-manager"</span>,</span><br><span class="line">    <span class="string">"key"</span>: &#123;</span><br><span class="line">        <span class="string">"algo"</span>: <span class="string">"rsa"</span>,</span><br><span class="line">        <span class="string">"size"</span>: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"hosts"</span>: [</span><br><span class="line">      <span class="string">"127.0.0.1"</span>,</span><br><span class="line">      <span class="string">"172.21.17.30"</span>,</span><br><span class="line">      <span class="string">"172.21.17.31"</span>,</span><br><span class="line">      <span class="string">"172.21.16.110"</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"names"</span>: [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="string">"C"</span>: <span class="string">"CN"</span>,</span><br><span class="line">        <span class="string">"ST"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">        <span class="string">"L"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">        <span class="string">"O"</span>: <span class="string">"system:kube-controller-manager"</span>,</span><br><span class="line">        <span class="string">"OU"</span>: <span class="string">"4Paradigm"</span></span><br><span class="line">      &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></li><li><p>hosts 列表包含所有 kube-controller-manager 节点 IP；</p></li><li><p>CN 和 O 均为 system:kube-controller-manager，kubernetes 内置的 ClusterRoleBindings system:kube-controller-manager 赋予 kube-controller-manager 工作所需的权限。</p></li><li><p>生成证书和私钥</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json   -profile=kubernetes kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager</span></span><br><span class="line"><span class="comment"># ls kube-controller-manager*pem</span></span><br><span class="line">kube-controller-manager-key.pem  kube-controller-manager.pem</span><br><span class="line"></span><br><span class="line"><span class="comment"># cp kube-controller-manager*pem /etc/kubernetes/ssl/</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="9-2、创建和分发-kubeconfig-文件"><a href="#9-2、创建和分发-kubeconfig-文件" class="headerlink" title="9.2、创建和分发 kubeconfig 文件"></a>9.2、创建和分发 kubeconfig 文件</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kube-controller-manager 使用 kubeconfig 文件访问 apiserver，该文件提供了 apiserver 地址、嵌入的 CA 证书和 kube-controller-manager 证书：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl config set-cluster kubernetes \</span></span><br><span class="line">  --certificate-authority=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --embed-certs=<span class="literal">true</span> \</span><br><span class="line">  --server=<span class="variable">$&#123;KUBE_APISERVER&#125;</span> \</span><br><span class="line">  --kubeconfig=kube-controller-manager.kubeconfig</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl config set-credentials system:kube-controller-manager \</span></span><br><span class="line">  --client-certificate=kube-controller-manager.pem \</span><br><span class="line">  --client-key=kube-controller-manager-key.pem \</span><br><span class="line">  --embed-certs=<span class="literal">true</span> \</span><br><span class="line">  --kubeconfig=kube-controller-manager.kubeconfig</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl config set-context system:kube-controller-manager \</span></span><br><span class="line">  --cluster=kubernetes \</span><br><span class="line">  --user=system:kube-controller-manager \</span><br><span class="line">  --kubeconfig=kube-controller-manager.kubeconfig</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl config use-context system:kube-controller-manager --kubeconfig=kube-controller-manager.kubeconfig</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># cp kube-controller-manager.kubeconfig /etc/kubernetes/</span></span><br></pre></td></tr></table></figure><h4 id="9-3、创建-kube-controller-manager-systemd-unit文件"><a href="#9-3、创建-kube-controller-manager-systemd-unit文件" class="headerlink" title="9.3、创建 kube-controller-manager systemd unit文件"></a>9.3、创建 kube-controller-manager systemd unit文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat /usr/lib/systemd/system/kube-controller-manager.service </span></span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes Controller Manager</span><br><span class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">WorkingDirectory=/var/<span class="built_in">log</span>/k8s/kube-controller-manager</span><br><span class="line">ExecStart=/usr/bin/kube-controller-manager \</span><br><span class="line">  --profiling \</span><br><span class="line">  --cluster-name=kubernetes \</span><br><span class="line">  --controllers=*,bootstrapsigner,tokencleaner \</span><br><span class="line">  --kube-api-qps=1000 \</span><br><span class="line">  --kube-api-burst=2000 \</span><br><span class="line">  --leader-elect \</span><br><span class="line">  --use-service-account-credentials\</span><br><span class="line">  --concurrent-service-syncs=2 \</span><br><span class="line">  --<span class="built_in">bind</span>-address=0.0.0.0 \</span><br><span class="line">  --secure-port=10252 \</span><br><span class="line">  --tls-cert-file=/etc/kubernetes/ssl/kube-controller-manager.pem \</span><br><span class="line">  --tls-private-key-file=/etc/kubernetes/ssl/kube-controller-manager-key.pem \</span><br><span class="line">  --port=0 \</span><br><span class="line">  --authentication-kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \</span><br><span class="line">  --client-ca-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --requestheader-allowed-names=<span class="string">""</span> \</span><br><span class="line">  --requestheader-client-ca-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --requestheader-extra-headers-prefix=<span class="string">"X-Remote-Extra-"</span> \</span><br><span class="line">  --requestheader-group-headers=X-Remote-Group \</span><br><span class="line">  --requestheader-username-headers=X-Remote-User \</span><br><span class="line">  --authorization-kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \</span><br><span class="line">  --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem \</span><br><span class="line">  --experimental-cluster-signing-duration=876000h \</span><br><span class="line">  --horizontal-pod-autoscaler-sync-period=10s \</span><br><span class="line">  --concurrent-deployment-syncs=10 \</span><br><span class="line">  --concurrent-gc-syncs=30 \</span><br><span class="line">  --node-cidr-mask-size=24 \</span><br><span class="line">  --service-cluster-ip-range=10.254.0.0/16 \</span><br><span class="line">  --pod-eviction-timeout=6m \</span><br><span class="line">  --terminated-pod-gc-threshold=10000 \</span><br><span class="line">  --root-ca-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem \</span><br><span class="line">  --kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \</span><br><span class="line">  --logtostderr=<span class="literal">true</span> \</span><br><span class="line">  --v=2</span><br><span class="line">Restart=on-failure</span><br><span class="line">RestartSec=5</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><ul><li>–port=0：关闭监听非安全端口（http），同时 –address 参数无效，–bind-address 参数有效；</li><li>–secure-port=10252、–bind-address=0.0.0.0: 在所有网络接口监听 10252 端口的 https /metrics 请求；</li><li>–kubeconfig：指定 kubeconfig 文件路径，kube-controller-manager 使用它连接和验证 kube-apiserver；</li><li>–authentication-kubeconfig 和 –authorization-kubeconfig：kube-controller-manager 使用它连接 apiserver，对 client 的请求进行认证和授权。kube-controller-manager 不再使用 –tls-ca-file 对请求 https metrics 的 Client 证书进行校验。如果没有配置这两个 kubeconfig 参数，则 client 连接 kube-controller-manager https 端口的请求会被拒绝(提示权限不足)。</li><li>–cluster-signing-*-file：签名 TLS Bootstrap 创建的证书；</li><li>–experimental-cluster-signing-duration：指定 TLS Bootstrap 证书的有效期；</li><li>–root-ca-file：放置到容器 ServiceAccount 中的 CA 证书，用来对 kube-apiserver 的证书进行校验；</li><li>–service-account-private-key-file：签名 ServiceAccount 中 Token 的私钥文件，必须和 kube-apiserver 的 –service-account-key-file 指定的公钥文件配对使用；</li><li>–service-cluster-ip-range ：指定 Service Cluster IP 网段，必须和 kube-apiserver 中的同名参数一致；</li><li>–leader-elect=true：集群运行模式，启用选举功能；被选为 leader 的节点负责处理工作，其它节点为阻塞状态；</li><li>–controllers=*,bootstrapsigner,tokencleaner：启用的控制器列表，tokencleaner 用于自动清理过期的 Bootstrap token；</li><li>–horizontal-pod-autoscaler-*：custom metrics 相关参数，支持 autoscaling/v2alpha1；</li><li>–tls-cert-file、–tls-private-key-file：使用 https 输出 metrics 时使用的 Server 证书和秘钥；</li><li>–use-service-account-credentials=true: kube-controller-manager 中各 controller 使用 serviceaccount 访问 kube-apiserver；</li></ul><h4 id="9-4、启动-kube-controller-manager-服务"><a href="#9-4、启动-kube-controller-manager-服务" class="headerlink" title="9.4、启动 kube-controller-manager 服务"></a>9.4、启动 kube-controller-manager 服务</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># mkdir -p /var/log/k8s/kube-controller-manager</span></span><br><span class="line"><span class="comment"># systemctl daemon-reload &amp;&amp; systemctl enable kube-controller-manager &amp;&amp; systemctl restart kube-controller-manager &amp;&amp; systemctl status kube-controller-manager</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # netstat -lnpt | grep kube-cont</span></span><br><span class="line">tcp6       0      0 :::10252                :::*                    LISTEN      8335/kube-controlle</span><br></pre></td></tr></table></figure><h4 id="9-5、查看输出的-metrics"><a href="#9-5、查看输出的-metrics" class="headerlink" title="9.5、查看输出的 metrics"></a>9.5、查看输出的 metrics</h4><p><strong>注意:</strong> 以下命令在 kube-controller-manager 节点上执行。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># curl -s --cacert /etc/kubernetes/ssl/ca.pem --cert /etc/kubernetes/ssl/admin.pem --key /etc/kubernetes/ssl/admin-key.pem https://172.21.17.30:10252/metrics |head</span></span><br><span class="line"><span class="comment"># HELP apiserver_audit_event_total Counter of audit events generated and sent to the audit backend.</span></span><br><span class="line"><span class="comment"># TYPE apiserver_audit_event_total counter</span></span><br><span class="line">apiserver_audit_event_total 0</span><br><span class="line"><span class="comment"># HELP apiserver_audit_requests_rejected_total Counter of apiserver requests rejected due to an error in audit logging backend.</span></span><br><span class="line"><span class="comment"># TYPE apiserver_audit_requests_rejected_total counter</span></span><br><span class="line">apiserver_audit_requests_rejected_total 0</span><br><span class="line"><span class="comment"># HELP apiserver_client_certificate_expiration_seconds Distribution of the remaining lifetime on the certificate used to authenticate a request.</span></span><br><span class="line"><span class="comment"># TYPE apiserver_client_certificate_expiration_seconds histogram</span></span><br><span class="line">apiserver_client_certificate_expiration_seconds_bucket&#123;le=<span class="string">"0"</span>&#125; 0</span><br><span class="line">apiserver_client_certificate_expiration_seconds_bucket&#123;le=<span class="string">"1800"</span>&#125; 0</span><br></pre></td></tr></table></figure><h4 id="9-6-kube-controller-manager-的权限"><a href="#9-6-kube-controller-manager-的权限" class="headerlink" title="9.6 kube-controller-manager 的权限"></a>9.6 kube-controller-manager 的权限</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ClusteRole system:kube-controller-manager 的权限很小，只能创建 secret、serviceaccount 等资源对象，各 controller 的权限分散到 ClusterRole system:controller:XXX 中：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl describe clusterrole system:kube-controller-manager</span></span><br><span class="line">Name:         system:kube-controller-manager</span><br><span class="line">Labels:       kubernetes.io/bootstrapping=rbac-defaults</span><br><span class="line">Annotations:  rbac.authorization.kubernetes.io/autoupdate: <span class="literal">true</span></span><br><span class="line">PolicyRule:</span><br><span class="line">  Resources                                  Non-Resource URLs  Resource Names  Verbs</span><br><span class="line">  ---------                                  -----------------  --------------  -----</span><br><span class="line">  secrets                                    []                 []              [create delete get update]</span><br><span class="line">  endpoints                                  []                 []              [create get update]</span><br><span class="line">  serviceaccounts                            []                 []              [create get update]</span><br><span class="line">  events                                     []                 []              [create patch update]</span><br><span class="line">  tokenreviews.authentication.k8s.io         []                 []              [create]</span><br><span class="line">  subjectaccessreviews.authorization.k8s.io  []                 []              [create]</span><br><span class="line">  configmaps                                 []                 []              [get]</span><br><span class="line">  namespaces                                 []                 []              [get]</span><br><span class="line">  *.*                                        []                 []              [list watch]</span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;需要在 kube-controller-manager 的启动参数中添加 –use-service-account-credentials=true 参数，这样 main controller 会为各 controller 创建对应的 ServiceAccount XXX-controller。内置的 ClusterRoleBinding system:controller:XXX 将赋予各 XXX-controller ServiceAccount 对应的 ClusterRole system:controller:XXX 权限</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get clusterrole|grep controller</span></span><br><span class="line">system:controller:attachdetach-controller                              4h52m</span><br><span class="line">system:controller:certificate-controller                               4h52m</span><br><span class="line">system:controller:clusterrole-aggregation-controller                   4h52m</span><br><span class="line">system:controller:cronjob-controller                                   4h52m</span><br><span class="line">system:controller:daemon-set-controller                                4h52m</span><br><span class="line">system:controller:deployment-controller                                4h52m</span><br><span class="line">system:controller:disruption-controller                                4h52m</span><br><span class="line">system:controller:endpoint-controller                                  4h52m</span><br><span class="line">system:controller:expand-controller                                    4h52m</span><br><span class="line">system:controller:generic-garbage-collector                            4h52m</span><br><span class="line">system:controller:horizontal-pod-autoscaler                            4h52m</span><br><span class="line">system:controller:job-controller                                       4h52m</span><br><span class="line">system:controller:namespace-controller                                 4h52m</span><br><span class="line">system:controller:node-controller                                      4h52m</span><br><span class="line">system:controller:persistent-volume-binder                             4h52m</span><br><span class="line">system:controller:pod-garbage-collector                                4h52m</span><br><span class="line">system:controller:pv-protection-controller                             4h52m</span><br><span class="line">system:controller:pvc-protection-controller                            4h52m</span><br><span class="line">system:controller:replicaset-controller                                4h52m</span><br><span class="line">system:controller:replication-controller                               4h52m</span><br><span class="line">system:controller:resourcequota-controller                             4h52m</span><br><span class="line">system:controller:route-controller                                     4h52m</span><br><span class="line">system:controller:service-account-controller                           4h52m</span><br><span class="line">system:controller:service-controller                                   4h52m</span><br><span class="line">system:controller:statefulset-controller                               4h52m</span><br><span class="line">system:controller:ttl-controller                                       4h52m</span><br><span class="line">system:kube-controller-manager                                         4h52m</span><br></pre></td></tr></table></figure><ul><li>以 deployment controller 为例：<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl describe clusterrole system:controller:deployment-controller</span></span><br><span class="line">Name:         system:controller:deployment-controller</span><br><span class="line">Labels:       kubernetes.io/bootstrapping=rbac-defaults</span><br><span class="line">Annotations:  rbac.authorization.kubernetes.io/autoupdate: <span class="literal">true</span></span><br><span class="line">PolicyRule:</span><br><span class="line">  Resources                          Non-Resource URLs  Resource Names  Verbs</span><br><span class="line">  ---------                          -----------------  --------------  -----</span><br><span class="line">  replicasets.apps                   []                 []              [create delete get list patch update watch]</span><br><span class="line">  replicasets.extensions             []                 []              [create delete get list patch update watch]</span><br><span class="line">  events                             []                 []              [create patch update]</span><br><span class="line">  pods                               []                 []              [get list update watch]</span><br><span class="line">  deployments.apps                   []                 []              [get list update watch]</span><br><span class="line">  deployments.extensions             []                 []              [get list update watch]</span><br><span class="line">  deployments.apps/finalizers        []                 []              [update]</span><br><span class="line">  deployments.apps/status            []                 []              [update]</span><br><span class="line">  deployments.extensions/finalizers  []                 []              [update]</span><br><span class="line">  deployments.extensions/status      []                 []              [update]</span><br></pre></td></tr></table></figure></li></ul><h4 id="9-7、查看当前的-leader"><a href="#9-7、查看当前的-leader" class="headerlink" title="9.7、查看当前的 leader"></a>9.7、查看当前的 leader</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get endpoints kube-controller-manager --namespace=kube-system  -o yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Endpoints</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    control-plane.alpha.kubernetes.io/leader: <span class="string">'&#123;"holderIdentity":"k8s-master-01-2.kxl_8890f530-d829-11e9-873f-fa163e5af833","leaseDurationSeconds":15,"acquireTime":"2019-09-16T06:00:15Z","renewTime":"2019-09-16T07:05:38Z","leaderTransitions":1&#125;'</span></span><br><span class="line">  creationTimestamp: <span class="string">"2019-09-16T02:27:06Z"</span></span><br><span class="line">  name: kube-controller-manager</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  resourceVersion: <span class="string">"25734"</span></span><br><span class="line">  selfLink: /api/v1/namespaces/kube-system/endpoints/kube-controller-manager</span><br><span class="line">  uid: 7a5b872e-d829-11e9-9b67-fa163effd55b</span><br></pre></td></tr></table></figure><p>当前的 leader 为k8s-master-01-2节点。</p><p>测试 kube-controller-manager 集群的高可用,停掉一个或两个节点的 kube-controller-manager 服务，观察其它节点的日志，看是否获取了 leader 权限。</p><h3 id="10、scheduler集群"><a href="#10、scheduler集群" class="headerlink" title="10、scheduler集群"></a>10、scheduler集群</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3 个节点，启动后将通过竞争选举机制产生一个 leader 节点，其它节点为阻塞状态。当 leader 节点不可用后，剩余节点将再次进行选举产生新的 leader 节点，从而保证服务的可用性。</p><p>为保证通信安全，本文档先生成 x509 证书和私钥，kube-scheduler 在如下两种情况下使用该证书：<br>1.与 kube-apiserver 的安全端口通信;<br>2.在安全端口(https，10251) 输出 prometheus 格式的 metrics；</p><h4 id="10-1、创建-kube-scheduler-证书和私钥"><a href="#10-1、创建-kube-scheduler-证书和私钥" class="headerlink" title="10.1、创建 kube-scheduler 证书和私钥"></a>10.1、创建 kube-scheduler 证书和私钥</h4><ul><li><p>创建证书签名请求</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat &gt; kube-scheduler-csr.json &lt;&lt;EOF</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"CN"</span>: <span class="string">"system:kube-scheduler"</span>,</span><br><span class="line">    <span class="string">"hosts"</span>: [</span><br><span class="line">      <span class="string">"127.0.0.1"</span>,</span><br><span class="line">      <span class="string">"172.21.17.30"</span>,</span><br><span class="line">      <span class="string">"172.21.17.31"</span>,</span><br><span class="line">      <span class="string">"172.21.16.110"</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"key"</span>: &#123;</span><br><span class="line">        <span class="string">"algo"</span>: <span class="string">"rsa"</span>,</span><br><span class="line">        <span class="string">"size"</span>: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"names"</span>: [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="string">"C"</span>: <span class="string">"CN"</span>,</span><br><span class="line">        <span class="string">"ST"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">        <span class="string">"L"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">        <span class="string">"O"</span>: <span class="string">"system:kube-scheduler"</span>,</span><br><span class="line">        <span class="string">"OU"</span>: <span class="string">"4Paradigm"</span></span><br><span class="line">      &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure></li><li><p>hosts 列表包含所有 kube-scheduler 节点 IP；</p></li><li><p>CN 和 O 均为 system:kube-scheduler，kubernetes 内置的 ClusterRoleBindings system:kube-scheduler 将赋予 kube-scheduler 工作所需的权限；</p></li><li><p>生成证书和私钥:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-scheduler-csr.json | cfssljson -bare kube-scheduler</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ls kube-scheduler*pem</span></span><br><span class="line">kube-scheduler-key.pem  kube-scheduler.pem</span><br><span class="line"></span><br><span class="line"><span class="comment"># cp kube-scheduler*pem  /etc/kubernetes/ssl/</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="10-2、创建和分发-kubeconfig-文件"><a href="#10-2、创建和分发-kubeconfig-文件" class="headerlink" title="10.2、创建和分发 kubeconfig 文件"></a>10.2、创建和分发 kubeconfig 文件</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kube-scheduler 使用 kubeconfig 文件访问 apiserver，该文件提供了 apiserver 地址、嵌入的 CA 证书和 kube-scheduler 证书：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl config set-cluster kubernetes \</span></span><br><span class="line">  --certificate-authority=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --embed-certs=<span class="literal">true</span> \</span><br><span class="line">  --server=<span class="variable">$&#123;KUBE_APISERVER&#125;</span> \</span><br><span class="line">  --kubeconfig=kube-scheduler.kubeconfig</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl config set-credentials system:kube-scheduler \</span></span><br><span class="line">  --client-certificate=kube-scheduler.pem \</span><br><span class="line">  --client-key=kube-scheduler-key.pem \</span><br><span class="line">  --embed-certs=<span class="literal">true</span> \</span><br><span class="line">  --kubeconfig=kube-scheduler.kubeconfig</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl config set-context system:kube-scheduler \</span></span><br><span class="line">  --cluster=kubernetes \</span><br><span class="line">  --user=system:kube-scheduler \</span><br><span class="line">  --kubeconfig=kube-scheduler.kubeconfig</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl config use-context system:kube-scheduler --kubeconfig=kube-scheduler.kubeconfig</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># cp kube-scheduler.kubeconfig /etc/kubernetes/</span></span><br></pre></td></tr></table></figure><h4 id="10-3、创建-kube-scheduler-配置文件"><a href="#10-3、创建-kube-scheduler-配置文件" class="headerlink" title="10.3、创建 kube-scheduler 配置文件"></a>10.3、创建 kube-scheduler 配置文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat &gt;kube-scheduler.yaml &lt;&lt;EOF</span></span><br><span class="line">apiVersion: kubescheduler.config.k8s.io/v1alpha1</span><br><span class="line">kind: KubeSchedulerConfiguration</span><br><span class="line">bindTimeoutSeconds: 600</span><br><span class="line">clientConnection:</span><br><span class="line">  burst: 200</span><br><span class="line">  kubeconfig: <span class="string">"/etc/kubernetes/kube-scheduler.kubeconfig"</span></span><br><span class="line">  qps: 100</span><br><span class="line">enableContentionProfiling: <span class="literal">false</span></span><br><span class="line">enableProfiling: <span class="literal">true</span></span><br><span class="line">hardPodAffinitySymmetricWeight: 1</span><br><span class="line">healthzBindAddress: <span class="comment">##NODE_IP##:10251</span></span><br><span class="line">leaderElection:</span><br><span class="line">  leaderElect: <span class="literal">true</span></span><br><span class="line">metricsBindAddress: <span class="comment">##NODE_IP##:10251</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># cp kube-scheduler.yaml /etc/kubernetes/</span></span><br></pre></td></tr></table></figure><ul><li>–kubeconfig：指定 kubeconfig 文件路径，kube-scheduler 使用它连接和验证 kube-apiserver；</li><li>–leader-elect=true：集群运行模式，启用选举功能；被选为 leader 的节点负责处理工作，其它节点为阻塞状态；</li></ul><h4 id="10-4、创建-kube-scheduler-systemd-unit-模板文件"><a href="#10-4、创建-kube-scheduler-systemd-unit-模板文件" class="headerlink" title="10.4、创建 kube-scheduler systemd unit 模板文件"></a>10.4、创建 kube-scheduler systemd unit 模板文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat /usr/lib/systemd/system/kube-scheduler.service </span></span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes Scheduler</span><br><span class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">WorkingDirectory=/var/<span class="built_in">log</span>/k8s/kube-scheduler</span><br><span class="line">ExecStart=/usr/bin/kube-scheduler \</span><br><span class="line">  --config=/etc/kubernetes/kube-scheduler.yaml \</span><br><span class="line">  --<span class="built_in">bind</span>-address=0.0.0.0 \</span><br><span class="line">  --secure-port=10259 \</span><br><span class="line">  --port=0 \</span><br><span class="line">  --tls-cert-file=/etc/kubernetes/ssl/kube-scheduler.pem \</span><br><span class="line">  --tls-private-key-file=/etc/kubernetes/ssl/kube-scheduler-key.pem \</span><br><span class="line">  --authentication-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \</span><br><span class="line">  --client-ca-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --requestheader-allowed-names=<span class="string">""</span> \</span><br><span class="line">  --requestheader-client-ca-file=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --requestheader-extra-headers-prefix=<span class="string">"X-Remote-Extra-"</span> \</span><br><span class="line">  --requestheader-group-headers=X-Remote-Group \</span><br><span class="line">  --requestheader-username-headers=X-Remote-User \</span><br><span class="line">  --authorization-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \</span><br><span class="line">  --logtostderr=<span class="literal">true</span> \</span><br><span class="line">  --v=2</span><br><span class="line">Restart=always</span><br><span class="line">RestartSec=5</span><br><span class="line">StartLimitInterval=0</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><h4 id="10-5、启动-kube-scheduler-服务"><a href="#10-5、启动-kube-scheduler-服务" class="headerlink" title="10.5、启动 kube-scheduler 服务"></a>10.5、启动 kube-scheduler 服务</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># mkdir -p /var/log/k8s/kube-scheduler</span></span><br><span class="line"><span class="comment"># systemctl daemon-reload &amp;&amp; systemctl enable kube-scheduler &amp;&amp; systemctl restart kube-scheduler &amp;&amp; systemctl status kube-scheduler</span></span><br></pre></td></tr></table></figure><h4 id="10-6、查看输出的-metrics"><a href="#10-6、查看输出的-metrics" class="headerlink" title="10.6、查看输出的 metrics"></a>10.6、查看输出的 metrics</h4><p>kube-scheduler 监听 10251 和 10251 端口：</p><ul><li>10251：接收 http 请求，非安全端口，不需要认证授权</li><li>10259：接收 https 请求，安全端口，需要认证授权</li></ul><p>两个接口都对外提供 /metrics 和 /healthz 的访问。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># netstat -lnpt |grep kube-sch</span></span><br><span class="line">tcp        0      0 172.21.17.31:10251      0.0.0.0:*               LISTEN      8441/kube-scheduler </span><br><span class="line">tcp6       0      0 :::10259                :::*                    LISTEN      8441/kube-scheduler</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># curl -s http://172.21.17.30:10251/metrics |head</span></span><br><span class="line"><span class="comment"># HELP apiserver_audit_event_total Counter of audit events generated and sent to the audit backend.</span></span><br><span class="line"><span class="comment"># TYPE apiserver_audit_event_total counter</span></span><br><span class="line">apiserver_audit_event_total 0</span><br><span class="line"><span class="comment"># HELP apiserver_audit_requests_rejected_total Counter of apiserver requests rejected due to an error in audit logging backend.</span></span><br><span class="line"><span class="comment"># TYPE apiserver_audit_requests_rejected_total counter</span></span><br><span class="line">apiserver_audit_requests_rejected_total 0</span><br><span class="line"><span class="comment"># HELP apiserver_client_certificate_expiration_seconds Distribution of the remaining lifetime on the certificate used to authenticate a request.</span></span><br><span class="line"><span class="comment"># TYPE apiserver_client_certificate_expiration_seconds histogram</span></span><br><span class="line">apiserver_client_certificate_expiration_seconds_bucket&#123;le=<span class="string">"0"</span>&#125; 0</span><br><span class="line">apiserver_client_certificate_expiration_seconds_bucket&#123;le=<span class="string">"1800"</span>&#125; 0</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># curl -s --cacert /etc/kubernetes/ssl/ca.pem --cert /etc/kubernetes/ssl/admin.pem --key /etc/kubernetes/ssl/admin-key.pem https://172.21.17.30:10259/metrics |head</span></span><br><span class="line"><span class="comment"># HELP apiserver_audit_event_total Counter of audit events generated and sent to the audit backend.</span></span><br><span class="line"><span class="comment"># TYPE apiserver_audit_event_total counter</span></span><br><span class="line">apiserver_audit_event_total 0</span><br><span class="line"><span class="comment"># HELP apiserver_audit_requests_rejected_total Counter of apiserver requests rejected due to an error in audit logging backend.</span></span><br><span class="line"><span class="comment"># TYPE apiserver_audit_requests_rejected_total counter</span></span><br><span class="line">apiserver_audit_requests_rejected_total 0</span><br><span class="line"><span class="comment"># HELP apiserver_client_certificate_expiration_seconds Distribution of the remaining lifetime on the certificate used to authenticate a request.</span></span><br><span class="line"><span class="comment"># TYPE apiserver_client_certificate_expiration_seconds histogram</span></span><br><span class="line">apiserver_client_certificate_expiration_seconds_bucket&#123;le=<span class="string">"0"</span>&#125; 0</span><br><span class="line">apiserver_client_certificate_expiration_seconds_bucket&#123;le=<span class="string">"1800"</span>&#125; 0</span><br></pre></td></tr></table></figure><h4 id="10-7、查看当前的-leader"><a href="#10-7、查看当前的-leader" class="headerlink" title="10.7、查看当前的 leader"></a>10.7、查看当前的 leader</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#  kubectl get endpoints kube-scheduler --namespace=kube-system  -o yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Endpoints</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    control-plane.alpha.kubernetes.io/leader: <span class="string">'&#123;"holderIdentity":"k8s-master-01.kxl_2a6e0bf9-d82b-11e9-b946-fa163effd55b","leaseDurationSeconds":15,"acquireTime":"2019-09-16T06:00:28Z","renewTime":"2019-09-16T07:41:57Z","leaderTransitions":1&#125;'</span></span><br><span class="line">  creationTimestamp: <span class="string">"2019-09-16T02:38:55Z"</span></span><br><span class="line">  name: kube-scheduler</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  resourceVersion: <span class="string">"29215"</span></span><br><span class="line">  selfLink: /api/v1/namespaces/kube-system/endpoints/kube-scheduler</span><br><span class="line">  uid: 20a04151-d82b-11e9-baf3-fa163e53d4c8</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kuberntes v1.14</tag>
      </tags>
  </entry>
  <entry>
    <title>路由器端口映射</title>
    <url>/2019/09/10/%E8%B7%AF%E7%94%B1%E5%99%A8%E7%AB%AF%E5%8F%A3%E6%98%A0%E5%B0%84/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><p>好记性不如烂笔头，h3c MSR3620路由器做端口映射到后端服务器,包含单个端口和端口段的映射</p><a id="more"></a><h3 id="单个端口的映射"><a href="#单个端口的映射" class="headerlink" title="单个端口的映射"></a>单个端口的映射</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[router] interface GigabitEthernet0/2 公网ip接口</span><br><span class="line">[router] nat server protocol tcp global 公网ip 80 inside 内网ip 80</span><br></pre></td></tr></table></figure><h3 id="多端口"><a href="#多端口" class="headerlink" title="多端口"></a>多端口</h3><p>vsftp可以使用这个</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[router] interface GigabitEthernet0/2 公网ip接口</span><br><span class="line">[router] nat server protocol tcp global current-interface 9000 9045 inside 内网ip 9000 9045</span><br></pre></td></tr></table></figure><p><strong>备注:</strong> 可以使用vsftp场景，<a href="https://xxlaila.github.io/2019/08/09/vsftpd%E5%AE%89%E8%A3%85/" target="_blank" rel="noopener">vsftp安装</a></p><!-- rebuild by neat -->]]></content>
      <categories>
        <category>网络设备</category>
      </categories>
      <tags>
        <tag>MSR3620</tag>
      </tags>
  </entry>
  <entry>
    <title>traefik https应用</title>
    <url>/2019/09/06/traefik-https%E5%BA%94%E7%94%A8/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;之前已经使用traefik服务作为入口，测试并访问了tomcat应用，之前是通过http来访问的，而我们在yaml文件里面也添加8443端口用于https访问，在实际环境中我们也是需要https来进行访问应用，通过traefik实现https，<a href="https://xxlaila.github.io/2019/09/05/traefik-ingress%E4%BD%BF%E7%94%A8/" target="_blank" rel="noopener">traefik http应用</a></p><h3 id="操作实践"><a href="#操作实践" class="headerlink" title="操作实践"></a>操作实践</h3><ul><li>这里我用了公司的证书，就是为了贴近真实，也满足测试需求，</li><li>创建一个secret，保存https证书</li></ul><a id="more"></a><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ls</span></span><br><span class="line">1_test.xxlaila.cn_bundle.crt  2_test.xxlaila.cn.key</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl create secret generic traefik-cert --from-file=1_test.xxlaila.cn_bundle.crt --from-file=2_test.xxlaila.cn.key -n kube-system</span></span><br><span class="line">secret/traefik-cert created</span><br></pre></td></tr></table></figure><p>把证书拷贝到k8s node节点，存放路径为/etc/kubernetes/certs。</p><h3 id="创建一个configmap，保存traefix的配置"><a href="#创建一个configmap，保存traefix的配置" class="headerlink" title="创建一个configmap，保存traefix的配置"></a>创建一个configmap，保存traefix的配置</h3><p>traefix中配置了把所有http请求全部rewrite为https的规则，并配置相应的证书位置</p><ul><li>traefik.toml<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">defaultEntryPoints = [<span class="string">"http"</span>,<span class="string">"https"</span>]</span><br><span class="line">[entryPoints]</span><br><span class="line">  [entryPoints.http]</span><br><span class="line">  address = <span class="string">":80"</span></span><br><span class="line">    [entryPoints.http.redirect]</span><br><span class="line">    entryPoint = <span class="string">"https"</span></span><br><span class="line">  [entryPoints.https]</span><br><span class="line">  address = <span class="string">":443"</span></span><br><span class="line">    [entryPoints.https.tls]</span><br><span class="line">      [[entryPoints.https.tls.certificates]]</span><br><span class="line">      certFile = <span class="string">"/etc/kubernetes/certs/1_test.xxlaila.cn_bundle.crt"</span></span><br><span class="line">      keyFile = <span class="string">"/etc/kubernetes/certs/2_test.xxlaila.cn.key"</span></span><br><span class="line"></span><br><span class="line">$ kubectl create configmap traefik-conf --from-file=traefik.toml -n kube-system</span><br><span class="line">configmap/traefik-conf created</span><br></pre></td></tr></table></figure></li></ul><p>把traefik.toml文件拷贝到k8s node节点,存放路径为/etc/kubernetes/conf。</p><h3 id="重新部署Traefix"><a href="#重新部署Traefix" class="headerlink" title="重新部署Traefix"></a>重新部署Traefix</h3><p>主要是要关联创建的secret和configMap，并挂载相对应的主机目录。</p><ul><li>deployment.yaml</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat deployment.yaml</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: traefik-ingress-lb</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: traefik-ingress-lb</span><br><span class="line">spec:</span><br><span class="line">  replicas: 2 <span class="comment"># 增加行</span></span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        k8s-app: traefik-ingress-lb</span><br><span class="line">        name: traefik-ingress-lb</span><br><span class="line">    spec:</span><br><span class="line">      terminationGracePeriodSeconds: 60</span><br><span class="line">      hostNetwork: <span class="literal">true</span></span><br><span class="line">      restartPolicy: Always</span><br><span class="line">      serviceAccountName: ingress</span><br><span class="line">      containers:</span><br><span class="line">      - image: traefik</span><br><span class="line">        name: traefik-ingress-lb</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - mountPath: <span class="string">"/etc/kubernetes/certs"</span></span><br><span class="line">          name: <span class="string">"ssl"</span></span><br><span class="line">        - mountPath: <span class="string">"/etc/kubernetes/conf"</span></span><br><span class="line">          name: <span class="string">"config"</span></span><br><span class="line">        resources:</span><br><span class="line">          limits:</span><br><span class="line">            cpu: 1000m</span><br><span class="line">            memory: 3000Mi</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 500m</span><br><span class="line">            memory: 2000Mi</span><br><span class="line">        ports:</span><br><span class="line">        - name: http</span><br><span class="line">          containerPort: 80</span><br><span class="line">          hostPort: 80</span><br><span class="line">        - name: admin</span><br><span class="line">          containerPort: 8580</span><br><span class="line">          hostPort: 8580</span><br><span class="line">        args:</span><br><span class="line">        - --configFile=/etc/kubernetes/conf/traefik.toml</span><br><span class="line">        - --web</span><br><span class="line">        - --web.address=:8580</span><br><span class="line">        - --kubernetes</span><br><span class="line"></span><br><span class="line">$ kubectl apply -f deployment.yaml </span><br><span class="line">deployment.extensions/traefik-ingress-lb configured</span><br></pre></td></tr></table></figure><h3 id="测试效果"><a href="#测试效果" class="headerlink" title="测试效果"></a>测试效果</h3><p>登陆traefik-ui界面,用原本http的访问，traefik会直接给我们重定向至https。<br><img src="https://img.xxlaila.cn/1567748749337.jpg" alt="img"></p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于traefik-ui使用的域名不是我们证书所支持的域名，所以这里提示不安全，修改之前创建的ingress，修改其中的域名为支持证书的域名</p><ul><li>traefik-ui.yaml<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: traefik-web-ui</span><br><span class="line">  namespace: kube-system </span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    k8s-app: traefik-ingress-lb</span><br><span class="line">  ports:</span><br><span class="line">  - name: web</span><br><span class="line">    port: 80</span><br><span class="line">    targetPort: 8580</span><br><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: traefik-web-ui</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: traefik.test.xxlaila.cn</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: /</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: traefik-web-ui</span><br><span class="line">          servicePort: web</span><br><span class="line"></span><br><span class="line">$ kubectl apply -f traefik-ui.yaml </span><br><span class="line">service/traefik-web-ui unchanged</span><br><span class="line">ingress.extensions/traefik-web-ui configured</span><br></pre></td></tr></table></figure></li></ul><p>修改hosts版定的域名进行访问<br><img src="https://img.xxlaila.cn/1567749086159.jpg" alt="img"></p><ul><li><p>修改之前部署的tomcat程序</p></li><li><p>ingress-tomcat.yaml</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: tomcat-test-web</span><br><span class="line">  namespace: default</span><br><span class="line">  annotations:</span><br><span class="line">    kubernetes.io/ingress.class: traefik</span><br><span class="line">    traefik.frontend.rule.type: PathPrefixStrip</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: tomcat.test.xxlaila.cn</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: /test1/</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: tomcat-test1</span><br><span class="line">          servicePort: 8080</span><br><span class="line">      - path: /test2/</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: tomcat-test2</span><br><span class="line">          servicePort: 8080</span><br><span class="line"></span><br><span class="line">$ kubectl apply -f ingress-tomcat.yaml </span><br><span class="line">ingress.extensions/tomcat-test-web configured</span><br></pre></td></tr></table></figure></li></ul><p>访问链接测试<br><img src="https://img.xxlaila.cn/1567749278980.jpg" alt="img"><br><img src="https://img.xxlaila.cn/1567749309772.jpg" alt="img"></p><h3 id="其他需求"><a href="#其他需求" class="headerlink" title="其他需求"></a>其他需求</h3><p>在我们真实的应用场景中，需求肯定有不同的，比如我所在的公司开发环境就要只是http和https，测试环境以上的就全部强制https。这就得分开进行配置</p><h4 id="同时支持http和https"><a href="#同时支持http和https" class="headerlink" title="同时支持http和https"></a>同时支持http和https</h4><p>把http中的rewrite代码改掉</p><ul><li>traefik.toml</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">defaultEntryPoints = [<span class="string">"http"</span>,<span class="string">"https"</span>]</span><br><span class="line">[entryPoints]</span><br><span class="line">  [entryPoints.http]</span><br><span class="line">  address = <span class="string">":80"</span></span><br><span class="line">    entryPoint = <span class="string">"https"</span></span><br><span class="line">  [entryPoints.https]</span><br><span class="line">  address = <span class="string">":443"</span></span><br><span class="line">    [entryPoints.https.tls]</span><br><span class="line">      [[entryPoints.https.tls.certificates]]</span><br><span class="line">      certFile = <span class="string">"/etc/kubernetes/certs/1_test.xxlaila.cn_bundle.crt"</span></span><br><span class="line">      keyFile = <span class="string">"/etc/kubernetes/certs/2_test.xxlaila.cn.key"</span></span><br><span class="line">      [[entryPoints.https.tls.certificates]]</span><br><span class="line">      certFile = <span class="string">"/etc/kubernetes/certs/1_dev.xxlaila.cn_bundle.crt"</span></span><br><span class="line">      keyFile = <span class="string">"/etc/kubernetes/certs/2_dev.xxlaila.cn.key"</span></span><br><span class="line"></span><br><span class="line">[file]</span><br><span class="line"></span><br><span class="line"><span class="comment"># rules</span></span><br><span class="line">[entryPoints]</span><br><span class="line">  [entryPoints.http]</span><br><span class="line">  address = <span class="string">":80"</span></span><br><span class="line">    [entryPoints.http.redirect]</span><br><span class="line">      regex = <span class="string">"^http://traefix.test.xxlaila.cn/(.*)"</span></span><br><span class="line">      replacement = <span class="string">"https://traefix.test.xxlaila.cn/<span class="variable">$1</span>"</span></span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>traefik</tag>
      </tags>
  </entry>
  <entry>
    <title>traefik ingress使用</title>
    <url>/2019/09/05/traefik-ingress%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:23 GMT+0800 (China Standard Time) --><h3 id="Traefik介绍"><a href="#Traefik介绍" class="headerlink" title="Traefik介绍"></a>Traefik介绍</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;简单的说，ingress就是从kubernetes集群外访问集群的入口，将用户的URL请求转发到不同的service上。Ingress相当于nginx、apache等负载均衡反向代理服务器，其中还包括规则定义，即URL的路由信息。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Traefik是一款开源的反向代理与负载均衡工具。它最大的优点是能够与常见的微服务系统直接整合，实现自动化动态配置。Traefik通过不断地跟 kubernetes API 打交道，实时的感知后端 service、pod 等变化，比如pod，service 增加与减少等；当得到这些变化信息后，Ingress自动更新配置并热重载 ，达到服务发现的作用。</p><a id="more"></a><p>traefix 整体架构图如下:<br><img src="https://img.xxlaila.cn/34238937snkhfskdy8923.png" alt="img"></p><h3 id="Traefik主要特性详解"><a href="#Traefik主要特性详解" class="headerlink" title="Traefik主要特性详解"></a>Traefik主要特性详解</h3><ul><li><p>自动熔断</p><ul><li>在集群中，当某一个服务大量出现请求错误，或者请求响应时间过久，或者返回500+错误状态码时，我们希望可以主动剔除该服务，也就是不在将请求转发到该服务上，而这一个过程是自动完成，不需要人工执行。Traefik 通过配置很容易就能帮我们实现，Traefik 可以通过定义策略来主动熔断服务。</li><li>NetworkErrorRatio() &gt; 0.5：监测服务错误率达到50%时，熔断</li><li>LatencyAtQuantileMS(50.0) &gt; 50：监测延时大于50ms时，熔断</li><li>ResponseCodeRatio(500, 600, 0, 600) &gt; 0.5：监测返回状态码为[500-600]在[0-600]区间占比超过50%时，熔断</li></ul></li><li><p>负载均衡策略</p><ul><li>Traefik 提供两种负载均衡策略支持。一种是 wrr（加权轮训调度算法），一种是 drr（动态加权循环调度算法）</li><li>wrr是默认的负载均衡策略，新创建的 service 权重都是一样为1，这样的话，请求会平均分给每个服务，但是这样很多时候会出现资源分配不均衡的问题，比如由于集群中每个机器配置不一样，而且服务消耗不一样，假设 A 资源使用率已经很高，而 B 属于空闲状态，如果还是均摊到每个服务的话，会加重 A 的负荷，这时候因该有一种策略能够主动识别并分担更多流量到 B 才对</li><li>drr 就更加智能，它是一种动态加权轮训调度方式，它会记录一段时间内转发到 A 的请求数，跟转发到 B 的请求数对比，转发数量多，说明处理速度快，响应时间快。如果 A 处理请求速度比 B 快，那么就会调整 A 的权重，接下来的一段时间，就会转发更多请求给 A，相应的 B 的转发就少一些。整个过程都在不断的调整权重，实现请求的合理分配，从而达到资源使用最大化</li></ul></li></ul><h3 id="部署Traefik-ingress"><a href="#部署Traefik-ingress" class="headerlink" title="部署Traefik ingress"></a>部署Traefik ingress</h3><ul><li><p>创建ingress-rbac.yaml，将用于service account验证。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat ingress-rbac.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: ingress</span><br><span class="line">  namespace: kube-system</span><br><span class="line">---</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: ingress</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: ingress</span><br><span class="line">    namespace: kube-system</span><br><span class="line">roleRef:</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: cluster-admin</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br></pre></td></tr></table></figure></li><li><p>创建Depeloyment部署traefik，如文件名为deployment.yaml</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: traefik-ingress-lb</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: traefik-ingress-lb</span><br><span class="line">spec:</span><br><span class="line">  replicas: 2 <span class="comment"># 增加行</span></span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        k8s-app: traefik-ingress-lb</span><br><span class="line">        name: traefik-ingress-lb</span><br><span class="line">    spec:</span><br><span class="line">      terminationGracePeriodSeconds: 60</span><br><span class="line">      hostNetwork: <span class="literal">true</span></span><br><span class="line">      restartPolicy: Always</span><br><span class="line">      serviceAccountName: ingress</span><br><span class="line">      containers:</span><br><span class="line">      - image: traefik</span><br><span class="line">        name: traefik-ingress-lb</span><br><span class="line">        resources:</span><br><span class="line">          limits:</span><br><span class="line">            cpu: 1000m</span><br><span class="line">            memory: 3000Mi</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 500m</span><br><span class="line">            memory: 2000Mi</span><br><span class="line">        ports:</span><br><span class="line">        - name: http</span><br><span class="line">          containerPort: 80</span><br><span class="line">          hostPort: 80</span><br><span class="line">        - name: admin</span><br><span class="line">          containerPort: 8580</span><br><span class="line">          hostPort: 8580</span><br><span class="line">        args:</span><br><span class="line">        - --web</span><br><span class="line">        - --web.address=:8580</span><br><span class="line">        - --kubernetes</span><br></pre></td></tr></table></figure></li></ul><ul><li><strong>注意</strong>: 我们这里用的是Deploy类型，没有限定该pod运行在哪个主机上。Traefik的端口是8580。</li></ul><ul><li>编写Traefik UI的ingress部署文件，如文件名为traefik-ui.yaml<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: traefik-web-ui</span><br><span class="line">  namespace: kube-system <span class="comment">#增加行</span></span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    k8s-app: traefik-ingress-lb</span><br><span class="line">  ports:</span><br><span class="line">  - name: web</span><br><span class="line">    port: 80</span><br><span class="line">    targetPort: 8580</span><br><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: traefik-web-ui</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: traefik.xxlaila.io</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: /</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: traefik-web-ui</span><br><span class="line">          servicePort: web</span><br></pre></td></tr></table></figure></li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</p><ul><li><code>backend</code>中要配置default namespace中启动的service名字。</li><li><code>path</code>就是URL地址后的路径，如<code>traefik.frontend.io/path</code>，service将会接受path这个路径</li><li><code>host</code>最好使用service-name.filed1.filed2.domain-name这种类似主机名称的命名方式，方便区分服务。</li></ul><ul><li><strong>逼逼一下</strong>: 目前我所在的公司后端微服务100+，前端60+，如果用传统nginx的local来匹配，估计要写死人，而且对于运维自动化来也不是很好做，再则是出了问题也还要去看一下是哪个应用；我们目前是通过每个服务每一个域名，域名是根据服务名来自动生成，除了几个特定对外公开的是特制的域名，其他的均采用这种机制，当有问题的时候，一下就能判断出那里出问题，很好定位，有域名有特殊配置的时候，也可以单独的进行设置，但是截止目前两年多来，运维拒绝这种特殊需求(有还有，很少，只有那么两三个)</li></ul><ul><li><p>配置完成后就可以启动treafik ingress了</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl apply -f ./</span></span><br><span class="line">deployment.extensions/traefik-ingress-lb created</span><br><span class="line">serviceaccount/ingress created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/ingress created</span><br><span class="line">service/traefik-web-ui created</span><br><span class="line">ingress.extensions/traefik-web-ui created</span><br></pre></td></tr></table></figure></li><li><p>查看是否部署成功</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get pods -n kube-system | grep traefik</span></span><br><span class="line"></span><br><span class="line">traefik-ingress-lb-5d7f658cfd-4vkjc     1/1     Running   0          29m</span><br><span class="line">traefik-ingress-lb-5d7f658cfd-7sszp     1/1     Running   0          19m</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl get ingress -o wide --all-namespaces </span></span><br><span class="line">NAMESPACE     NAME                HOSTS                ADDRESS   PORTS   AGE</span><br><span class="line">kube-system   traefik-web-ui      traefik.xxlaila.io             80      29m</span><br></pre></td></tr></table></figure></li></ul><p>在浏览器绑定hosts域名解析，node的ip地址，在浏览器输入traefik.xxlaila.io即可访问了<br><img src="https://img.xxlaila.cn/1567676343800.jpg" alt="img"></p><p>左侧蓝色部分列出的是所有的前端(frontends)，右侧绿色部分是所有的后端(backend)。</p><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>下面模拟部署一个程序，以Nginx 为例，并使用drr动态轮训加权策略。</p><ul><li><p>nginx-deployment.yaml</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">apiVersion: apps/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-pod</span><br><span class="line">spec:</span><br><span class="line">  replicas: 2</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx-pod</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.15.5</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-service</span><br><span class="line">  annotations:</span><br><span class="line">    traefik.ingress.kubernetes.io/load-balancer-method: drr</span><br><span class="line">spec:</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        name: nginx-service</span><br><span class="line">        namespace: default</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: nginx-pod</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80</span><br><span class="line">    targetPort: 80</span><br><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ingress</span><br><span class="line">  annotations:</span><br><span class="line">    kubernetes.io/ingress.class: traefik</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: k8s.nginx.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - backend:</span><br><span class="line">          serviceName: nginx-service</span><br><span class="line">          servicePort: 80</span><br></pre></td></tr></table></figure></li><li><p>创建nginx</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl apply -f nginx-deployment.yaml </span></span><br><span class="line">deployment.apps/nginx-pod created</span><br><span class="line">service/nginx-service created</span><br><span class="line">ingress.extensions/nginx-ingress created</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl get pods</span></span><br></pre></td></tr></table></figure></li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;所有访问这些地址的流量都会发送给172.16.0.180这台主机，就是我们启动traefik的主机。Traefik会解析http请求header里的Host参数将流量转发给Ingress配置里的相应service。<br><img src="https://img.xxlaila.cn/1567676984150.jpg" alt="img"></p><p>客户端绑定host，浏览器进行访问: <a href="http://k8s.nginx.com" target="_blank" rel="noopener">http://k8s.nginx.com</a><br><img src="https://img.xxlaila.cn/1567677018297.jpg" alt="img"></p><p>在K8s集群节点上访问测试</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># curl -x 172.21.16.204 http://k8s.nginx.com</span></span><br><span class="line">&lt;html&gt;&lt;body&gt;&lt;h1&gt;503 Service Unavailable&lt;/h1&gt;</span><br><span class="line">No server is available to handle this request.</span><br><span class="line">&lt;/body&gt;&lt;/html&gt;</span><br><span class="line">[root@k8s ~]<span class="comment"># curl -x 172.21.16.204:80 http://k8s.nginx.com</span></span><br><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;</span><br><span class="line">&lt;title&gt;Welcome to nginx!&lt;/title&gt;</span><br><span class="line">&lt;style&gt;</span><br><span class="line">    body &#123;</span><br><span class="line">        width: 35em;</span><br><span class="line">        margin: 0 auto;</span><br><span class="line">        font-family: Tahoma, Verdana, Arial, sans-serif;</span><br><span class="line">    &#125;</span><br><span class="line">&lt;/style&gt;</span><br><span class="line">&lt;/head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;</span><br><span class="line">&lt;p&gt;If you see this page, the nginx web server is successfully installed and</span><br><span class="line">working. Further configuration is required.&lt;/p&gt;</span><br><span class="line"></span><br><span class="line">&lt;p&gt;For online documentation and support please refer to</span><br><span class="line">&lt;a href=<span class="string">"http://nginx.org/"</span>&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;</span><br><span class="line">Commercial support is available at</span><br><span class="line">&lt;a href=<span class="string">"http://nginx.com/"</span>&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;</span><br><span class="line"></span><br><span class="line">&lt;p&gt;&lt;em&gt;Thank you <span class="keyword">for</span> using nginx.&lt;/em&gt;&lt;/p&gt;</span><br><span class="line">&lt;/body&gt;</span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></table></figure><h4 id="ingress配置同域名不同路径代理web应用"><a href="#ingress配置同域名不同路径代理web应用" class="headerlink" title="ingress配置同域名不同路径代理web应用"></a>ingress配置同域名不同路径代理web应用</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;很多时候我们不想配置太多的域名来区别应用，使用同域名分路径的方式来区别应用就简洁方便很多。ingress也提供了相关的配置。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;假设两个应用tomcat-test1和tomcat-test2。这里可配置域名tomcat.xxlaila.io，通过路径test1、test2来分别代理两个tomcat应用。其中，分路径配置需添加配置：traefik.frontend.rule.type: PathPrefixStrip,首先，我先创建tomcat-test1和tomcat-test2的pod和service，其中8080为tomcat的http端口，8443为tomcat的https端口，本例中仅使用http端口测试。</p><ul><li><p>tomcat-test1.yaml</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: tomcat-test1</span><br><span class="line">  labels: </span><br><span class="line">    app: tomcat-test1</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1 </span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: tomcat-test1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: tomcat-test1</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: tomcat-test1</span><br><span class="line">        image: tomcat</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 8443</span><br><span class="line">        - containerPort: 8080</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: tomcat-test1</span><br><span class="line">  labels:</span><br><span class="line">    name: tomcat-test1</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - port: 8443</span><br><span class="line">    targetPort: 8443</span><br><span class="line">  selector:</span><br><span class="line">    app: tomcat-test1</span><br><span class="line">  ports:</span><br><span class="line">  - port: 8080 </span><br><span class="line">    targetPort: 8080</span><br><span class="line">  selector:</span><br><span class="line">    app: tomcat-test1</span><br></pre></td></tr></table></figure></li><li><p>tomcat-test2.yaml</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: tomcat-test2</span><br><span class="line">  labels: </span><br><span class="line">    app: tomcat-test2</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1 </span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: tomcat-test2</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: tomcat-test2</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: tomcat-test2</span><br><span class="line">        image: manjeetchauhan211/tomcat_test2</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 8443</span><br><span class="line">        - containerPort: 8080</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: tomcat-test2</span><br><span class="line">  labels:</span><br><span class="line">    name: tomcat-test2</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - port: 8443</span><br><span class="line">    targetPort: 8443</span><br><span class="line">  ports:</span><br><span class="line">  - port: 8080 </span><br><span class="line">    targetPort: 8080</span><br><span class="line">  selector:</span><br><span class="line">    app: tomcat-test2</span><br></pre></td></tr></table></figure></li><li><p>执行创建</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl apply -f ./</span></span><br><span class="line"></span><br><span class="line">$ kubectl get deployment</span><br><span class="line">NAME           READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">nginx-pod      2/2     2            2           17h</span><br><span class="line">tomcat-test1   1/1     1            1           42m</span><br><span class="line">tomcat-test2   1/1     1            1           42m</span><br><span class="line"></span><br><span class="line">$ kubectl get svc</span><br><span class="line">NAME             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE</span><br><span class="line">kubernetes       ClusterIP   10.254.0.1       &lt;none&gt;        443/TCP    5d23h</span><br><span class="line">nginx-service    ClusterIP   10.254.149.69    &lt;none&gt;        80/TCP     17h</span><br><span class="line">tomcat-test1     ClusterIP   10.254.195.108   &lt;none&gt;        8080/TCP   42m</span><br><span class="line">tomcat-test2     ClusterIP   10.254.6.88      &lt;none&gt;        8080/TCP   42m</span><br><span class="line">traefik-web-ui   ClusterIP   10.254.22.102    &lt;none&gt;        80/TCP     17h</span><br></pre></td></tr></table></figure></li></ul><p>创建test1的ingress，来发布tomcat-test1服务</p><ul><li>ingress-tomcat1.yam<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat ingress-tomcat1.yaml </span><br><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: tomcat-test1-web</span><br><span class="line">  namespace: default</span><br><span class="line">  annotations:</span><br><span class="line">    kubernetes.io/ingress.class: traefik</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: tomcat.xxlaila.io</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: /</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: tomcat-test1</span><br><span class="line">          servicePort: 8080</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl create -f ingress-tomcat.yaml</span></span><br></pre></td></tr></table></figure></li></ul><p>在traefix-ui界面上，可以看到已经有了一个<code>tomcat.xxlaila.io</code>的域名规则.<br><img src="https://img.xxlaila.cn/1567739051461.jpg" alt="img"></p><p>在hosts文件添加tomcat.xxlaila.io绑定来进行访问<br><img src="https://img.xxlaila.cn/1567739162707.jpg" alt="img"></p><h5 id="ingress配置同域名对应location"><a href="#ingress配置同域名对应location" class="headerlink" title="ingress配置同域名对应location"></a>ingress配置同域名对应location</h5><ul><li><p>ingress-tomcat.yaml</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: tomcat-test-web</span><br><span class="line">  namespace: default</span><br><span class="line">  annotations:</span><br><span class="line">    kubernetes.io/ingress.class: traefik</span><br><span class="line">    traefik.frontend.rule.type: PathPrefixStrip</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: tomcat.xxlaila.io</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: /test1/</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: tomcat-test1</span><br><span class="line">          servicePort: 8080</span><br><span class="line">      - path: /test2/</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: tomcat-test2</span><br><span class="line">          servicePort: 8080</span><br></pre></td></tr></table></figure></li><li><p>创建并查看</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl create -f ingress-tomcat.yaml </span><br><span class="line"></span><br><span class="line">$ kubectl describe ingress tomcat-test-web</span><br><span class="line">Name:             tomcat-test-web</span><br><span class="line">Namespace:        default</span><br><span class="line">Address:          </span><br><span class="line">Default backend:  default-http-backend:80 (&lt;none&gt;)</span><br><span class="line">Rules:</span><br><span class="line">  Host               Path  Backends</span><br><span class="line">  ----               ----  --------</span><br><span class="line">  tomcat.xxlaila.io  </span><br><span class="line">                     /test1/   tomcat-test1:8080 (&lt;none&gt;)</span><br><span class="line">                     /test2/   tomcat-test2:8080 (&lt;none&gt;)</span><br><span class="line">Annotations:</span><br><span class="line">  traefik.frontend.rule.type:                        PathPrefixStrip</span><br><span class="line">  kubectl.kubernetes.io/last-applied-configuration:  &#123;<span class="string">"apiVersion"</span>:<span class="string">"extensions/v1beta1"</span>,<span class="string">"kind"</span>:<span class="string">"Ingress"</span>,<span class="string">"metadata"</span>:&#123;<span class="string">"annotations"</span>:&#123;<span class="string">"kubernetes.io/ingress.class"</span>:<span class="string">"traefik"</span>,<span class="string">"traefik.frontend.rule.type"</span>:<span class="string">"PathPrefixStrip"</span>&#125;,<span class="string">"name"</span>:<span class="string">"tomcat-test-web"</span>,<span class="string">"namespace"</span>:<span class="string">"default"</span>&#125;,<span class="string">"spec"</span>:&#123;<span class="string">"rules"</span>:[&#123;<span class="string">"host"</span>:<span class="string">"tomcat.xxlaila.io"</span>,<span class="string">"http"</span>:&#123;<span class="string">"paths"</span>:[&#123;<span class="string">"backend"</span>:&#123;<span class="string">"serviceName"</span>:<span class="string">"tomcat-test1"</span>,<span class="string">"servicePort"</span>:8080&#125;,<span class="string">"path"</span>:<span class="string">"/test1/"</span>&#125;,&#123;<span class="string">"backend"</span>:&#123;<span class="string">"serviceName"</span>:<span class="string">"tomcat-test2"</span>,<span class="string">"servicePort"</span>:8080&#125;,<span class="string">"path"</span>:<span class="string">"/test2/"</span>&#125;]&#125;&#125;]&#125;&#125;</span><br><span class="line"></span><br><span class="line">  kubernetes.io/ingress.class:  traefik</span><br><span class="line">Events:                         &lt;none&gt;</span><br></pre></td></tr></table></figure></li></ul><h3 id="给节点设置label"><a href="#给节点设置label" class="headerlink" title="给节点设置label"></a>给节点设置label</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于是 Kubernetes DeamonSet 这种方式部署 Traefik，所以需要提前给节点设置 Label，这样当程序部署时 Pod 会自动调度到设置 Label 的点上。</p><h4 id="节点设置-Label-标签"><a href="#节点设置-Label-标签" class="headerlink" title="节点设置 Label 标签"></a>节点设置 Label 标签</h4><ul><li>格式：kubectl label nodes [节点名] [key=value]<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"> kubectl get nodes</span><br><span class="line">NAME            STATUS   ROLES    AGE    VERSION</span><br><span class="line">172.21.16.204   Ready    &lt;none&gt;   7d5h   v1.13.3</span><br><span class="line">172.21.16.240   Ready    &lt;none&gt;   7d2h   v1.13.3</span><br><span class="line">172.21.16.87    Ready    &lt;none&gt;   7d2h   v1.13.3</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl label nodes 172.21.16.204 IngressProxy=true</span></span><br><span class="line">node/172.21.16.204 labeled</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看节点label设置是否成功</span></span><br><span class="line"><span class="comment"># kubectl get nodes --show-labels</span></span><br><span class="line">NAME            STATUS   ROLES    AGE    VERSION   LABELS</span><br><span class="line">172.21.16.204   Ready    &lt;none&gt;   7d5h   v1.13.3   IngressProxy=<span class="literal">true</span>,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=172.21.16.204,node.kubernetes.io/k8s-node=<span class="literal">true</span></span><br><span class="line">172.21.16.240   Ready    &lt;none&gt;   7d2h   v1.13.3   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=172.21.16.240,node.kubernetes.io/k8s-node=<span class="literal">true</span></span><br><span class="line">172.21.16.87    Ready    &lt;none&gt;   7d2h   v1.13.3   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=172.21.16.87,node.kubernetes.io/k8s-node=<span class="literal">true</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="修改Traefix部署文件"><a href="#修改Traefix部署文件" class="headerlink" title="修改Traefix部署文件"></a>修改Traefix部署文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat deployment.yaml</span></span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: traefik-ingress-lb</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: traefik-ingress-lb</span><br><span class="line">spec:</span><br><span class="line">  replicas: 2 <span class="comment"># 增加行</span></span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        k8s-app: traefik-ingress-lb</span><br><span class="line">        name: traefik-ingress-lb</span><br><span class="line">    spec:</span><br><span class="line">      terminationGracePeriodSeconds: 60</span><br><span class="line">      volumes:</span><br><span class="line">      - name: ssl</span><br><span class="line">        secret:</span><br><span class="line">          secretName: traefik-cert</span><br><span class="line">      - name: config</span><br><span class="line">        configMap:</span><br><span class="line">          name: traefik-conf</span><br><span class="line">      hostNetwork: <span class="literal">true</span></span><br><span class="line">      restartPolicy: Always</span><br><span class="line">      serviceAccountName: ingress</span><br><span class="line">      containers:</span><br><span class="line">      - image: traefik</span><br><span class="line">        name: traefik-ingress-lb</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - mountPath: <span class="string">"/etc/kubernetes/certs"</span></span><br><span class="line">          name: <span class="string">"ssl"</span></span><br><span class="line">        - mountPath: <span class="string">"/etc/kubernetes/conf"</span></span><br><span class="line">          name: <span class="string">"config"</span></span><br><span class="line">        resources:</span><br><span class="line">          limits:</span><br><span class="line">            cpu: 1000m</span><br><span class="line">            memory: 3000Mi</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 500m</span><br><span class="line">            memory: 2000Mi</span><br><span class="line">        ports:</span><br><span class="line">        - name: http</span><br><span class="line">          containerPort: 80</span><br><span class="line">          hostPort: 80</span><br><span class="line">        - name: admin</span><br><span class="line">          containerPort: 8580</span><br><span class="line">          hostPort: 8580</span><br><span class="line">        args:</span><br><span class="line">        - --configFile=/etc/kubernetes/conf/traefik.toml</span><br><span class="line">        - --web</span><br><span class="line">        - --web.address=:8580</span><br><span class="line">        - --kubernetes</span><br><span class="line">      nodeSelector:</span><br><span class="line">        IngressProxy: <span class="string">"true"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行部署即可</span></span><br></pre></td></tr></table></figure><ul><li>traefix-ui界面上可以看到<br><img src="https://img.xxlaila.cn/1567739737385.jpg" alt="img"></li></ul><p>从describe信息和ui界面上可以看到，tomcat.test.k8s分别有了/test1/和/test2/的域名代理以及相对应的后端<br><img src="https://img.xxlaila.cn/1567739822127.jpg" alt="img"><br><img src="https://img.xxlaila.cn/1567739856570.jpg" alt="img"></p><p><a href="https://xuchao918.github.io/2019/03/01/Kubernetes-traefik-ingress%E4%BD%BF%E7%94%A8/" target="_blank" rel="noopener">参考文献2</a><br><a href="https://blog.51cto.com/icenycmh/2124502" target="_blank" rel="noopener">参考文献1</a></p><!-- rebuild by neat -->]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>traefik</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s-helm</title>
    <url>/2019/09/04/k8s-helm/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Helm类似与linux下面的yum，Helm是一个用于kubernetes的包管理器，每一个包为一个chart，一个chart是一个目录，常常会对目录进行打包压缩，形成一个${name}-version.tgz的格式进行传输和存储。</p><ul><li>对于应用发布者而言，可以通过Helm打包应用，管理应用依赖关系，管理应用版本并发布应用到软件仓库。</li><li>对于使用者而言，使用Helm后不用需要了解Kubernetes的Yaml语法并编写应用部署文件，可以通过Helm下载并在kubernetes上安装需要的应用。</li></ul><p>Helm还提供了kubernetes上的软件部署，删除，升级，回滚应用的强大功能</p><a id="more"></a><h3 id="1、helm组件"><a href="#1、helm组件" class="headerlink" title="1、helm组件"></a>1、helm组件</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Helm 是一个命令行下的客户端工具。主要用于 Kubernetes 应用程序 Chart 的创建、打包、发布以及创建和管理本地和远程的 Chart 仓库。</p><h4 id="1-1、Tiller"><a href="#1-1、Tiller" class="headerlink" title="1.1、Tiller"></a>1.1、Tiller</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Tiller 是 Helm 的服务端，部署在 Kubernetes 集群中。Tiller 用于接收 Helm 的请求，并根据 Chart 生成 Kubernetes 的部署文件（ Helm 称为 Release ），然后提交给 Kubernetes 创建应用。Tiller 还提供了 Release 的升级、删除、回滚等一系列功能。</p><h4 id="1-2、Chart"><a href="#1-2、Chart" class="headerlink" title="1.2、Chart"></a>1.2、Chart</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Helm 的软件包，采用 TAR 格式。类似于 APT 的 DEB 包或者 YUM 的 RPM 包，其包含了一组定义 Kubernetes 资源相关的 YAML 文件</p><h4 id="1-3、Repoistory"><a href="#1-3、Repoistory" class="headerlink" title="1.3、Repoistory"></a>1.3、Repoistory</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Helm 的软件仓库，Repository 本质上是一个 Web 服务器，该服务器保存了一系列的 Chart 软件包以供用户下载，并且提供了一个该 Repository 的 Chart 包的清单文件以供查询。Helm 可以同时管理多个不同的 Repository</p><h4 id="1-4、Release"><a href="#1-4、Release" class="headerlink" title="1.4、Release"></a>1.4、Release</h4><p>使用 helm install 命令在 Kubernetes 集群中部署的 Chart 称为 Release</p><h3 id="2、helm安装"><a href="#2、helm安装" class="headerlink" title="2、helm安装"></a>2、helm安装</h3><ul><li>下载helm<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># wget https://storage.googleapis.com/kubernetes-helm/helm-v2.14.3-linux-amd64.tar.gz</span></span><br><span class="line"><span class="comment"># tar zxf helm-v2.14.3-linux-amd64.tar.gz &amp;&amp; mv linux-amd64/&#123;helm,tiller&#125; /usr/bin</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="2-1、创建分蘖服务帐户"><a href="#2-1、创建分蘖服务帐户" class="headerlink" title="2.1、创建分蘖服务帐户"></a>2.1、创建分蘖服务帐户</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl create serviceaccount tiller --namespace kube-system</span></span><br><span class="line">serviceaccount/tiller created</span><br></pre></td></tr></table></figure><h4 id="2-2、授予分蘖集群管理员角色"><a href="#2-2、授予分蘖集群管理员角色" class="headerlink" title="2.2、授予分蘖集群管理员角色"></a>2.2、授予分蘖集群管理员角色</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl create clusterrolebinding tiller-admin-binding --clusterrole=cluster-admin --serviceaccount=kube-system:tiller</span></span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/tiller-admin-binding created</span><br></pre></td></tr></table></figure><h4 id="2-2、安装tiller"><a href="#2-2、安装tiller" class="headerlink" title="2.2、安装tiller"></a>2.2、安装tiller</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># helm init --service-account tiller --upgrade -i docker.io/sapcc/tiller:v2.14.3 --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts</span></span><br></pre></td></tr></table></figure><h5 id="2-2-1、检查是否安装成功"><a href="#2-2-1、检查是否安装成功" class="headerlink" title="2.2.1、检查是否安装成功"></a>2.2.1、检查是否安装成功</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl -n kube-system get pods|grep tiller</span></span><br><span class="line">tiller-deploy-75b8f8575d-fplck          1/1     Running   0          17h</span><br><span class="line"></span><br><span class="line"><span class="comment"># helm version</span></span><br><span class="line">Client: &amp;version.Version&#123;SemVer:<span class="string">"v2.14.3"</span>, GitCommit:<span class="string">"0e7f3b6637f7af8fcfddb3d2941fcc7cbebb0085"</span>, GitTreeState:<span class="string">"clean"</span>&#125;</span><br><span class="line">Server: &amp;version.Version&#123;SemVer:<span class="string">"v2.14.3"</span>, GitCommit:<span class="string">"0e7f3b6637f7af8fcfddb3d2941fcc7cbebb0085"</span>, GitTreeState:<span class="string">"clean"</span>&#125;</span><br></pre></td></tr></table></figure><ul><li><strong>错误</strong>: 这里安装完成后执行<code>helm version</code>提示错误，内容如下:<br><code>E0904 18:51:07.730671 22845 portforward.go:391] an error occurred forwarding 38767 -&gt; 44134: error forwarding port 44134 to pod b52064300cfa79e6d83795535584f89c97c33dc91ea39c024492b7b40e3fb68e, uid : unable to do port forwarding: socat not found.</code>这个错误需要在客户端安装一个<a href="https://github.com/helm/helm/issues/1371" target="_blank" rel="noopener">socat插件</a></li></ul><ul><li><p>在node安装socat</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># yum install -y socat</span></span><br></pre></td></tr></table></figure></li><li><p>修改helm第三方存储库(可选)</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># helm repo add stable https://burdenbear.github.io/kube-charts-mirror/</span></span><br><span class="line"><span class="comment"># helm repo list</span></span><br><span class="line">NAME     URL                                             </span><br><span class="line"><span class="built_in">local</span>    http://127.0.0.1:8879/charts                    </span><br><span class="line">monocular https://helm.github.io/monocular                </span><br><span class="line">stable   https://burdenbear.github.io/kube-charts-mirror/</span><br></pre></td></tr></table></figure></li></ul><h3 id="3、测试和启动本地helm-web"><a href="#3、测试和启动本地helm-web" class="headerlink" title="3、测试和启动本地helm web"></a>3、测试和启动本地helm web</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># helm list</span></span><br><span class="line"><span class="comment"># helm search</span></span><br><span class="line"><span class="comment"># helm search mysql --versions</span></span><br><span class="line"><span class="comment"># helm repo list</span></span><br><span class="line"><span class="comment"># helm serve --address 0.0.0.0:8879 &amp;</span></span><br></pre></td></tr></table></figure><p><img src="https://img.xxlaila.cn/1567651293339.jpg" alt="img"></p><h3 id="4、helm-web-ui"><a href="#4、helm-web-ui" class="headerlink" title="4、helm web ui"></a>4、helm web ui</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;helm安装以后，经常使用helm cli命来进行部署还是比较吃力的，而且对于有些人不喜欢cli的来说，是一个非常痛苦的事情，这里介绍一款kubeapps，Kubeapps是一个基于Web的UI，用于在Kubernetes集群中部署和管理应用程序。 Kubeapps允许您：</p><ul><li>从图表存储库中浏览和部署Helm图表</li><li>检查，升级和删除群集中安装的基于Helm的应用程序</li><li>添加自定义和私有图表存储库（支持ChartMuseum和JFrog Artifactory)</li><li>从服务目录和可用的Service Brokers浏览和配置外部服务</li><li>使用服务目录绑定将基于Helm的应用程序连接到外部服务</li><li>基于Kubernetes基于角色的访问控制的安全身份验证和授权</li></ul><h4 id="4-1、安装kubeapps"><a href="#4-1、安装kubeapps" class="headerlink" title="4.1、安装kubeapps"></a>4.1、安装kubeapps</h4><p>使用Helm图表安装<a href="https://github.com/kubeapps/kubeapps" target="_blank" rel="noopener">最新版本的Kubeapps</a></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># helm repo add bitnami https://charts.bitnami.com/bitnami</span></span><br><span class="line"><span class="comment"># helm install --name kubeapps --namespace kubeapps bitnami/kubeapps</span></span><br></pre></td></tr></table></figure><h4 id="4-2、启动kubeappsDashboard"><a href="#4-2、启动kubeappsDashboard" class="headerlink" title="4.2、启动kubeappsDashboard"></a>4.2、启动kubeappsDashboard</h4><p>安装Kubeapps后，运行以下命令从系统安全访问Kubeapps Dashboard</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># export POD_NAME=$(kubectl get pods --namespace kubeapps -l "app=kubeapps" -o jsonpath="&#123;.items[0].metadata.name&#125;")</span></span><br><span class="line"><span class="comment"># kubectl port-forward -n kubeapps $POD_NAME --address 0.0.0.0 8081:8080 &amp;</span></span><br><span class="line"><span class="comment"># 把容器的8080 映射到本地的8081端口，用于浏览器访问</span></span><br></pre></td></tr></table></figure><p><img src="https://img.xxlaila.cn/1567650844980.jpg" alt="img"></p><h4 id="4-3、创建token"><a href="#4-3、创建token" class="headerlink" title="4.3、创建token"></a>4.3、创建token</h4><p>访问仪表板需要Kubernetes API令牌才能通过Kubernetes API服务器进行身份验证</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl create serviceaccount kubeapps-operator</span></span><br><span class="line">serviceaccount/kubeapps-operator created</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl create clusterrolebinding kubeapps-operator --clusterrole=cluster-admin --serviceaccount=default:kubeapps-operator</span></span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/kubeapps-operator created</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取token</span></span><br><span class="line"><span class="comment"># kubectl get secret $(kubectl get serviceaccount kubeapps-operator -o jsonpath='&#123;.secrets[].name&#125;') -o jsonpath='&#123;.data.token&#125;' | base64 --decode</span></span><br></pre></td></tr></table></figure><p><img src="https://img.xxlaila.cn/1567650971425.jpg" alt="img"></p><h5 id="4-3-1、创建token访问脚本"><a href="#4-3-1、创建token访问脚本" class="headerlink" title="4.3.1、创建token访问脚本"></a>4.3.1、创建token访问脚本</h5><p>每次访问kubeapps的token 都要输入一长串，这里我们写一个shell脚本，放在<code>/usr/bin</code>目录，需要的时候执行命令即可，这样方便用于记</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># vim /usr/bin/kubeapps</span></span><br><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">kubectl get secret $(kubectl get serviceaccount kubeapps-operator -o jsonpath=<span class="string">'&#123;.secrets[].name&#125;'</span>) -o jsonpath=<span class="string">'&#123;.data.token&#125;'</span> | base64 --decode</span><br><span class="line"><span class="comment"># chmod +x /usr/bin/kubeapps</span></span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>helm</tag>
      </tags>
  </entry>
  <entry>
    <title>metrics-server安装季</title>
    <url>/2019/09/04/metrics-server%E5%AE%89%E8%A3%85%E5%AD%A3/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:23 GMT+0800 (China Standard Time) --><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;metrics-server 通过 kube-apiserver 发现所有节点，然后调用 kubelet APIs（通过 https 接口）获得各节点（Node）和 Pod 的 CPU、Memory 等资源使用情况。Kubernetes 1.12 开始，kubernetes 的安装脚本移除了 Heapster，从 1.13 开始完全移除了对 Heapster 的支持，Heapster 不再被维护。</p><ul><li>替代方案如下:<ul><li>用于支持自动扩缩容的 CPU/memory HPA metrics：metrics-server</li><li>通用的监控方案：使用第三方可以获取 Prometheus 格式监控指标的监控系统，如 Prometheus Operator</li><li>事件传输：使用第三方工具来传输、归档 kubernetes events</li></ul></li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;使用 metrics-server 替代 Heapster，将无法在 dashboard 中以图形展示 Pod 的内存和 CPU 情况，需要通过 Prometheus、Grafana 等监控方案来弥补。</p><a id="more"></a><h4 id="1、监控架构"><a href="#1、监控架构" class="headerlink" title="1、监控架构"></a>1、监控架构</h4><p><img src="https://img.xxlaila.cn/2748678bdjsg848sd.png" alt="img"></p><h4 id="2、安装-metrics-server"><a href="#2、安装-metrics-server" class="headerlink" title="2、安装 metrics-server"></a>2、安装 metrics-server</h4><ul><li>从 github clone 源码<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># git clone https://github.com/xxlaila/kubernetes-yaml.git</span></span><br><span class="line"><span class="comment"># cd kubernetes-yaml/metrics-server</span></span><br><span class="line"><span class="comment"># ls</span></span><br><span class="line">aggregated-metrics-reader.yaml  auth-delegator.yaml  auth-reader.yaml  metrics-apiservice.yaml  metrics-server-deployment.yaml  metrics-server-service.yaml  resource-reader.yaml</span><br></pre></td></tr></table></figure></li></ul><ul><li><strong>注意</strong>: 之前在安装的时候遇到很多坑，而且网上看了教程基本上不能用，很坑，自己看网上教程，然后根据每一个错误来进行解决，终于，功夫不负有心人，花了一天半终于搞定啦。</li></ul><h4 id="3、metrics-server-文件修改"><a href="#3、metrics-server-文件修改" class="headerlink" title="3、metrics-server 文件修改"></a>3、metrics-server 文件修改</h4><p>metrics-server yaml文件这里文件已经修改好了，可以直接拿来用，<a href="https://github.com/kubernetes-incubator/metrics-server/issues/247" target="_blank" rel="noopener">参考文献</a>，</p><h5 id="3-1、metrics-server-deployment-yaml"><a href="#3-1、metrics-server-deployment-yaml" class="headerlink" title="3.1、metrics-server-deployment.yaml"></a>3.1、metrics-server-deployment.yaml</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat metrics-server-deployment.yaml</span></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: metrics-server</span><br><span class="line">  namespace: kube-system</span><br><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: metrics-server</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: metrics-server</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      k8s-app: metrics-server</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: metrics-server</span><br><span class="line">      labels:</span><br><span class="line">        k8s-app: metrics-server</span><br><span class="line">    spec:</span><br><span class="line">      serviceAccountName: metrics-server</span><br><span class="line">      hostNetwork: <span class="literal">true</span>   <span class="comment">#增加行</span></span><br><span class="line">      volumes:</span><br><span class="line">      <span class="comment"># mount in tmp so we can safely use from-scratch images and/or read-only containers</span></span><br><span class="line">      - name: tmp-dir</span><br><span class="line">        emptyDir: &#123;&#125;</span><br><span class="line">      containers:</span><br><span class="line">      - name: metrics-server</span><br><span class="line">        image: mirrorgooglecontainers/metrics-server-amd64:v0.3.3</span><br><span class="line">        imagePullPolicy: Always</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - mountPath: /etc/ssl/kubernetes/</span><br><span class="line">          name: ca-ssl</span><br><span class="line">        <span class="built_in">command</span>:   <span class="comment"># command内容均为增加</span></span><br><span class="line">        - /metrics-server</span><br><span class="line">        - --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP</span><br><span class="line">        - --requestheader-client-ca-file=/etc/ssl/kubernetes/front-proxy-ca.pem</span><br><span class="line">        - --kubelet-insecure-tls=<span class="literal">true</span></span><br><span class="line">      volumes:</span><br><span class="line">       - name: ca-ssl</span><br><span class="line">         hostPath:</span><br><span class="line">          path: /etc/kubernetes/ssl</span><br></pre></td></tr></table></figure><h5 id="3-2、"><a href="#3-2、" class="headerlink" title="3.2、"></a>3.2、</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat resource-reader.yaml</span></span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRole</span><br><span class="line">metadata:</span><br><span class="line">  name: system:metrics-server</span><br><span class="line">rules:</span><br><span class="line">- apiGroups:</span><br><span class="line">  - <span class="string">""</span></span><br><span class="line">  resources:</span><br><span class="line">  - pods</span><br><span class="line">  - nodes</span><br><span class="line">  - nodes/stats</span><br><span class="line">  - namespaces</span><br><span class="line">  verbs:</span><br><span class="line">  - get</span><br><span class="line">  - list</span><br><span class="line">  - watch</span><br><span class="line">- apiGroups: <span class="comment"># 增加</span></span><br><span class="line">  - <span class="string">"extensions"</span></span><br><span class="line">  resources:</span><br><span class="line">  - deployments</span><br><span class="line">  verbs:</span><br><span class="line">  - get</span><br><span class="line">  - list</span><br><span class="line">  - update</span><br><span class="line">  - watch</span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: system:metrics-server</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: system:metrics-server</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: metrics-server</span><br><span class="line">  namespace: kube-system</span><br></pre></td></tr></table></figure><h4 id="4、准备证书"><a href="#4、准备证书" class="headerlink" title="4、准备证书"></a>4、准备证书</h4><p>这些证书文件主要用在Metrics API aggregator 上,<a href="https://blog.51cto.com/ylw6006/2114338" target="_blank" rel="noopener">参考文献</a></p><ul><li><p>front-proxy-ca-csr.json</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat front-proxy-ca-csr.json </span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"CN"</span>: <span class="string">"kubernetes"</span>,</span><br><span class="line">  <span class="string">"key"</span>: &#123;</span><br><span class="line">    <span class="string">"algo"</span>: <span class="string">"rsa"</span>,</span><br><span class="line">    <span class="string">"size"</span>: 2048</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>front-proxy-client-csr.json</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="string">"CN"</span>: <span class="string">"front-proxy-client"</span>,</span><br><span class="line">  <span class="string">"key"</span>: &#123;</span><br><span class="line">    <span class="string">"algo"</span>: <span class="string">"rsa"</span>,</span><br><span class="line">    <span class="string">"size"</span>: 2048</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h5 id="4-1、生成证书"><a href="#4-1、生成证书" class="headerlink" title="4.1、生成证书"></a>4.1、生成证书</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cfssl gencert   -initca front-proxy-ca-csr.json | cfssljson -bare front-proxy-ca</span></span><br><span class="line"><span class="comment"># cfssl gencert \</span></span><br><span class="line"> -ca=front-proxy-ca.pem \</span><br><span class="line"> -ca-key=front-proxy-ca-key.pem \</span><br><span class="line"> -config=/root/ssl/kubernetes-gencert.json \</span><br><span class="line"> -profile=kubernetes \</span><br><span class="line"> front-proxy-client-csr.json | cfssljson -bare front-proxy-client</span><br><span class="line"><span class="comment"># ls *.pem</span></span><br><span class="line">front-proxy-ca-key.pem  front-proxy-ca.pem  front-proxy-client-key.pem  front-proxy-client.pem</span><br></pre></td></tr></table></figure><ul><li>证书生成完成后，吧证书复制到所有的master节点和node节点</li></ul><h4 id="5、master修改配置文件"><a href="#5、master修改配置文件" class="headerlink" title="5、master修改配置文件"></a>5、master修改配置文件</h4><h4 id="5-1、apiserver"><a href="#5-1、apiserver" class="headerlink" title="5.1、apiserver"></a>5.1、apiserver</h4><p>在apiserver配置文件里面增加如下配置</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">--runtime-config=api/all=<span class="literal">true</span> \</span><br><span class="line">--<span class="built_in">enable</span>-aggregator-routing=<span class="literal">true</span> \</span><br><span class="line">--requestheader-client-ca-file=/etc/kubernetes/ssl/front-proxy-ca.pem \</span><br><span class="line">--requestheader-allowed-names=aggregator \</span><br><span class="line">--requestheader-extra-headers-prefix=X-Remote-Extra- \</span><br><span class="line">--requestheader-group-headers=X-Remote-Group \</span><br><span class="line">--requestheader-username-headers=X-Remote-User \</span><br><span class="line">--proxy-client-cert-file=/etc/kubernetes/ssl/front-proxy-client.pem \</span><br><span class="line">--proxy-client-key-file=/etc/kubernetes/ssl/front-proxy-client-key.pem \</span><br></pre></td></tr></table></figure><ul><li>apiserver配置文件KUBE_API_ARGS内容如下<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">KUBE_API_ARGS=<span class="string">" --allow-privileged=true \</span></span><br><span class="line"><span class="string">                --anonymous-auth=false \</span></span><br><span class="line"><span class="string">                --alsologtostderr \</span></span><br><span class="line"><span class="string">                --apiserver-count=3 \</span></span><br><span class="line"><span class="string">                --audit-log-maxage=30 \</span></span><br><span class="line"><span class="string">                --audit-log-maxbackup=3 \</span></span><br><span class="line"><span class="string">                --audit-log-maxsize=100 \</span></span><br><span class="line"><span class="string">                --enable-aggregator-routing=true \</span></span><br><span class="line"><span class="string">                --audit-log-path=/var/log/kube-audit/audit.log \</span></span><br><span class="line"><span class="string">                --audit-policy-file=/etc/kubernetes/audit-policy.yaml \</span></span><br><span class="line"><span class="string">                --authorization-mode=Node,RBAC \</span></span><br><span class="line"><span class="string">                --client-ca-file=/etc/kubernetes/ssl/kubernetes-ca.pem \</span></span><br><span class="line"><span class="string">                --enable-bootstrap-token-auth \</span></span><br><span class="line"><span class="string">                --enable-garbage-collector \</span></span><br><span class="line"><span class="string">                --enable-logs-handler \</span></span><br><span class="line"><span class="string">                --endpoint-reconciler-type=lease \</span></span><br><span class="line"><span class="string">                --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem \</span></span><br><span class="line"><span class="string">                --etcd-certfile=/etc/etcd/ssl/etcd.pem \</span></span><br><span class="line"><span class="string">                --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem \</span></span><br><span class="line"><span class="string">                --etcd-compaction-interval=0s \</span></span><br><span class="line"><span class="string">                --event-ttl=168h0m0s \</span></span><br><span class="line"><span class="string">                --kubelet-https=true \</span></span><br><span class="line"><span class="string">                --kubelet-certificate-authority=/etc/kubernetes/ssl/kubernetes-ca.pem \</span></span><br><span class="line"><span class="string">                --kubelet-client-certificate=/etc/kubernetes/ssl/kubelet-api-admin.pem \</span></span><br><span class="line"><span class="string">                --kubelet-client-key=/etc/kubernetes/ssl/kubelet-api-admin-key.pem \</span></span><br><span class="line"><span class="string">                --kubelet-timeout=3s \</span></span><br><span class="line"><span class="string">                --runtime-config=api/all=true \</span></span><br><span class="line"><span class="string">                --service-node-port-range=30000-50000 \</span></span><br><span class="line"><span class="string">                --service-account-key-file=/etc/kubernetes/ssl/kubernetes-ca.pem \</span></span><br><span class="line"><span class="string">                --tls-cert-file=/etc/kubernetes/ssl/kube-apiserver.pem \</span></span><br><span class="line"><span class="string">                --tls-private-key-file=/etc/kubernetes/ssl/kube-apiserver-key.pem \</span></span><br><span class="line"><span class="string">                --requestheader-client-ca-file=/etc/kubernetes/ssl/front-proxy-ca.pem \</span></span><br><span class="line"><span class="string">                --requestheader-allowed-names=aggregator \</span></span><br><span class="line"><span class="string">                --requestheader-extra-headers-prefix=X-Remote-Extra- \</span></span><br><span class="line"><span class="string">                --requestheader-group-headers=X-Remote-Group \</span></span><br><span class="line"><span class="string">                --requestheader-username-headers=X-Remote-User \</span></span><br><span class="line"><span class="string">                --proxy-client-cert-file=/etc/kubernetes/ssl/front-proxy-client.pem \</span></span><br><span class="line"><span class="string">                --proxy-client-key-file=/etc/kubernetes/ssl/front-proxy-client-key.pem \</span></span><br><span class="line"><span class="string">                --v=2"</span></span><br></pre></td></tr></table></figure></li></ul><h5 id="5-2、kube-control-manager"><a href="#5-2、kube-control-manager" class="headerlink" title="5.2、kube-control-manager"></a>5.2、kube-control-manager</h5><p>在controller-manager文件增加如下配置</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">--horizontal-pod-autoscaler-use-rest-clients=<span class="literal">true</span> \</span><br></pre></td></tr></table></figure><ul><li>kube-control-manager配置文件KUBE_CONTROLLER_MANAGER_ARGS如下<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">KUBE_CONTROLLER_MANAGER_ARGS=<span class="string">"  --address=0.0.0.0 \</span></span><br><span class="line"><span class="string">                                --authentication-kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \</span></span><br><span class="line"><span class="string">                                --authorization-kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \</span></span><br><span class="line"><span class="string">                                --bind-address=0.0.0.0 \</span></span><br><span class="line"><span class="string">                                --cluster-name=kubernetes \</span></span><br><span class="line"><span class="string">                                --cluster-signing-cert-file=/etc/kubernetes/ssl/kubernetes-ca.pem \</span></span><br><span class="line"><span class="string">                                --cluster-signing-key-file=/etc/kubernetes/ssl/kubernetes-ca-key.pem \</span></span><br><span class="line"><span class="string">                                --client-ca-file=/etc/kubernetes/ssl/kubernetes-ca.pem \</span></span><br><span class="line"><span class="string">                                --controllers=*,bootstrapsigner,tokencleaner \</span></span><br><span class="line"><span class="string">                                --deployment-controller-sync-period=10s \</span></span><br><span class="line"><span class="string">                                --experimental-cluster-signing-duration=87600h0m0s \</span></span><br><span class="line"><span class="string">                                --enable-garbage-collector=true \</span></span><br><span class="line"><span class="string">                                --kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \</span></span><br><span class="line"><span class="string">                                --leader-elect=true \</span></span><br><span class="line"><span class="string">                                --node-monitor-grace-period=20s \</span></span><br><span class="line"><span class="string">                                --node-monitor-period=5s \</span></span><br><span class="line"><span class="string">                                --port=10252 \</span></span><br><span class="line"><span class="string">                                --pod-eviction-timeout=2m0s \</span></span><br><span class="line"><span class="string">                                --requestheader-client-ca-file=/etc/kubernetes/ssl/kubernetes-ca.pem \</span></span><br><span class="line"><span class="string">                                --terminated-pod-gc-threshold=50 \</span></span><br><span class="line"><span class="string">                                --tls-cert-file=/etc/kubernetes/ssl/kube-controller-manager.pem \</span></span><br><span class="line"><span class="string">                                --tls-private-key-file=/etc/kubernetes/ssl/kube-controller-manager-key.pem \</span></span><br><span class="line"><span class="string">                                --root-ca-file=/etc/kubernetes/ssl/kubernetes-ca.pem \</span></span><br><span class="line"><span class="string">                                --secure-port=10257 \</span></span><br><span class="line"><span class="string">                                --service-cluster-ip-range=10.254.0.0/16 \</span></span><br><span class="line"><span class="string">                                --service-account-private-key-file=/etc/kubernetes/ssl/kubernetes-ca-key.pem \</span></span><br><span class="line"><span class="string">                                --use-service-account-credentials=true \</span></span><br><span class="line"><span class="string">                                --horizontal-pod-autoscaler-use-rest-clients=true \</span></span><br><span class="line"><span class="string">                                --v=2"</span></span><br></pre></td></tr></table></figure></li></ul><h5 id="5-3、重启服务"><a href="#5-3、重启服务" class="headerlink" title="5.3、重启服务"></a>5.3、重启服务</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># systemctl  restart kube-apiserver.service &amp;&amp;systemctl  restart kube-controller-manager</span></span><br></pre></td></tr></table></figure><h4 id="6、node节点配置文件修改"><a href="#6、node节点配置文件修改" class="headerlink" title="6、node节点配置文件修改"></a>6、node节点配置文件修改</h4><p>node 节点修改修改kubelet文件</p><ul><li><p>kubelet配置文件完成的KUBELET_ARGS参数</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">KUBELET_ARGS=<span class="string">"  --address=0.0.0.0 \</span></span><br><span class="line"><span class="string">                --allow-privileged \</span></span><br><span class="line"><span class="string">                --anonymous-auth=false \</span></span><br><span class="line"><span class="string">                --authorization-mode=Webhook \</span></span><br><span class="line"><span class="string">                --authentication-token-webhook=true \</span></span><br><span class="line"><span class="string">                --bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \</span></span><br><span class="line"><span class="string">                --client-ca-file=/etc/kubernetes/ssl/kubernetes-ca.pem \</span></span><br><span class="line"><span class="string">                --cgroup-driver=cgroupfs \</span></span><br><span class="line"><span class="string">                --cert-dir=/etc/kubernetes/ssl \</span></span><br><span class="line"><span class="string">                --cluster-dns=10.254.0.2 \</span></span><br><span class="line"><span class="string">                --cluster-domain=cluster.local \</span></span><br><span class="line"><span class="string">                --eviction-soft=imagefs.available&lt;15%,memory.available&lt;512Mi,nodefs.available&lt;15%,nodefs.inodesFree&lt;10% \</span></span><br><span class="line"><span class="string">                --eviction-soft-grace-period=imagefs.available=3m,memory.available=1m,nodefs.available=3m,nodefs.inodesFree=1m \</span></span><br><span class="line"><span class="string">                --eviction-hard=imagefs.available&lt;10%,memory.available&lt;256Mi,nodefs.available&lt;10%,nodefs.inodesFree&lt;5% \</span></span><br><span class="line"><span class="string">                --eviction-max-pod-grace-period=30 \</span></span><br><span class="line"><span class="string">                --image-gc-high-threshold=80 \</span></span><br><span class="line"><span class="string">                --image-gc-low-threshold=70 \</span></span><br><span class="line"><span class="string">                --image-pull-progress-deadline=30s \</span></span><br><span class="line"><span class="string">                --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \</span></span><br><span class="line"><span class="string">                --max-pods=100 \</span></span><br><span class="line"><span class="string">                --minimum-image-ttl-duration=720h0m0s \</span></span><br><span class="line"><span class="string">                --node-labels=node.kubernetes.io/k8s-node=true \</span></span><br><span class="line"><span class="string">                --pod-infra-container-image=docker.io/kubernetes/pause:latest \</span></span><br><span class="line"><span class="string">                --port=10250 \</span></span><br><span class="line"><span class="string">                --read-only-port=0 \</span></span><br><span class="line"><span class="string">                --rotate-certificates \</span></span><br><span class="line"><span class="string">                --rotate-server-certificates \</span></span><br><span class="line"><span class="string">                --fail-swap-on=false \</span></span><br><span class="line"><span class="string">                --v=2"</span></span><br></pre></td></tr></table></figure></li><li><p>重启kubelet</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># systemctl restart kubelet</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="7、创建metrics"><a href="#7、创建metrics" class="headerlink" title="7、创建metrics"></a>7、创建metrics</h4><p>通过yaml文件创建对应的资源</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl create -f ./</span></span><br></pre></td></tr></table></figure><h5 id="7-1、查看运行情况"><a href="#7-1、查看运行情况" class="headerlink" title="7.1、查看运行情况"></a>7.1、查看运行情况</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl -n kube-system get pods -l k8s-app=metrics-server</span></span><br><span class="line">NAME                              READY   STATUS    RESTARTS   AGE</span><br><span class="line">metrics-server-84b786c9bb-7trdr   1/1     Running   0          62m</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl get svc -n kube-system  metrics-server</span></span><br><span class="line">NAME             TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">metrics-server   ClusterIP   10.254.45.238   &lt;none&gt;        443/TCP   3h6m</span><br></pre></td></tr></table></figure><h5 id="7-2、获取v1beta1-metrics-k8s-io并验证"><a href="#7-2、获取v1beta1-metrics-k8s-io并验证" class="headerlink" title="7.2、获取v1beta1.metrics.k8s.io并验证"></a>7.2、获取v1beta1.metrics.k8s.io并验证</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;之前<code>v1beta1.metrics.k8s.io kube-system/metrics-server True 3h</code>参数一直是<code>v1beta1.metrics.k8s.io kube-system/metrics-server False (FailedDiscoveryCheck) 16m</code>,</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#  kubectl get apiservice</span></span><br><span class="line">NAME                                    SERVICE                      AVAILABLE   AGE</span><br><span class="line">v1.                                     Local                        True        4d</span><br><span class="line">v1.apps                                 Local                        True        4d</span><br><span class="line">v1.authentication.k8s.io                Local                        True        4d</span><br><span class="line">v1.authorization.k8s.io                 Local                        True        4d</span><br><span class="line">v1.autoscaling                          Local                        True        4d</span><br><span class="line">v1.batch                                Local                        True        4d</span><br><span class="line">v1.networking.k8s.io                    Local                        True        4d</span><br><span class="line">v1.rbac.authorization.k8s.io            Local                        True        4d</span><br><span class="line">v1.storage.k8s.io                       Local                        True        4d</span><br><span class="line">v1alpha1.admissionregistration.k8s.io   Local                        True        4d</span><br><span class="line">v1alpha1.auditregistration.k8s.io       Local                        True        4d</span><br><span class="line">v1alpha1.rbac.authorization.k8s.io      Local                        True        4d</span><br><span class="line">v1alpha1.scheduling.k8s.io              Local                        True        4d</span><br><span class="line">v1alpha1.settings.k8s.io                Local                        True        4d</span><br><span class="line">v1alpha1.storage.k8s.io                 Local                        True        4d</span><br><span class="line">v1beta1.admissionregistration.k8s.io    Local                        True        4d</span><br><span class="line">v1beta1.apiextensions.k8s.io            Local                        True        4d</span><br><span class="line">v1beta1.apps                            Local                        True        4d</span><br><span class="line">v1beta1.authentication.k8s.io           Local                        True        4d</span><br><span class="line">v1beta1.authorization.k8s.io            Local                        True        4d</span><br><span class="line">v1beta1.batch                           Local                        True        4d</span><br><span class="line">v1beta1.certificates.k8s.io             Local                        True        4d</span><br><span class="line">v1beta1.coordination.k8s.io             Local                        True        4d</span><br><span class="line">v1beta1.events.k8s.io                   Local                        True        4d</span><br><span class="line">v1beta1.extensions                      Local                        True        4d</span><br><span class="line">v1beta1.metrics.k8s.io                  kube-system/metrics-server   True        3h</span><br><span class="line">v1beta1.policy                          Local                        True        4d</span><br><span class="line">v1beta1.rbac.authorization.k8s.io       Local                        True        4d</span><br><span class="line">v1beta1.scheduling.k8s.io               Local                        True        4d</span><br><span class="line">v1beta1.storage.k8s.io                  Local                        True        4d</span><br><span class="line">v1beta2.apps                            Local                        True        4d</span><br><span class="line">v2alpha1.batch                          Local                        True        4d</span><br><span class="line">v2beta1.autoscaling                     Local                        True        4d</span><br><span class="line">v2beta2.autoscaling                     Local                        True        4d</span><br></pre></td></tr></table></figure><h4 id="8、查看-metrics-server-输出的-metrics"><a href="#8、查看-metrics-server-输出的-metrics" class="headerlink" title="8、查看 metrics-server 输出的 metrics"></a>8、查看 metrics-server 输出的 metrics</h4><h5 id="8-1、通过-kube-apiserver-或-kubectl-proxy-访问"><a href="#8-1、通过-kube-apiserver-或-kubectl-proxy-访问" class="headerlink" title="8.1、通过 kube-apiserver 或 kubectl proxy 访问"></a>8.1、通过 kube-apiserver 或 kubectl proxy 访问</h5><ul><li><a href="https://172.21.17.31:6443/apis/metrics.k8s.io/v1beta1/nodes" target="_blank" rel="noopener">https://172.21.17.31:6443/apis/metrics.k8s.io/v1beta1/nodes</a></li><li><a href="https://172.21.17.31:6443/apis/metrics.k8s.io/v1beta1/nodes/" target="_blank" rel="noopener">https://172.21.17.31:6443/apis/metrics.k8s.io/v1beta1/nodes/</a></li><li><a href="https://172.21.17.31:6443/apis/metrics.k8s.io/v1beta1/pods" target="_blank" rel="noopener">https://172.21.17.31:6443/apis/metrics.k8s.io/v1beta1/pods</a></li><li><a href="https://172.21.17.31:6443/apis/metrics.k8s.io/v1beta1/namespace//pods/" target="_blank" rel="noopener">https://172.21.17.31:6443/apis/metrics.k8s.io/v1beta1/namespace//pods/</a></li></ul><h5 id="8-2、直接使用-kubectl-命令访问"><a href="#8-2、直接使用-kubectl-命令访问" class="headerlink" title="8.2、直接使用 kubectl 命令访问"></a>8.2、直接使用 kubectl 命令访问</h5><ul><li>kubectl get –raw /apis/metrics.k8s.io/v1beta1/nodes</li><li>kubectl get –raw /apis/metrics.k8s.io/v1beta1/pods</li><li>kubectl get –raw /apis/metrics.k8s.io/v1beta1/nodes/</li><li>kubectl get –raw /apis/metrics.k8s.io/v1beta1/namespace//pods/</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get --raw "/apis/metrics.k8s.io/v1beta1" | jq .</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"kind"</span>: <span class="string">"APIResourceList"</span>,</span><br><span class="line">  <span class="string">"apiVersion"</span>: <span class="string">"v1"</span>,</span><br><span class="line">  <span class="string">"groupVersion"</span>: <span class="string">"metrics.k8s.io/v1beta1"</span>,</span><br><span class="line">  <span class="string">"resources"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"name"</span>: <span class="string">"nodes"</span>,</span><br><span class="line">      <span class="string">"singularName"</span>: <span class="string">""</span>,</span><br><span class="line">      <span class="string">"namespaced"</span>: <span class="literal">false</span>,</span><br><span class="line">      <span class="string">"kind"</span>: <span class="string">"NodeMetrics"</span>,</span><br><span class="line">      <span class="string">"verbs"</span>: [</span><br><span class="line">        <span class="string">"get"</span>,</span><br><span class="line">        <span class="string">"list"</span></span><br><span class="line">      ]</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"name"</span>: <span class="string">"pods"</span>,</span><br><span class="line">      <span class="string">"singularName"</span>: <span class="string">""</span>,</span><br><span class="line">      <span class="string">"namespaced"</span>: <span class="literal">true</span>,</span><br><span class="line">      <span class="string">"kind"</span>: <span class="string">"PodMetrics"</span>,</span><br><span class="line">      <span class="string">"verbs"</span>: [</span><br><span class="line">        <span class="string">"get"</span>,</span><br><span class="line">        <span class="string">"list"</span></span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl get --raw "/apis/metrics.k8s.io/v1beta1/nodes" | jq .</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"kind"</span>: <span class="string">"NodeMetricsList"</span>,</span><br><span class="line">  <span class="string">"apiVersion"</span>: <span class="string">"metrics.k8s.io/v1beta1"</span>,</span><br><span class="line">  <span class="string">"metadata"</span>: &#123;</span><br><span class="line">    <span class="string">"selfLink"</span>: <span class="string">"/apis/metrics.k8s.io/v1beta1/nodes"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"items"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"metadata"</span>: &#123;</span><br><span class="line">        <span class="string">"name"</span>: <span class="string">"172.21.16.204"</span>,</span><br><span class="line">        <span class="string">"selfLink"</span>: <span class="string">"/apis/metrics.k8s.io/v1beta1/nodes/172.21.16.204"</span>,</span><br><span class="line">        <span class="string">"creationTimestamp"</span>: <span class="string">"2019-09-04T07:00:44Z"</span></span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="string">"timestamp"</span>: <span class="string">"2019-09-04T07:00:40Z"</span>,</span><br><span class="line">      <span class="string">"window"</span>: <span class="string">"30s"</span>,</span><br><span class="line">      <span class="string">"usage"</span>: &#123;</span><br><span class="line">        <span class="string">"cpu"</span>: <span class="string">"63788460n"</span>,</span><br><span class="line">        <span class="string">"memory"</span>: <span class="string">"1033152Ki"</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"metadata"</span>: &#123;</span><br><span class="line">        <span class="string">"name"</span>: <span class="string">"172.21.16.240"</span>,</span><br><span class="line">        <span class="string">"selfLink"</span>: <span class="string">"/apis/metrics.k8s.io/v1beta1/nodes/172.21.16.240"</span>,</span><br><span class="line">        <span class="string">"creationTimestamp"</span>: <span class="string">"2019-09-04T07:00:44Z"</span></span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="string">"timestamp"</span>: <span class="string">"2019-09-04T07:00:40Z"</span>,</span><br><span class="line">      <span class="string">"window"</span>: <span class="string">"30s"</span>,</span><br><span class="line">      <span class="string">"usage"</span>: &#123;</span><br><span class="line">        <span class="string">"cpu"</span>: <span class="string">"41797865n"</span>,</span><br><span class="line">        <span class="string">"memory"</span>: <span class="string">"837420Ki"</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"metadata"</span>: &#123;</span><br><span class="line">        <span class="string">"name"</span>: <span class="string">"172.21.16.87"</span>,</span><br><span class="line">        <span class="string">"selfLink"</span>: <span class="string">"/apis/metrics.k8s.io/v1beta1/nodes/172.21.16.87"</span>,</span><br><span class="line">        <span class="string">"creationTimestamp"</span>: <span class="string">"2019-09-04T07:00:44Z"</span></span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="string">"timestamp"</span>: <span class="string">"2019-09-04T07:00:34Z"</span>,</span><br><span class="line">      <span class="string">"window"</span>: <span class="string">"30s"</span>,</span><br><span class="line">      <span class="string">"usage"</span>: &#123;</span><br><span class="line">        <span class="string">"cpu"</span>: <span class="string">"37347688n"</span>,</span><br><span class="line">        <span class="string">"memory"</span>: <span class="string">"851232Ki"</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>/apis/metrics.k8s.io/v1beta1/nodes 和 /apis/metrics.k8s.io/v1beta1/pods 返回的 usage 包含 CPU 和 Memory；</li></ul><h4 id="使用-kubectl-top"><a href="#使用-kubectl-top" class="headerlink" title="使用 kubectl top"></a>使用 kubectl top</h4><p>使用 kubectl top 命令查看集群节点资源使用情况,kubectl top 命令从 metrics-server 获取集群节点基本的指标信息</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl top node</span></span><br><span class="line">NAME            CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   </span><br><span class="line">172.21.16.204   69m          1%     1008Mi          13%       </span><br><span class="line">172.21.16.240   41m          2%     817Mi           23%       </span><br><span class="line">172.21.16.87    39m          1%     831Mi           23%</span><br></pre></td></tr></table></figure><p>metrics到这里就已经成功的部署，参数没有一一介绍，后期有时间在列出来</p><p>这里还有很多参考的文档没有一一列出来，主要是浏览器被关闭啦，感谢那些参考的文档，😊😊😊</p><!-- rebuild by neat -->]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>metrics-server</tag>
      </tags>
  </entry>
  <entry>
    <title>kubelet提供api请求接口</title>
    <url>/2019/09/04/kubelet%E6%8F%90%E4%BE%9Bapi%E8%AF%B7%E6%B1%82%E6%8E%A5%E5%8F%A3/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:23 GMT+0800 (China Standard Time) --><h3 id="kubelet-提供的-API-接口认证"><a href="#kubelet-提供的-API-接口认证" class="headerlink" title="kubelet 提供的 API 接口认证"></a>kubelet 提供的 API 接口认证</h3><p><a href="https://xxlaila.github.io/2019/08/10/kubernetes-node%E8%8A%82%E7%82%B9%E5%AE%89%E8%A3%85/" target="_blank" rel="noopener">node安装参考</a></p><p>kubelet 启动后监听多个端口，用于接收 kube-apiserver 或其它客户端发送的请求：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@k8s-3 ~]<span class="comment">#  netstat -lnpt|grep kubelet</span></span><br><span class="line">tcp        0      0 127.0.0.1:46395         0.0.0.0:*               LISTEN      8941/kubelet        </span><br><span class="line">tcp        0      0 127.0.0.1:10248         0.0.0.0:*               LISTEN      8941/kubelet        </span><br><span class="line">tcp6       0      0 :::10250                :::*                    LISTEN      8941/kubelet</span><br></pre></td></tr></table></figure><ul><li><strong>10248</strong>: healthz http 服务</li><li><strong>10250</strong>: https 服务，访问该端口时需要认证和授权（即使访问 /healthz 也需要）</li><li>未开启只读端口 10255</li><li>从 K8S v1.10 开始，去除了 –cadvisor-port 参数（默认 4194 端口），不支持访问 cAdvisor UI &amp; API</li></ul><a id="more"></a><p>kubelet 接收 10250 端口的 https 请求，可以访问如下资源：</p><ul><li>/pods、/runningpods</li><li>/metrics、/metrics/cadvisor、/metrics/probes</li><li>/spec</li><li>/stats、/stats/container</li><li>/logs</li><li>/run/、/exec/, /attach/, /portForward/, /containerLogs/<br><a href="https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/server/server.go#L434:3" target="_blank" rel="noopener">详情参考</a></li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;由于关闭了匿名认证，同时开启了 webhook 授权，所有访问 10250 端口 https API 的请求都需要被认证和授权。<br>&nbsp;&nbsp;&nbsp;&nbsp;预定义的 ClusterRole system:kubelet-api-admin 授予访问 kubelet 所有 API 的权限(kube-apiserver 使用的 kubernetes 证书 User 授予了该权限)：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl describe clusterrole system:kubelet-api-admin</span></span><br><span class="line">Name:         system:kubelet-api-admin</span><br><span class="line">Labels:       kubernetes.io/bootstrapping=rbac-defaults</span><br><span class="line">Annotations:  rbac.authorization.kubernetes.io/autoupdate: <span class="literal">true</span></span><br><span class="line">PolicyRule:</span><br><span class="line">  Resources      Non-Resource URLs  Resource Names  Verbs</span><br><span class="line">  ---------      -----------------  --------------  -----</span><br><span class="line">  nodes/<span class="built_in">log</span>      []                 []              [*]</span><br><span class="line">  nodes/metrics  []                 []              [*]</span><br><span class="line">  nodes/proxy    []                 []              [*]</span><br><span class="line">  nodes/spec     []                 []              [*]</span><br><span class="line">  nodes/stats    []                 []              [*]</span><br><span class="line">  nodes          []                 []              [get list watch proxy]</span><br></pre></td></tr></table></figure><h3 id="kubelet-api-认证和授权"><a href="#kubelet-api-认证和授权" class="headerlink" title="kubelet api 认证和授权"></a>kubelet api 认证和授权</h3><p>kubelet 配置了如下认证参数:</p><ul><li><strong>–anonymous-auth=false</strong>: 设置为 false，不允许匿名�访问 10250 端口</li><li><strong>–authentication-token-webhook=true</strong>: 指定签名客户端证书的 CA 证书，开启 HTTPs 证书认证</li><li><strong>–client-ca-file=/etc/kubernetes/ssl/kubernetes-ca.pem</strong>: 开启 HTTPs bearer token 认证</li><li><strong>–authorization-mode=Webhook</strong>: 开启 RBAC 授权</li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;kubelet 收到请求后，使用 clientCAFile 对证书签名进行认证，或者查询 bearer token 是否有效。如果两者都没通过，则拒绝请求，提示 Unauthorized</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@k8s-master-01-2 ~]<span class="comment"># curl -s --cacert /etc/kubernetes/ssl/kubernetes-ca.pem https://172.21.16.204:10250/metrics</span></span><br><span class="line">Unauthorized</span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;通过认证后，kubelet 使用 SubjectAccessReview API 向 kube-apiserver 发送请求，查询证书或 token 对应的 user、group 是否有操作资源的权限(RBAC)；</p><h3 id="证书认证和授权"><a href="#证书认证和授权" class="headerlink" title="证书认证和授权"></a>证书认证和授权</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 权限不足</span></span><br><span class="line"><span class="comment"># curl -s --cacert /etc/kubernetes/ssl/kubernetes-ca.pem  --cert /etc/kubernetes/ssl/kube-controller-manager.pem --key /etc/kubernetes/ssl/kube-controller-manager-key.pem https://172.21.16.204:10250/metrics</span></span><br><span class="line">Forbidden (user=system:kube-controller-manager, verb=get, resource=nodes, subresource=metrics)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用部署 kubectl 命令行工具时创建的、具有最高权限的 admin 证书</span></span><br><span class="line"><span class="comment"># curl -s --cacert /etc/kubernetes/ssl/kubernetes-ca.pem --cert /etc/kubernetes/ssl/admin.pem --key /etc/kubernetes/ssl/admin-key.pem https://172.21.16.204:10250/metrics|head</span></span><br><span class="line"><span class="comment"># HELP apiserver_audit_event_total Counter of audit events generated and sent to the audit backend.</span></span><br><span class="line"><span class="comment"># TYPE apiserver_audit_event_total counter</span></span><br><span class="line">apiserver_audit_event_total 0</span><br><span class="line"><span class="comment"># HELP apiserver_audit_requests_rejected_total Counter of apiserver requests rejected due to an error in audit logging backend.</span></span><br><span class="line"><span class="comment"># TYPE apiserver_audit_requests_rejected_total counter</span></span><br><span class="line">apiserver_audit_requests_rejected_total 0</span><br><span class="line"><span class="comment"># HELP apiserver_client_certificate_expiration_seconds Distribution of the remaining lifetime on the certificate used to authenticate a request.</span></span><br><span class="line"><span class="comment"># TYPE apiserver_client_certificate_expiration_seconds histogram</span></span><br><span class="line">apiserver_client_certificate_expiration_seconds_bucket&#123;le=<span class="string">"0"</span>&#125; 0</span><br><span class="line">apiserver_client_certificate_expiration_seconds_bucket&#123;le=<span class="string">"21600"</span>&#125; 0</span><br></pre></td></tr></table></figure><ul><li><strong>注意</strong>: –cacert、–cert、–key 的参数值必须是文件路径，否则返回 401 Unauthorized；</li></ul><h4 id="bear-token-认证和授权"><a href="#bear-token-认证和授权" class="headerlink" title="bear token 认证和授权"></a>bear token 认证和授权</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;创建一个 ServiceAccount，将它和 ClusterRole system:kubelet-api-admin 绑定，从而具有调用 kubelet API 的权限</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">horization.k8s.io/kubelet-api-test created</span><br><span class="line"><span class="comment"># SECRET=$(kubectl get secrets | grep kubelet-api-test | awk '&#123;print $1&#125;')</span></span><br><span class="line"><span class="comment"># TOKEN=$(kubectl describe secret $&#123;SECRET&#125; | grep -E '^token' | awk '&#123;print $2&#125;')</span></span><br><span class="line"><span class="comment"># echo $&#123;TOKEN&#125;</span></span><br><span class="line">eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6Imt1YmVsZXQtYXBpLXRlc3QtdG9rZW4tNGJra3MiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoia3ViZWxldC1hcGktdGVzdCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6Ijk3MDRhZDEwLWNlYjQtMTFlOS04ZDIwLWZhMTYzZTVhZjgzMyIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0Omt1YmVsZXQtYXBpLXRlc3QifQ.ishvOaC5tppYKDNpEOXIiVhVtgjyqzjySZjzndot5Z5U9MkY9LN8ZSMWRe6lNsB1UuTgEWTsHlG3OIRfExnHehYhWIt59V9e39KKbeY17hHoT-RZSaD6GoB449t_vUdIJedd1FGZ8DckQvDr6X5fMuD7MSU3vRL077j-uls-y4IW5kaJHeAGJfc6eWoCnv96DCbI8mQ8yuYbwLFpfIPLb4u6FPkwMQL2KXy6FhWPY1va6zAh4LdjGWhH6IAkKleq0aqfMwvmlnk1_OUmnmBoGJGuB96IwqBATP0jFzrd-Sv6af3RsSYz2r8YzJUj3kat9bd__HNCCXampYYr8ffu8YEdn-J9p6HK13FWU4O9QSIDrRONNIOpUXclJ-ov3z6N1hiIcVq5UJU6xR2z4ccvPXmH9Sj7p8CquqKEuobZxK97TFtECGlb2Ex43u4t0UHRo23UCQA-qP2Zs4-U2Zmf_qu3I-Lm7jzuYzXFCAb27yZx_XOUY-ycnKhtM6PpUfVKhkcHfWBOYY-QtBEbYf6yHRqCWcjrsZ63C_B56qAYaU5ca3hAcr6RBuHmmHISGESlLbmrpGgJ_ajd5mrJSh3Z_qdqu-Xt0Ya0NLfXgAcGi5n8xWJLztRTeyHrFTj7g82MqERUfFk9bdLHcz77xmrNLnhRZ87GW9sTZLw8QRUjF1g</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># curl -s --cacert /etc/kubernetes/ssl/kubernetes-ca.pem -H "Authorization: Bearer $&#123;TOKEN&#125;" https://172.21.16.204:10250/metrics|head</span></span><br><span class="line"><span class="comment"># HELP apiserver_audit_event_total Counter of audit events generated and sent to the audit backend.</span></span><br><span class="line"><span class="comment"># TYPE apiserver_audit_event_total counter</span></span><br><span class="line">apiserver_audit_event_total 0</span><br><span class="line"><span class="comment"># HELP apiserver_audit_requests_rejected_total Counter of apiserver requests rejected due to an error in audit logging backend.</span></span><br><span class="line"><span class="comment"># TYPE apiserver_audit_requests_rejected_total counter</span></span><br><span class="line">apiserver_audit_requests_rejected_total 0</span><br><span class="line"><span class="comment"># HELP apiserver_client_certificate_expiration_seconds Distribution of the remaining lifetime on the certificate used to authenticate a request.</span></span><br><span class="line"><span class="comment"># TYPE apiserver_client_certificate_expiration_seconds histogram</span></span><br><span class="line">apiserver_client_certificate_expiration_seconds_bucket&#123;le=<span class="string">"0"</span>&#125; 0</span><br><span class="line">apiserver_client_certificate_expiration_seconds_bucket&#123;le=<span class="string">"21600"</span>&#125; 0</span><br></pre></td></tr></table></figure><h4 id="cadvisor-和-metrics"><a href="#cadvisor-和-metrics" class="headerlink" title="cadvisor 和 metrics"></a>cadvisor 和 metrics</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;cadvisor 是内嵌在 kubelet 二进制中的，统计所在节点各容器的资源(CPU、内存、磁盘、网卡)使用情况的服务.</p><blockquote><p>在访问api-server安全端口之前，我们需要做一些操作才能访问，否则无法进行访问</p></blockquote><h5 id="在浏览器访问kube-apiserver安全端口"><a href="#在浏览器访问kube-apiserver安全端口" class="headerlink" title="在浏览器访问kube-apiserver安全端口"></a>在浏览器访问kube-apiserver安全端口</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;提示证书不被信任,这是因为 kube-apiserver 的 server 证书是我们创建的根证书 ca.pem 签名的，需要将根证书 ca.pem 导入操作系统，并设置永久信任。<br><img src="https://img.xxlaila.cn/1567564092242.jpg" alt="img"></p><p>&nbsp;&nbsp;&nbsp;&nbsp;给浏览器生成一个 client 证书，访问 apiserver 的 6443 https 端口时使用。这里使用部署 kubectl 命令行工具时创建的 admin 证书、私钥和上面的 ca 证书，创建一个浏览器可以使用 PKCS#12/PFX 格式的证书：</p><ul><li><p>会提示输入密码，这里密码需要记住，一会倒入证书到浏览器的时候需要</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># openssl pkcs12 -export -out admin.pfx -inkey admin-key.pem -in admin.pem -certfile kubernetes-ca.pem</span></span><br><span class="line">Enter Export Password:</span><br><span class="line">Verifying - Enter Export Password:</span><br></pre></td></tr></table></figure></li><li><p>将创建的 admin.pfx 导入到系统的证书中,<br><img src="https://img.xxlaila.cn/1567564289155.jpg" alt="img"></p></li></ul><p>再次访问 apiserver 地址，提示选择一个浏览器证书，这里选中上面导入的 admin.pfx<br><img src="https://img.xxlaila.cn/1567564393274.jpg" alt="img"></p><ul><li><p>提示需要输入系统的密码,这里是mac的电脑<br><img src="https://img.xxlaila.cn/1567564441293.jpg" alt="img"></p></li><li><p>被授权访问 kube-apiserver 的安全端口<br><img src="https://img.xxlaila.cn/1567564526664.jpg" alt="img"></p></li></ul><h5 id="客户端选择证书的原理"><a href="#客户端选择证书的原理" class="headerlink" title="客户端选择证书的原理"></a>客户端选择证书的原理</h5><ul><li>证书选择是在客户端和服务端 SSL/TLS 握手协商阶段商定的；</li><li>服务端如果要求客户端提供证书，则在握手时会向客户端发送一个它接受的 CA 列表；</li><li>客户端查找它的证书列表(一般是操作系统的证书，对于 Mac 为 keychain)，看有没有被 CA 签名的证书，如果有，则将它们提供给用户选择（证书的私钥）；</li><li>用户选择一个证书私钥，然后客户端将使用它和服务端通信；</li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;在浏览器访问 <a href="https://172.21.16.204:10250/metrics" target="_blank" rel="noopener">https://172.21.16.204:10250/metrics</a> 和 <a href="https://172.21.16.204:10250/metrics/cadvisor" target="_blank" rel="noopener">https://172.21.16.204:10250/metrics/cadvisor</a> 分别返回 kubelet 和 cadvisor 的 metrics。<br><img src="https://img.xxlaila.cn/1567563858215.jpg" alt="img"><br><img src="https://img.xxlaila.cn/1567564733531.jpg" alt="img"></p><ul><li><strong>原因</strong>: kubelet配置文件设置<code>--anonymous-auth=false</code>不允许匿名证书访问 10250 的 https 服务,所以我们才需要配置证书来进行访问</li></ul><!-- rebuild by neat -->]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubelet</tag>
      </tags>
  </entry>
  <entry>
    <title>centos-nfs-512错误</title>
    <url>/2019/09/03/centos-nfs-512%E9%94%99%E8%AF%AF/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><p>nfs 错误kernel: NFS: nfs4_discover_server_trunking unhandled error -512. Exiting with error EIO</p><p>&nbsp;&nbsp;&nbsp;&nbsp;很久没挂载过nfs，忘记客户端怎么挂在nfs的了，服务端很早就安装好了，今天一台客户机需要挂载nfs，然后居然报错了，然后找了一圈居然没找到怎么解决，然后又重新看了一次centos nfs的配置，于是乎就搞定了</p><p>在挂载nfs的提示很慢，长时间无响应，强行结束看看是什么问题，查看日志</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ sudo tail -f /var/<span class="built_in">log</span>/messages</span><br><span class="line">Sep  3 11:23:51 dev-application kernel: NFS: nfs4_discover_server_trunking unhandled error -512. Exiting with error EIO</span><br></pre></td></tr></table></figure><a id="more"></a><h3 id="1、在客户端查看rpcbind-service"><a href="#1、在客户端查看rpcbind-service" class="headerlink" title="1、在客户端查看rpcbind.service"></a>1、在客户端查看rpcbind.service</h3><p>在客户端查看<code>rpcbind.service</code>是否正常启动，没有正常启动，就需要启动，没有安装服务，就需要安装</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ sudo yum -y install rpcbind</span><br><span class="line">$ sudo systemctl start rpcbind.service &amp;&amp; sudo systemctl <span class="built_in">enable</span> rpcbind.service</span><br></pre></td></tr></table></figure><ul><li>再次挂载的时候还是挂载补上，因为第一次挂载的时候客户端向服务器端发送了数据请求，而且还没有正常的断开链接，可以在服务端用<code>netstat -anp|grep tcp</code>查看有没有客户端的链接信息,状态是<code>ESTABLISHED</code>，有的话，我们需要把它结束掉</li></ul><h3 id="2、linux-结束tcp会话"><a href="#2、linux-结束tcp会话" class="headerlink" title="2、linux 结束tcp会话"></a>2、linux 结束tcp会话</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 目前不知道什么命令,后期更新</span></span><br><span class="line"><span class="comment"># 重启nfs服务</span></span><br><span class="line">$ sudo systemctl restart nfs.service</span><br></pre></td></tr></table></figure><h3 id="3、客户端挂载"><a href="#3、客户端挂载" class="headerlink" title="3、客户端挂载"></a>3、客户端挂载</h3><p>再次来到客户机挂载，可以成功的挂载</p><h3 id="4、rpcbind服务介绍"><a href="#4、rpcbind服务介绍" class="headerlink" title="4、rpcbind服务介绍"></a>4、rpcbind服务介绍</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;共享和加载NFS文件系统需要服务，红帽企业Linux使用核心级的支持和守护进程的组合来提供NFS文件共享.NFS依靠远程过程调用(RPC)在客户端和服务器端路由请求。在Linux下RPC服务由portmap服务控制。</p><h4 id="4-1、为了共享和加载NFS文件系统，下面的服务要一起工作"><a href="#4-1、为了共享和加载NFS文件系统，下面的服务要一起工作" class="headerlink" title="4.1、为了共享和加载NFS文件系统，下面的服务要一起工作:"></a>4.1、为了共享和加载NFS文件系统，下面的服务要一起工作:</h4><ul><li>nfs - 启动相应RPC服务进程来服务对于NFS文件系统的请求.</li><li>nfslock - 一个可选的服务，用于启动相应的RPC进程，允许NFS客户端在服务器上对文件加锁.</li><li>portmap - Linux的RPC服务,它响应RPC服务的请求和与请求的RPC服务建立连接.</li></ul><h4 id="4-2、RPC进程在后台一起工作服务于NFS服务"><a href="#4-2、RPC进程在后台一起工作服务于NFS服务" class="headerlink" title="4.2、RPC进程在后台一起工作服务于NFS服务"></a>4.2、RPC进程在后台一起工作服务于NFS服务</h4><ul><li>rpc.mountd - 这个进程接受来自NFS客户端的加载请求和验证请求的文件系统正在被输出.这个进程由NFS服务自动启动，不需要用户的配置.</li><li>rpc.nfsd - 这个进程是NFS服务器.它和Linux核心一起工作来满足NFS客户端的动态需求，例如提供为每个NFS客户端的每次请求服务器线程.这个进程对应于nfs服务.</li><li>rpc.lockd - 一个可选的进程，它允许NFS客户端在服务器上对文件加锁.这个进程对应于nfslock服务.</li><li>rpc.statd - 这个进程实现了网络状态监控(NSM)RPC协议,通知NFS客户端什么时候一个NFS服务器非正常重启动.这个进程被nfslock服务自动启动，不需要用户的配置.</li><li>rpc.rquotad - 这个进程对于远程用户提供用户配额信息. 这个进程被nfs服务自动启动，不需要用户的配置.</li></ul><!-- rebuild by neat -->]]></content>
      <categories>
        <category>Centos</category>
      </categories>
      <tags>
        <tag>nfs</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s集群部署heapster</title>
    <url>/2019/09/02/k8s%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2heapster/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><p>kubernetes集群启用tls认证部署heapster，在部署期间遇到了很多的坑，走过很多雷，这里记录一下,不过在新版本中heapster被metrics-server代替了，metrics-server后篇介绍和使用</p><h2 id="1、准备工作"><a href="#1、准备工作" class="headerlink" title="1、准备工作"></a>1、准备工作</h2><p>下载需要的文件，这里用之前k8s-heapster部署的文件拿来进行修改，<a href="https://github.com/xxlaila/kubernetes-yaml/" target="_blank" rel="noopener">文件地址</a></p><h3 id="2、执行文件创建"><a href="#2、执行文件创建" class="headerlink" title="2、执行文件创建"></a>2、执行文件创建</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cd kubernetes-yaml/heapster-influxdb-grafana</span></span><br><span class="line"><span class="comment"># kubectl apply -f ./</span></span><br></pre></td></tr></table></figure><a id="more"></a><ul><li><strong>错误提示</strong>: 这里在执行创建后，没有图像显示，查看pods日志发现错误<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get pods -n kube-system|grep he</span></span><br><span class="line">heapster-7b7b4754d-5p7tb                1/1     Running   0          17m</span><br><span class="line"><span class="comment"># kubectl logs heapster-7b7b4754d-5p7tb -n kube-system</span></span><br><span class="line">E0902 10:57:11.804543       1 reflector.go:190] k8s.io/heapster/metrics/processors/namespace_based_enricher.go:89: Failed to list *v1.Namespace: namespaces is forbidden: User <span class="string">"system:serviceaccount:kube-system:heapster"</span> cannot list resource <span class="string">"namespaces"</span> <span class="keyword">in</span> API group <span class="string">""</span> at the cluster scope: RBAC: clusterrole.rbac.authorization.k8s.io <span class="string">"system:heapster"</span> not found</span><br><span class="line">E0902 10:57:12.071117       1 reflector.go:190] k8s.io/heapster/metrics/util/util.go:30: Failed to list *v1.Node: nodes is forbidden: User <span class="string">"system:serviceaccount:kube-system:heapster"</span> cannot list resource <span class="string">"nodes"</span> <span class="keyword">in</span> API group <span class="string">""</span> at the cluster scope: RBAC: clusterrole.rbac.authorization.k8s.io <span class="string">"system:heapster"</span> not found</span><br></pre></td></tr></table></figure></li></ul><ul><li>排错<br>查看ClusterRole: system:heapster的权限,发现并没有</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl describe clusterrole system:heapster</span></span><br><span class="line">Error from server (NotFound): clusterroles.rbac.authorization.k8s.io <span class="string">"system:heapster"</span> not found</span><br></pre></td></tr></table></figure><p>提示这个错误，应该是我之前部署过，然后修改过权限，不小心给删掉啦，这里需要吧权限重建一次就好了，<a href="https://www.cnblogs.com/vincenshen/p/9638162.html" target="_blank" rel="noopener">参考文献</a></p><ul><li>新建heapster-clusterrole.yaml<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat heapster-clusterrole.yaml </span></span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRole</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    rbac.authorization.kubernetes.io/autoupdate: <span class="string">"true"</span></span><br><span class="line">  labels:</span><br><span class="line">    kubernetes.io/bootstrapping: rbac-defaults</span><br><span class="line">  name: system:heapster</span><br><span class="line">rules:</span><br><span class="line">- apiGroups:</span><br><span class="line">  - <span class="string">""</span></span><br><span class="line">  resources:</span><br><span class="line">  - events</span><br><span class="line">  - namespaces</span><br><span class="line">  - nodes</span><br><span class="line">  - pods</span><br><span class="line">  - nodes/stats</span><br><span class="line">  verbs:</span><br><span class="line">  - create</span><br><span class="line">  - get</span><br><span class="line">  - list</span><br><span class="line">  - watch</span><br><span class="line">- apiGroups:</span><br><span class="line">  - extensions</span><br><span class="line">  resources:</span><br><span class="line">  - deployments</span><br><span class="line">  verbs:</span><br><span class="line">  - get</span><br><span class="line">  - list</span><br><span class="line">  - watch</span><br></pre></td></tr></table></figure></li></ul><p>并执行 kubectl apply -f heapster-clusterrole.yaml,在次查看日志，发现之前的错误没有了，但是又出现了一个新的错误<br><code>E0902 11:27:05.025300 1 manager.go:101] Error in scraping containers from kubelet:172.21.16.204:10250: failed to get all container stats from Kubelet URL &quot;https://172.21.16.204:10250/stats/container/&quot;: request failed - &quot;401 Unauthorized&quot;, response: &quot;Unauthorized&quot;</code></p><ul><li><p>修改heapster-clusterrole.yaml文件，在文件里面我们添加几个权限</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat heapster-clusterrole.yaml </span></span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRole</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    rbac.authorization.kubernetes.io/autoupdate: <span class="string">"true"</span></span><br><span class="line">  labels:</span><br><span class="line">    kubernetes.io/bootstrapping: rbac-defaults</span><br><span class="line">  name: system:heapster</span><br><span class="line">rules:</span><br><span class="line">- apiGroups:</span><br><span class="line">  - <span class="string">""</span></span><br><span class="line">  resources:</span><br><span class="line">  - events</span><br><span class="line">  - namespaces</span><br><span class="line">  - nodes</span><br><span class="line">  - pods</span><br><span class="line">  - nodes/stats</span><br><span class="line">  verbs:</span><br><span class="line">  - create</span><br><span class="line">  - get</span><br><span class="line">  - list</span><br><span class="line">  - watch</span><br><span class="line">- apiGroups:</span><br><span class="line">  - extensions</span><br><span class="line">  resources:</span><br><span class="line">  - deployments</span><br><span class="line">  verbs:</span><br><span class="line">  - get</span><br><span class="line">  - list</span><br><span class="line">  - update</span><br><span class="line">  - watch</span><br><span class="line">- apiGroups:</span><br><span class="line">  - <span class="string">""</span></span><br><span class="line">  resources:</span><br><span class="line">  - nodes/stats</span><br><span class="line">  verbs:</span><br><span class="line">  - get</span><br></pre></td></tr></table></figure></li><li><p>执行创建</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl apply -f heapster-clusterrole.yaml</span></span><br><span class="line"><span class="comment"># 重新创建heapster</span></span><br><span class="line"><span class="comment"># kubectl delete -f heapster.yaml</span></span><br><span class="line"><span class="comment"># kubectl create -f heapster.yaml</span></span><br></pre></td></tr></table></figure></li></ul><p>完成以后我们继续看heapster pod的日志，发现日志里面还是出现<code>401 Unauthorized&quot;, response: &quot;Unauthorized&quot;</code>，我们需要修改node节点 kubelet 启动的配置参数，添加<code>--authentication-token-webhook</code>参数: 使用tokenreview API来进行令牌认证。Kubelet 在配置的 API server 上调用 TokenReview API 以确定来自 bearer token 的用户信息。<a href="https://k8smeetup.github.io/docs/admin/kubelet-authentication-authorization/" target="_blank" rel="noopener">官方参考</a>，<a href="https://github.com/kubernetes-retired/heapster/issues/1936" target="_blank" rel="noopener">github上错误解决</a>， <a href="https://jimmysong.io/posts/user-authentication-in-kubernetes/" target="_blank" rel="noopener">参数文章学习</a></p><p>到此为止: 错误没有啦，但是界面数据没出来，稍等片刻</p><!-- rebuild by neat -->]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>heapster</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s配置Dashboard</title>
    <url>/2019/08/29/k8s%E9%85%8D%E7%BD%AEDashboard/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><p>&nbsp;&nbsp;&nbsp;&nbsp;K8S Dashboard是官方的一个基于WEB的用户界面，专门用来管理K8S集群，并可展示集群的状态。K8S集群安装好后默认没有包含Dashboard，我们需要额外创建它。</p><h3 id="1、安装dashboard"><a href="#1、安装dashboard" class="headerlink" title="1、安装dashboard"></a>1、安装dashboard</h3><h4 id="1-1、下载准备需要的文件"><a href="#1-1、下载准备需要的文件" class="headerlink" title="1.1、下载准备需要的文件"></a>1.1、下载准备需要的文件</h4><p>经过修改过后的文件，已经可以正常使用的<a href="https://github.com/xxlaila/kubernetes-yaml/" target="_blank" rel="noopener">文件</a></p><a id="more"></a><ul><li>创建dashboard<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@k8s-master-01 dashboard]<span class="comment"># kubectl create -f kubernetes-dashboard.yaml </span></span><br><span class="line">secret/kubernetes-dashboard-certs created</span><br><span class="line">serviceaccount/kubernetes-dashboard created</span><br><span class="line">role.rbac.authorization.k8s.io/kubernetes-dashboard-minimal created</span><br><span class="line">rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard-minimal created</span><br><span class="line">deployment.apps/kubernetes-dashboard created</span><br><span class="line">service/kubernetes-dashboard created</span><br></pre></td></tr></table></figure></li></ul><h4 id="1-2、查看服务状态和pod"><a href="#1-2、查看服务状态和pod" class="headerlink" title="1.2、查看服务状态和pod"></a>1.2、查看服务状态和pod</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@k8s-master-01 ~]<span class="comment"># kubectl get service --all-namespaces</span></span><br><span class="line">NAMESPACE     NAME                   TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)         AGE</span><br><span class="line">default       kubernetes             ClusterIP   10.254.0.1      &lt;none&gt;        443/TCP         18h</span><br><span class="line">kube-system   coredns                ClusterIP   10.254.0.10     &lt;none&gt;        53/UDP,53/TCP   16h</span><br><span class="line">kube-system   kubernetes-dashboard   NodePort    10.254.51.226   &lt;none&gt;        443:30001/TCP   15h</span><br></pre></td></tr></table></figure><ul><li><p>查看service描述</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@k8s-master-01 ~]<span class="comment"># kubectl describe  service kubernetes-dashboard -n kube-system</span></span><br><span class="line">Name:                     kubernetes-dashboard</span><br><span class="line">Namespace:                kube-system</span><br><span class="line">Labels:                   k8s-app=kubernetes-dashboard</span><br><span class="line">Annotations:              &lt;none&gt;</span><br><span class="line">Selector:                 k8s-app=kubernetes-dashboard</span><br><span class="line">Type:                     NodePort</span><br><span class="line">IP:                       10.254.51.226</span><br><span class="line">Port:                     &lt;<span class="built_in">unset</span>&gt;  443/TCP</span><br><span class="line">TargetPort:               8443/TCP</span><br><span class="line">NodePort:                 &lt;<span class="built_in">unset</span>&gt;  30001/TCP</span><br><span class="line">Endpoints:                10.254.39.3:8443</span><br><span class="line">Session Affinity:         None</span><br><span class="line">External Traffic Policy:  Cluster</span><br><span class="line">Events:                   &lt;none&gt;</span><br></pre></td></tr></table></figure></li><li><p>查看pod描述</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@k8s-master-01 ~]<span class="comment"># kubectl describe pod kubernetes-dashboard-6c655d9445-6zntr --namespace=kube-system</span></span><br><span class="line">Name:           kubernetes-dashboard-6c655d9445-6zntr</span><br><span class="line">Namespace:      kube-system</span><br><span class="line">Node:           172.21.17.31/172.21.17.31</span><br><span class="line">Start Time:     Thu, 29 Aug 2019 17:47:20 +0800</span><br><span class="line">Labels:         k8s-app=kubernetes-dashboard</span><br><span class="line">                pod-template-hash=6c655d9445</span><br><span class="line">Annotations:    &lt;none&gt;</span><br><span class="line">Status:         Running</span><br><span class="line">IP:             10.254.39.3</span><br></pre></td></tr></table></figure></li></ul><h3 id="2、授权Dashboard账户集群管理权限"><a href="#2、授权Dashboard账户集群管理权限" class="headerlink" title="2、授权Dashboard账户集群管理权限"></a>2、授权Dashboard账户集群管理权限</h3><p>若果不进行授权操作，打开dashboard会报错，如下图<br><img src="https://img.xxlaila.cn/WechatIMG28864.png" alt="img"></p><ul><li><p>新建kubrnetes-dashboard-admin-rbac.yaml</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat kubernetes-dashboard-admin-rbac.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: admin-user</span><br><span class="line">  namespace: kube-system</span><br><span class="line">---</span><br><span class="line"><span class="comment"># Create ClusterRoleBinding</span></span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: admin-user</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: cluster-admin</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: admin-user</span><br><span class="line">  namespace: kube-system</span><br></pre></td></tr></table></figure></li><li><p>执行创建</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl create -f kubernetes-dashboard-admin-rbac.yaml</span></span><br></pre></td></tr></table></figure></li></ul><p>找到kubernete-dashboard-admin的token，复制token在dashboard页面进行登录，</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@k8s-master-01 dashboard]<span class="comment"># kubectl -n kube-system get secret | grep admin-user</span></span><br><span class="line">admin-user-token-qv49g             kubernetes.io/service-account-token   3      15h</span><br><span class="line"></span><br><span class="line">[root@k8s-master-01 dashboard]<span class="comment"># kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '&#123;print $1&#125;')</span></span><br><span class="line">Name:         admin-user-token-qv49g</span><br><span class="line">Namespace:    kube-system</span><br><span class="line">Labels:       &lt;none&gt;</span><br><span class="line">Annotations:  kubernetes.io/service-account.name: admin-user</span><br><span class="line">              kubernetes.io/service-account.uid: ea3f0e3f-ca42-11e9-8716-fa163effd55b</span><br><span class="line"></span><br><span class="line">Type:  kubernetes.io/service-account-token</span><br><span class="line"></span><br><span class="line">Data</span><br><span class="line">====</span><br><span class="line">ca.crt:     1359 bytes</span><br><span class="line">namespace:  11 bytes</span><br><span class="line">token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLXF2NDlnIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJlYTNmMGUzZi1jYTQyLTExZTktODcxNi1mYTE2M2VmZmQ1NWIiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.AbdsJdgi9d0rCYrmvoJkWf32HKSMT03OyOX55aRhPptjzIjDcGxxQYecT0w58N7Z_2L2RwTBfOrm4B3wTEDfFZKgYsnGJQOzJMtZDN9w5YJg2xGQ27E3KisTbbQzd_I5DgxSZWW75GwWf756_bIQpWuXNRO_KjheyWuNNv0tSEYRiXpcboSQpb-8R-Km-vP85mxke6s5cJFSk0WLMjFWow1vOF1ns23NZ5nslEmYOMZF3_Fxybh3LbiCyrpD4c0FtfRcXaBIBqACeyCPRriYMIIJq3OJjI-DzuqUedu1x2xH2prB4mNjxlKt2-7q0M1zCuvm5JhW_LzWgveu9ni2ig</span><br></pre></td></tr></table></figure><h3 id="3、配置文件修改说明"><a href="#3、配置文件修改说明" class="headerlink" title="3、配置文件修改说明"></a>3、配置文件修改说明</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;dashboard 文件被修改，默认的token失效的时间是900秒，15分钟，每15分钟就要进行一次认证，这样对于运维人员来说就不是特别的方便，我们可以通过修改token-ttl参数来设置，主要是修改dashborad的yaml文件，并重新建立即可</p><h4 id="3-1、在配置文件修改-添加"><a href="#3-1、在配置文件修改-添加" class="headerlink" title="3.1、在配置文件修改/添加"></a>3.1、在配置文件修改/添加</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ports:</span><br><span class="line">- containerPort: 8443</span><br><span class="line">  protocol: TCP</span><br><span class="line">args:</span><br><span class="line">  - --auto-generate-certificates</span><br><span class="line">  - --token-ttl=43200</span><br></pre></td></tr></table></figure><ul><li>重建pod<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@k8s-master-01 dashboard]<span class="comment"># kubectl apply -f kubernetes-dashboard.yaml</span></span><br></pre></td></tr></table></figure></li></ul><p>我们可以输入任意节点的ip加30001端口就可以访问dashboard, https://{ip}:30001。</p><h3 id="其他操作"><a href="#其他操作" class="headerlink" title="其他操作"></a>其他操作</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;每天我们来公司要登录dashboard的时候都要去输入一次token，每次去获取token的时候都要输入很长的一串，这里为了方便，可以写一个脚本，要token的时候执行一下脚本，就可以。</p><ul><li><p>创建脚本</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># vim kube-token</span></span><br><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk <span class="string">'&#123;print $1&#125;'</span>)</span><br></pre></td></tr></table></figure></li><li><p>设置脚本</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># chmod +x kube-token</span></span><br><span class="line"><span class="comment"># mv kube-token /usr/bin</span></span><br></pre></td></tr></table></figure></li></ul><!-- rebuild by neat -->]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>dashboard</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s删除node重新加入</title>
    <url>/2019/08/29/k8s%E5%88%A0%E9%99%A4node%E9%87%8D%E6%96%B0%E5%8A%A0%E5%85%A5/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><p>&nbsp;&nbsp;&nbsp;&nbsp;有时候k8s node 在加入集群的时候不经意的时候弄错啦某些东西，这时候可以把这个node删除，然后重新加入，删除节点之前我们需要做一下常规化的操作，来保障运行在该节点的pod迁移到其他的node上。</p><a id="more"></a><ul><li><p>1、先驱赶上面的pod</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@k8s-master-01-2 kubernetes]<span class="comment"># kubectl drain 172.21.110 --delete-local-data</span></span><br><span class="line">node/172.21.110 cordoned</span><br><span class="line">node/172.21.110 drained</span><br></pre></td></tr></table></figure></li><li><p>2、删除节点</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@k8s-master-01-2 kubernetes]<span class="comment"># kubectl delete node 172.21.110</span></span><br><span class="line">node <span class="string">"172.21.110"</span> deleted</span><br></pre></td></tr></table></figure></li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;<code>kubectl delete</code> 命令本身是通用的，可以进行任何资源的删除<code>kubectl delete type typename</code>，type是资源类型，可以是<code>node, pod, rs, rc, deployment, service</code>等等，typename是这个资源的名称</p><ul><li><p>3、查看node是否被删除</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@k8s-master-01-2 kubernetes]<span class="comment"># kubectl get nodes</span></span><br><span class="line">NAME           STATUS   ROLES    AGE   VERSION</span><br><span class="line">172.21.17.30   Ready    &lt;none&gt;   20m   v1.13.3</span><br><span class="line">172.21.17.31   Ready    &lt;none&gt;   10m   v1.13.3</span><br></pre></td></tr></table></figure></li><li><p>4、彻底删除node<br>进入该节点。删除<code>kubelet.kubeconfig</code></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@k8s-master-01-3 kubernetes]<span class="comment"># rm -rf  kubelet.kubeconfig</span></span><br></pre></td></tr></table></figure></li><li><p>4、node重新加入集群<br>&nbsp;&nbsp;&nbsp;&nbsp;当我们的node执行删除以后，重新启动kubelet服务以后。node又会自动的加入到集群里面来，怎么彻底的删除，让后重启kubelet的时候重新像集群里面发出csr请求，集群重新通过该节点的csr请求吧该节点加入到集群来，<code>kubelet.kubeconfig</code>也重新生成</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@k8s-master-01-2 kubernetes]<span class="comment"># kubectl get csr</span></span><br><span class="line">NAME                                                   AGE     REQUESTOR           CONDITION</span><br><span class="line">node-csr-H1CAqJw4VZYY67-tk4Akuso_uuPPwpj3d5jK3xcL88M   8m      kubelet-bootstrap   Approved,Issued</span><br><span class="line">node-csr-YPvpbITaxGBrOxuCpGiY7jrGpPNSZ4sdbKhSkUEcdnc   7m54s   kubelet-bootstrap   Approved,Issued</span><br><span class="line">node-csr-odhUT58g0mdVuZdUeclj7doEpUmWzv1YzaiJYQaPeek   5s      kubelet-bootstrap   Pending</span><br><span class="line">node-csr-u4oi5e0Upt-ZmejSDEFm9Q0RU3wZf9bThU_o51nclgg   17m     kubelet-bootstrap   Approved,Issued</span><br></pre></td></tr></table></figure></li><li><p>5、重新通过csr</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@k8s-master-01-2 kubernetes]<span class="comment"># kubectl certificate approve node-csr-odhUT58g0mdVuZdUeclj7doEpUmWzv1YzaiJYQaPeek </span></span><br><span class="line">certificatesigningrequest.certificates.k8s.io/node-csr-odhUT58g0mdVuZdUeclj7doEpUmWzv1YzaiJYQaPeek approved</span><br></pre></td></tr></table></figure></li><li><p>6、在集群查看节点</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@k8s-master-01-2 kubernetes]<span class="comment"># kubectl get nodes</span></span><br><span class="line">NAME            STATUS   ROLES    AGE   VERSION</span><br><span class="line">172.21.16.110   Ready    &lt;none&gt;   36s   v1.13.3</span><br><span class="line">172.21.17.30    Ready    &lt;none&gt;   28m   v1.13.3</span><br><span class="line">172.21.17.31    Ready    &lt;none&gt;   17m   v1.13.3</span><br></pre></td></tr></table></figure></li></ul><!-- rebuild by neat -->]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>node</tag>
      </tags>
  </entry>
  <entry>
    <title>Centos route策略</title>
    <url>/2019/08/28/Centos-route%E7%AD%96%E7%95%A5/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><blockquote><p>场景:<br>&nbsp;&nbsp;&nbsp;&nbsp;公司业务在一个新的云平台上线，该云平台使用的是比较传统的VMware vSphere来做的虚拟化，而且该云平台网络也是我们不清楚的，反正就是不能设置(不能像现在主流云平台自定义网络或者是地址段)，是一个政府的云平台，具体各种奇葩的限制就不说啦，你懂的……</p></blockquote><a id="more"></a><p>&nbsp;&nbsp;&nbsp;&nbsp;根据我们自身业务的需求，我们需要一个内网地址段，和对方协商后给我们开了一个10网段，20的子网，然后要了两台带有外网ip的服务器，一个用来做跳板机，一个用来做反向代理。然后等了两天对方吧服务器给我们开好了，我们登录跳板机，发现只有外网ip的服务器才能上网，其他的均不能上网，做nat也不能上，但是跳板机是公网，没有内网，可以通内网的ip服务器，所以猜测网络策略肯定是他们在交换机上做的。然后我们自己yum源来安装一些中间件。（不说了，后面还有一堆的奇葩问题，进入正题吧）</p><h3 id="后期问题"><a href="#后期问题" class="headerlink" title="后期问题"></a>后期问题</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;当我们把业务部署上线以后，运行一段时间后出现微服务之间调用超时，然后我们登录服务器排查网络问题，在服务器之间ping内网没问题，ping注册中心、数据库都没问题。服务器有公网，然后公网之间ping都没问题，网络层面没有任何的问题，查看系统日志也有发现错误，然后单独的吧借口地址拿出来进行<code>curl</code>能过但是有点慢，然后就吧这个命令写在脚本里面，让他每30秒跑一次，然后追加日志，跑了两三个小时，我们查看日志，日志里面也有超时现象，奇怪了，然后吧这个问题联系对方云平台的工程师，系统工程师、网络工程师拉群讨论，最后对方系统工程师提示我们让我们吧路由的<code>Metric</code>值两个网卡不要一样，对<code>Metric</code>刚开始一年懵逼，先不管，解决问题再说：</p><ul><li><p>修改Metric(内网卡)</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ route -n</span><br><span class="line">$ sudo route add -net 10.10.20.0/22 dev ens160 metric 98</span><br><span class="line">$ sudo route  del -net 10.10.20.0/22 dev ens160 metric 100</span><br></pre></td></tr></table></figure></li><li><p>记住是先添加删除，顺序不要颠倒</p></li><li><p>查看路由</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ route -n</span><br><span class="line">Kernel IP routing table</span><br><span class="line">Destination     Gateway         Genmask         Flags Metric Ref    Use Iface</span><br><span class="line">0.0.0.0         119.126.116.254 0.0.0.0         UG    100    0        0 ens192</span><br><span class="line">10.10.20.0      0.0.0.0         255.255.252.0   U     98     0        0 ens160</span><br><span class="line">119.126.116.128 0.0.0.0         255.255.255.128 U     100    0        0 ens192</span><br></pre></td></tr></table></figure></li><li><p>ens192(外网卡)</p></li><li><p>ens160(内网卡)</p></li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;涉及到的业务服务器都修改，修改Metric值以后，我们<code>curl</code>一次，速度比以前快多了，又跑了一次脚本，观察了几个小时没问题，观察了两天，业务没有出现问题，日志里面也没有出现超时，问题得到解决</p><h3 id="Metric-介绍"><a href="#Metric-介绍" class="headerlink" title="Metric 介绍"></a>Metric 介绍</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;为路由指定所需跃点数的整数值（范围是 1 ~ 9999），它用来在路由表里的多个路由中选择与转发包中的目标地址最为匹配的路由。所选的路由具有最少的跃点数。跃点数能够反映跃点的数量、路径的速度、路径可靠性、路径吞吐量以及管理属性。Metric的值越小，优先级越高；如果两块网卡的Metric的值相同，就会出现抢占优先级继而网卡冲突，将会有一块网卡无法连接</p><p>更多介绍<a href="https://www.cyberciti.biz/faq/what-is-a-routing-table/" target="_blank" rel="noopener">参考</a></p><!-- rebuild by neat -->]]></content>
      <categories>
        <category>Centos</category>
      </categories>
      <tags>
        <tag>route</tag>
        <tag>Metric</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s部署ingress</title>
    <url>/2019/08/26/k8s%E9%83%A8%E7%BD%B2ingress/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:23 GMT+0800 (China Standard Time) --><p>在kubernetes 集群中，一个服务安装以后怎么对外提供访问，外部用户怎么来访问我们容器中业务。</p><p><img src="https://img.xxlaila.cn/2373874sds43.png" alt="img"></p><h3 id="1、Ingress-介绍"><a href="#1、Ingress-介绍" class="headerlink" title="1、Ingress 介绍"></a>1、Ingress 介绍</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;Kubernetes 暴露服务的方式目前只有三种：LoadBlancer Service、NodePort Service、Ingress；本文主要通过Ingress来访问</p><h3 id="2、Ingress-是什么"><a href="#2、Ingress-是什么" class="headerlink" title="2、Ingress 是什么"></a>2、Ingress 是什么</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;Ingress 就是能利用 Nginx、Haproxy 啥的负载均衡器暴露集群内服务的工具问题来了，集群内服务想要暴露出去面临着几个问题：</p><a id="more"></a><ul><li><p>Pod 漂移问题<br>&nbsp;&nbsp;&nbsp;&nbsp;众所周知 Kubernetes 具有强大的副本控制能力，能保证在任意副本(Pod)挂掉时自动从其他机器启动一个新的，还可以动态扩容等，总之一句话，这个 Pod 可能在任何时刻出现在任何节点上，也可能在任何时刻死在任何节点上；那么自然随着 Pod 的创建和销毁，Pod IP 肯定会动态变化；那么如何把这个动态的 Pod IP 暴露出去？这里借助于 Kubernetes 的 Service 机制，Service 可以以标签的形式选定一组带有指定标签的 Pod，并监控和自动负载他们的 Pod IP，那么我们向外暴露只暴露 Service IP 就行了；这就是 NodePort 模式：即在每个节点上开起一个端口，然后转发到内部 Service IP 上，如下图所示：<br><img src="https://img.xxlaila.cn/4dfs98347sdhsfs.png" alt="img"></p></li><li><p>端口管理问题<br>&nbsp;&nbsp;&nbsp;&nbsp;采用 NodePort 方式暴露服务面临一个坑爹的问题是，服务一旦多起来，NodePort 在每个节点上开启的端口会及其庞大，而且难以维护；这时候引出的思考问题是 “能不能使用 Nginx 啥的只监听一个端口，比如 80，然后按照域名向后转发？” 简单的实现就是使用 DaemonSet 在每个 node 上监听 80，然后写好规则，因为 Nginx 外面绑定了宿主机 80 端口(就像 NodePort)，本身又在集群内，那么向后直接转发到相应 Service IP 就行了，如下图所示<br><img src="https://img.xxlaila.cn/85793kdfksdo43.png" alt="img"></p></li><li><p>域名分配及动态更新问题<br>&nbsp;&nbsp;&nbsp;&nbsp;从上面的思路，采用 Nginx 似乎已经解决了问题，但是其实这里面有一个很大缺陷：每次有新服务加入怎么改 Nginx 配置？总不能手动改或者来个 Rolling Update 前端 Nginx Pod 吧？这时候 “伟大而又正直勇敢的” Ingress 登场，如果不算上面的 Nginx，Ingress 只有两大组件：Ingress Controller 和 Ingress<br>&nbsp;&nbsp;&nbsp;&nbsp;Ingress 简单的理解就是 你原来要改 Nginx 配置，然后配置各种域名对应哪个 Service，现在把这个动作抽象出来，变成一个 Ingress 对象，你可以用 yml 创建，每次不要去改 Nginx 了，直接改 yml 然后创建/更新就行了；那么问题来了：”Nginx 咋整？”<br>&nbsp;&nbsp;&nbsp;&nbsp;Ingress Controller 这东西就是解决 “Nginx 咋整” 的；Ingress Controoler 通过与 Kubernetes API 交互，动态的去感知集群中 Ingress 规则变化，然后读取他，按照他自己模板生成一段 Nginx 配置，再写到 Nginx Pod 里，最后 reload 一下，工作流程如下图:</p></li></ul><p><img src="https://img.xxlaila.cn/3248kjfiy4789wodjkshf3.png" alt="img"><br>&nbsp;&nbsp;&nbsp;&nbsp;当然在实际应用中，最新版本 Kubernetes 已经将 Nginx 与 Ingress Controller 合并为一个组件，所以 Nginx 无需单独部署，只需要部署 Ingress Controller 即可。</p><h3 id="3、Nginx-Ingress"><a href="#3、Nginx-Ingress" class="headerlink" title="3、Nginx Ingress"></a>3、Nginx Ingress</h3><h4 id="3-1、下载官方文件"><a href="#3-1、下载官方文件" class="headerlink" title="3.1、下载官方文件"></a>3.1、下载官方文件</h4><p>官方的mandatory.yaml文件里面包含了ingress RBAC，重要的组件 <a href="https://github.com/kubernetes/ingress-nginx/tree/nginx-0.20.0/deploy" target="_blank" rel="noopener">Nginx+Ingres Controller</a></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat mandatory.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Namespace</span><br><span class="line">metadata:</span><br><span class="line">  name: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: ingress-nginx</span><br><span class="line">    app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">kind: ConfigMap</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-configuration</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: ingress-nginx</span><br><span class="line">    app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">kind: ConfigMap</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: tcp-services</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: ingress-nginx</span><br><span class="line">    app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">kind: ConfigMap</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: udp-services</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: ingress-nginx</span><br><span class="line">    app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ingress-serviceaccount</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: ingress-nginx</span><br><span class="line">    app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">kind: ClusterRole</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ingress-clusterrole</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: ingress-nginx</span><br><span class="line">    app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - <span class="string">""</span></span><br><span class="line">    resources:</span><br><span class="line">      - configmaps</span><br><span class="line">      - endpoints</span><br><span class="line">      - nodes</span><br><span class="line">      - pods</span><br><span class="line">      - secrets</span><br><span class="line">    verbs:</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - <span class="string">""</span></span><br><span class="line">    resources:</span><br><span class="line">      - nodes</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - <span class="string">""</span></span><br><span class="line">    resources:</span><br><span class="line">      - services</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - <span class="string">"extensions"</span></span><br><span class="line">    resources:</span><br><span class="line">      - ingresses</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - <span class="string">""</span></span><br><span class="line">    resources:</span><br><span class="line">      - events</span><br><span class="line">    verbs:</span><br><span class="line">      - create</span><br><span class="line">      - patch</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - <span class="string">"extensions"</span></span><br><span class="line">    resources:</span><br><span class="line">      - ingresses/status</span><br><span class="line">    verbs:</span><br><span class="line">      - update</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">kind: Role</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ingress-role</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: ingress-nginx</span><br><span class="line">    app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - <span class="string">""</span></span><br><span class="line">    resources:</span><br><span class="line">      - configmaps</span><br><span class="line">      - pods</span><br><span class="line">      - secrets</span><br><span class="line">      - namespaces</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - <span class="string">""</span></span><br><span class="line">    resources:</span><br><span class="line">      - configmaps</span><br><span class="line">    resourceNames:</span><br><span class="line">      <span class="comment"># Defaults to "&lt;election-id&gt;-&lt;ingress-class&gt;"</span></span><br><span class="line">      <span class="comment"># Here: "&lt;ingress-controller-leader&gt;-&lt;nginx&gt;"</span></span><br><span class="line">      <span class="comment"># This has to be adapted if you change either parameter</span></span><br><span class="line">      <span class="comment"># when launching the nginx-ingress-controller.</span></span><br><span class="line">      - <span class="string">"ingress-controller-leader-nginx"</span></span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - update</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - <span class="string">""</span></span><br><span class="line">    resources:</span><br><span class="line">      - configmaps</span><br><span class="line">    verbs:</span><br><span class="line">      - create</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - <span class="string">""</span></span><br><span class="line">    resources:</span><br><span class="line">      - endpoints</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">kind: RoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ingress-role-nisa-binding</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: ingress-nginx</span><br><span class="line">    app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: Role</span><br><span class="line">  name: nginx-ingress-role</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: nginx-ingress-serviceaccount</span><br><span class="line">    namespace: ingress-nginx</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ingress-clusterrole-nisa-binding</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: ingress-nginx</span><br><span class="line">    app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: nginx-ingress-clusterrole</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: nginx-ingress-serviceaccount</span><br><span class="line">    namespace: ingress-nginx</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ingress-controller</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    app.kubernetes.io/name: ingress-nginx</span><br><span class="line">    app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app.kubernetes.io/name: ingress-nginx</span><br><span class="line">      app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app.kubernetes.io/name: ingress-nginx</span><br><span class="line">        app.kubernetes.io/part-of: ingress-nginx</span><br><span class="line">      annotations:</span><br><span class="line">        prometheus.io/port: <span class="string">"10254"</span></span><br><span class="line">        prometheus.io/scrape: <span class="string">"true"</span></span><br><span class="line">    spec:</span><br><span class="line">      serviceAccountName: nginx-ingress-serviceaccount</span><br><span class="line">      hostNetwork: <span class="literal">true</span></span><br><span class="line">      containers:</span><br><span class="line">        - name: nginx-ingress-controller</span><br><span class="line">          image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.22.0</span><br><span class="line">          args:</span><br><span class="line">            - /nginx-ingress-controller</span><br><span class="line">            - --configmap=$(POD_NAMESPACE)/nginx-configuration</span><br><span class="line">            - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services</span><br><span class="line">            - --udp-services-configmap=$(POD_NAMESPACE)/udp-services</span><br><span class="line">            - --publish-service=$(POD_NAMESPACE)/ingress-nginx</span><br><span class="line">            - --annotations-prefix=nginx.ingress.kubernetes.io</span><br><span class="line">     --default-ssl-certificate=$(POD_NAMESPACE)/ingress-secret</span><br><span class="line">      --default-backend-service=$(POD_NAMESPACE)/default-http-backend</span><br><span class="line">          securityContext:</span><br><span class="line">            allowPrivilegeEscalation: <span class="literal">true</span></span><br><span class="line">            capabilities:</span><br><span class="line">              drop:</span><br><span class="line">                - ALL</span><br><span class="line">              add:</span><br><span class="line">                - NET_BIND_SERVICE</span><br><span class="line">            <span class="comment"># www-data -&gt; 33</span></span><br><span class="line">            runAsUser: 33</span><br><span class="line">          env:</span><br><span class="line">            - name: POD_NAME</span><br><span class="line">              valueFrom:</span><br><span class="line">                fieldRef:</span><br><span class="line">                  fieldPath: metadata.name</span><br><span class="line">            - name: POD_NAMESPACE</span><br><span class="line">              valueFrom:</span><br><span class="line">                fieldRef:</span><br><span class="line">                  fieldPath: metadata.namespace</span><br><span class="line">          ports:</span><br><span class="line">            - name: http</span><br><span class="line">              containerPort: 80</span><br><span class="line">              hostPort: 80</span><br><span class="line">            - name: https</span><br><span class="line">              containerPort: 443</span><br><span class="line">              hostPort: 443</span><br><span class="line">          livenessProbe:</span><br><span class="line">            failureThreshold: 3</span><br><span class="line">            httpGet:</span><br><span class="line">              path: /healthz</span><br><span class="line">              port: 10254</span><br><span class="line">              scheme: HTTP</span><br><span class="line">            initialDelaySeconds: 10</span><br><span class="line">            periodSeconds: 10</span><br><span class="line">            successThreshold: 1</span><br><span class="line">            timeoutSeconds: 1</span><br><span class="line">          readinessProbe:</span><br><span class="line">            failureThreshold: 3</span><br><span class="line">            httpGet:</span><br><span class="line">              path: /healthz</span><br><span class="line">              port: 10254</span><br><span class="line">              scheme: HTTP</span><br><span class="line">            periodSeconds: 10</span><br><span class="line">            successThreshold: 1</span><br><span class="line">            timeoutSeconds: 1</span><br><span class="line">---</span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;<code>hostNetwork: true</code>是增加的， 官方的 Ingress Controller 有个坑，默认注释了hostNetwork 工作方式。以防止端口的在宿主机的冲突。没有绑定到宿主机 80 端口，也就是说前端 Nginx 没有监听宿主机 80 端口；所以需要把配置搞下来自己加一下 hostNetwork。</p><h4 id="3-2、部署默认后端"><a href="#3-2、部署默认后端" class="headerlink" title="3.2、部署默认后端"></a>3.2、部署默认后端</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;我们知道 前端的 Nginx 最终要负载到后端 service 上，那么如果访问不存在的域名咋整？官方给出的建议是部署一个 默认后端，对于未知请求全部负载到这个默认后端上；这个后端啥也不干，就是返回 404，部署如下</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat default-backend.yaml </span></span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: default-http-backend</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: default-http-backend</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        k8s-app: default-http-backend</span><br><span class="line">    spec:</span><br><span class="line">      terminationGracePeriodSeconds: 60</span><br><span class="line">      containers:</span><br><span class="line">      - name: default-http-backend</span><br><span class="line">        <span class="comment"># Any image is permissable as long as:</span></span><br><span class="line">        <span class="comment"># 1. It serves a 404 page at /</span></span><br><span class="line">        <span class="comment"># 2. It serves 200 on a /healthz endpoint</span></span><br><span class="line">        image: docker.io/xxlaila/defaultbackend:1.4</span><br><span class="line">        livenessProbe:</span><br><span class="line">          httpGet:</span><br><span class="line">            path: /healthz</span><br><span class="line">            port: 8080</span><br><span class="line">            scheme: HTTP</span><br><span class="line">          initialDelaySeconds: 30</span><br><span class="line">          timeoutSeconds: 5</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 8080</span><br><span class="line">        resources:</span><br><span class="line">          limits:</span><br><span class="line">            cpu: 10m</span><br><span class="line">            memory: 20Mi</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 10m</span><br><span class="line">            memory: 20Mi</span><br><span class="line"><span class="comment">#      nodeSelector:</span></span><br><span class="line"><span class="comment">#        kubernetes.io/hostname: 172.21.16.231</span></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: default-http-backend</span><br><span class="line">  namespace: ingress-nginx</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: default-http-backend</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80</span><br><span class="line">    targetPort: 8080</span><br><span class="line">  selector:</span><br><span class="line">    k8s-app: default-http-backend</span><br></pre></td></tr></table></figure><h4 id="3-3、执行创建-完成后可以看到"><a href="#3-3、执行创建-完成后可以看到" class="headerlink" title="3.3、执行创建,完成后可以看到"></a>3.3、执行创建,完成后可以看到</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl create -f mandatory.yaml </span></span><br><span class="line"><span class="comment"># kubectl create -f default-backend.yaml</span></span><br></pre></td></tr></table></figure><p><img src="https://img.xxlaila.cn/34dnksjfh384yksfkjdsfsd.png" alt="img"></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get pods -n ingress-nginx</span></span><br><span class="line">NAME                                       READY   STATUS    RESTARTS   AGE</span><br><span class="line">default-http-backend-66cdcb6c7d-pb9sp      1/1     Running   0          8h</span><br><span class="line">nginx-ingress-controller-69585dbb4-m6fcm   1/1     Running   0          8h</span><br><span class="line"><span class="comment"># kubectl get svc -n ingress-nginx</span></span><br><span class="line">NAME                   TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">default-http-backend   ClusterIP   10.102.60.154   &lt;none&gt;        80/TCP    8h</span><br></pre></td></tr></table></figure><h3 id="4、部署-Ingress"><a href="#4、部署-Ingress" class="headerlink" title="4、部署 Ingress"></a>4、部署 Ingress</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;从上面可以知道 Ingress 就是个规则，指定哪个域名转发到哪个 Service，所以说首先我们得有个 Service，当然 Service 去哪找这里就不管了；这里默认为已经有了两个可用的 Service，以下以 jenkins、Dashboard 为例<br>&nbsp;&nbsp;&nbsp;&nbsp;先写一个 Ingress 文件，语法格式啥的请参考 官方文档，由于我的 jenkins在kube-ops，Dashboard 在kube-system 这个命名空间，所以要指定 namespace.参考下面实例</p><h4 id="4-1、部署jenkins实例"><a href="#4-1、部署jenkins实例" class="headerlink" title="4.1、部署jenkins实例"></a>4.1、部署jenkins实例</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat jenkins-ingress.yaml</span></span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: jenkins-ingress</span><br><span class="line">  namespace: kube-ops</span><br><span class="line">  annotations:</span><br><span class="line">    kubernetes.io/ingress.class: <span class="string">"nginx"</span></span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: ci.xxlaila.io</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - backend:</span><br><span class="line">          serviceName: jenkins2</span><br><span class="line">          servicePort: 8080</span><br><span class="line"><span class="comment"># kubectl create -f jenkins-ingress.yaml</span></span><br></pre></td></tr></table></figure><p>执行域名解析到ip地址，访问jenkins</p><p><img src="https://img.xxlaila.cn/1566808344775.jpg" alt="img"></p><h4 id="4-2、部署kubernetes-dashboard"><a href="#4-2、部署kubernetes-dashboard" class="headerlink" title="4.2、部署kubernetes-dashboard"></a>4.2、部署kubernetes-dashboard</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat nginx-kubernetes-dashboard.yaml </span></span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: dashboard-ingress</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  annotations:</span><br><span class="line">    nginx.ingress.kubernetes.io/ingress.class: nginx</span><br><span class="line">    nginx.ingress.kubernetes.io/secure-backends: <span class="string">"true"</span></span><br><span class="line">    nginx.ingress.kubernetes.io/ssl-passthrough: <span class="string">"true"</span></span><br><span class="line">spec:</span><br><span class="line">  tls:</span><br><span class="line">  - hosts:</span><br><span class="line">    - k8s.xxlaila.io</span><br><span class="line">    secretName: ingress-secret</span><br><span class="line">  rules:</span><br><span class="line">    - host: dashboard.xxlaila.io</span><br><span class="line">      http:</span><br><span class="line">        paths:</span><br><span class="line">        - path: /</span><br><span class="line">          backend:</span><br><span class="line">            serviceName: kubernetes-dashboard</span><br><span class="line">            servicePort: 443</span><br></pre></td></tr></table></figure><p><img src="https://img.xxlaila.cn/786jg656iojhf0.png" alt="img"></p><h3 id="5、部署-Ingress-TLS"><a href="#5、部署-Ingress-TLS" class="headerlink" title="5、部署 Ingress TLS"></a>5、部署 Ingress TLS</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;上面已经做好了 Ingress，接下来配置TLS ；官方给出的样例很简单，大致步骤就两步：创建一个含有证书的 secret、在 Ingress 开启证书；但是官方的有坑，下面是操作步骤</p><h4 id="5-1、创建证书"><a href="#5-1、创建证书" class="headerlink" title="5.1、创建证书"></a>5.1、创建证书</h4><p>首先第一步当然要有个证书，由于我这个 Ingress 有两个服务域名，所以证书要支持两个域名；生成证书命令如下：</p><ul><li><p>生成CA证书</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># mkdir cert &amp;&amp; cd cert</span></span><br></pre></td></tr></table></figure></li><li><p>编辑 openssl 配置</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cp /etc/pki/tls/openssl.cnf .</span></span><br></pre></td></tr></table></figure></li><li><p>修改主要配置</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># vi openssl.cnf</span></span><br><span class="line">	[req]</span><br><span class="line">	req_extensions = v3_req <span class="comment"># 这行默认注释关着的 把注释删掉</span></span><br></pre></td></tr></table></figure></li><li><p>增加配置</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># vi openssl.cnf</span></span><br><span class="line">[ v3_req ]</span><br><span class="line">basicConstraints = CA:FALSE</span><br><span class="line">keyUsage = nonRepudiation, digitalSignature, keyEncipherment</span><br><span class="line">subjectAltName = @alt_names</span><br><span class="line">[alt_names]</span><br><span class="line">DNS.1 = dashboard.mritd.me		<span class="comment">#需要增加的域名</span></span><br><span class="line">DNS.2 = kibana.mritd.me</span><br></pre></td></tr></table></figure></li><li><p>生成证书</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># openssl genrsa -out ingress-key.pem 2048</span></span><br><span class="line"><span class="comment"># openssl req -new -key ingress-key.pem -out ingress.csr -subj "/CN=kube-ingress" -config openssl.cnf</span></span><br><span class="line"><span class="comment"># openssl x509 -req -in ingress.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out ingress.pem -days 365 -extensions v3_req -extfile openssl.cnf</span></span><br></pre></td></tr></table></figure></li><li><p>查看生成后的证书</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ls</span></span><br><span class="line">ca-key.pem  ca.pem  ca.srl  ingress-key.pem  ingress.csr  ingress.pem  openssl.cnf</span><br></pre></td></tr></table></figure></li></ul><h4 id="5-2、创建-secret"><a href="#5-2、创建-secret" class="headerlink" title="5.2、创建 secret"></a>5.2、创建 secret</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;创建好证书以后，需要将证书内容放到 secret 中，secret 中全部内容需要 base64 编码，然后注意去掉换行符(变成一行)；以下是我的 secret 样例(上一步中 ingress.pem 是证书crt，ingress-key.pem 是证书的 key)</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># vim ingress-secret.yml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">data:</span><br><span class="line">  tls.crt: MIIDAjCCAeqgAwIBAgIJAIUNBpFKFrg4MA0GCSqGSIb3DQEBCwUAMBIxEDAOBgNVBAMMB2t1YmUtY2EwHhcNMTkwMTI5MTAzMzQ3WhcNMjAwMTI5MTAzMzQ3WjAXMRUwEwYDVQQDDAxrdWJlLWluZ3Jlc3MwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC0rLcpYHdqVjN84vCJYF2l61F+LYuPRczPNWyo8Rba4XpT6MMMqoGqgmI164r4of2klBEMPZ0dm1mJaYnjb1Zq/qzVUlqaednxfXsr6u8Xm0a6l7ep+Yr+XcRISZC9AjgyqlFtgjzbNJauHkHTy0i+jqV2A4SkVUT2whBqF00WEKC6kQLhw4Ab1XBG5aOK2Jz4TZdP+Mw4n3AsihycHgjhFvhGNixKl4mpfHfLvFeKxmBa8ZoWT+3AGgkX186EXhdhsfdYqHeLT2TvwsqbUJI8E8F7nleo5VMifr9KGHxaCJ+ynr/WFX1c+0wYAGmSKsw4CnXh4EE+IvaeQjBz8O2HAgMBAAGjVjBUMAkGA1UdEwQCMAAwCwYDVR0PBAQDAgXgMDoGA1UdEQQzMDGCFmNpLnp4Yy5raW5neHVubGlhbi5jb22CF2s4cy56eGMua2luZ3h1bmxpYW4uY29tMA0GCSqGSIb3DQEBCwUAA4IBAQCShThVXg3Fnkrm82sHowxCEc9UG9uzOY2LbxhVN7mcm8U0cXy3acAXdKWLHUwdZnOxNJytpaBBWb/6KFFKrIekaSK+tSD+oRISJf43c1tbt0QEpplUaDagQ35NANyQY2VHQntDdVK4/NNJULbHNEqsu19vDDvmDFi0aLHvqPFAvlGnEBPgO1Ac297pDR2thHyMCGBzRKTQOJy2q3HKexa1pItHjAVmv/k71HBeTJ1en2tFHLUlR0kEhYYPBeRclVZ1oYWn7THRaBW8NtugdM6mxVNFAWQTq5goP0VcW44lWYiUF6mX/UZa2c+5FwsGWdSwbTqmZj7ptA2QcveUcN2/</span><br><span class="line">  tls.key: MIIEowIBAAKCAQEAtKy3KWB3alYzfOLwiWBdpetRfi2Lj0XMzzVsqPEW2uF6U+jDDKqBqoJiNeuK+KH9pJQRDD2dHZtZiWmJ429Wav6s1VJamnnZ8X17K+rvF5tGupe3qfmK/l3ESEmQvQI4MqpRbYI82zSWrh5B08tIvo6ldgOEpFVE9sIQahdNFhCgupEC4cOAG9VwRuWjitic+E2XT/jMOJ9wLIocnB4I4Rb4RjYsSpeJqXx3y7xXisZgWvGaFk/twBoJF9fOhF4XYbH3WKh3i09k78LKm1CSPBPBe55XqOVTIn6/Shh8Wgifsp6/1hV9XPtMGABpkirMOAp14eBBPiL2nkIwc/DthwIDAQABAoIBACLj97sV1fnDC85iRPFCmtMfzmz/fqP8ZsDdIE6/wBok0OrDWGdpxgCXjT+8bOn23nSZ43DptR2ykmfm6anyJk4jQF0xui16uovYH6ErjWCRq+b8xYsdlanpka4kBr95XkDqgy8Sp43taevWDABKkZG7Gljf9Q2HKfo9H85dEZXg7RKKz77wahcHpZofthNo0s5kkH2ckVpjwh5svM+M/gm8KhZkKayndr1ezEAmndT+K/P4EVuYbpPQyk5A6E1G7Dh9M2TczZZa6oiR5etloDk2sOIYxysIZqAblyMN2IrIO0hf4nGqSmfe0TZMCKzSXoVOV2Qe+ckJ+mSyOPcdbIECgYEA3EMWrrr4oc9R8o35mQ2FDCG0FUyXrCo9SN/CyhScdeUx1IRV00CBwL340H1CwzEpj5g4bj6LGGSeEw2g8qfPoPdzr1yxNnCv43dhaGrWRrSxGL6kGfKmIlbgHTPIuzRkAXGWiCgo5JnzPjQtjmpUTmwmxXwqDih8kpRsBSTsmx0CgYEA0f1ORGtmrj/bRBQU0NF+ztTVFJQbPV3lTHl21Qgd8QcEIcOHqzpI2fmkeGZdyYQXIfODA/X+PFng+Z6J5ubtvplN5jPzpQuQRtIZ47NRmtgn4DoH+GAqxIb2hFbwXpXuSR/LelFtzxb91nGiVKpdizvuwnitgOuAIbgGHYbgpfMCgYAvvA5fYb/eeWq+EUzFgauS3H8FmqrIMgNEFtJFL0BVQI2TC/b5qGI2XjVdIbhlSvNB3nBkXAOTDsM/R9XYoMubi+UzXPg+3x8PQeEHWxgDDMfQoAg6Y17j1EYPrhhTkeAWfAJukZ2DJWYU1gQFeD+7Gy8v31/R365XqfjbCIyKdQKBgD+/3tr2oB2WVUK9tfQPJag1BNtSe1KOBubImULjS/O4ZZC6g51//E3wc/X5Xc+nwj4UZ1n0fFJmFt6xOrxWryaF9BhG/VjFwe8+KY3vCn8v0CtKctD8oP842e4jVqXgbo7UkDl6LxQHrthDdzys2+lBMKLpcAMLe8LA01pzcA/xAoGBAK3KXGXz0AsM+5FBbUFVZFOROUQLcd+N8esHnSqD8i7/J0PJsxm9irutuLht0cYK7Fcq65fYmZ4lrJO7bpiBRzONR78lMgm/rcKfCCNr4o4n6almvuFxA+jGgEM2W7iYb9pW+FFOqIOjl0cSHy2hsN7seGv3JBcz5StRjtwuSWf6</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  name: ingress-secret</span><br><span class="line">  namespace: kube-system</span><br><span class="line"><span class="built_in">type</span>: Opaque</span><br></pre></td></tr></table></figure><ul><li>创建完成后 create<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl create -f ingress-secret.yml</span></span><br><span class="line">secret/ingress-secret created</span><br></pre></td></tr></table></figure></li></ul><h4 id="5-3、快速创建"><a href="#5-3、快速创建" class="headerlink" title="5.3、快速创建"></a>5.3、快速创建</h4><p>5.2步骤可以简化创建，可以执行一条命令进行创建，</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl create secret tls ingress-secret --key cert/ingress-key.pem --cert cert/ingress.pem</span></span><br></pre></td></tr></table></figure><h4 id="5-4、重新部署-Ingress"><a href="#5-4、重新部署-Ingress" class="headerlink" title="5.4、重新部署 Ingress"></a>5.4、重新部署 Ingress</h4><p>在tls生成完成后，需要重新部署Ingress，让Ingress能够家在tls。修改配置文件</p><h5 id="5-4-1、jenkins-tls"><a href="#5-4-1、jenkins-tls" class="headerlink" title="5.4.1、jenkins tls"></a>5.4.1、jenkins tls</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># vi jenkins-ingress.yaml</span></span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: jenkins-ingress</span><br><span class="line">  namespace: kube-ops</span><br><span class="line">  annotations:</span><br><span class="line">    kubernetes.io/ingress.class: <span class="string">"nginx"</span></span><br><span class="line">spec:</span><br><span class="line">  tls:</span><br><span class="line">  - hosts:</span><br><span class="line">    - ci.xxlaila.io</span><br><span class="line">    secretName: ingress-secret</span><br><span class="line">  rules:</span><br><span class="line">  - host: ci.xxlaila.io</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - backend:</span><br><span class="line">          serviceName: jenkins2</span><br><span class="line">          servicePort: 8080</span><br><span class="line"><span class="comment"># kubect create -f jenkins-ingress.yaml</span></span><br></pre></td></tr></table></figure><p>访问jenkins域名，这里输入http访问会强制跳转到https<br><img src="https://img.xxlaila.cn/1566808668453.jpg" alt="img"><br><img src="https://img.xxlaila.cn/1566808711171.jpg" alt="img"></p><h5 id="5-4-2、kubernetes-dashboard-tls"><a href="#5-4-2、kubernetes-dashboard-tls" class="headerlink" title="5.4.2、kubernetes dashboard tls"></a>5.4.2、kubernetes dashboard tls</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># vim nginx-kubernetes-dashboard.yaml </span></span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: dashboard-ingress</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  annotations:</span><br><span class="line">    kubernetes.io/ingress.class: <span class="string">"nginx"</span></span><br><span class="line">spec:</span><br><span class="line">  tls:</span><br><span class="line">  - hosts:</span><br><span class="line">    - k8s.xxlaila.io</span><br><span class="line">    secretName: ingress-secret</span><br><span class="line">  rules:</span><br><span class="line">  - host: k8s.xxlaila.io</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - backend:</span><br><span class="line">          serviceName: kubernetes-dashboard</span><br><span class="line">          servicePort: 80</span><br><span class="line"><span class="comment"># kubectl create -f nginx-kubernetes-dashboard.yaml</span></span><br></pre></td></tr></table></figure><h3 id="6、ingress-高级用法"><a href="#6、ingress-高级用法" class="headerlink" title="6、ingress 高级用法"></a>6、ingress 高级用法</h3><p><img src="https://img.xxlaila.cn/34324kjsdfh8234ks.png" alt="img"></p><ul><li>lvs 反向代理到 物理nginx。完成https拆包，继承nginx所有功能</li><li>nginx 反向代理到ingress-control。 ingress-control 有两种部署方式 。<ul><li>ingress-control 使用nodePort 方式暴漏服务</li><li>ingress-control 使用hostNetwork 方式暴漏服务</li></ul></li></ul><h3 id="7、总结分析"><a href="#7、总结分析" class="headerlink" title="7、总结分析"></a>7、总结分析</h3><ul><li>ingress-control 在自己的所属的namespace=ingress, 是可以夸不同namespace提供反向代理服.</li><li>如果需要提供夸NS 访问ingress，先给 ingress-control创建RBAC</li><li>ingress-control 使用hostnetwork 模式 性能比使用service nodePort 性能好很多。因为hostnetwork 是直接获取pod 的IP？</li></ul><!-- rebuild by neat -->]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>Ingress</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s部署Weave Scope</title>
    <url>/2019/08/26/k8s%E9%83%A8%E7%BD%B2Weave-Scope/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><h3 id="1、Weave-Scope介绍"><a href="#1、Weave-Scope介绍" class="headerlink" title="1、Weave Scope介绍"></a>1、Weave Scope介绍</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;weave scope 是Docker和Kubernetes的故障排除和监控，自动生成应用程序的地图，能够直观地了解，监控和控制基于容器的，基于微服务的应用程序。可以实时的了解docker容器，选择容器基础架构的概述，或关注特定的微服务。轻松识别和纠正问题，确保集装箱化应用的稳定性和性能，查看容器的上下文指标，标记和元数据。轻松地在容器内的进程之间导航，以运行容器，在可扩展的可排序表中进行排列。使用给定主机或服务的最大CPU或内存轻松找到容器。直接与您的容器交互：暂停，重新启动和停止容器。启动命令行。全部不离开范围浏览器窗口。</p><p><img src="https://img.xxlaila.cn/378246bsjfhsajkdq.png" alt="img"></p><h3 id="2、部署weave-scope"><a href="#2、部署weave-scope" class="headerlink" title="2、部署weave scope"></a>2、部署weave scope</h3><p>初次学习，直接下载官方配置文件，没有经过任何修改，不过相信自己随着学习的进步，会逐渐深入。<a href="https://github.com/weaveworks/scope" target="_blank" rel="noopener">github地址</a>，<a href="https://www.weave.works/docs/cloud/latest/overview/" target="_blank" rel="noopener">官方地址</a></p><a id="more"></a><h4 id="2-1、下载配置文件"><a href="#2-1、下载配置文件" class="headerlink" title="2.1、下载配置文件"></a>2.1、下载配置文件</h4><p><a href="https://github.com/weaveworks/scope/tree/master/examples/k8s" target="_blank" rel="noopener">下载配置文件</a>,配置文件如下列表:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ls -l</span></span><br><span class="line">cluster-role-binding.yaml</span><br><span class="line">cluster-role.yaml</span><br><span class="line">deploy.yaml</span><br><span class="line">ds.yaml</span><br><span class="line">ns.yaml</span><br><span class="line">psp.yaml</span><br><span class="line">sa.yaml</span><br><span class="line">scope.yaml</span><br><span class="line">svc.yaml</span><br></pre></td></tr></table></figure><ul><li>执行创建<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl create -f ./</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="2-2、查看部署"><a href="#2-2、查看部署" class="headerlink" title="2.2、查看部署"></a>2.2、查看部署</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get svc,pods -n weave</span></span><br><span class="line">NAME                      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">service/weave-scope-app   ClusterIP   10.99.252.207   &lt;none&gt;        80/TCP    25m</span><br><span class="line"></span><br><span class="line">NAME                                   READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod/weave-scope-agent-gpwmw            1/1     Running   0          24m</span><br><span class="line">pod/weave-scope-agent-lmf22            1/1     Running   0          24m</span><br><span class="line">pod/weave-scope-agent-r8vft            1/1     Running   0          24m</span><br><span class="line">pod/weave-scope-agent-rph5p            1/1     Running   0          24m</span><br><span class="line">pod/weave-scope-agent-zfrnc            1/1     Running   0          24m</span><br><span class="line">pod/weave-scope-app-5c46dd7467-s8cp8   1/1     Running   0          24m</span><br></pre></td></tr></table></figure><p><img src="https://img.xxlaila.cn/3489hdnsjkhd8324sd.png" alt="img"></p><h4 id="2-3、查看服务状态"><a href="#2-3、查看服务状态" class="headerlink" title="2.3、查看服务状态"></a>2.3、查看服务状态</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get services -n weave</span></span><br><span class="line">NAME              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">weave-scope-app   ClusterIP   10.99.252.207   &lt;none&gt;        80/TCP    27m</span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;到这里weave-scope部署完成，但是我们需要进行访问，这里我们通过之前学习的nginx Ingress来对scope配置一个域名，然后吧域名解析到制定的ip地址上进行访问</p><h3 id="3、配置scope域名"><a href="#3、配置scope域名" class="headerlink" title="3、配置scope域名"></a>3、配置scope域名</h3><p>新建立一个yaml文件，如下</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat scope-ingress.yaml</span></span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: scope-ingress</span><br><span class="line">  namespace: weave</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">    - host: scope.xxlaila.io</span><br><span class="line">      http:</span><br><span class="line">        paths:</span><br><span class="line">        - path: /</span><br><span class="line">          backend:</span><br><span class="line">            serviceName: weave-scope-app</span><br><span class="line">            servicePort: 80</span><br><span class="line"><span class="comment"># kubectl create -f scope-ingress.yaml</span></span><br></pre></td></tr></table></figure><blockquote><p>通过域名访问：<a href="http://scope.xxlaila.io" target="_blank" rel="noopener">http://scope.xxlaila.io</a></p></blockquote><p><img src="https://img.xxlaila.cn/287346jskbdjy784kjs.png" alt="img"><br><img src="https://img.xxlaila.cn/3o4wndk9234sd.png" alt="img"><br><img src="https://img.xxlaila.cn/34873i24dfsd.png" alt="img"><br><img src="https://img.xxlaila.cn/458dskjfhu2y4skdsds.png" alt="img"></p><!-- rebuild by neat -->]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>Weave Scope</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s部署zookeeper集群</title>
    <url>/2019/08/24/k8s%E9%83%A8%E7%BD%B2zookeeper%E9%9B%86%E7%BE%A4/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><p>zk属于有状态服务，需要连接外部存储，吧数据存放在数据盘里面，否则容器挂了，数据没有了</p><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><p>准备zk的yaml文件</p><a id="more"></a><h3 id="1、配置zk-data文件"><a href="#1、配置zk-data文件" class="headerlink" title="1、配置zk-data文件"></a>1、配置zk-data文件</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat zk-data.yaml</span></span><br><span class="line">kind: PersistentVolume</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: k8s-pv-zk1</span><br><span class="line">  annotations:</span><br><span class="line">    volume.beta.kubernetes.io/storage-class: <span class="string">"anything"</span></span><br><span class="line">  labels:</span><br><span class="line">    <span class="built_in">type</span>: <span class="built_in">local</span></span><br><span class="line">spec:</span><br><span class="line">  capacity:</span><br><span class="line">    storage: 3Gi</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteOnce</span><br><span class="line">  hostPath:</span><br><span class="line">    path: <span class="string">"/var/lib/zookeeper"</span></span><br><span class="line">  persistentVolumeReclaimPolicy: Recycle</span><br><span class="line">---</span><br><span class="line">kind: PersistentVolume</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: k8s-pv-zk2</span><br><span class="line">  annotations:</span><br><span class="line">    volume.beta.kubernetes.io/storage-class: <span class="string">"anything"</span></span><br><span class="line">  labels:</span><br><span class="line">    <span class="built_in">type</span>: <span class="built_in">local</span></span><br><span class="line">spec:</span><br><span class="line">  capacity:</span><br><span class="line">    storage: 3Gi</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteOnce</span><br><span class="line">  hostPath:</span><br><span class="line">    path: <span class="string">"/var/lib/zookeeper"</span></span><br><span class="line">  persistentVolumeReclaimPolicy: Recycle</span><br><span class="line">---</span><br><span class="line">kind: PersistentVolume</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: k8s-pv-zk3</span><br><span class="line">  annotations:</span><br><span class="line">    volume.beta.kubernetes.io/storage-class: <span class="string">"anything"</span></span><br><span class="line">  labels:</span><br><span class="line">    <span class="built_in">type</span>: <span class="built_in">local</span></span><br><span class="line">spec:</span><br><span class="line">  capacity:</span><br><span class="line">    storage: 3Gi</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteOnce</span><br><span class="line">  hostPath:</span><br><span class="line">    path: <span class="string">"/var/lib/zookeeper"</span></span><br><span class="line">  persistentVolumeReclaimPolicy: Recycle</span><br><span class="line"><span class="comment"># cat zookeeper.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: zk-hs</span><br><span class="line">  labels:</span><br><span class="line">    app: zk</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - port: 2888</span><br><span class="line">    name: server</span><br><span class="line">  - port: 3888</span><br><span class="line">    name: leader-election</span><br><span class="line">  clusterIP: None</span><br><span class="line">  selector:</span><br><span class="line">    app: zk</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: zk-cs</span><br><span class="line">  labels:</span><br><span class="line">    app: zk</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - port: 2181</span><br><span class="line">    name: client</span><br><span class="line">  selector:</span><br><span class="line">    app: zk</span><br><span class="line">---</span><br><span class="line">apiVersion: policy/v1beta1</span><br><span class="line">kind: PodDisruptionBudget</span><br><span class="line">metadata:</span><br><span class="line">  name: zk-pdb</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: zk</span><br><span class="line">  maxUnavailable: 1</span><br><span class="line">---</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: StatefulSet</span><br><span class="line">metadata:</span><br><span class="line">  name: zk</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: zk</span><br><span class="line">  serviceName: zk-hs</span><br><span class="line">  replicas: 3</span><br><span class="line">  updateStrategy:</span><br><span class="line">    <span class="built_in">type</span>: RollingUpdate</span><br><span class="line">  podManagementPolicy: Parallel</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: zk</span><br><span class="line">    spec:</span><br><span class="line">      affinity:</span><br><span class="line">        podAntiAffinity:</span><br><span class="line">          requiredDuringSchedulingIgnoredDuringExecution:</span><br><span class="line">            - labelSelector:</span><br><span class="line">                matchExpressions:</span><br><span class="line">                  - key: <span class="string">"app"</span></span><br><span class="line">                    operator: In</span><br><span class="line">                    values:</span><br><span class="line">                    - zk</span><br><span class="line">              topologyKey: <span class="string">"kubernetes.io/hostname"</span></span><br><span class="line">      containers:</span><br><span class="line">      - name: kubernetes-zookeeper</span><br><span class="line">        imagePullPolicy: Always</span><br><span class="line">        image: <span class="string">"xxlaila/kubernetes-zookeeper:1.0-3.4.10"</span></span><br><span class="line">        resources:</span><br><span class="line">          requests:</span><br><span class="line">            memory: <span class="string">"1Gi"</span></span><br><span class="line">            cpu: <span class="string">"0.5"</span></span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 2181</span><br><span class="line">          name: client</span><br><span class="line">        - containerPort: 2888</span><br><span class="line">          name: server</span><br><span class="line">        - containerPort: 3888</span><br><span class="line">          name: leader-election</span><br><span class="line">        <span class="built_in">command</span>:</span><br><span class="line">        - sh</span><br><span class="line">        - -c</span><br><span class="line">        - <span class="string">"start-zookeeper \</span></span><br><span class="line"><span class="string">          --servers=3 \</span></span><br><span class="line"><span class="string">          --data_dir=/var/lib/zookeeper/data \</span></span><br><span class="line"><span class="string">          --data_log_dir=/var/lib/zookeeper/data/log \</span></span><br><span class="line"><span class="string">          --conf_dir=/opt/zookeeper/conf \</span></span><br><span class="line"><span class="string">          --client_port=2181 \</span></span><br><span class="line"><span class="string">          --election_port=3888 \</span></span><br><span class="line"><span class="string">          --server_port=2888 \</span></span><br><span class="line"><span class="string">          --tick_time=2000 \</span></span><br><span class="line"><span class="string">          --init_limit=10 \</span></span><br><span class="line"><span class="string">          --sync_limit=5 \</span></span><br><span class="line"><span class="string">          --heap=512M \</span></span><br><span class="line"><span class="string">          --max_client_cnxns=60 \</span></span><br><span class="line"><span class="string">          --snap_retain_count=3 \</span></span><br><span class="line"><span class="string">          --purge_interval=12 \</span></span><br><span class="line"><span class="string">          --max_session_timeout=40000 \</span></span><br><span class="line"><span class="string">          --min_session_timeout=4000 \</span></span><br><span class="line"><span class="string">          --log_level=INFO"</span></span><br><span class="line">        readinessProbe:</span><br><span class="line">          <span class="built_in">exec</span>:</span><br><span class="line">            <span class="built_in">command</span>:</span><br><span class="line">            - sh</span><br><span class="line">            - -c</span><br><span class="line">            - <span class="string">"zookeeper-ready 2181"</span></span><br><span class="line">          initialDelaySeconds: 10</span><br><span class="line">          timeoutSeconds: 5</span><br><span class="line">        livenessProbe:</span><br><span class="line">          <span class="built_in">exec</span>:</span><br><span class="line">            <span class="built_in">command</span>:</span><br><span class="line">            - sh</span><br><span class="line">            - -c</span><br><span class="line">            - <span class="string">"zookeeper-ready 2181"</span></span><br><span class="line">          initialDelaySeconds: 10</span><br><span class="line">          timeoutSeconds: 5</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: datadir</span><br><span class="line">          mountPath: /var/lib/zookeeper</span><br><span class="line">      securityContext:</span><br><span class="line">        runAsUser: 1000</span><br><span class="line">        fsGroup: 1000</span><br><span class="line">  volumeClaimTemplates:</span><br><span class="line">  - metadata:</span><br><span class="line">      name: datadir</span><br><span class="line">      annotations:</span><br><span class="line">        volume.beta.kubernetes.io/storage-class: <span class="string">"managed-nfs-storage"</span></span><br><span class="line">    spec:</span><br><span class="line">      accessModes: [ <span class="string">"ReadWriteOnce"</span> ]</span><br><span class="line">      resources:</span><br><span class="line">        requests:</span><br><span class="line">          storage: 3Gi</span><br></pre></td></tr></table></figure><h3 id="2、执行部署"><a href="#2、执行部署" class="headerlink" title="2、执行部署"></a>2、执行部署</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl create -f zookeeper.yaml -n kube-dev</span></span><br></pre></td></tr></table></figure><h3 id="3、查看部署"><a href="#3、查看部署" class="headerlink" title="3、查看部署"></a>3、查看部署</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get pod -o wide -n kube-dev</span></span><br><span class="line">NAME                            READY   STATUS    RESTARTS   AGE     IP            NODE            NOMINATED NODE   READINESS GATES</span><br><span class="line">zk-0                            1/1     Running   0          6m13s   10.254.62.4   172.21.17.15    &lt;none&gt;           &lt;none&gt;</span><br><span class="line">zk-1                            1/1     Running   0          6m12s   10.254.21.4   172.21.16.96    &lt;none&gt;           &lt;none&gt;</span><br><span class="line">zk-2                            1/1     Running   0          6m12s   10.254.96.4   172.21.16.193   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><h3 id="4、查看持久卷申明"><a href="#4、查看持久卷申明" class="headerlink" title="4、查看持久卷申明"></a>4、查看持久卷申明</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get pv -o wide -n kube-dev</span></span><br><span class="line">NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                             STORAGECLASS          REASON   AGE</span><br><span class="line">pvc-d1cb6a1c-407d-11e9-9436-fa163e14c5bd   3Gi        RWO            Delete           Bound    kube-dev/datadir-zk-0             managed-nfs-storage            6m18s</span><br><span class="line">pvc-d20a95ec-407d-11e9-9436-fa163e14c5bd   3Gi        RWO            Delete           Bound    kube-dev/datadir-zk-1             managed-nfs-storage            6m18s</span><br><span class="line">pvc-d23577af-407d-11e9-9436-fa163e14c5bd   3Gi        RWO            Delete           Bound    kube-dev/datadir-zk-2             managed-nfs-storage            6m23s</span><br></pre></td></tr></table></figure><h3 id="5、查看pvc"><a href="#5、查看pvc" class="headerlink" title="5、查看pvc"></a>5、查看pvc</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get pvc -o wide -n kube-dev</span></span><br><span class="line">NAME                     STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS          AGE</span><br><span class="line">datadir-zk-0             Bound    pvc-d1cb6a1c-407d-11e9-9436-fa163e14c5bd   3Gi        RWO            managed-nfs-storage   6m38s</span><br><span class="line">datadir-zk-1             Bound    pvc-d20a95ec-407d-11e9-9436-fa163e14c5bd   3Gi        RWO            managed-nfs-storage   6m37s</span><br><span class="line">datadir-zk-2             Bound    pvc-d23577af-407d-11e9-9436-fa163e14c5bd   3Gi        RWO            managed-nfs-storage   6m37s</span><br></pre></td></tr></table></figure><h3 id="6、验证集群是否工作正常"><a href="#6、验证集群是否工作正常" class="headerlink" title="6、验证集群是否工作正常"></a>6、验证集群是否工作正常</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># for i in 0 1 2; do kubectl exec zk-$i zkServer.sh status -n kube-dev; done</span></span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /usr/bin/../etc/zookeeper/zoo.cfg</span><br><span class="line">Mode: follower</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /usr/bin/../etc/zookeeper/zoo.cfg</span><br><span class="line">Mode: leader</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /usr/bin/../etc/zookeeper/zoo.cfg</span><br><span class="line">Mode: follower</span><br></pre></td></tr></table></figure><h3 id="7、集群的访问地址"><a href="#7、集群的访问地址" class="headerlink" title="7、集群的访问地址"></a>7、集群的访问地址</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">server.1=zk-0.zk-hs.kube-dev.svc.cluster.local.:2888:3888</span><br><span class="line">server.2=zk-1.zk-hs.kube-dev.svc.cluster.local.:2888:3888</span><br><span class="line">server.3=zk-2.zk-hs.kube-dev.svc.cluster.local.:2888:3888</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s部署coredns</title>
    <url>/2019/08/24/k8s%E9%83%A8%E7%BD%B2coredns/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><p>&nbsp;&nbsp;&nbsp;&nbsp;k8s集群中的应用通常是通过ingress实现微服务发布的，前文介绍过在K8S集群中使用traefik实现服务的自动发布，其实现方式是traefik通过集群的DNS服务来解析service对应的集群地址（clusterip），从而将用户的访问请求转发到集群地址上。因此，在部署完集群后的第一件事情应该是配置DNS服务，目前可选的方案有skydns, kube-dns, coredns。<br>&nbsp;&nbsp;&nbsp;&nbsp;kubedns是Kubernetes中的一个内置插件，目前作为一个独立的开源项目维护，见<a href="https://github.com/kubernetes/dns。该DNS服务器利用SkyDNS的库来为Kubernetes" target="_blank" rel="noopener">https://github.com/kubernetes/dns。该DNS服务器利用SkyDNS的库来为Kubernetes</a> pod和服务提供DNS请求。CoreDNS项目是SkyDNS2的作者，Miek Gieben采用更模块化，可扩展的框架构建,将此DNS服务器作为KubeDNS的替代品。CoreDNS作为CNCF中的托管的一个项目，在Kuberentes1.9版本中，使用kubeadm方式安装的集群可以通过以下命令直接安装CoreDNS。kubeadm init –feature-gates=CoreDNS=true</p><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><p>准备coredns的<a href="https://github.com/coredns/deployment.git" target="_blank" rel="noopener">yaml文件</a></p><a id="more"></a><p>首先我们的查看<code>cat /etc/kubernetes/kubelet</code> dns的ip地址是多少，这里我的是<code>10.254.0.2</code>，根据自己的情况进行修改</p><ul><li><p>执行创建</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ./deploy.sh -i 10.254.0.2 | kubectl apply -f -</span></span><br><span class="line">serviceaccount/coredns created</span><br><span class="line">clusterrole.rbac.authorization.k8s.io/system:coredns created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/system:coredns created</span><br><span class="line">configmap/coredns created</span><br><span class="line">deployment.apps/coredns created</span><br><span class="line">service/kube-dns created</span><br></pre></td></tr></table></figure></li><li><p>擦看coredns信息</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get pod,svc,deployment,rc -n kube-system|grep dns</span></span><br><span class="line">pod/coredns-799775f9b6-mgdc9                1/1     Running   0          12m</span><br><span class="line">pod/coredns-799775f9b6-v95lp                1/1     Running   0          12m</span><br><span class="line">service/kube-dns               ClusterIP   10.254.0.2       &lt;none&gt;        53/UDP,53/TCP,9153/TCP   12m</span><br><span class="line"></span><br><span class="line">deployment.extensions/coredns                2/2     2            2           12m</span><br></pre></td></tr></table></figure></li></ul><h3 id="部署-DNS-自动扩容"><a href="#部署-DNS-自动扩容" class="headerlink" title="部署 DNS 自动扩容"></a>部署 DNS 自动扩容</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;在大规模集群的情况下，可能需要集群 DNS 自动扩容，具体文档请参考 DNS Horizontal Autoscaler，DNS 扩容算法可参考 Github，如有需要请自行修改；以下为具体配置</p><ul><li><p>dns-horizontal-autoscaler.yaml</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kind: ServiceAccount</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: kube-dns-autoscaler</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    addonmanager.kubernetes.io/mode: Reconcile</span><br><span class="line">---</span><br><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: system:kube-dns-autoscaler</span><br><span class="line">  labels:</span><br><span class="line">    addonmanager.kubernetes.io/mode: Reconcile</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups: [<span class="string">""</span>]</span><br><span class="line">    resources: [<span class="string">"nodes"</span>]</span><br><span class="line">    verbs: [<span class="string">"list"</span>]</span><br><span class="line">  - apiGroups: [<span class="string">""</span>]</span><br><span class="line">    resources: [<span class="string">"replicationcontrollers/scale"</span>]</span><br><span class="line">    verbs: [<span class="string">"get"</span>, <span class="string">"update"</span>]</span><br><span class="line">  - apiGroups: [<span class="string">"extensions"</span>]</span><br><span class="line">    resources: [<span class="string">"deployments/scale"</span>, <span class="string">"replicasets/scale"</span>]</span><br><span class="line">    verbs: [<span class="string">"get"</span>, <span class="string">"update"</span>]</span><br><span class="line"><span class="comment"># Remove the configmaps rule once below issue is fixed:</span></span><br><span class="line"><span class="comment"># kubernetes-incubator/cluster-proportional-autoscaler#16</span></span><br><span class="line">  - apiGroups: [<span class="string">""</span>]</span><br><span class="line">    resources: [<span class="string">"configmaps"</span>]</span><br><span class="line">    verbs: [<span class="string">"get"</span>, <span class="string">"create"</span>]</span><br><span class="line">---</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: system:kube-dns-autoscaler</span><br><span class="line">  labels:</span><br><span class="line">    addonmanager.kubernetes.io/mode: Reconcile</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: kube-dns-autoscaler</span><br><span class="line">    namespace: kube-system</span><br><span class="line">roleRef:</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: system:kube-dns-autoscaler</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: kube-dns-autoscaler</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: kube-dns-autoscaler</span><br><span class="line">    kubernetes.io/cluster-service: <span class="string">"true"</span></span><br><span class="line">    addonmanager.kubernetes.io/mode: Reconcile</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      k8s-app: kube-dns-autoscaler</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        k8s-app: kube-dns-autoscaler</span><br><span class="line">      annotations:</span><br><span class="line">        scheduler.alpha.kubernetes.io/critical-pod: <span class="string">''</span></span><br><span class="line">    spec:</span><br><span class="line">      priorityClassName: system-cluster-critical</span><br><span class="line">      containers:</span><br><span class="line">      - name: autoscaler</span><br><span class="line">        image: gcr.azk8s.cn/google_containers/cluster-proportional-autoscaler-amd64:1.1.2-r2</span><br><span class="line">        resources:</span><br><span class="line">            requests:</span><br><span class="line">                cpu: <span class="string">"20m"</span></span><br><span class="line">                memory: <span class="string">"10Mi"</span></span><br><span class="line">        <span class="built_in">command</span>:</span><br><span class="line">          - /cluster-proportional-autoscaler</span><br><span class="line">          - --namespace=kube-system</span><br><span class="line">          - --configmap=kube-dns-autoscaler</span><br><span class="line">          <span class="comment"># Should keep target in sync with cluster/addons/dns/kube-dns.yaml.base</span></span><br><span class="line">          - --target=Deployment/coredns</span><br><span class="line">          <span class="comment"># When cluster is using large nodes(with more cores), "coresPerReplica" should dominate.</span></span><br><span class="line">          <span class="comment"># If using small nodes, "nodesPerReplica" should dominate.</span></span><br><span class="line">          - --default-params=&#123;<span class="string">"linear"</span>:&#123;<span class="string">"coresPerReplica"</span>:256,<span class="string">"nodesPerReplica"</span>:16,<span class="string">"preventSinglePointFailure"</span>:<span class="literal">true</span>&#125;&#125;</span><br><span class="line">          - --logtostderr=<span class="literal">true</span></span><br><span class="line">          - --v=2</span><br><span class="line">      tolerations:</span><br><span class="line">      - key: <span class="string">"CriticalAddonsOnly"</span></span><br><span class="line">        operator: <span class="string">"Exists"</span></span><br><span class="line">      serviceAccountName: kube-dns-autoscaler</span><br></pre></td></tr></table></figure></li><li><p>执行创建</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl apply -f dns-horizontal-autoscaler.yaml </span></span><br><span class="line">serviceaccount/kube-dns-autoscaler created</span><br><span class="line">clusterrole.rbac.authorization.k8s.io/system:kube-dns-autoscaler created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/system:kube-dns-autoscaler created</span><br><span class="line">deployment.apps/kube-dns-autoscaler created</span><br></pre></td></tr></table></figure></li></ul><p>执行创建以后我们可以看到pod 创建了两个dns</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get pods -n kube-system|grep coredns</span></span><br><span class="line">coredns-68676b6b88-pw9c2                1/1     Running   0          7m18s</span><br><span class="line">coredns-68676b6b88-tgbbv                1/1     Running   0          100m</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>coredns</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s部署mysql</title>
    <url>/2019/08/24/k8s%E9%83%A8%E7%BD%B2mysql/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><p>&nbsp;&nbsp;&nbsp;&nbsp;后端存储利用nfs来进行存储数据，nfs安装不阐述，需要注意注意的是在创建mysql 的共享目录的时候参数设定<code>/data/mysql *(rw,sync,no_root_squash,no_subtree_check)</code></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ sudo systemctl  restart nfs.service</span><br><span class="line">$ sudo exportfs -arv</span><br></pre></td></tr></table></figure><h3 id="1、创建mysql存储"><a href="#1、创建mysql存储" class="headerlink" title="1、创建mysql存储"></a>1、创建mysql存储</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat mysql-pvc.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolume</span><br><span class="line">metadata:</span><br><span class="line">  name: pvc001</span><br><span class="line">  namespace: kube-ops</span><br><span class="line">spec:</span><br><span class="line">  capacity:</span><br><span class="line">    storage: 10Gi</span><br><span class="line">  accessModes:</span><br><span class="line">  - ReadWriteMany</span><br><span class="line">  persistentVolumeReclaimPolicy: Delete</span><br><span class="line">  nfs:</span><br><span class="line">    server: 172.21.16.240</span><br><span class="line">    path: /data/mysql</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: mysql-pvc</span><br><span class="line">  namespace: kube-ops</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteMany</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 10Gi</span><br></pre></td></tr></table></figure><a id="more"></a><h3 id="2、部署mysql"><a href="#2、部署mysql" class="headerlink" title="2、部署mysql"></a>2、部署mysql</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat mysql-deploy.yaml</span></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: mysql-deploy</span><br><span class="line">  namespace: kube-ops</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      name: mysql-ops</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels: </span><br><span class="line">        name: mysql-ops</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">        - name: mysql</span><br><span class="line">          image: mysql:8.0.12</span><br><span class="line">          imagePullPolicy: IfNotPresent</span><br><span class="line">          env:</span><br><span class="line">          - name: MYSQL_ROOT_PASSWORD</span><br><span class="line">            value: <span class="string">"noc-mysql"</span></span><br><span class="line">          ports:</span><br><span class="line">            - containerPort: 3306</span><br><span class="line">          volumeMounts:</span><br><span class="line">            - name: mysql-persistent-storage</span><br><span class="line">              mountPath: <span class="string">"/var/lib/mysql"</span></span><br><span class="line">      volumes:</span><br><span class="line">        - name: mysql-persistent-storage</span><br><span class="line">          persistentVolumeClaim:</span><br><span class="line">            claimName: mysql-pvc</span><br></pre></td></tr></table></figure><h3 id="3、设置端口映射"><a href="#3、设置端口映射" class="headerlink" title="3、设置端口映射"></a>3、设置端口映射</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat mysql-svc.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: mysql-svc</span><br><span class="line">  namespace: kube-ops</span><br><span class="line">  labels: </span><br><span class="line">    name: mysql-svc</span><br><span class="line">spec:</span><br><span class="line">  <span class="built_in">type</span>: NodePort</span><br><span class="line">  ports:</span><br><span class="line">  - port: 3306</span><br><span class="line">    protocol: TCP</span><br><span class="line">    targetPort: 3306</span><br><span class="line">    name: http</span><br><span class="line">    nodePort: 30003</span><br><span class="line">  selector:</span><br><span class="line">    name: mysql-ops</span><br></pre></td></tr></table></figure><h3 id="4、查看pod部署"><a href="#4、查看pod部署" class="headerlink" title="4、查看pod部署"></a>4、查看pod部署</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get pod -n kube-ops</span></span><br></pre></td></tr></table></figure><p><img src="https://img.xxlaila.cn/8364iqhkdaskda.png" alt="img"></p><h3 id="5、查看mysql部署哪个node节点"><a href="#5、查看mysql部署哪个node节点" class="headerlink" title="5、查看mysql部署哪个node节点"></a>5、查看mysql部署哪个node节点</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get pod -o wide -n kube-ops</span></span><br></pre></td></tr></table></figure><p><img src="https://img.xxlaila.cn/42joud02jef24.png" alt="img"></p><h3 id="6、进入mysql容器"><a href="#6、进入mysql容器" class="headerlink" title="6、进入mysql容器"></a>6、进入mysql容器</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ sudo docker ps -a</span><br><span class="line">$ sudo docker <span class="built_in">exec</span> -it ded7f2990db5 /bin/bash</span><br><span class="line">root@mysql-deploy-6dc5d9786b-lgkxk:/<span class="comment"># mysql -h127.0.0.1 -uroot -pnoc-mysql</span></span><br></pre></td></tr></table></figure><h4 id="6-1、设置mysql"><a href="#6-1、设置mysql" class="headerlink" title="6.1、设置mysql"></a>6.1、设置mysql</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; alter user <span class="string">'root'</span>@<span class="string">'%'</span> identified with mysql_native_password by<span class="string">'root'</span>;</span><br><span class="line">mysql&gt; alter  user <span class="string">'root'</span>@<span class="string">'%'</span> identified by <span class="string">'mysql'</span>;</span><br><span class="line">mysql&gt; alter  user <span class="string">'root'</span>@<span class="string">'%'</span> identified by <span class="string">'noc-mysql'</span>;</span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;退出mysql和容器，执行quit;退出mysql，按ctrl+p+q从容器中返回node主机。利用navicat 通过node主机的ip地址和端口30003连接mysql数据库<br><img src="https://img.xxlaila.cn/2348wndssmadklj3.png" alt="img"></p><!-- rebuild by neat -->]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s角色访问RBAC</title>
    <url>/2019/08/24/k8s%E8%A7%92%E8%89%B2%E8%AE%BF%E9%97%AERBAC/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><h3 id="1、rbac介绍"><a href="#1、rbac介绍" class="headerlink" title="1、rbac介绍"></a>1、rbac介绍</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;Kubernetes中的两个用于配置信息的重要资源对象：ConfigMap和Secret，其实到这里我们基本上学习的内容已经覆盖到Kubernetes中一些重要的资源对象了，来部署一个应用程序是完全没有问题的了。在我们演示一个完整的示例之前，我们还需要给大家讲解一个重要的概念：RBAC - 基于角色的访问控制。<br>&nbsp;&nbsp;&nbsp;&nbsp;RBAC使用rbac.authorization.k8s.io API Group 来实现授权决策，允许管理员通过 Kubernetes API 动态配置策略，要启用RBAC，需要在 apiserver 中添加参数–authorization-mode=RBAC，如果使用的kubeadm安装的集群，1.6 版本以上的都默认开启了RBAC，可以通过查看 Master 节点上 apiserver 的静态Pod定义文件：</p><h3 id="2、-kubernetes-关于空间权限赋予"><a href="#2、-kubernetes-关于空间权限赋予" class="headerlink" title="2、 kubernetes 关于空间权限赋予"></a>2、 kubernetes 关于空间权限赋予</h3><h4 id="1、获取并查看"><a href="#1、获取并查看" class="headerlink" title="1、获取并查看"></a>1、获取并查看</h4><ul><li>Role</li><li>ClusterRole</li><li>RoleBinding</li><li>ClusterRoleBinding</li></ul><a id="more"></a><ul><li><p>1.1、查看kube-system namespace下的所有role</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl get role -n kube-system</span><br></pre></td></tr></table></figure></li><li><p>1.2、查看某个role定义的资源权限</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl get role &lt;role-name&gt; -n kube-system -o yaml</span><br></pre></td></tr></table></figure></li><li><p>1.3、查看kube-system namespace下所有的rolebinding</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl get rolebinding -n kube-system</span><br></pre></td></tr></table></figure></li><li><p>1.4、查看kube-system namespace下的某个rolebinding详细信息（绑定的Role和subject）</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl get rolebinding &lt;rolebind-name&gt; -n kube-system -o yaml</span><br></pre></td></tr></table></figure></li><li><p>1.5、查看集群所有的clusterrole</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl get clusterrole</span><br></pre></td></tr></table></figure></li><li><p>1.6、查看某个clusterrole定义的资源权限详细信息</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl get clusterrole &lt;clusterrole-name&gt; -o yaml</span><br></pre></td></tr></table></figure></li><li><p>1.7、查看所有的clusterrolebinding</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl get clusterrolebinding</span><br></pre></td></tr></table></figure></li><li><p>1.8、查看某一clusterrolebinding的详细信息</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl get clusterrolebinding &lt;clusterrolebinding-name&gt; -o yaml</span><br></pre></td></tr></table></figure></li></ul><h4 id="2、kubectl命令可以用于在命名空间内或者整个集群内授予角色。"><a href="#2、kubectl命令可以用于在命名空间内或者整个集群内授予角色。" class="headerlink" title="2、kubectl命令可以用于在命名空间内或者整个集群内授予角色。"></a>2、kubectl命令可以用于在命名空间内或者整个集群内授予角色。</h4><ul><li><p>在某一特定名字空间内授予Role或者ClusterRole。示例如下：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl create rolebinding</span><br></pre></td></tr></table></figure></li><li><p>在名为”acme”的名字空间中将admin ClusterRole授予用户”bob”：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl create rolebinding bob-admin-binding --clusterrole=admin --user=bob --namespace=acme</span><br></pre></td></tr></table></figure></li><li><p>在名为”acme”的名字空间中将view ClusterRole授予服务账户”myapp”：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl create rolebinding myapp-view-binding --clusterrole=view --serviceaccount=acme:myapp --namespace=acme</span><br><span class="line">kubectl create clusterrolebinding</span><br></pre></td></tr></table></figure></li></ul><h4 id="3、在整个集群中授予ClusterRole，包括所有名字空间。示例如下："><a href="#3、在整个集群中授予ClusterRole，包括所有名字空间。示例如下：" class="headerlink" title="3、在整个集群中授予ClusterRole，包括所有名字空间。示例如下："></a>3、在整个集群中授予ClusterRole，包括所有名字空间。示例如下：</h4><ul><li><p>在整个集群范围内将cluster-admin ClusterRole授予用户”root”：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl create clusterrolebinding root-cluster-admin-binding --clusterrole=cluster-admin --user=root</span><br></pre></td></tr></table></figure></li><li><p>在整个集群范围内将system:node ClusterRole授予用户”kubelet”：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl create clusterrolebinding kubelet-node-binding --clusterrole=system:node --user=kubelet</span><br></pre></td></tr></table></figure></li><li><p>在整个集群范围内将view ClusterRole授予名字空间”acme”内的服务账户”myapp”：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl create clusterrolebinding myapp-view-binding --clusterrole=view --serviceaccount=acme:myapp</span><br></pre></td></tr></table></figure></li></ul><h4 id="4、对某一个namespace赋予jenkins部署权限"><a href="#4、对某一个namespace赋予jenkins部署权限" class="headerlink" title="4、对某一个namespace赋予jenkins部署权限"></a>4、对某一个namespace赋予jenkins部署权限</h4><ul><li><p>查看kube-ops 下面的角色</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get role -n kube-ops</span></span><br><span class="line">NAME       AGE</span><br><span class="line">jenkins2   2d6h</span><br></pre></td></tr></table></figure></li><li><p>查看role定义的资源权限</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get role jenkins2 -n kube-ops -o yaml</span></span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: Role</span><br><span class="line">metadata:</span><br><span class="line">  creationTimestamp: <span class="string">"2019-01-14T03:07:25Z"</span></span><br><span class="line">  name: jenkins2</span><br><span class="line">  namespace: kube-ops</span><br><span class="line">  resourceVersion: <span class="string">"2389179"</span></span><br><span class="line">  selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/kube-ops/roles/jenkins2</span><br><span class="line">  uid: 84762132-17a9-11e9-8991-fa163e14c5bd</span><br><span class="line">rules:</span><br><span class="line">- apiGroups:</span><br><span class="line">  - <span class="string">""</span></span><br><span class="line">  resources:</span><br><span class="line">  - pods</span><br><span class="line">  verbs:</span><br><span class="line">  - create</span><br><span class="line">  - delete</span><br><span class="line">  - get</span><br><span class="line">  - list</span><br><span class="line">  - patch</span><br><span class="line">  - update</span><br><span class="line">  - watch</span><br><span class="line">- apiGroups:</span><br><span class="line">  - <span class="string">""</span></span><br><span class="line">  resources:</span><br><span class="line">  - pods/<span class="built_in">exec</span></span><br><span class="line">  verbs:</span><br><span class="line">  - create</span><br><span class="line">  - delete</span><br><span class="line">  - get</span><br><span class="line">  - list</span><br><span class="line">  - patch</span><br><span class="line">  - update</span><br><span class="line">  - watch</span><br><span class="line">- apiGroups:</span><br><span class="line">  - <span class="string">""</span></span><br><span class="line">  resources:</span><br><span class="line">  - pods/<span class="built_in">log</span></span><br><span class="line">  verbs:</span><br><span class="line">  - get</span><br><span class="line">  - list</span><br><span class="line">  - watch</span><br><span class="line">- apiGroups:</span><br><span class="line">  - <span class="string">""</span></span><br><span class="line">  resources:</span><br><span class="line">  - secrets</span><br><span class="line">  verbs:</span><br><span class="line">  - get</span><br></pre></td></tr></table></figure></li><li><p>创建jenkins2的权限</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@k8s-zxc-test-3 ~]<span class="comment"># kubectl -n kube-system create sa jenkins2</span></span><br><span class="line">serviceaccount/jenkins2 created</span><br></pre></td></tr></table></figure></li><li><p>授权访问</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@k8s-zxc-test-3 ~]<span class="comment"># kubectl create clusterrolebinding jenkins2 --clusterrole cluster-admin --serviceaccount=kube-ops:jenkins2</span></span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/jenkins2 created</span><br></pre></td></tr></table></figure></li></ul><!-- rebuild by neat -->]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>RBAC</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s heapster</title>
    <url>/2019/08/24/k8s-heapster/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><h3 id="1、heapster-介绍"><a href="#1、heapster-介绍" class="headerlink" title="1、heapster 介绍"></a>1、heapster 介绍</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;Heapster是容器集群监控和性能分析工具,支持Kubernetes和CoreOS。<br>&nbsp;&nbsp;&nbsp;&nbsp;Kubernetes有个监控agent—cAdvisor。在每个kubernetes Node上都会运行cAdvisor,它会收集本机以及容器的监控数据(cpu,memory,filesystem,network,uptime)。在较新的版本中，K8S已经将cAdvisor功能集成到kubelet组件中。每个Node节点可以直接进行web访问。</p><h3 id="2、heapster-安装"><a href="#2、heapster-安装" class="headerlink" title="2、heapster 安装"></a>2、heapster 安装</h3><p>下载heapster的<a href="https://github.com/kubernetes-retired/heapster/tree/master/deploy/kube-config" target="_blank" rel="noopener">yaml文件</a>，下载完成后我们需要对文件修改，以满足我们的的需求.</p><h4 id="2-1、grafana修改"><a href="#2-1、grafana修改" class="headerlink" title="2.1、grafana修改"></a>2.1、grafana修改</h4><p>grafana添加nodePort: 30003让grafana支持外部访问，我们可以通过这个端口进行但单独的页面配置。</p><a id="more"></a><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat grafana.yaml</span></span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: monitoring-grafana</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        task: monitoring</span><br><span class="line">        k8s-app: grafana</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: grafana</span><br><span class="line">        image: k8s.gcr.io/heapster-grafana-amd64:v5.0.4</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 3000</span><br><span class="line">          protocol: TCP</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - mountPath: /etc/ssl/certs</span><br><span class="line">          name: ca-certificates</span><br><span class="line">          readOnly: <span class="literal">true</span></span><br><span class="line">        - mountPath: /var</span><br><span class="line">          name: grafana-storage</span><br><span class="line">        env:</span><br><span class="line">        - name: INFLUXDB_HOST</span><br><span class="line">          value: monitoring-influxdb</span><br><span class="line">        - name: GF_SERVER_HTTP_PORT</span><br><span class="line">          value: <span class="string">"3000"</span></span><br><span class="line">          <span class="comment"># The following env variables are required to make Grafana accessible via</span></span><br><span class="line">          <span class="comment"># the kubernetes api-server proxy. On production clusters, we recommend</span></span><br><span class="line">          <span class="comment"># removing these env variables, setup auth for grafana, and expose the grafana</span></span><br><span class="line">          <span class="comment"># service using a LoadBalancer or a public IP.</span></span><br><span class="line">        - name: GF_AUTH_BASIC_ENABLED</span><br><span class="line">          value: <span class="string">"false"</span></span><br><span class="line">        - name: GF_AUTH_ANONYMOUS_ENABLED</span><br><span class="line">          value: <span class="string">"true"</span></span><br><span class="line">        - name: GF_AUTH_ANONYMOUS_ORG_ROLE</span><br><span class="line">          value: Admin</span><br><span class="line">        - name: GF_SERVER_ROOT_URL</span><br><span class="line">          <span class="comment"># If you're only using the API Server proxy, set this value instead:</span></span><br><span class="line">          <span class="comment"># value: /api/v1/namespaces/kube-system/services/monitoring-grafana/proxy</span></span><br><span class="line">          value: /</span><br><span class="line">      volumes:</span><br><span class="line">      - name: ca-certificates</span><br><span class="line">        hostPath:</span><br><span class="line">          path: /etc/ssl/certs</span><br><span class="line">      - name: grafana-storage</span><br><span class="line">        emptyDir: &#123;&#125;</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    <span class="comment"># For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons)</span></span><br><span class="line">    <span class="comment"># If you are NOT using this as an addon, you should comment out this line.</span></span><br><span class="line">    kubernetes.io/cluster-service: <span class="string">'true'</span></span><br><span class="line">    kubernetes.io/name: monitoring-grafana</span><br><span class="line">  name: monitoring-grafana</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  <span class="comment"># In a production setup, we recommend accessing Grafana through an external Loadbalancer</span></span><br><span class="line">  <span class="comment"># or through a public IP.</span></span><br><span class="line">  <span class="comment"># type: LoadBalancer</span></span><br><span class="line">  <span class="comment"># You could also use NodePort to expose the service at a randomly-generated port</span></span><br><span class="line">  <span class="built_in">type</span>: NodePort</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80</span><br><span class="line">    targetPort: 3000</span><br><span class="line">    nodePort: 30003</span><br><span class="line">  selector:</span><br><span class="line">    k8s-app: grafana</span><br></pre></td></tr></table></figure><h4 id="2-2、heapster文件修改"><a href="#2-2、heapster文件修改" class="headerlink" title="2.2、heapster文件修改"></a>2.2、heapster文件修改</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">- --<span class="built_in">source</span>=kubernetes:https://kubernetes.default?useServiceAccount=<span class="literal">true</span>&amp;kubeletHttps=<span class="literal">true</span>&amp;kubeletPort=10250&amp;insecure=<span class="literal">true</span></span><br><span class="line">连接k8s api的地址，默认是kubernetes.default，后面一段加入用户认证，端口，以及https;</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">- --sink=influxdb:http://monitoring-influxdb.kube-system.svc:8086</span><br><span class="line">指定 influxdb数据库的地址，这个在infuxdb文件里面有这个域名</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat heapster.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: heapster</span><br><span class="line">  namespace: kube-system</span><br><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: heapster</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        task: monitoring</span><br><span class="line">        k8s-app: heapster</span><br><span class="line">    spec:</span><br><span class="line">      serviceAccountName: heapster</span><br><span class="line">      containers:</span><br><span class="line">      - name: heapster</span><br><span class="line">        image: k8s.gcr.io/heapster-amd64:v1.5.4</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        <span class="built_in">command</span>:</span><br><span class="line">        - /heapster</span><br><span class="line">        - --<span class="built_in">source</span>=kubernetes:https://kubernetes.default?useServiceAccount=<span class="literal">true</span>&amp;kubeletHttps=<span class="literal">true</span>&amp;kubeletPort=10250&amp;insecure=<span class="literal">true</span></span><br><span class="line">        - --sink=influxdb:http://monitoring-influxdb.kube-system.svc:8086</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    task: monitoring</span><br><span class="line">    <span class="comment"># For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons)</span></span><br><span class="line">    <span class="comment"># If you are NOT using this as an addon, you should comment out this line.</span></span><br><span class="line">    kubernetes.io/cluster-service: <span class="string">'true'</span></span><br><span class="line">    kubernetes.io/name: Heapster</span><br><span class="line">  name: heapster</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80</span><br><span class="line">    targetPort: 8082</span><br><span class="line">  selector:</span><br><span class="line">    k8s-app: heapster</span><br></pre></td></tr></table></figure><h3 id="3、执行创建heapster"><a href="#3、执行创建heapster" class="headerlink" title="3、执行创建heapster"></a>3、执行创建heapster</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubect create -f ./</span></span><br></pre></td></tr></table></figure><p>在执行创建完成后，等待一会显示图像<br><img src="https://img.xxlaila.cn/459348skndiy923s.png" alt="img"><br><img src="https://img.xxlaila.cn/34293knsdalsk0329.png" alt="img"></p><h4 id="3-1、访问grafana是否正常"><a href="#3-1、访问grafana是否正常" class="headerlink" title="3.1、访问grafana是否正常"></a>3.1、访问grafana是否正常</h4><p>前面在grafana文件里面增加了nodePoer: 30003的端口，我们可以通过任意节点ip:30003进行访问grafana界面。</p><p><img src="https://img.xxlaila.cn/453847sndkniuy234wds.png" alt="img"><br><strong>可以进行配置grafana。</strong></p><!-- rebuild by neat -->]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>heapster</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s部署eureka集群</title>
    <url>/2019/08/24/k8s%E9%83%A8%E7%BD%B2eureka%E9%9B%86%E7%BE%A4/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:23 GMT+0800 (China Standard Time) --><p>eureka 不阐述介绍，这里直接开始在kubernetes下部署eureka集群</p><h3 id="1、配置文件的增加"><a href="#1、配置文件的增加" class="headerlink" title="1、配置文件的增加"></a>1、配置文件的增加</h3><p>eureka 只一个有状态的服务，部署有状态服务我们可以使用StatefulSet</p><h4 id="1-1、增加dockerfile"><a href="#1-1、增加dockerfile" class="headerlink" title="1.1、增加dockerfile"></a>1.1、增加dockerfile</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat Dockerfile</span><br><span class="line">FROM docker.io/xxlaila/centos7.6-jdk1.8:latest</span><br><span class="line">MAINTAINER xxlaila <span class="string">"cq_xxlaila@163.com"</span></span><br><span class="line"><span class="comment"># Install dependent plugin</span></span><br><span class="line"></span><br><span class="line">ADD target/kxl-eureka.jar /opt/webapps/kxl-eureka.jar</span><br><span class="line">ADD application.yaml /opt/webapps/application.yaml</span><br><span class="line"></span><br><span class="line">WORKDIR /opt/webapps</span><br><span class="line"></span><br><span class="line">EXPOSE 8080</span><br><span class="line"></span><br><span class="line">ENTRYPOINT [<span class="string">"java"</span>, <span class="string">"-jar"</span>, <span class="string">"-Dspring.profiles.active=dev"</span>, <span class="string">"kxl-eureka.jar"</span>]</span><br></pre></td></tr></table></figure><a id="more"></a><h4 id="1-2、修改配置文件"><a href="#1-2、修改配置文件" class="headerlink" title="1.2、修改配置文件"></a>1.2、修改配置文件</h4><p>在做eureka集群的时候，application.yaml的配置文件很重要，配置文件做不好，将会直接影响到eureka的启动，还有集群的模式</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat application.yaml</span><br><span class="line">spring:</span><br><span class="line">  application:</span><br><span class="line">    name: kxl-eureka</span><br><span class="line">server:</span><br><span class="line">  port: 8761</span><br><span class="line">eureka:</span><br><span class="line">  instance:</span><br><span class="line"><span class="comment">#    prefer-ip-address: true</span></span><br><span class="line">    hostname: <span class="variable">$&#123;EUREKA_HOST_NAME:peer1&#125;</span> <span class="comment">#服务主机名</span></span><br><span class="line">    appname: <span class="variable">$&#123;spring.application.name&#125;</span>  <span class="comment">#服务名称，默认为 unknow 这里直接取 spring.application.name 了</span></span><br><span class="line">    <span class="comment"># server 从最后一次收到心跳到移除废弃服务的超时时间（秒）</span></span><br><span class="line">    lease-expiration-duration-in-seconds: 90</span><br><span class="line">    <span class="comment"># client 给 server 发送心跳的间隔时间（秒），比 lease-expiration-duration-in-seconds 小</span></span><br><span class="line">    lease-renewal-interval-in-seconds: 30</span><br><span class="line"></span><br><span class="line">  client:</span><br><span class="line">    serviceUrl:</span><br><span class="line">      defaultZone: <span class="variable">$&#123;EUREKA_URL_LIST:http://peer1:8761/eureka/&#125;</span> <span class="comment"># 指定服务中心 eureka server的地址</span></span><br><span class="line">    <span class="comment"># client 是否从eureka上拉取注册信息， server模式可关掉</span></span><br><span class="line">    fetch-registry: <span class="variable">$&#123;BOOL_FETCH:true&#125;</span>   <span class="comment"># 是否拉取 eureka server 的注册信息。 默认为true</span></span><br><span class="line">    <span class="comment"># client 是否注册到eureka上， server模式可关掉</span></span><br><span class="line">    register-with-eureka: <span class="variable">$&#123;BOOL_REGISTER:true&#125;</span>  <span class="comment"># 是否把服务中心本身当做eureka client 注册。默认为true</span></span><br><span class="line">    <span class="comment"># client 间隔多久去拉去服务信息(秒)</span></span><br><span class="line">    registry-fetch-interval-seconds: 30</span><br><span class="line">  server:</span><br><span class="line">    <span class="comment"># 自我保护机制，应对网络闪断情况，大面积丢失过多的client，不删除服务</span></span><br><span class="line">    <span class="built_in">enable</span>-self-preservation: <span class="variable">$&#123;SELF_PRESERVATION:true&#125;</span>     <span class="comment"># 是否开启自我保护。 默认为 true.</span></span><br><span class="line">    <span class="comment"># 每分钟心跳数 实际/期望，如果小于阈值(threshold)，则触发自我保护机制</span></span><br><span class="line">    renewal-percent-threshold: 0.85</span><br><span class="line">    <span class="comment"># 扫描失效服务的间隔时间（毫秒）</span></span><br><span class="line">    eviction-interval-timer-in-ms: 60000</span><br><span class="line"></span><br><span class="line">  application:</span><br><span class="line">    name: <span class="variable">$&#123;EUREKA_APPLICATION_NAME:eureka-server&#125;</span></span><br></pre></td></tr></table></figure><h4 id="1-3、创建-eureka-docker镜像"><a href="#1-3、创建-eureka-docker镜像" class="headerlink" title="1.3、创建 eureka docker镜像"></a>1.3、创建 eureka docker镜像</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker build -t xxlaila/kxl-eureka:v1 .</span><br><span class="line">$ docker push xxlaila/kxl-eureka:v1</span><br></pre></td></tr></table></figure><h3 id="2、在k8s创建eureka集群"><a href="#2、在k8s创建eureka集群" class="headerlink" title="2、在k8s创建eureka集群"></a>2、在k8s创建eureka集群</h3><h4 id="2-1、创建eureka集群"><a href="#2-1、创建eureka集群" class="headerlink" title="2.1、创建eureka集群"></a>2.1、创建eureka集群</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat kxl-eureka.yaml</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: eureka</span><br><span class="line">  namespace: kube-dev</span><br><span class="line">  labels:</span><br><span class="line">    app: eureka</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - port: 8761</span><br><span class="line">    name: eureka</span><br><span class="line">  clusterIP: None</span><br><span class="line">  selector:</span><br><span class="line">    app: eureka</span><br><span class="line">---</span><br><span class="line">apiVersion: apps/v1beta1</span><br><span class="line">kind: StatefulSet</span><br><span class="line">metadata:</span><br><span class="line">  name: eureka</span><br><span class="line">  namespace: kube-dev</span><br><span class="line">spec:</span><br><span class="line">  serviceName: <span class="string">"eureka"</span></span><br><span class="line">  replicas: 3</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: eureka</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: eureka</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: eureka</span><br><span class="line">        image:  docker.io/xxlaila/kxl-eureka:v1</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 8761</span><br><span class="line">        env:</span><br><span class="line">        - name: MY_POD_NAME</span><br><span class="line">          valueFrom:</span><br><span class="line">            fieldRef:</span><br><span class="line">              fieldPath: metadata.name</span><br><span class="line">          <span class="comment"># Due to camelcase issues with "defaultZone" and "preferIpAddress", _JAVA_OPTIONS is used here</span></span><br><span class="line">        - name: eureka_client_serviceUrl_defaultZone</span><br><span class="line">          <span class="comment">#value: http://eureka-0.eureka.&lt;namespace&gt;.svc.cluster.local:8761/eureka/,http://eureka-1.eureka.&lt;namespace&gt;.svc.cluster.local:8761/eureka/</span></span><br><span class="line">          value: http://eureka-0.eureka:8761/eureka/,http://eureka-1.eureka:8761/eureka/,http://eureka-2.eureka:8761/eureka/</span><br><span class="line">        - name: EUREKA_CLIENT_REGISTERWITHEUREKA</span><br><span class="line">          value: <span class="string">"true"</span></span><br><span class="line">        - name: EUREKA_CLIENT_FETCHREGISTRY</span><br><span class="line">          value: <span class="string">"true"</span></span><br><span class="line">          <span class="comment"># In the docker image, this is set to localhost. Otherwise, we could leave this empty.</span></span><br><span class="line">          <span class="comment"># The hostnames must match with the the eureka serviceUrls, otherwise the replicas are reported as unavailable in the eureka dashboard</span></span><br><span class="line">        - name: EUREKA_INSTANCE_HOSTNAME</span><br><span class="line">          <span class="comment">#value: "$(MY_POD_NAME).eureka.&lt;namespace.svc.cluster.local"</span></span><br><span class="line">          value: <span class="string">"<span class="variable">$(MY_POD_NAME)</span>.eureka"</span></span><br><span class="line">          <span class="comment">#value: eureka</span></span><br><span class="line">          <span class="comment"># For the other (stateless) services, this should probably be set to true, since their pods have no DNS-resolvable  hostnames</span></span><br><span class="line">       <span class="comment">#- name: EUREKA_INSTANCE_PREFERIPADDRESS</span></span><br><span class="line">       <span class="comment">#  value: "false"</span></span><br><span class="line">  <span class="comment"># No need to start the pods in order. We just need the stable network identity</span></span><br><span class="line">  podManagementPolicy: <span class="string">"Parallel"</span></span><br></pre></td></tr></table></figure><p>在做这个的时候其实遇到了很多坑，也是参考一些文章才完成的,<a href="https://github.com/kingschan1204/blog/issues/5" target="_blank" rel="noopener">参考文献</a></p><h4 id="2-2、创建eureka-Ingress"><a href="#2-2、创建eureka-Ingress" class="headerlink" title="2.2、创建eureka Ingress"></a>2.2、创建eureka Ingress</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat eureka-ingress.yaml</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: eureka-ingress</span><br><span class="line">  namespace: kube-dev</span><br><span class="line">  annotations:</span><br><span class="line">    kubernetes.io/ingress.class: <span class="string">"nginx"</span></span><br><span class="line">    nginx.ingress.kubernetes.io/secure-backends: <span class="string">"true"</span></span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: eureka.xxlaila.io</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - backend:</span><br><span class="line">          serviceName: eureka</span><br><span class="line">          servicePort: 8761</span><br></pre></td></tr></table></figure><h4 id="2-3、执行创建"><a href="#2-3、执行创建" class="headerlink" title="2.3、执行创建"></a>2.3、执行创建</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl apply -f .</span><br><span class="line">$ kubectl get pods -n kube-dev</span><br><span class="line">NAME       READY   STATUS    RESTARTS   AGE</span><br><span class="line">eureka-0   1/1     Running   0          21m</span><br><span class="line">eureka-1   1/1     Running   0          21m</span><br><span class="line">eureka-2   1/1     Running   0          21m</span><br><span class="line">$ kubectl get pods,svc,rs -n kube-dev</span><br><span class="line">NAME           READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod/eureka-0   1/1     Running   0          21m</span><br><span class="line">pod/eureka-1   1/1     Running   0          21m</span><br><span class="line">pod/eureka-2   1/1     Running   0          21m</span><br><span class="line"></span><br><span class="line">NAME             TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE</span><br><span class="line">service/eureka   ClusterIP   None         &lt;none&gt;        8761/TCP   21m</span><br></pre></td></tr></table></figure><h4 id="2-4、访问验证"><a href="#2-4、访问验证" class="headerlink" title="2.4、访问验证"></a>2.4、访问验证</h4><p><img src="https://img.xxlaila.cn/34239uskdnksjda.png" alt="img"><br>通过域名访问：<br><img src="https://img.xxlaila.cn/89745jkndklsajkdhsajkbfa.png" alt="img"></p><h3 id="3、eureka-环境"><a href="#3、eureka-环境" class="headerlink" title="3、eureka 环境"></a>3、eureka 环境</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;根据2.4小节可以看到，enviroonment为test，我们在dockerfile指定的为dev，所以这里就有点差池，但是查看了一下资料，这个要么就写多个application.yaml的配置文件，要么就打多个包，这样就比较麻烦，而且考虑到公司微服务的特殊性，既要满足于公司的微服务架构，有要考虑的模版的通用性，还需要考虑运维维护的便捷性。下面一起来看看基于公司的定制化来改变这个局限性。</p><h4 id="3-1、公司的系统环境变量"><a href="#3-1、公司的系统环境变量" class="headerlink" title="3.1、公司的系统环境变量"></a>3.1、公司的系统环境变量</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CONFIG_API_SERVER=http://api.conf.xxlaila.io</span><br><span class="line">RUN_CLUSTER=default</span><br><span class="line">RUN_MODE=AUTO</span><br><span class="line">RUN_ENV=demo</span><br><span class="line">CONFIG_API_SERVER：公司配置中心api的地址，app拉取配置中心的配置</span><br><span class="line">RUN_CLUSTER：默认集群，做灰度发布使用</span><br><span class="line">RUN_MODE：</span><br><span class="line">RUN_ENV：当前系统所运行的环境</span><br></pre></td></tr></table></figure><h4 id="3-2、修改eureka的配置文件"><a href="#3-2、修改eureka的配置文件" class="headerlink" title="3.2、修改eureka的配置文件"></a>3.2、修改eureka的配置文件</h4><p>增加配置eureka:<code>environment: ${RUN_ENV}</code></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat application.yaml</span><br><span class="line">spring:</span><br><span class="line">  application:</span><br><span class="line">    name: kxl-eureka</span><br><span class="line">server:</span><br><span class="line">  port: 8761</span><br><span class="line">eureka:</span><br><span class="line">  environment: <span class="variable">$&#123;RUN_ENV&#125;</span></span><br><span class="line">  instance:</span><br><span class="line"><span class="comment">#    prefer-ip-address: true</span></span><br><span class="line">    hostname: <span class="variable">$&#123;EUREKA_HOST_NAME:peer1&#125;</span></span><br><span class="line">    appname: <span class="variable">$&#123;spring.application.name&#125;</span></span><br><span class="line">    <span class="comment"># server 从最后一次收到心跳到移除废弃服务的超时时间（秒）</span></span><br><span class="line">    lease-expiration-duration-in-seconds: 90</span><br><span class="line">    <span class="comment"># client 给 server 发送心跳的间隔时间（秒），比 lease-expiration-duration-in-seconds 小</span></span><br><span class="line">    lease-renewal-interval-in-seconds: 30</span><br><span class="line"></span><br><span class="line">  client:</span><br><span class="line">    serviceUrl:</span><br><span class="line">      defaultZone: <span class="variable">$&#123;EUREKA_URL_LIST:http://peer1:8761/eureka/&#125;</span></span><br><span class="line">    <span class="comment"># client 是否从eureka上拉取注册信息， server模式可关掉</span></span><br><span class="line">    fetch-registry: <span class="variable">$&#123;BOOL_FETCH:true&#125;</span></span><br><span class="line">    <span class="comment"># client 是否注册到eureka上， server模式可关掉</span></span><br><span class="line">    register-with-eureka: <span class="variable">$&#123;BOOL_REGISTER:true&#125;</span></span><br><span class="line">    <span class="comment"># client 间隔多久去拉去服务信息(秒)</span></span><br><span class="line">    registry-fetch-interval-seconds: 30</span><br><span class="line">  server:</span><br><span class="line">    <span class="comment"># 自我保护机制，应对网络闪断情况，大面积丢失过多的client，不删除服务</span></span><br><span class="line">    <span class="built_in">enable</span>-self-preservation: <span class="variable">$&#123;SELF_PRESERVATION:true&#125;</span></span><br><span class="line">    <span class="comment"># 每分钟心跳数 实际/期望，如果小于阈值(threshold)，则触发自我保护机制</span></span><br><span class="line">    renewal-percent-threshold: 0.85</span><br><span class="line">    <span class="comment"># 扫描失效服务的间隔时间（毫秒）</span></span><br><span class="line">    eviction-interval-timer-in-ms: 60000</span><br><span class="line"></span><br><span class="line">  application:</span><br><span class="line">    name: <span class="variable">$&#123;EUREKA_APPLICATION_NAME:eureka-server&#125;</span></span><br></pre></td></tr></table></figure><h4 id="3-3、修改eureka部署文件"><a href="#3-3、修改eureka部署文件" class="headerlink" title="3.3、修改eureka部署文件"></a>3.3、修改eureka部署文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat kxl-eureka.yaml </span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: eureka</span><br><span class="line">  namespace: kube-dev</span><br><span class="line">  labels:</span><br><span class="line">    app: eureka</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - port: 8761</span><br><span class="line">    name: eureka</span><br><span class="line">  clusterIP: None</span><br><span class="line">  selector:</span><br><span class="line">    app: eureka</span><br><span class="line">---</span><br><span class="line">apiVersion: apps/v1beta1</span><br><span class="line">kind: StatefulSet</span><br><span class="line">metadata:</span><br><span class="line">  name: eureka</span><br><span class="line">  namespace: kube-dev</span><br><span class="line">spec:</span><br><span class="line">  serviceName: <span class="string">"eureka"</span></span><br><span class="line">  replicas: 3</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: eureka</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: eureka</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: eureka</span><br><span class="line">        image:  docker.io/xxlaila/kxl-eureka:v2</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 8761</span><br><span class="line">        env:</span><br><span class="line">        - name: MY_POD_NAME</span><br><span class="line">          valueFrom:</span><br><span class="line">            fieldRef:</span><br><span class="line">              fieldPath: metadata.name</span><br><span class="line">          <span class="comment"># Due to camelcase issues with "defaultZone" and "preferIpAddress", _JAVA_OPTIONS is used here</span></span><br><span class="line">        - name: eureka_client_serviceUrl_defaultZone</span><br><span class="line">          <span class="comment">#value: http://eureka-0.eureka.&lt;namespace&gt;.svc.cluster.local:8761/eureka/,http://eureka-1.eureka.&lt;namespace&gt;.svc.cluster.local:8761/eureka/</span></span><br><span class="line">          value: http://eureka-0.eureka:8761/eureka/,http://eureka-1.eureka:8761/eureka/,http://eureka-2.eureka:8761/eureka/</span><br><span class="line">        - name: EUREKA_CLIENT_REGISTERWITHEUREKA</span><br><span class="line">          value: <span class="string">"true"</span></span><br><span class="line">        - name: EUREKA_CLIENT_FETCHREGISTRY</span><br><span class="line">          value: <span class="string">"true"</span></span><br><span class="line">          <span class="comment"># In the docker image, this is set to localhost. Otherwise, we could leave this empty.</span></span><br><span class="line">          <span class="comment"># The hostnames must match with the the eureka serviceUrls, otherwise the replicas are reported as unavailable in the eureka dashboard</span></span><br><span class="line">        - name: EUREKA_INSTANCE_HOSTNAME</span><br><span class="line">          <span class="comment">#value: "$(MY_POD_NAME).eureka.&lt;namespace.svc.cluster.local"</span></span><br><span class="line">          value: <span class="string">"<span class="variable">$(MY_POD_NAME)</span>.eureka"</span></span><br><span class="line">        - name: RUN_ENV</span><br><span class="line">          value: <span class="built_in">test</span></span><br><span class="line">        - name: CONFIG_API_SERVER</span><br><span class="line">          value: http://api.conf.xxlaila.io</span><br><span class="line">        - name: RUN_CLUSTER</span><br><span class="line">          value: default</span><br><span class="line">        - name: RUN_MODE</span><br><span class="line">          value: AUTO</span><br><span class="line">          <span class="comment">#value: eureka</span></span><br><span class="line">          <span class="comment"># For the other (stateless) services, this should probably be set to true, since their pods have no DNS-resolvable  hostnames</span></span><br><span class="line">       <span class="comment">#- name: EUREKA_INSTANCE_PREFERIPADDRESS</span></span><br><span class="line">       <span class="comment">#  value: "false"</span></span><br><span class="line">  <span class="comment"># No need to start the pods in order. We just need the stable network identity</span></span><br><span class="line">  podManagementPolicy: <span class="string">"Parallel"</span></span><br></pre></td></tr></table></figure><h4 id="3-4、重建pod"><a href="#3-4、重建pod" class="headerlink" title="3.4、重建pod"></a>3.4、重建pod</h4><p>pod重建以后我们经过访问可以看到</p><p><img src="https://img.xxlaila.cn/453khskdha324.png" alt="img"><br><img src="https://img.xxlaila.cn/3746dfnks324.png" alt="img"></p><p><em>后续持续优化</em></p><!-- rebuild by neat -->]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>eureka</tag>
      </tags>
  </entry>
  <entry>
    <title>jira接入LDAP</title>
    <url>/2019/08/24/jira%E6%8E%A5%E5%85%A5LDAP/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><blockquote><p>场景:<br>&nbsp;&nbsp;&nbsp;&nbsp;之前介绍了jira 和confluence的账户结合，jira和confluence可以使用一个账户，有人员离职之后直接在jira吧用户禁用即可，一端操作，方便两端，但是随着公司人员越来越多，这样的方式已经不在适合这种了，来一个用户就需要去创建，对运维来说，这是重复的工作，提升不了任何效率，而且枯草无味。这里我们就可以使用ldap，jira和confluence都是支持ldap，ldap的好处，这里不阐述，下面来看看如何配置jira介入ldap。confluence还是接入jira，这样我们就只操作ladp和jira，简单省事。</p></blockquote><blockquote><p>问题点:<br>&nbsp;&nbsp;&nbsp;&nbsp;由于在建立jira和confluence的时候还没有ldap，ldap是后期才接入的，所以这里就存在于怎么吧以前有jira登录的账户认证切换到ldap。而且不影响之前的文档，但是用户权限会影响，问题不大，可以添加。下面开始操作</p></blockquote><a id="more"></a><h3 id="1、进入jira用户管理页面"><a href="#1、进入jira用户管理页面" class="headerlink" title="1、进入jira用户管理页面"></a>1、进入jira用户管理页面</h3><p><img src="https://img.xxlaila.cn/image2018-7-12_11-12-55.png" alt="img"></p><h3 id="2、选择ldap，进入ldap配置页面"><a href="#2、选择ldap，进入ldap配置页面" class="headerlink" title="2、选择ldap，进入ldap配置页面"></a>2、选择ldap，进入ldap配置页面</h3><p><img src="https://img.xxlaila.cn/image2019-6-12_15-32-32.png" alt="img"><br><img src="https://img.xxlaila.cn/image2019-6-12_15-32-48.png" alt="img"></p><h3 id="3、高级设置"><a href="#3、高级设置" class="headerlink" title="3、高级设置"></a>3、高级设置</h3><p><img src="https://img.xxlaila.cn/image2019-6-12_15-33-17.png" alt="img"></p><h3 id="4、配置用户模式"><a href="#4、配置用户模式" class="headerlink" title="4、配置用户模式"></a>4、配置用户模式</h3><p><img src="https://img.xxlaila.cn/image2019-6-12_15-33-35.png" alt="img"></p><h3 id="5、设置组模式"><a href="#5、设置组模式" class="headerlink" title="5、设置组模式"></a>5、设置组模式</h3><p><img src="https://img.xxlaila.cn/image2019-6-12_15-33-55.png" alt="img"></p><h3 id="6、设置成员模式"><a href="#6、设置成员模式" class="headerlink" title="6、设置成员模式"></a>6、设置成员模式</h3><p>这里ldap一定要存在与ladp的group里面<br><img src="https://img.xxlaila.cn/image2019-6-12_15-34-23.png" alt="img"></p><h3 id="7、测试并保存"><a href="#7、测试并保存" class="headerlink" title="7、测试并保存"></a>7、测试并保存</h3><p>这里测试账户一定是ladp的账户<br><img src="https://img.xxlaila.cn/image2019-6-12_15-34-58.png" alt="img"></p><h3 id="8、同步账户"><a href="#8、同步账户" class="headerlink" title="8、同步账户"></a>8、同步账户</h3><p><img src="https://img.xxlaila.cn/image2019-6-12_15-35-50.png" alt="img"></p><h3 id="9、用ladp账户登录测试"><a href="#9、用ladp账户登录测试" class="headerlink" title="9、用ladp账户登录测试"></a>9、用ladp账户登录测试</h3><p><img src="https://img.xxlaila.cn/image2019-6-12_15-36-53.png" alt="img"></p><h2 id="jira账户切换至ldap"><a href="#jira账户切换至ldap" class="headerlink" title="jira账户切换至ldap"></a>jira账户切换至ldap</h2><blockquote><p>jira配置ldap以后默认是本地账户和ldap是同时存在的，用户可以用以前的本地账户和ldap都登录，这里实现ldap登录，禁止本地登录。</p></blockquote><h3 id="前置条件"><a href="#前置条件" class="headerlink" title="前置条件"></a>前置条件</h3><ul><li>jira的本地账户和ldap的账户名称必须一样</li><li>操作前请备份数据库</li><li>用户邮箱保持一致</li><li>密码无所谓，不需要统一</li></ul><h3 id="查看用户信息关联"><a href="#查看用户信息关联" class="headerlink" title="查看用户信息关联"></a>查看用户信息关联</h3><p>登录jira的数据库。然后找到cwd_user表</p><h4 id="cwd-user表"><a href="#cwd-user表" class="headerlink" title="cwd_user表"></a>cwd_user表</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;jira的前台不能直接去更改，只有更改数据库，进入数据库，找到一张cwd_user的表，里面包含了所有用户的登录账号信息，其中有一个字段directory_id的，这个字段我们可以看到本地账户的id是1，ldap同步过来的账户是10001，如下图：<br><img src="https://img.xxlaila.cn/1566617986314.jpg" alt="img"></p><p>&nbsp;&nbsp;&nbsp;&nbsp;继续看该表的credential字段，密码也有区别,本地账户是有一串加密后的字符串，ldap认证的是nopass，包括后面的external_id 也是有区别的，如下图<br><img src="https://img.xxlaila.cn/1566618124910.jpg" alt="img"></p><h4 id="cwd-directory表"><a href="#cwd-directory表" class="headerlink" title="cwd_directory表"></a>cwd_directory表</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;在打开cwd_directory表，里面有两条数据，一个对应的是ldap，一个对应的本地，和cwd_user是对应的，如图：<br><img src="https://img.xxlaila.cn/image2019-6-13_10-0-47.png" alt="img"></p><h4 id="修改数据库"><a href="#修改数据库" class="headerlink" title="修改数据库"></a>修改数据库</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;记住ldap目录的id，然后在cwd_user表里面删除ldap的相同账户的整条记录，因为要伪装原本系统自带的目录服务器，原来编辑的文件和内容为原先这个用户的id。修改directory_id 为ldap的id，还有一个需要修改的地方为CREDENTIAL的字段，把它修改为nopass。修改完成后需要重启jira，不重启不会生效，而且登录服务器还会报错。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;这里jira和confluence是做了关联的，jira修改以后，confluence也可以进行登录，无需在confluence在设置一次ldap;修改完成后，以前用户的管理员权限有问题，重新添加一次即可。不会影响其他的。设置文档，ldap权限要选择为只读，且为本地组。然后吧jia和confluence的普通组添加进去，否则用户进来没有权限</p><!-- rebuild by neat -->]]></content>
      <categories>
        <category>jira</category>
      </categories>
      <tags>
        <tag>ldap</tag>
      </tags>
  </entry>
  <entry>
    <title>confluence与jira账户打通</title>
    <url>/2019/08/24/confluence%E4%B8%8Ejira%E8%B4%A6%E6%88%B7%E6%89%93%E9%80%9A/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><p><a href="http://xxlaila.github.io/2019/08/24/confluence-install/" target="_blank" rel="noopener">confluence</a>安装</p><h3 id="登录confluence"><a href="#登录confluence" class="headerlink" title="登录confluence"></a>登录confluence</h3><p>点击用户管理</p><a id="more"></a><p><img src="https://img.xxlaila.cn/1566615435701.jpg" alt="img"></p><ul><li>点击用户目录<br><img src="https://img.xxlaila.cn/1566615500778.jpg" alt="img"></li></ul><h3 id="开始配置和jira的连接"><a href="#开始配置和jira的连接" class="headerlink" title="开始配置和jira的连接"></a>开始配置和jira的连接</h3><ul><li>点击添加目录</li></ul><p><img src="https://img.xxlaila.cn/1566615580951.jpg" alt="img"><br><img src="https://img.xxlaila.cn/1566616502650.jpg" alt="img"><br><img src="https://img.xxlaila.cn/1566616536151.jpg" alt="img"></p><p>点击测试连接，连接成功以后，点击测试保存。回到界面可以点击同步<br><img src="https://img.xxlaila.cn/1566616609967.jpg" alt="img"></p><!-- rebuild by neat -->]]></content>
      <categories>
        <category>confluence</category>
      </categories>
      <tags>
        <tag>confluence,jira</tag>
      </tags>
  </entry>
  <entry>
    <title>jira安装和配置</title>
    <url>/2019/08/24/jira%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;JIRA是Atlassian公司出品的项目与事务跟踪工具，被广泛应用于缺陷跟踪、客户服务、需求收集、流程审批、任务跟踪、项目跟踪和敏捷管理等工作领域。</p><ul><li>环境准备</li></ul><table><thead><tr><th>应用</th><th>服务器配置</th><th>操作系统</th><th>插件</th></tr></thead><tbody><tr><td>mysql 5.6 +</td><td>2/4G/50G</td><td>centos 7.4</td><td></td></tr><tr><td>jira</td><td>4/8G/200G</td><td>centos 7.4</td><td>jdk1.8</td></tr></tbody></table><a id="more"></a><h3 id="安装JIRA"><a href="#安装JIRA" class="headerlink" title="安装JIRA"></a>安装JIRA</h3><ul><li>执行可执行文件<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ./atlassian-jira-software-7.2.2-x64.bin</span></span><br></pre></td></tr></table></figure></li></ul><p><img src="https://img.xxlaila.cn/1524710136913-900.png" alt="img"></p><ul><li><p>安装完成后停止JIRA</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">＃/etc/init.d/jira stop</span><br></pre></td></tr></table></figure></li><li><p>复制破解包和数据库驱动连接器</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">＃cp mysql-connector-java-5.1.39-bin.jar atlassian-extras-3.1.2.jar /opt/atlassian/jira/atlassian-jira/WEB-INF/lib/</span><br></pre></td></tr></table></figure></li><li><p>启动JIRA</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">＃/etc/init.d/jira start</span><br></pre></td></tr></table></figure></li></ul><p>登陆网页控制台进行配置，这里略过，这里这是完成，无法截图(后期有机会安装截图补上)，设置以后的截图<br><img src="https://img.xxlaila.cn/1524710090658-810.png" alt="img"></p><h3 id="配置JIRA的邮件服务器"><a href="#配置JIRA的邮件服务器" class="headerlink" title="配置JIRA的邮件服务器"></a>配置JIRA的邮件服务器</h3><p><img src="https://img.xxlaila.cn/1524710057086-739.png" alt="img"><br><img src="https://img.xxlaila.cn/1524710038583-516.png" alt="img"></p><p>配置完成后点击最下面的测试连接</p><p><img src="https://img.xxlaila.cn/1524710027432-202.png" alt="img"></p><!-- rebuild by neat -->]]></content>
      <categories>
        <category>jira</category>
      </categories>
      <tags>
        <tag>jira</tag>
      </tags>
  </entry>
  <entry>
    <title>git清空commit记录方法</title>
    <url>/2019/08/24/git%E6%B8%85%E7%A9%BAcommit%E8%AE%B0%E5%BD%95%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><blockquote><p>说明:<br>&nbsp;&nbsp;&nbsp;&nbsp;例如将代码提交到git仓库，将一些敏感信息提交，所以需要删除提交记录以彻底清除提交信息，以得到一个干净的仓库且代码不变</p></blockquote><h3 id="1-Checkout"><a href="#1-Checkout" class="headerlink" title="1.Checkout"></a>1.Checkout</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ git checkout --orphan latest_branch</span><br></pre></td></tr></table></figure><h3 id="2-Add-all-the-files"><a href="#2-Add-all-the-files" class="headerlink" title="2. Add all the files"></a>2. Add all the files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ git add -A</span><br></pre></td></tr></table></figure><h3 id="3-Commit-the-changes"><a href="#3-Commit-the-changes" class="headerlink" title="3. Commit the changes"></a>3. Commit the changes</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ git commit -am <span class="string">"commit message"</span></span><br></pre></td></tr></table></figure><a id="more"></a><h3 id="4-Delete-the-branch"><a href="#4-Delete-the-branch" class="headerlink" title="4. Delete the branch"></a>4. Delete the branch</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ git branch -D master</span><br></pre></td></tr></table></figure><h3 id="5-Rename-the-current-branch-to-master"><a href="#5-Rename-the-current-branch-to-master" class="headerlink" title="5.Rename the current branch to master"></a>5.Rename the current branch to master</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ git branch -m master</span><br></pre></td></tr></table></figure><h3 id="6-Finally-force-update-your-repository"><a href="#6-Finally-force-update-your-repository" class="headerlink" title="6.Finally, force update your repository"></a>6.Finally, force update your repository</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ git push -f origin master</span><br></pre></td></tr></table></figure><h2 id="git-主干和分支同步"><a href="#git-主干和分支同步" class="headerlink" title="git 主干和分支同步"></a>git 主干和分支同步</h2><h3 id="1、远程分支"><a href="#1、远程分支" class="headerlink" title="1、远程分支"></a>1、远程分支</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">查看当前的远程分支</span><br><span class="line">$ git remote -v</span><br></pre></td></tr></table></figure><h3 id="2、增加远程分支"><a href="#2、增加远程分支" class="headerlink" title="2、增加远程分支"></a>2、增加远程分支</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ git remote add latest https://github.com/xxlaila/work.git</span><br></pre></td></tr></table></figure><h3 id="3、更新主库的远程分支"><a href="#3、更新主库的远程分支" class="headerlink" title="3、更新主库的远程分支"></a>3、更新主库的远程分支</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">更新远程分支</span><br><span class="line">$ git fetch latest</span><br></pre></td></tr></table></figure><h3 id="4、合并主库的最新代码"><a href="#4、合并主库的最新代码" class="headerlink" title="4、合并主库的最新代码"></a>4、合并主库的最新代码</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">合并主库的最新代码</span><br><span class="line">$ git rebase latest/dev</span><br></pre></td></tr></table></figure><h3 id="5、推送本地代码到自身远程仓库"><a href="#5、推送本地代码到自身远程仓库" class="headerlink" title="5、推送本地代码到自身远程仓库"></a>5、推送本地代码到自身远程仓库</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">推送远程仓库</span><br><span class="line">$ git push</span><br></pre></td></tr></table></figure><h2 id="git-代码信息统计"><a href="#git-代码信息统计" class="headerlink" title="git 代码信息统计"></a>git 代码信息统计</h2><blockquote><p>说明: 公司每个季度都要对公司开发人员的工作量进行整体评估，评估git上所有代码库的commit数量和修改文件的总行数</p></blockquote><h3 id="统计时间区间commit数量"><a href="#统计时间区间commit数量" class="headerlink" title="统计时间区间commit数量"></a>统计时间区间commit数量</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">log</span> --oneline --since==2019-04-1 --until=2019-06-30 | wc -l</span><br></pre></td></tr></table></figure><h3 id="统计添加修改的代码行数"><a href="#统计添加修改的代码行数" class="headerlink" title="统计添加修改的代码行数"></a>统计添加修改的代码行数</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">log</span> --<span class="built_in">stat</span> --since=<span class="string">"2019-4-01"</span> --before=<span class="string">"2019-06-30"</span> |perl -ne <span class="string">'END &#123; print $c &#125; $c += $1 if /(\d+) insertions/'</span></span><br></pre></td></tr></table></figure><h2 id="git-stash使用"><a href="#git-stash使用" class="headerlink" title="git stash使用"></a>git stash使用</h2><blockquote><p>场景:<br>&nbsp;&nbsp;&nbsp;&nbsp;今天遇到一个特殊的场景，写的一个python程序到服务器上有一个小bug运行报错，然后就直接在服务器上调试，修改了程序，程序正常跑起，然而本地也要修改，修改的时候同时修改了其他的地方，然后提交了git上，这时，需要在服务器上更新最新的代码，但是服务器上的代码和git上的不一致，这就会导致错误，这时git的强大之处就体现出来了。</p></blockquote><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ git status</span><br><span class="line">能够将所有未提交的修改（工作区和暂存区）保存至堆栈中，用于后续恢复当前工作目录。</span><br></pre></td></tr></table></figure><ul><li>更新代码<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ git pull</span><br></pre></td></tr></table></figure></li></ul><p>将当前stash中的内容弹出，并应用到当前分支对应的工作目录上。</p><blockquote><p>注:<br>&nbsp;&nbsp;&nbsp;&nbsp;该命令将堆栈中最近保存的内容删除（栈是先进后出）</p></blockquote><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ git stash pop</span><br></pre></td></tr></table></figure><p>更多的git stash 命令详解请<a href="https://git-scm.com/book/zh/v1/Git-%E5%B7%A5%E5%85%B7-%E5%82%A8%E8%97%8F%EF%BC%88Stashing%EF%BC%89" target="_blank" rel="noopener">点击</a></p><!-- rebuild by neat -->]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>confluence_install</title>
    <url>/2019/08/24/confluence-install/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:23 GMT+0800 (China Standard Time) --><p>&nbsp;&nbsp;&nbsp;&nbsp;Confluence是一个专业的企业知识管理与协同软件，也可以用于构建企业wiki。使用简单，但它强大的编辑和站点管理特征能够帮助团队成员之间共享信息、文档协作、集体讨论，信息推送。为团队提供一个协作环境。在这里，团队成员齐心协力，各擅其能，协同地编写文档和管理项目。从此打破不同团队、不同部门以及个人之间信息孤岛的僵局，Confluence真正实现了组织资源共享。</p><p>环境准备</p><table><thead><tr><th>系统版本</th><th>插件</th><th>软件</th><th>版本</th><th>服务配置</th></tr></thead><tbody><tr><td>centos 7.4</td><td></td><td>mysql</td><td>5.6+</td><td>2/4G/50G</td></tr><tr><td>centos 7.4</td><td>jdk 1.8</td><td>confluence</td><td>6.12.2</td><td>4/8G/200G</td></tr></tbody></table><p>confluence 6.12.2安装并破解，mysql 版本这里使用的是5.7.24</p><a id="more"></a><h2 id="安装mysql-5-7-24-版本"><a href="#安装mysql-5-7-24-版本" class="headerlink" title="安装mysql 5.7.24 版本"></a>安装mysql 5.7.24 版本</h2><h3 id="安装mysql"><a href="#安装mysql" class="headerlink" title="安装mysql"></a>安装mysql</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># rpm -ivh http://dev.mysql.com/get/mysql57-community-release-el7-7.noarch.rpm</span></span><br><span class="line"><span class="comment"># yum list |grep "mysql"</span></span><br><span class="line"><span class="comment"># yum install -y mysql-community-server</span></span><br></pre></td></tr></table></figure><ul><li><p>动mysql</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># systemctl start mysqld.service</span></span><br><span class="line"><span class="comment"># systemctl enable mysqld.service</span></span><br></pre></td></tr></table></figure></li><li><p>修改myslq密码</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># grep 'temporary password' /var/log/mysqld.log</span></span><br><span class="line">mysql&gt; SET PASSWORD = PASSWORD(<span class="string">'news password'</span>);</span><br><span class="line">mysql&gt; ALTER USER <span class="string">'root'</span>@<span class="string">'localhost'</span> PASSWORD EXPIRE NEVER;</span><br><span class="line">mysql&gt; flush privileges;</span><br></pre></td></tr></table></figure></li></ul><h3 id="修改mysql的配置文件，用于支持confluence的最低安装需求"><a href="#修改mysql的配置文件，用于支持confluence的最低安装需求" class="headerlink" title="修改mysql的配置文件，用于支持confluence的最低安装需求"></a>修改mysql的配置文件，用于支持confluence的最低安装需求</h3><blockquote><p>在my.cnf配置文件[mysqld]里面添加下面配置参数</p></blockquote><ul><li><p>将默认字符集指定为UTF-8</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">character-set-server=utf8</span><br><span class="line">collation-server=utf8_bin</span><br></pre></td></tr></table></figure></li><li><p>将默认存储引擎设置为InnoDB</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">default-storage-engine=INNODB</span><br></pre></td></tr></table></figure></li><li><p>指定值max_allowed_packet至少为256M</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">max_allowed_packet=512M</span><br></pre></td></tr></table></figure></li><li><p>指定值 innodb_log_file_size 至少为2GB</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">innodb_log_file_size=2GB</span><br></pre></td></tr></table></figure></li><li><p>确保数据库的全局事务隔离级别已设置为READ-COMMITTED</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">transaction-isolation=READ-COMMITTED</span><br></pre></td></tr></table></figure></li><li><p>检查二进制日志记录格式是否配置为使用“基于行”的二进制日志记录</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">binlog_format=row</span><br></pre></td></tr></table></figure></li><li><p>确保sql_mode参数未指定NO_AUTO_VALUE_ON_ZERO</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sql_mode = <span class="string">"ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION"</span></span><br></pre></td></tr></table></figure></li><li><p>重启mysql数据库</p></li></ul><h3 id="为Confluence创建数据库用户和数据库"><a href="#为Confluence创建数据库用户和数据库" class="headerlink" title="为Confluence创建数据库用户和数据库"></a>为Confluence创建数据库用户和数据库</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql&gt; CREATE DATABASE confluence CHARACTER SET utf8 COLLATE utf8_bin;</span><br><span class="line">mysql&gt; GRANT ALL PRIVILEGES ON confluence.* TO confluence@<span class="string">'localhost'</span> IDENTIFIED BY <span class="string">'password'</span></span><br><span class="line">mysql&gt; flush privileges;</span><br></pre></td></tr></table></figure><h2 id="安装Confluence"><a href="#安装Confluence" class="headerlink" title="安装Confluence"></a>安装Confluence</h2><p>下载Confluence，这里下载bin文件进行安装，<a href="https://www.atlassian.com/software/confluence/download-archives" target="_blank" rel="noopener">下载地址</a>,下载的版本为atlassian-confluence-6.12.2-x64.bin,包有点大，需要等待……</p><ul><li><p>赋予权限</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># chmod a+x atlassian-confluence-6.12.2-x64.bin</span></span><br></pre></td></tr></table></figure></li><li><p>开始安装</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ./atlassian-confluence-6.12.2-x64.bin</span></span><br></pre></td></tr></table></figure></li><li><p>安装过程中需要做一些基本的配置，详情查看我的配置</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Unpacking JRE ...</span><br><span class="line">Starting Installer ...</span><br><span class="line"></span><br><span class="line">This will install Confluence 6.12.2 on your computer.</span><br><span class="line">OK [o, Enter], Cancel [c]</span><br><span class="line">o （输入o同意）</span><br><span class="line">Click Next to <span class="built_in">continue</span>, or Cancel to <span class="built_in">exit</span> Setup.</span><br><span class="line"></span><br><span class="line">Choose the appropriate installation or upgrade option.</span><br><span class="line">Please choose one of the following:</span><br><span class="line">Express Install (uses default settings) [1], </span><br><span class="line">Custom Install (recommended <span class="keyword">for</span> advanced users) [2, Enter], </span><br><span class="line">Upgrade an existing Confluence installation [3]</span><br><span class="line">2   (选择2自定义安装，我们可以进行一些定制的配置)</span><br></pre></td></tr></table></figure></li><li><p>开始进行安装参数的配置</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Select the folder <span class="built_in">where</span> you would like Confluence 6.12.2 to be installed,</span><br><span class="line"><span class="keyword">then</span> click Next.</span><br><span class="line">Where should Confluence 6.12.2 be installed?</span><br><span class="line">[/opt/atlassian/confluence](安装目录,目录变化可以在这里输入，这里直接回车)</span><br><span class="line"></span><br><span class="line">Default location <span class="keyword">for</span> Confluence data</span><br><span class="line">[/var/atlassian/application-data/confluence]（数据的存放目录，这里我们修改到我们的数据盘）</span><br><span class="line">/opt/atlassian/confluence-data/</span><br><span class="line"></span><br><span class="line">Configure <span class="built_in">which</span> ports Confluence will use.</span><br><span class="line">Confluence requires two TCP ports that are not being used by any other</span><br><span class="line">applications on this machine. The HTTP port is <span class="built_in">where</span> you will access</span><br><span class="line">Confluence through your browser. The Control port is used to Startup and</span><br><span class="line">Shutdown Confluence.</span><br><span class="line">Use default ports (HTTP: 8090, Control: 8000) - Recommended [1, Enter], Set custom value <span class="keyword">for</span> HTTP and Control ports [2]</span><br><span class="line">（这里是设置使用的端口，默认即可）</span><br><span class="line"></span><br><span class="line">Confluence can be run <span class="keyword">in</span> the background.</span><br><span class="line">You may choose to run Confluence as a service, <span class="built_in">which</span> means it will start</span><br><span class="line">automatically whenever the computer restarts.</span><br><span class="line">Install Confluence as Service?</span><br><span class="line">Yes [y, Enter], No [n]</span><br><span class="line">y</span><br><span class="line"></span><br><span class="line">Extracting files ...</span><br><span class="line">                                                                           </span><br><span class="line"></span><br><span class="line">Please <span class="built_in">wait</span> a few moments <span class="keyword">while</span> we configure Confluence.</span><br><span class="line"></span><br><span class="line">Installation of Confluence 6.12.2 is complete</span><br><span class="line">Start Confluence now?</span><br><span class="line">Yes [y, Enter], No [n]</span><br><span class="line">y</span><br><span class="line"></span><br><span class="line">Please <span class="built_in">wait</span> a few moments <span class="keyword">while</span> Confluence starts up.</span><br><span class="line">Launching Confluence ...</span><br><span class="line">输入y回车后Confluence会进行后台安装，这里等待安装完成即可</span><br><span class="line"></span><br><span class="line">Installation of Confluence 6.12.2 is complete</span><br><span class="line">Your installation of Confluence 6.12.2 is now ready and can be accessed via</span><br><span class="line">your browser.</span><br><span class="line">Confluence 6.12.2 can be accessed at http://localhost:8090</span><br><span class="line">安装完成</span><br></pre></td></tr></table></figure></li></ul><blockquote><p>通过浏览器访问试一下: <a href="http://ip:8090" target="_blank" rel="noopener">http://ip:8090</a></p></blockquote><h2 id="进行访问配置"><a href="#进行访问配置" class="headerlink" title="进行访问配置"></a>进行访问配置</h2><h3 id="安装nginx-进行方向代理访问"><a href="#安装nginx-进行方向代理访问" class="headerlink" title="安装nginx 进行方向代理访问"></a>安装nginx 进行方向代理访问</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># rpm -Uvh http://nginx.org/packages/centos/7/noarch/RPMS/nginx-release-centos-7-0.el7.ngx.noarch.rpm</span></span><br><span class="line"><span class="comment"># yum install nginx -y</span></span><br><span class="line"><span class="comment"># systemctl start nginx.service</span></span><br><span class="line"><span class="comment"># systemctl enable nginx.service</span></span><br><span class="line">配置nginx的upstream这里将不再阐述</span><br></pre></td></tr></table></figure><h3 id="接下来通过浏览器进行配置"><a href="#接下来通过浏览器进行配置" class="headerlink" title="接下来通过浏览器进行配置"></a>接下来通过浏览器进行配置</h3><ul><li>打开页面</li></ul><p><img src="https://img.xxlaila.cn/1566609337530.jpg" alt="img"></p><ul><li>设置语言为中文和产品安装</li></ul><p><img src="https://img.xxlaila.cn/1566609486131.jpg" alt="img"></p><p>在下面的一个界面需要记住服务器的ID，这个ip在后面破解的时候需要的</p><ul><li>停止confluence<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># /etc/init.d/confluence stop</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="破解Confluence"><a href="#破解Confluence" class="headerlink" title="破解Confluence"></a>破解Confluence</h3><ul><li>在本地下载破解器</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># wget https://github.com/xxlaila/work/blob/master/zip/confluence.zip</span></span><br><span class="line"><span class="comment"># unzip confluence.zip</span></span><br></pre></td></tr></table></figure><ul><li><p>在服务器上把atlassian-extras-decoder-v2-3.4.1.jar进行如下操作</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cd /opt/atlassian/confluence/confluence/WEB-INF/lib</span></span><br><span class="line"><span class="comment"># cp atlassian-extras-decoder-v2-3.4.1.jar /opt/atlassian-extras-2.4.jar</span></span><br><span class="line"><span class="comment"># mv atlassian-extras-decoder-v2-3.4.1.jar atlassian-extras-decoder-v2-3.4.1.jar.bak</span></span><br></pre></td></tr></table></figure></li><li><p>把/opt/atlassian-extras-2.4.jar下载到本地,在本地启动Confluence破解器</p></li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ java -jar confluence_keygen.jar</span><br></pre></td></tr></table></figure><ul><li>点击.patch! 选择下载到本地的atlassian-extras-2.4.jar包，文件类型不变，点击打开，自动生产一个新的atlassian-extras-2.4.jar包</li></ul><p><img src="https://img.xxlaila.cn/FA8682F205BB1655E20AAD392DF13417.jpg" alt="img"></p><ul><li>把服务器id输入到server id，name项随便输入，名称不要过短，店家.gen!生成授权吗，然后把授权复制到confluence框里面</li></ul><p><img src="https://img.xxlaila.cn/CB99372F30342EBABC1125510FBC50B9.jpg" alt="img"></p><ul><li>把新生成的包上传到/opt/atlassian/confluence/confluence/WEB-INF/lib/目录下面</li></ul><h3 id="下载mysql驱动"><a href="#下载mysql驱动" class="headerlink" title="下载mysql驱动"></a>下载mysql驱动</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.47.zip</span></span><br></pre></td></tr></table></figure><ul><li><p>完成后进行解压，并把mysql-connector-java-5.1.47-bin.jar 复制到lib目录下面</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cp mysql-connector-java-5.1.47-bin.jar /opt/atlassian/confluence/confluence/WEB-INF/lib</span></span><br><span class="line"><span class="comment"># /etc/init.d/confluence start</span></span><br></pre></td></tr></table></figure></li><li><p>设置数据库</p></li></ul><p><img src="https://img.xxlaila.cn/EFF70DEA9DFBCA88E24BE83BEE9DFFC8.jpg" alt="img"><br><img src="https://img.xxlaila.cn/BF70624FCFA9E5ECFD4E121E02D08FD3.jpg" alt="img"></p><blockquote><p>这是完成以后进行测试，是否联通，在下一步（需要进行等待，后台在生成数据库）,生成完成后，系统会跳转到另外一个页面，这里忘记截图,是进行数据导入、站点恢复等</p></blockquote><ul><li><p>重新打开网址连接<br><img src="https://img.xxlaila.cn/BEE317A48B4CEE0407638F47CDBDF31F.jpg" alt="img"></p></li><li><p>由于这里没有ladp和jira,所以选择在confluence中管理用户和组</p></li><li><p>设置系统管理账户</p></li></ul><p><img src="https://img.xxlaila.cn/93FDD95874C661340531C27CC77298FD.jpg" alt="img"><br><img src="https://img.xxlaila.cn/6B1F3A2EBF1B3DAF1A962317AA59DC15.jpg" alt="img"></p><blockquote><p>这里跳过个人头像设定,头像设定完成后会提示你新建一个空间</p></blockquote><p><img src="https://img.xxlaila.cn/FCE130BD8B121A290010FEA2C2342898.jpg" alt="img"></p><ul><li>查看授权期限</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">设置——&gt;一般设置——&gt;管理——&gt;授权细节</span><br></pre></td></tr></table></figure><p><img src="https://img.xxlaila.cn/EEB25C9704F4C76C30E1DF32A2E1EDE0.jpg" alt="img"></p><!-- rebuild by neat -->]]></content>
      <categories>
        <category>confluence</category>
      </categories>
      <tags>
        <tag>confluence</tag>
      </tags>
  </entry>
  <entry>
    <title>nexus3搭建npm私服</title>
    <url>/2019/08/23/nexus3%E6%90%AD%E5%BB%BAnpm%E7%A7%81%E6%9C%8D/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;公司前端全是nodejs的，nodejs在install的时候往往是连接外网，或者是设置taobao源，即使是设置了taobao源，但是还是解决不了慢的问题，为此搭建了一个内部的npm私服，这里用google一下有很多都可以来进行搭建npm私服，然后也看到了nexus也可以来做，正好maven私服也是用的这个，都是3版本，为此选择了nexus来做npm的私服，和maven一套便于维护。</p><h3 id="nexus安装"><a href="#nexus安装" class="headerlink" title="nexus安装"></a>nexus安装</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;不介绍，安装完成nexus后，在浏览器打开并进行登录，第一次安装登录nexus的默认用户<code>admin</code>,默认密码是<code>admin123</code></p><a id="more"></a><h3 id="1、创建repository"><a href="#1、创建repository" class="headerlink" title="1、创建repository"></a>1、创建repository</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;Nexus Repository Manager 3 可以用于多种类型的包管理。此处我们要搭建的是npm包管理私服。登录在界面点击下图所示按钮。<br><img src="https://img.xxlaila.cn/1566524933898.jpg" alt="img"></p><ul><li>进入设置界面<br><img src="https://img.xxlaila.cn/1566525058628.jpg" alt="img"></li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;上图中左面菜单有很多功能。可以在 Security 下的 Users 可以创建用户并设置用户权限，修改用户信息。Logging 下的 Log Viewer 可以查看运行日志。而本次配置主要用到了 Repository -&gt; Repositories 和 Security -&gt; Realms 两项</p><ul><li>首先在 Repositories 创建仓库</li></ul><p><img src="https://img.xxlaila.cn/1566526054409.jpg" alt="img"></p><ul><li><p>接下来会进入到 Repositorty 的选择：（npm 有三种）<br><img src="https://img.xxlaila.cn/1566526144738.jpg" alt="img"></p></li><li><p>第一种：代理 npm 仓库</p></li></ul><p><a href="https://help.sonatype.com/repomanager3/formats/npm-registry" target="_blank" rel="noopener">Proxying npm Registries</a>可产看官方文档</p><p>&nbsp;&nbsp;&nbsp;&nbsp;将公共 npm 服务器的资源代理缓存，减少重复下载，加快开发人员和CI服务器的下载速度。创建时选择 npm(proxy) ，只需填写 Name 和 Remote storage （公有库域名）即可。<br><img src="https://img.xxlaila.cn/1566526375641.jpg" alt="img"></p><ul><li>第二种：私有 npm 仓库<br><a href="https://help.sonatype.com/repomanager3/formats/npm-registry" target="_blank" rel="noopener">Private npm Registries</a>官方文档</li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;用于 上传自己的npm包 以及第三方npm包。同样的创建步骤，只不过选择的 仓库类型为 npm(hosted)。 只填写 Name 即可</p><p><img src="https://img.xxlaila.cn/1566526835511.jpg" alt="img"></p><ul><li>第三种：npm 仓库组<br><a href="https://help.sonatype.com/repomanager3/formats/npm-registry" target="_blank" rel="noopener">Grouping npm Registries</a>官方文档</li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;用于将多个内部或外部 npm 仓库统一为一个 npm仓库。被添加到 npm仓库组 中的 其他仓库内的包都能够通过该 npm仓库组 访问到。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;例如：可以新建一个npm仓库组将 上面两个刚刚创建的 npm 仓库都添加进去。这样可以通过这个 npm仓库组，既可以访问 公有npm仓库 又可以访问自己的 私有npm仓库。<br>&nbsp;&nbsp;&nbsp;&nbsp;仓库类型为 npm(group)，起一个名字 Name，然后选择需要添加到组里的 其他 npm 仓库。此处我选择的是 npm-kxl-external 和 npm-kxl-internal</p><p><img src="https://img.xxlaila.cn/1566527053731.jpg" alt="img"></p><p>&nbsp;&nbsp;&nbsp;&nbsp;仓库都创建完毕了。接下来需要验证一下是否可用,在 Repositories 中点击创建的 仓库。可以查看该仓库的 URL。<br>在项目目录下创建 .npmrc 文件。文件内容为：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">registry=http://172.21.16.90:8081/repository/npm-kxl-all/</span><br></pre></td></tr></table></figure><p>然后随便安装一个 包 试试（日志级别设置为 info）：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ npm --loglevel info install react</span><br></pre></td></tr></table></figure><p><img src="https://img.xxlaila.cn/1566528241640.jpg" alt="img"></p><p>如图。确实是从设置的 npm 私服下载的react。成功</p><h3 id="发布到-npm-私服"><a href="#发布到-npm-私服" class="headerlink" title="发布到 npm 私服"></a>发布到 npm 私服</h3><p>除了从 npm 仓库安装依赖。我们还需要将公司内部的 代码打包 发布到 npm 的私服。这里没什么特殊的，就是需要设置一下 Nexus Repository Manager 的权限。这样才能使用 npm login 认证登录到我们的私服。</p><p><img src="https://img.xxlaila.cn/1566528346695.jpg" alt="img"></p><p>此处在 Realms 下。将 npm Bearer Token Realm 添加到 Active 列表内保存即可。<br>然后可以执行（登录 私有npm仓库）：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm login --registry=http://172.21.16.90:8081/repository/npm-kxl-internal/</span><br><span class="line">Username: admin</span><br><span class="line">Password: </span><br><span class="line">Email: (this IS public) 1@qq.com</span><br><span class="line">Logged <span class="keyword">in</span> as admin on http://172.21.16.90:8081/repository/npm-kxl-internal/.</span><br></pre></td></tr></table></figure><p>执行命令，提示填写账号密码和邮箱，验证通过后将会在 用户主目录下的 .npmrc 文件中插入一条 此仓库 url 和对应的 token。</p><p><img src="https://img.xxlaila.cn/1566528452423.jpg" alt="img"></p><p>在确保项目有 package.json 前提下，执行：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm publish --registry=http://172.21.16.90:8081/repository/npm-kxl-internal/</span><br></pre></td></tr></table></figure><p>至此，使用 Nexus Repository Manager 3 搭建 npm 私服结束。整体流程并不复杂，文档很详尽,直接读文档可能会遗漏一些东西。可以参考<a href="https://help.sonatype.com/repomanager3/formats/npm-registry" target="_blank" rel="noopener">官方文档</a></p><p><a href="https://xxlaila.github.io/2019/10/15/nexus配置ldap/" target="_blank" rel="noopener">nexus ldap配置</a></p><!-- rebuild by neat -->]]></content>
      <categories>
        <category>nexus</category>
      </categories>
      <tags>
        <tag>npm</tag>
      </tags>
  </entry>
  <entry>
    <title>nginx URL 斜杠问题</title>
    <url>/2019/08/22/nginx-URL-%E6%96%9C%E6%9D%A0%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>今天公司新上的一个前端应用遇到一个问题，那就是在微信登录界面扫码登录之后，微信回调给我们的地址多加了一个斜杠;</p><blockquote><p>错误的地址:<a href="http://a.xxlaila.com/wx.html/?code=011amZet0h1IUf19Fvht0jg4ft0amZeN" target="_blank" rel="noopener">http://a.xxlaila.com/wx.html/?code=011amZet0h1IUf19Fvht0jg4ft0amZeN</a><br>正确的地址:<a href="http://a.xxlaila.com/wx.html?code=011amZet0h1IUf19Fvht0jg4ft0amZeN" target="_blank" rel="noopener">http://a.xxlaila.com/wx.html?code=011amZet0h1IUf19Fvht0jg4ft0amZeN</a></p></blockquote><p>在nginx上配置需要吧这个斜杠删除掉。用户才能正常的访问；</p><h3 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h3><p>在配置文件里面增加如下配置项</p><a id="more"></a><ul><li>删除URL结尾的斜杠</li></ul><p><em>rewrite ^/(.</em>)/$ /$1 permanent;*</p><figure class="highlight nginx"><table><tr><td class="code"><pre><span class="line"><span class="section">server</span> &#123;</span><br><span class="line">  <span class="attribute">listen</span> <span class="number">80</span>;</span><br><span class="line">  <span class="attribute">charset</span> utf-<span class="number">8</span>;</span><br><span class="line">  <span class="attribute">server_name</span>  a.xxlaila.com;</span><br><span class="line">  <span class="attribute">rewrite</span><span class="regexp"> ^/(.*)/$</span> /<span class="variable">$1</span> <span class="literal">permanent</span>;</span><br><span class="line">  <span class="attribute">index</span> index.html index.htm index.jsp index.php;</span><br><span class="line">  <span class="attribute">root</span> /opt/webapps/a.xxlaila.com;</span><br><span class="line"></span><br><span class="line">  <span class="attribute">location</span> <span class="regexp">~* /</span> &#123;</span><br><span class="line">    <span class="attribute">try_files</span> <span class="variable">$uri</span> <span class="variable">$uri</span>/ /index.html;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="attribute">access_log</span>   /var/log/nginx/a.xxlaila.com.access.log main;</span><br><span class="line"><span class="comment">#  error_log   /var/log/nginx/a.xxlaila.com.error.log debug;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>在URL结尾添加斜杠<br>在配置文件增加如下配置项目</li></ul><p><em>rewrite ^(.</em>[^/])$ $1/ permanent;*</p><figure class="highlight nginx"><table><tr><td class="code"><pre><span class="line"><span class="section">server</span> &#123;</span><br><span class="line">  <span class="attribute">listen</span> <span class="number">80</span>;</span><br><span class="line">  <span class="attribute">charset</span> utf-<span class="number">8</span>;</span><br><span class="line">  <span class="attribute">server_name</span>  a.xxlaila.com;</span><br><span class="line">  <span class="attribute">rewrite</span><span class="regexp"> ^(.*[^/])$</span> <span class="variable">$1</span>/ <span class="literal">permanent</span>;</span><br><span class="line">  <span class="attribute">index</span> index.html index.htm index.jsp index.php;</span><br><span class="line">  <span class="attribute">root</span> /opt/webapps/a.xxlaila.com;</span><br><span class="line"></span><br><span class="line">  <span class="attribute">location</span> <span class="regexp">~* /</span> &#123;</span><br><span class="line">    <span class="attribute">try_files</span> <span class="variable">$uri</span> <span class="variable">$uri</span>/ /index.html;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="attribute">access_log</span>   /var/log/nginx/a.xxlaila.com.access.log main;</span><br><span class="line"><span class="comment">#  error_log   /var/log/nginx/a.xxlaila.com.error.log debug;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="问题2"><a href="#问题2" class="headerlink" title="问题2"></a>问题2</h2><p>在浏览器访问某一个url/页面的时候，通常有时候带有.html的一个扩展名，现需求是带<code>.html</code>和不带<code>.html</code>都可以访问</p><h3 id="例"><a href="#例" class="headerlink" title="例"></a>例</h3><p>增加如下配置文件</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (!-e <span class="variable">$request_filename</span>) &#123;    </span><br><span class="line">       rewrite ^(.*)$ /<span class="variable">$1</span>.html last;</span><br><span class="line">       <span class="built_in">break</span>;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">  listen 80;</span><br><span class="line">  charset utf-8;</span><br><span class="line">  server_name  a.xxlaila.com;</span><br><span class="line">  rewrite ^/(.*)/$ /<span class="variable">$1</span> permanent;</span><br><span class="line">  index index.html index.htm index.jsp index.php;</span><br><span class="line">  root /opt/webapps/a.xxlaila.com;</span><br><span class="line"></span><br><span class="line">  location ~* / &#123;</span><br><span class="line">    <span class="keyword">if</span> (!-e <span class="variable">$request_filename</span>) &#123;</span><br><span class="line">       rewrite ^(.*)$ /<span class="variable">$1</span>.html last;</span><br><span class="line">       <span class="built_in">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    try_files <span class="variable">$uri</span> <span class="variable">$uri</span>/ /index.html;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  access_log   /var/<span class="built_in">log</span>/nginx/a.xxlaila.com.access.log main;</span><br><span class="line"><span class="comment">#  error_log   /var/log/nginx/a.xxlaila.com.error.log debug;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      <categories>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title>kubernetes-ci/cd-(四)</title>
    <url>/2019/08/20/kubernetes-ci-cd-%E5%9B%9B/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><h2 id="1、Blue-Ocean"><a href="#1、Blue-Ocean" class="headerlink" title="1、Blue Ocean"></a>1、Blue Ocean</h2><p>安装Blue Ocean插件</p><h3 id="1-1、创建pipeline"><a href="#1-1、创建pipeline" class="headerlink" title="1.1、创建pipeline"></a>1.1、创建pipeline</h3><p><img src="https://img.xxlaila.cn/348knfnsdlds.png" alt="img"></p><ul><li>配置代码库的地址</li><li>然后配置授权账户</li></ul><p><img src="https://img.xxlaila.cn/9857jksdhfjkhdsfds.png" alt="img"></p><p>在这儿之前git库里面必须存在于jenkinsfile文件，pipeline会自动去扫描代码库里面的分支，然后根据每一个分支建立一个类似于job的形式，然后我们可以根据每一个分支进行部署，可以执行定时触发，部署</p><p><img src="https://img.xxlaila.cn/382dklfjdskjfs.png" alt="img"></p><p>这儿，只有一个分支存在于jenkinsfile，所以只显示一个分支，如下图：</p><!-- rebuild by neat -->]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>jenkins, ci/cd</tag>
      </tags>
  </entry>
  <entry>
    <title>kubernetes-ci/cd-(三)</title>
    <url>/2019/08/20/kubernetes-ci-cd-%E4%B8%89/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><p>&nbsp;&nbsp;&nbsp;&nbsp;jenkins 配置完成后，最终实现的是ci/cd，在编译的过程中，经常会遇到后端java的，前端nodejs的，这里就需要进行一个k8s在调度的时候生产pod来进行指定pod进行编译</p><h3 id="1、制作容器"><a href="#1、制作容器" class="headerlink" title="1、制作容器"></a>1、制作容器</h3><p>自定义一个容器，里面包含了 java，nodejs的所需要的环境，同时需要同步容器的时间，包含来jenkins的node</p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat Dockerfile</span></span><br><span class="line"><span class="attr">FROM</span> <span class="string">docker.io/centos:latest</span></span><br><span class="line"><span class="attr">MAINTAINER</span> <span class="string">xxlaila "cq_xxlaila@163.com"</span></span><br><span class="line"><span class="comment"># Install dependent plugin</span></span><br><span class="line"><span class="attr">ENV</span> <span class="string">VERSION v10.15.1</span></span><br><span class="line"><span class="attr">RUN</span> <span class="string">yum install -y wget \</span></span><br><span class="line">    <span class="attr">git</span> <span class="string">\</span></span><br><span class="line">    <span class="meta">java-1.8.0-openjdk.x86_64</span> <span class="string">\</span></span><br><span class="line">    <span class="meta">&amp;&amp;</span> <span class="string">curl -sL https://rpm.nodesource.com/setup_11.x | bash - \</span></span><br><span class="line">    <span class="meta">&amp;&amp;</span> <span class="string">yum install -y gcc gcc-c++ make \</span></span><br><span class="line">    <span class="meta">&amp;&amp;</span> <span class="string">yum install -y nodejs \</span></span><br><span class="line">    <span class="meta">&amp;&amp;</span> <span class="string">yum clean all</span></span><br><span class="line"><span class="comment"># System variable setting</span></span><br><span class="line"><span class="attr">RUN</span> <span class="string">echo "LANG=zh_CN.UTF-8" &gt;&gt; /etc/locale.conf \</span></span><br><span class="line">    <span class="meta">&amp;&amp;</span> <span class="string">source /etc/locale.conf \</span></span><br><span class="line">    <span class="meta">&amp;&amp;</span> <span class="string">cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \</span></span><br><span class="line">    <span class="meta">&amp;&amp;</span> <span class="string">echo "Asia/shanghai" &gt;&gt; /etc/timezone \</span></span><br><span class="line">    <span class="meta">&amp;&amp;</span> <span class="string">groupadd -g 10000 jenkins \</span></span><br><span class="line">    <span class="meta">&amp;&amp;</span> <span class="string">useradd -g jenkins -u 10000 jenkins</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="attr">EXPOSE</span> <span class="string">50000</span></span><br></pre></td></tr></table></figure><a id="more"></a><ul><li><p>执行容器打包</p><figure class="highlight gcode"><table><tr><td class="code"><pre><span class="line"><span class="attr"># docker build -t centos7</span><span class="number">.6</span>/<span class="symbol">node11</span>:latest .\</span><br></pre></td></tr></table></figure></li><li><p>推送容器到私有镜像仓库</p><figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line"># docker tag centos7<span class="number">.6</span>/node11:latest docker.io/xxlaila/centos<span class="number">-7</span>-jdk1<span class="number">.8</span>-nodejs11<span class="number">.10</span>-jenkins:latest</span><br><span class="line"># docker push docker.io/xxlaila/centos<span class="number">-7</span>-jdk1<span class="number">.8</span>-nodejs11<span class="number">.10</span>-jenkins:latest</span><br></pre></td></tr></table></figure></li></ul><h3 id="2、jenkins的配置"><a href="#2、jenkins的配置" class="headerlink" title="2、jenkins的配置"></a>2、jenkins的配置</h3><h4 id="2-1、系统配置"><a href="#2-1、系统配置" class="headerlink" title="2.1、系统配置"></a>2.1、系统配置</h4><p>jenkins——&gt;系统管理——&gt;系统设置<br><strong>名称</strong>：kubernetes<br><strong>地址</strong>：<a href="https://kubernetes.default.svc.cluster.local" target="_blank" rel="noopener">https://kubernetes.default.svc.cluster.local</a><br><strong>jenkins地址</strong>：<a href="http://jenkins2.kube-ops.svc.cluster.local:8080" target="_blank" rel="noopener">http://jenkins2.kube-ops.svc.cluster.local:8080</a><br><img src="https://img.xxlaila.cn/489kdngkdhfkodsmf.png" alt="img"></p><h4 id="2-2、增加一个kubenetes-pod-templates"><a href="#2-2、增加一个kubenetes-pod-templates" class="headerlink" title="2.2、增加一个kubenetes pod templates"></a>2.2、增加一个kubenetes pod templates</h4><p><img src="https://img.xxlaila.cn/83jknfkdslfds.png" alt="img"></p><h4 id="2-3、配置容器环境"><a href="#2-3、配置容器环境" class="headerlink" title="2.3、配置容器环境"></a>2.3、配置容器环境</h4><p><img src="https://img.xxlaila.cn/42clkdsjfkldsfs.png" alt="img"></p><h4 id="2-4、配置权限"><a href="#2-4、配置权限" class="headerlink" title="2.4、配置权限"></a>2.4、配置权限</h4><p><img src="https://img.xxlaila.cn/3486238kmxnfksd.png" alt="img"></p><h3 id="3、测试job"><a href="#3、测试job" class="headerlink" title="3、测试job"></a>3、测试job</h3><p>建立一个test job 的pipeline来进行容器是否正常</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">node (<span class="string">'agent-node'</span>)&#123;</span><br><span class="line">    container(<span class="string">'nodejs'</span>) &#123;</span><br><span class="line">        sh <span class="string">'whoami'</span></span><br><span class="line">        sh <span class="string">'hostname'</span></span><br><span class="line">        sh <span class="string">'echo $PATH'</span></span><br><span class="line">        sh <span class="string">'npm version'</span></span><br><span class="line">        sh <span class="string">'node -v'</span></span><br><span class="line">        sh <span class="string">'npx -v'</span></span><br><span class="line">        sh <span class="string">'java -version'</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="3-1、建立pipeline"><a href="#3-1、建立pipeline" class="headerlink" title="3.1、建立pipeline"></a>3.1、建立pipeline</h4><h5 id="3-1-1、建立一个后端"><a href="#3-1-1、建立一个后端" class="headerlink" title="3.1.1、建立一个后端"></a>3.1.1、建立一个后端</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">node(<span class="string">'agent-build'</span>) &#123;</span><br><span class="line">   stage(<span class="string">'Clone'</span>) &#123;</span><br><span class="line">      <span class="built_in">echo</span> <span class="string">"1.Clone Stage"</span></span><br><span class="line">      git credentialsId:<span class="string">'gitlabUser'</span>, url: <span class="string">'http://gitlab.xxlaila.com/plat/middleware/kxl-eureka.git'</span></span><br><span class="line">    &#125;</span><br><span class="line">    stage(<span class="string">'build'</span>) &#123;</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"2. Start build <span class="variable">$&#123;JOB_NAME&#125;</span>"</span></span><br><span class="line">        sh <span class="string">'/opt/bin/mvn clean package'</span></span><br><span class="line">    &#125;</span><br><span class="line">    stage(<span class="string">'show package'</span>) &#123;</span><br><span class="line">        sh <span class="string">'pwd'</span></span><br><span class="line">        sh <span class="string">'ls -ltrh target/'</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="3-1-2、建立一个前端"><a href="#3-1-2、建立一个前端" class="headerlink" title="3.1.2、建立一个前端"></a>3.1.2、建立一个前端</h4><figure class="highlight vim"><table><tr><td class="code"><pre><span class="line">node(<span class="string">'agent-build'</span>) &#123;</span><br><span class="line">    stage(<span class="string">'Clone'</span>) &#123;</span><br><span class="line">      <span class="keyword">echo</span> <span class="string">"1.Clone Stage"</span></span><br><span class="line">      git credentialsId:<span class="string">'gitlabUser'</span>, ur<span class="variable">l:</span> <span class="string">'http://gitlab.xxlaila.com/front-end/portal/kts-platform.git'</span></span><br><span class="line">    &#125;</span><br><span class="line">    stage(<span class="string">'install'</span>) &#123;</span><br><span class="line">        container(<span class="string">'nodejs'</span>)&#123;</span><br><span class="line">            <span class="keyword">echo</span> <span class="string">"2. Start install $&#123;JOB_NAME&#125;"</span></span><br><span class="line">            <span class="keyword">sh</span> <span class="string">'node -v'</span></span><br><span class="line">            <span class="keyword">sh</span> <span class="string">'npm install'</span></span><br><span class="line">            <span class="keyword">sh</span> <span class="string">'npm run build production'</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    stage(<span class="string">'show package'</span>) &#123;</span><br><span class="line">        <span class="keyword">sh</span> <span class="string">'pwd'</span></span><br><span class="line">        <span class="keyword">sh</span> <span class="string">'ls -ltrh'</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>ci/cd</tag>
      </tags>
  </entry>
  <entry>
    <title>kubernetes-ci/cd-(二)</title>
    <url>/2019/08/20/kubernetes-ci-cd-%E4%BA%8C/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:23 GMT+0800 (China Standard Time) --><h1 id="基于jenkins-pipeline进行部署"><a href="#基于jenkins-pipeline进行部署" class="headerlink" title="基于jenkins  pipeline进行部署"></a>基于jenkins pipeline进行部署</h1><h2 id="1、jenkins-pipeline介绍"><a href="#1、jenkins-pipeline介绍" class="headerlink" title="1、jenkins pipeline介绍"></a>1、jenkins pipeline介绍</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;要实现在 Jenkins 中的构建工作，可以有多种方式，我们这里采用比较常用的 Pipeline 这种方式。Pipeline，简单来说，就是一套运行在 Jenkins 上的工作流框架，将原来独立运行于单个或者多个节点的任务连接起来，实现单个任务难以完成的复杂流程编排和可视化的工作。</p><p>Jenkins Pipeline 有几个核心概念:</p><ul><li>Node：节点，一个 Node 就是一个 Jenkins 节点，Master 或者 Agent，是执行 Step 的具体运行环境，比如我们之前动态运行的 Jenkins Slave 就是一个 Node 节点</li><li>Stage：阶段，一个 Pipeline 可以划分为若干个 Stage，每个 Stage 代表一组操作，比如：Build、Test、Deploy，Stage 是一个逻辑分组的概念，可以跨多个 Node</li><li>Step：步骤，Step 是最基本的操作单元，可以是打印一句话，也可以是构建一个 Docker 镜像，由各类 Jenkins 插件提供，比如命令：sh ‘make’，就相当于我们平时 shell 终端中执行 make 命令一样。</li></ul><p>那么我们如何创建 Jenkins Pipline 呢？</p><ul><li>Pipeline 脚本是由 Groovy 语言实现的，但是我们没必要单独去学习 Groovy，当然你会的话最好</li><li>Pipeline 支持两种语法：Declarative(声明式)和 Scripted Pipeline(脚本式)语法</li><li>Pipeline 也有两种创建方法：可以直接在 Jenkins 的 Web UI 界面中输入脚本；也可以通过创建一个 Jenkinsfile 脚本文件放入项目源码库中</li><li>一般我们都推荐在 Jenkins 中直接从源代码控制(SCMD)中直接载入 Jenkinsfile Pipeline 这种方法创建一个简单的 Pipeline<blockquote><p>我们这里来给大家快速创建一个简单的 Pipeline，直接在 Jenkins 的 Web UI 界面中输入脚本运行。</p></blockquote></li><li>新建 Job：在 Web UI 中点击 New Item -&gt; 输入名称：pipeline-demo -&gt; 选择下面的 Pipeline -&gt; 点击 OK</li><li>配置：在最下方的 Pipeline 区域输入如下 Script 脚本，然后点击保存。</li></ul><a id="more"></a><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">shell node &#123;</span><br><span class="line">    stage(<span class="string">'Clone'</span>) &#123; </span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"1.Clone Stage"</span> </span><br><span class="line">    &#125; </span><br><span class="line">    stage(<span class="string">'Test'</span>) &#123; </span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"2.Test Stage"</span> </span><br><span class="line">    &#125; </span><br><span class="line">    stage(<span class="string">'Build'</span>) &#123; </span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"3.Build Stage"</span> </span><br><span class="line">        </span><br><span class="line">    &#125; </span><br><span class="line">    stage(<span class="string">'Deploy'</span>) &#123;</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"4. Deploy Stage"</span> </span><br><span class="line">    &#125;    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>构建：点击左侧区域的 Build Now，可以看到 Job 开始构建了<blockquote><p>隔一会儿，构建完成，可以点击左侧区域的 Console Output，我们就可以看到如下输出信息：</p></blockquote></li></ul><p><img src="https://img.xxlaila.cn/sdsdsjid23874823ehsj.png" alt="img"></p><ul><li>在 Slave 中构建任务<br>&nbsp;&nbsp;&nbsp;&nbsp;上面我们创建了一个简单的 Pipeline 任务，但是我们可以看到这个任务并没有在 Jenkins 的 Slave 中运行，那么如何让我们的任务跑在 Slave 中呢？还记得上节课我们在添加 Slave Pod 的时候，一定要记住添加的 label 吗？没错，我们就需要用到这个 label，我们重新编辑上面创建的 Pipeline 脚本，给 node 添加一个 label 属性，如下:</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">node(<span class="string">'jnlp-agent'</span>) &#123;</span><br><span class="line">    stage(<span class="string">'Clone'</span>) &#123;</span><br><span class="line">      <span class="built_in">echo</span> <span class="string">"1.Clone Stage"</span></span><br><span class="line">    &#125;</span><br><span class="line">    stage(<span class="string">'Test'</span>) &#123;</span><br><span class="line">      <span class="built_in">echo</span> <span class="string">"2.Test Stage"</span></span><br><span class="line">    &#125;</span><br><span class="line">    stage(<span class="string">'Build'</span>) &#123;</span><br><span class="line">      <span class="built_in">echo</span> <span class="string">"3.Build Stage"</span></span><br><span class="line">    &#125;</span><br><span class="line">    stage(<span class="string">'Deploy'</span>) &#123;</span><br><span class="line">      <span class="built_in">echo</span> <span class="string">"4. Deploy Stage"</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里只是给 node 添加了一个jnlp-agent这样的一个label，然后我们保存，构建之前查看下 kubernetes 集群中的 Pod：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[<span class="built_in">test</span>] [root@k8s-zxc-test-3 ~]<span class="comment"># kubectl get pods -n kube-ops</span></span><br><span class="line">NAME                        READY   STATUS              RESTARTS   AGE</span><br><span class="line">jenkins2-696b8fbdbb-q24nm   1/1     Running             0          45h</span><br><span class="line">jnlp-agent-342fv            0/1     ContainerCreating   0          0s</span><br><span class="line">[<span class="built_in">test</span>] [root@k8s-zxc-test-3 ~]<span class="comment"># kubectl get pods -n kube-ops</span></span><br><span class="line">NAME                        READY   STATUS              RESTARTS   AGE</span><br><span class="line">jenkins2-696b8fbdbb-q24nm   1/1     Running             0          45h</span><br><span class="line">jnlp-agent-342fv            0/1     ContainerCreating   0          1s</span><br></pre></td></tr></table></figure><p><img src="https://img.xxlaila.cn/dhf482390dsfjkdsg.png" alt="img"></p><ul><li><p>kubernetes 界面显示<br><img src="https://img.xxlaila.cn/9374hkdhskfjsdd.png" alt="img"></p></li><li><p>jenkins执行结果显示<br><img src="https://img.xxlaila.cn/23cvkndiuyriens.png" alt="img"></p></li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp; 证明我们当前的任务在跑在上面动态生成的这个 Pod 中，也符合我们的预期。我们回到 Job 的主界面，也可以看到大家可能比较熟悉的 Stage View 界面：<br><img src="https://img.xxlaila.cn/45ksfh9whnkxa.png" alt="img"></p><h5 id="部署-Kubernetes-应用"><a href="#部署-Kubernetes-应用" class="headerlink" title="部署 Kubernetes 应用"></a>部署 Kubernetes 应用</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;我们已经知道了如何在 Jenkins Slave 中构建任务了，那么如何来部署一个原生的 Kubernetes 应用呢？ 要部署 Kubernetes 应用，我们就得对我们之前部署应用的流程要非常熟悉才行，我们之前的流程是怎样的：</p><ul><li>1、编写代码</li><li>2、测试</li><li>3、编写 Dockerfile</li><li>4、构建打包 Docker 镜像</li><li>5、推送 Docker 镜像到仓库</li><li>6、编写 Kubernetes YAML 文件</li><li>7、更改 YAML 文件中 Docker 镜像 TAG</li><li>8、利用 kubectl 工具部署应用</li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;我们之前在 Kubernetes 环境中部署一个原生应用的流程应该基本上是上面这些流程吧？现在我们就需要把上面这些流程放入 Jenkins 中来自动帮我们完成(当然编码除外)，从测试到更新 YAML 文件属于 CI 流程，后面部署属于 CD 的流程。如果按照我们上面的示例，我们现在要来编写一个 Pipeline 的脚本。</p><ul><li><p>修改test-spring-social-wechat-sample pipeline脚本</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">node(<span class="string">'jnlp-agent'</span>) &#123;</span><br><span class="line">   stage(<span class="string">'Clone'</span>) &#123;</span><br><span class="line">      <span class="built_in">echo</span> <span class="string">"1.Clone Stage"</span></span><br><span class="line">    &#125;</span><br><span class="line">    stage(<span class="string">'Test'</span>) &#123;</span><br><span class="line">      <span class="built_in">echo</span> <span class="string">"2.Test Stage"</span></span><br><span class="line">    &#125;</span><br><span class="line">    stage(<span class="string">'Build'</span>) &#123;</span><br><span class="line">      <span class="built_in">echo</span> <span class="string">"3.Build Docker Image Stage"</span></span><br><span class="line">    &#125;</span><br><span class="line">    stage(<span class="string">'Push'</span>) &#123;</span><br><span class="line">      <span class="built_in">echo</span> <span class="string">"4.Push Docker Image Stage"</span></span><br><span class="line">    &#125;</span><br><span class="line">    stage(<span class="string">'YAML'</span>) &#123;</span><br><span class="line">      <span class="built_in">echo</span> <span class="string">"5. Change YAML File Stage"</span></span><br><span class="line">    &#125;</span><br><span class="line">    stage(<span class="string">'Deploy'</span>) &#123;</span><br><span class="line">      <span class="built_in">echo</span> <span class="string">"6. Deploy Stage"</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>1）、增加git地址，进行代码的clone</p><figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="title">stage</span><span class="params">(<span class="string">'Clone'</span>)</span></span> &#123;</span><br><span class="line">   echo <span class="string">"1.Clone Stage"</span></span><br><span class="line">   git url: <span class="string">"https://github.com/xxlaila/jenkins-demo.git"</span></span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></li><li><p>2）、进行测试</p><figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="title">stage</span><span class="params">(<span class="string">'Test'</span>)</span></span> &#123;</span><br><span class="line">  echo <span class="string">"2.Test Stage"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>3）、构建一个docker镜像</p><figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="title">stage</span><span class="params">(<span class="string">'Build'</span>)</span></span> &#123;</span><br><span class="line">  echo <span class="string">"3.Build Docker Image Stage"</span></span><br><span class="line">  sh <span class="string">"docker build -t xxlaila/jenkins-demo:$&#123;build_tag&#125; ."</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;平时构建的时候是不是都是直接使用docker build命令进行构建就行了，那么这个地方呢？我们上节课给大家提供的 Slave Pod 的镜像里面是不是采用的 Docker In Docker 的方式，也就是说我们也可以直接在 Slave 中使用 docker build 命令，所以我们这里直接使用 sh 直接执行 docker build 命令即可，但是镜像的 tag 呢？如果我们使用镜像 tag，则每次都是 latest 的 tag，这对于以后的排查或者回滚之类的工作会带来很大麻烦，我们这里采用和git commit的记录为镜像的 tag，这里有一个好处就是镜像的 tag 可以和 git 提交记录对应起来，也方便日后对应查看。但是由于这个 tag 不只是我们这一个 stage 需要使用，下一个推送镜像是不是也需要，所以这里我们把这个 tag 编写成一个公共的参数，把它放在 Clone 这个 stage 中，修改前面两个 stage:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">stage(<span class="string">'Clone'</span>) &#123;</span><br><span class="line">      <span class="built_in">echo</span> <span class="string">"1.Clone Stage"</span></span><br><span class="line">      git url: <span class="string">"https://github.com/xxlaila/jenkins-demo.git"</span></span><br><span class="line">      script &#123;</span><br><span class="line">        build_tag = sh(returnStdout: <span class="literal">true</span>, script: <span class="string">'git rev-parse --short HEAD'</span>).trim()</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    stage(<span class="string">'Build'</span>) &#123;</span><br><span class="line">      <span class="built_in">echo</span> <span class="string">"3.Build Docker Image Stage"</span></span><br><span class="line">      sh <span class="string">"docker build -t xxlaila/jenkins-demo:<span class="variable">$&#123;build_tag&#125;</span> ."</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><ul><li>4）、推送镜像<br>&nbsp;&nbsp;&nbsp;&nbsp;镜像构建完成了，现在我们就需要将此处构建的镜像推送到镜像仓库中去，当然如果你有私有镜像仓库也可以，这里还没有自己搭建私有的仓库，所以直接使用 docker hub 即可。<br>&nbsp;&nbsp;&nbsp;&nbsp;我们知道 docker hub 是公共的镜像仓库，任何人都可以获取上面的镜像，但是要往上推送镜像我们就需要用到一个帐号了，所以我们需要提前注册一个 docker hub 的帐号，记住用户名和密码，我们这里需要使用。正常来说我们在本地推送 docker 镜像的时候，是不是需要使用docker login命令，然后输入用户名和密码，认证通过后，就可以使用docker push命令来推送本地的镜像到 docker hub 上面去了，如果是这样的话，我们这里的 Pipeline 是不是就该这样写了：</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">stage(<span class="string">'Push'</span>) &#123;</span><br><span class="line">      <span class="built_in">echo</span> <span class="string">"4.Push Docker Image Stage"</span></span><br><span class="line">      sh <span class="string">"docker login -u cq_xxlaila@163.com -p 111111"</span></span><br><span class="line">      sh <span class="string">"docker push xxlaila/jenkins-demo:<span class="variable">$&#123;build_tag&#125;</span>"</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;如果只是在 Jenkins 的 Web UI 界面中来完成这个任务的话，我们这里的 Pipeline 是可以这样写的，但是我们是不是推荐使用 Jenkinsfile 的形式放入源码中进行版本管理，这样的话我们直接把 docker 仓库的用户名和密码暴露给别人这样很显然是非常非常不安全的，更何况我们这里使用的是 github 的公共代码仓库，所有人都可以直接看到我们的源码，所以我们应该用一种方式来隐藏用户名和密码这种私密信息，幸运的是 Jenkins 为我们提供了解决方法。<br>&nbsp;&nbsp;&nbsp;&nbsp;在首页点击 Credentials -&gt; Stores scoped to Jenkins 下面的 Jenkins -&gt; Global credentials (凭据) -&gt;system(系统)-&gt;全局凭据 (unrestricted)-&gt; 左侧的 Add Credentials( 添加凭据)：添加一个 Username with password 类型的认证信息，如下：<br><img src="https://img.xxlaila.cn/374kdjfkskfdsd.png" alt="img"></p><p>&nbsp;&nbsp;&nbsp;&nbsp;Add Credentials 输入 docker hub 的用户名和密码，ID 部分我们输入dockerHub，注意，这个值非常重要，在后面 Pipeline 的脚本中我们需要使用到这个 ID 值。<br>&nbsp;&nbsp;&nbsp;&nbsp;有了上面的 docker hub 的用户名和密码的认证信息，现在修改 Pipeline 中的第四部，使用这里的用户名和密码：</p><figure class="highlight reasonml"><table><tr><td class="code"><pre><span class="line">stage('Push') &#123;</span><br><span class="line">     echo <span class="string">"4.Push Docker Image Stage"</span></span><br><span class="line">     <span class="keyword">with</span><span class="constructor">Credentials([<span class="params">usernamePassword</span>(<span class="params">credentialsId</span>: '<span class="params">dockerHub</span>', <span class="params">passwordVariable</span>: '<span class="params">dockerHubPassword</span>', <span class="params">usernameVariable</span>: '<span class="params">dockerHubUser</span>')</span>]) &#123;</span><br><span class="line">         sh <span class="string">"docker login -u $&#123;dockerHubUser&#125; -p $&#123;dockerHubPassword&#125;"</span></span><br><span class="line">         sh <span class="string">"docker push xxlaila/jenkins-demo:$&#123;build_tag&#125;"</span></span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><blockquote><p><em>注意</em>:<br>&nbsp;&nbsp;&nbsp;&nbsp;我们这里在 stage 中使用了一个新的函数withCredentials，其中有一个credentialsId值就是我们刚刚创建的 ID 值，然后就可以在脚本中直接使用这里两个变量值来直接替换掉之前的登录 docker hub 的用户名和密码，这样操作就相对来说就很安全了，只是传递进去了两个变量而已，别人并不知道真正用户名和密码，只有我们自己的 Jenkins 平台上添加的才知道。<br><em>测试结果</em>:</p></blockquote><p><img src="https://img.xxlaila.cn/4ykjdbfjdshfkojdsl.png" alt="img"></p><ul><li>5）、更改 YAML<br>&nbsp;&nbsp;&nbsp;&nbsp;上面已经完成了镜像的打包、推送的工作，接下来我们是不是应该更新 Kubernetes 系统中应用的镜像版本了，当然为了方便维护，我们都是用 YAML 文件的形式来编写应用部署规则，比如我们这里的 YAML 文件：(k8s.yaml)<figure class="highlight dts"><table><tr><td class="code"><pre><span class="line"><span class="meta"># cat k8s.yaml</span></span><br><span class="line"><span class="symbol">apiVersion:</span> extensions/v1beta1</span><br><span class="line"><span class="symbol">kind:</span> Deployment</span><br><span class="line"><span class="symbol">metadata:</span></span><br><span class="line"><span class="symbol">  name:</span> jenkins-demo</span><br><span class="line"><span class="symbol">  namespace:</span> default</span><br><span class="line"><span class="symbol">spec:</span></span><br><span class="line"><span class="symbol">  template:</span></span><br><span class="line"><span class="symbol">    metadata:</span></span><br><span class="line"><span class="symbol">      labels:</span></span><br><span class="line"><span class="symbol">        app:</span> jenkins-demo</span><br><span class="line"><span class="symbol">    spec:</span></span><br><span class="line"><span class="symbol">      containers:</span></span><br><span class="line">      - image: xxlaila/jenkins-demo:<span class="params">&lt;BUILD_TAG&gt;</span></span><br><span class="line"><span class="symbol">        imagePullPolicy:</span> IfNotPresent</span><br><span class="line"><span class="symbol">        name:</span> jenkins-demo</span><br><span class="line"><span class="symbol">        env:</span></span><br><span class="line">        - name: branch</span><br><span class="line"><span class="symbol">          value:</span> <span class="params">&lt;BRANCH_NAME&gt;</span></span><br></pre></td></tr></table></figure></li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;使用一个 Deployment 资源对象来管理 Pod，该 Pod 使用的就是我们上面推送的镜像，唯一不同的地方是 Docker 镜像的 tag 不是我们平常见的具体的 tag，而是一个 的标识，实际上如果我们将这个标识替换成上面的 Docker 镜像的 tag，是不是就是最终我们本次构建需要使用到的镜像？怎么替换呢？其实也很简单，我们使用一个sed命令就可以实现了：</p><figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="title">stage</span><span class="params">(<span class="string">'YAML'</span>)</span></span> &#123;</span><br><span class="line">      echo <span class="string">"5. Change YAML File Stage"</span></span><br><span class="line">      sh <span class="string">"sed -i 's/&lt;BUILD_TAG&gt;/$&#123;build_tag&#125;/' k8s.yaml"</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><blockquote><p>sed 命令就是将 k8s.yaml 文件中的 标识给替换成变量 build_tag 的值。</p></blockquote><ul><li>6）、部署<br>&nbsp;&nbsp;&nbsp;&nbsp;Kubernetes 应用的 YAML 文件已经更改完成了，之前我们手动的环境下，是不是直接使用 kubectl apply 命令就可以直接更新应用。当然这里只是写入到了 Pipeline 里面，思路都是一样的：</li></ul><figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="title">stage</span><span class="params">(<span class="string">'Deploy'</span>)</span></span> &#123;</span><br><span class="line">      echo <span class="string">"6. Deploy Stage"</span></span><br><span class="line">      sh <span class="string">"kubectl apply -f k8s.yaml"</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><ul><li>点击jenkins进行构建<br><img src="https://img.xxlaila.cn/56hjkshdfdksnfkldsj.png" alt="img"></li></ul><blockquote><p>当然，这里部署失败，先别管，证明流程是对的，可以这么走</p></blockquote><p><img src="https://img.xxlaila.cn/3947dnfdsflskfjdsf.png" alt="img"></p><p>&nbsp;&nbsp;&nbsp;&nbsp;以上的配置基本已经完成，但是我们的实际项目实践过程中，可能还需要一些人工干预的步骤，比如我们提交了一次代码，测试也通过了，镜像也打包上传了，但是这个版本并不一定就是要立刻上线到生产环境的。我们可能需要将该版本先发布到测试环境、QA 环境、或者预览环境之类的，总之直接就发布到线上环境去还是挺少见的，所以我们需要增加人工确认的环节，一般都是在 CD 的环节才需要人工干预，比如我们这里的最后两步，我们就可以在前面加上确认，比如：</p><figure class="highlight vim"><table><tr><td class="code"><pre><span class="line">stage(<span class="string">'YAML'</span>) &#123;</span><br><span class="line">      <span class="keyword">echo</span> <span class="string">"5. Change YAML File Stage"</span></span><br><span class="line">      def userInput = <span class="built_in">input</span>(</span><br><span class="line">          id: <span class="string">'userInput'</span>,</span><br><span class="line">          message: <span class="string">'Choose a deploy environment'</span>,</span><br><span class="line">          parameter<span class="variable">s:</span> [</span><br><span class="line">              [</span><br><span class="line">                  #clas<span class="variable">s:</span> <span class="string">'ChoiceParameterDefinition'</span>,</span><br><span class="line">                  choice<span class="variable">s:</span> <span class="string">"Dev\nTest\nUat\nDemo\nPord"</span>,</span><br><span class="line">                  name: <span class="string">'Env'</span></span><br><span class="line">                  ]</span><br><span class="line">              ]</span><br><span class="line">          )</span><br><span class="line">          <span class="keyword">echo</span> <span class="string">"This is a deploy step to $&#123;userInput.Env&#125;"</span></span><br><span class="line">          <span class="keyword">sh</span> <span class="string">"sed -i 's/&lt;BUILD_TAG&gt;/$&#123;build_tag&#125;/' k8s.yaml"</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;这里使用了 input 关键字，里面使用一个 Choice 的列表来让用户进行选择，然后在我们选择了部署环境后，我们当然也可以针对不同的环境再做一些操作，比如可以给不同环境的 YAML 文件部署到不同的 namespace 下面去，增加不同的标签等等操作：</p><figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="title">stage</span><span class="params">(<span class="string">'Deploy'</span>)</span></span> &#123;</span><br><span class="line">      echo <span class="string">"6. Deploy Stage"</span></span><br><span class="line">      <span class="keyword">if</span> (userInput<span class="selector-class">.Env</span> == <span class="string">"Dev"</span>)&#123;</span><br><span class="line">          <span class="comment">// deploy dev stuff</span></span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (userInput<span class="selector-class">.Env</span> == <span class="string">"Test"</span>)&#123;</span><br><span class="line">          <span class="comment">// deploy test stuff</span></span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="comment">// deploy prod stuff</span></span><br><span class="line">      &#125;</span><br><span class="line">      sh <span class="string">"kubectl apply -f k8s.yaml"</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>由于这一步也属于部署的范畴，所以我们可以将最后两步都合并成一步，我们最终的 Pipeline 脚本如下：</p><figure class="highlight vim"><table><tr><td class="code"><pre><span class="line">node(<span class="string">'node-jnlp'</span>) &#123;</span><br><span class="line">    stage(<span class="string">'Clone'</span>) &#123;</span><br><span class="line">        <span class="keyword">echo</span> <span class="string">"1.Clone Stage"</span></span><br><span class="line">        git ur<span class="variable">l:</span> <span class="string">"https://github.com/xxlaila/jenkins-demo.git"</span></span><br><span class="line">        script &#123;</span><br><span class="line">            build_tag = <span class="keyword">sh</span>(returnStdou<span class="variable">t:</span> true, <span class="keyword">scrip</span><span class="variable">t:</span> <span class="string">'git rev-parse --short HEAD'</span>).trim()</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    stage(<span class="string">'Test'</span>) &#123;</span><br><span class="line">      <span class="keyword">echo</span> <span class="string">"2.Test Stage"</span></span><br><span class="line">    &#125;</span><br><span class="line">    stage(<span class="string">'Build'</span>) &#123;</span><br><span class="line">        <span class="keyword">echo</span> <span class="string">"3.Build Docker Image Stage"</span></span><br><span class="line">        <span class="keyword">sh</span> <span class="string">"docker build -t xxlaila/jenkins-demo:$&#123;build_tag&#125; ."</span></span><br><span class="line">    &#125;</span><br><span class="line">    stage(<span class="string">'Push'</span>) &#123;</span><br><span class="line">        <span class="keyword">echo</span> <span class="string">"4.Push Docker Image Stage"</span></span><br><span class="line">        withCredentials([usernamePassword(credentialsId: <span class="string">'dockerHub'</span>, passwordVariable: <span class="string">'dockerHubPassword'</span>, usernameVariable: <span class="string">'dockerHubUser'</span>)]) &#123;</span><br><span class="line">            <span class="keyword">sh</span> <span class="string">"docker login -u $&#123;dockerHubUser&#125; -p $&#123;dockerHubPassword&#125;"</span></span><br><span class="line">            <span class="keyword">sh</span> <span class="string">"docker push cnych/jenkins-demo:$&#123;build_tag&#125;"</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    stage(<span class="string">'Deploy'</span>) &#123;</span><br><span class="line">        <span class="keyword">echo</span> <span class="string">"5. Deploy Stage"</span></span><br><span class="line">        def userInput = <span class="built_in">input</span>(</span><br><span class="line">            id: <span class="string">'userInput'</span>,</span><br><span class="line">            message: <span class="string">'Choose a deploy environment'</span>,</span><br><span class="line">            parameter<span class="variable">s:</span> [</span><br><span class="line">                [</span><br><span class="line">                    $clas<span class="variable">s:</span> <span class="string">'ChoiceParameterDefinition'</span>,</span><br><span class="line">                    choice<span class="variable">s:</span> <span class="string">"Dev\nQA\nProd"</span>,</span><br><span class="line">                    name: <span class="string">'Env'</span></span><br><span class="line">                ]</span><br><span class="line">            ]</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">echo</span> <span class="string">"This is a deploy step to $&#123;userInput&#125;"</span></span><br><span class="line">        <span class="keyword">sh</span> <span class="string">"sed -i 's/&lt;BUILD_TAG&gt;/$&#123;build_tag&#125;/' k8s.yaml"</span></span><br><span class="line">        <span class="keyword">if</span> (userInput == <span class="string">"Dev"</span>) &#123;</span><br><span class="line">            // deploy dev stuff</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (userInput == <span class="string">"QA"</span>)&#123;</span><br><span class="line">            // deploy <span class="keyword">qa</span> stuff</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            // deploy prod stuff</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">sh</span> <span class="string">"kubectl apply -f k8s.yaml"</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><em>错误</em>: 在jenkins执行构建的时候提示:</p><p><img src="https://img.xxlaila.cn/2846djkfhklsdjdklsd.png" alt="img"></p><blockquote><p>没有权限进行部署，下面进行权限的分配。</p></blockquote><ul><li><p>查看kube-ops 下面的角色</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get role -n kube-ops</span></span><br><span class="line">NAME       AGE</span><br><span class="line">jenkins2   2d6h</span><br></pre></td></tr></table></figure></li><li><p>查看role定义的资源权限</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get role jenkins2 -n kube-ops -o yaml</span></span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: Role</span><br><span class="line">metadata:</span><br><span class="line">  creationTimestamp: <span class="string">"2019-01-14T03:07:25Z"</span></span><br><span class="line">  name: jenkins2</span><br><span class="line">  namespace: kube-ops</span><br><span class="line">  resourceVersion: <span class="string">"2389179"</span></span><br><span class="line">  selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/kube-ops/roles/jenkins2</span><br><span class="line">  uid: 84762132-17a9-11e9-8991-fa163e14c5bd</span><br><span class="line">rules:</span><br><span class="line">- apiGroups:</span><br><span class="line">  - <span class="string">""</span></span><br><span class="line">  resources:</span><br><span class="line">  - pods</span><br><span class="line">  verbs:</span><br><span class="line">  - create</span><br><span class="line">  - delete</span><br><span class="line">  - get</span><br><span class="line">  - list</span><br><span class="line">  - patch</span><br><span class="line">  - update</span><br><span class="line">  - watch</span><br><span class="line">- apiGroups:</span><br><span class="line">  - <span class="string">""</span></span><br><span class="line">  resources:</span><br><span class="line">  - pods/<span class="built_in">exec</span></span><br><span class="line">  verbs:</span><br><span class="line">  - create</span><br><span class="line">  - delete</span><br><span class="line">  - get</span><br><span class="line">  - list</span><br><span class="line">  - patch</span><br><span class="line">  - update</span><br><span class="line">  - watch</span><br><span class="line">- apiGroups:</span><br><span class="line">  - <span class="string">""</span></span><br><span class="line">  resources:</span><br><span class="line">  - pods/<span class="built_in">log</span></span><br><span class="line">  verbs:</span><br><span class="line">  - get</span><br><span class="line">  - list</span><br><span class="line">  - watch</span><br><span class="line">- apiGroups:</span><br><span class="line">  - <span class="string">""</span></span><br><span class="line">  resources:</span><br><span class="line">  - secrets</span><br><span class="line">  verbs:</span><br><span class="line">  - get</span><br></pre></td></tr></table></figure></li><li><p>创建jenkins2的权限</p><figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">[<span class="symbol">root@</span>k8s-zxc-test<span class="number">-3</span> ~]# kubectl -n kube-system create sa jenkins2</span><br><span class="line">serviceaccount/jenkins2 created</span><br></pre></td></tr></table></figure></li><li><p>授权访问</p><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line">[root@k8s-zxc-test<span class="number">-3</span> ~]# kubectl <span class="built_in">create</span> clusterrolebinding jenkins2 <span class="comment">--clusterrole cluster-admin --serviceaccount=kube-ops:jenkins2</span></span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.<span class="built_in">io</span>/jenkins2 created</span><br></pre></td></tr></table></figure></li></ul><!-- rebuild by neat -->]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>ci/cd</tag>
      </tags>
  </entry>
  <entry>
    <title>zabbix企业微信告警</title>
    <url>/2019/08/20/zabbix%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E5%91%8A%E8%AD%A6/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><p>&nbsp;&nbsp;&nbsp;&nbsp;Zabbix可以通过多种方式把告警信息发送到指定人，常用的有邮件，短信报警方式，但是越来越多的企业开始使用zabbix结合微信作为主要的告警方式，这样可以及时有效的把告警信息推送到接收人，方便告警的及时处理。<br>&nbsp;&nbsp;&nbsp;&nbsp;微信企业号需要先在企业通信录新建该员工，该员工才能关注该企业号，这样就能实现告警信息的私密性。如果使用公众号，则只要所有关注了该公众号的人都能收到告警消息，容易造成信息泄露。而且员工数少于200人的企业号是不用钱的，也没有任何申请限制.</p><h3 id="1、脚本存放目录"><a href="#1、脚本存放目录" class="headerlink" title="1、脚本存放目录"></a>1、脚本存放目录</h3><p>/usr/lib/zabbix/alertscripts，脚本的权限是zabbix 账户，具有可执行权限</p><a id="more"></a><figure class="highlight nix"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat wechat.py</span></span><br><span class="line"><span class="comment">#!/usr/bin/python2.7</span></span><br><span class="line"><span class="comment">#_*_coding:utf-8 _*_</span></span><br><span class="line"><span class="built_in">import</span> requests,sys,json</span><br><span class="line"><span class="built_in">import</span> urllib3</span><br><span class="line">urllib3.disable_warnings()</span><br><span class="line">reload(sys)</span><br><span class="line">sys.setdefaultencoding('utf-<span class="number">8</span>')</span><br><span class="line">def GetToken(Corpid,Secret):</span><br><span class="line">    <span class="attr">Url</span> = <span class="string">"https://qyapi.weixin.qq.com/cgi-bin/gettoken"</span></span><br><span class="line">    <span class="attr">Data</span> = &#123;</span><br><span class="line">        <span class="string">"corpid"</span>:Corpid,</span><br><span class="line">        <span class="string">"corpsecret"</span>:Secret</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="attr">r</span> = requests.get(<span class="attr">url=Url,params=Data,verify=False)</span></span><br><span class="line">    <span class="attr">Token</span> = r.json()['access_token']</span><br><span class="line">    return Token</span><br><span class="line">def SendMessage(Token,User,Agentid,Subject,Content):</span><br><span class="line">    <span class="attr">Url</span> = <span class="string">"https://qyapi.weixin.qq.com/cgi-bin/message/send?access_token=%s"</span> % Token</span><br><span class="line">    <span class="attr">Data</span> = &#123;</span><br><span class="line">        <span class="string">"touser"</span>: User,                                 <span class="comment"># 企业号中的用户帐号，在zabbix用户Media中配置，如果配置不正常，将按部门发送。</span></span><br><span class="line">        <span class="comment">#"totag": Tagid,                                # 企业号中的标签id，群发使用（推荐）</span></span><br><span class="line">        <span class="string">"toparty"</span>: <span class="string">"2"</span>,                            <span class="comment"># 企业号中的部门id，群发时使用。</span></span><br><span class="line">        <span class="string">"msgtype"</span>: <span class="string">"text"</span>,                              <span class="comment"># 消息类型。</span></span><br><span class="line">        <span class="string">"agentid"</span>: Agentid,                             <span class="comment"># 企业号中的应用id。</span></span><br><span class="line">        <span class="string">"text"</span>: &#123;</span><br><span class="line">            <span class="string">"content"</span>: Subject + '\n' + Content</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">"safe"</span>: <span class="string">"0"</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="attr">r</span> = requests.post(<span class="attr">url=Url,data=json.dumps(Data),verify=False)</span></span><br><span class="line">    return r.text</span><br><span class="line"><span class="keyword">if</span> <span class="attr">__name__</span> == '__main__':</span><br><span class="line">    <span class="attr">User</span> = sys.argv[<span class="number">1</span>]                                                              <span class="comment"># zabbix传过来的第一个参数</span></span><br><span class="line">    <span class="attr">Subject</span> = sys.argv[<span class="number">2</span>]                                                           <span class="comment"># zabbix传过来的第二个参数</span></span><br><span class="line">    <span class="attr">Content</span> = sys.argv[<span class="number">3</span>]                                                           <span class="comment"># zabbix传过来的第三个参数</span></span><br><span class="line">    <span class="attr">Corpid</span> = <span class="string">"wwa9c9999"</span>                                                   <span class="comment"># CorpID是企业号的标识</span></span><br><span class="line">    <span class="attr">Secret</span> = <span class="string">"VGbZvXJ5RiLskdksh2dkhaskdu92uihsjdhjksadh"</span>     <span class="comment"># Secret是管理组凭证密钥</span></span><br><span class="line">    <span class="comment">#Tagid = "1"                                                                     # 通讯录标签ID</span></span><br><span class="line">    <span class="attr">Agentid</span> = <span class="string">"1000001"</span>                                                                   <span class="comment"># 应用ID</span></span><br><span class="line">    <span class="comment">#Partyid = "1"                                                                  # 部门ID</span></span><br><span class="line">    <span class="attr">Token</span> = GetToken(Corpid, Secret)</span><br><span class="line">    <span class="attr">Status</span> = SendMessage(Token,User,Agentid,Subject,Content)</span><br><span class="line">    print Status</span><br></pre></td></tr></table></figure><h3 id="2、重要参数介绍"><a href="#2、重要参数介绍" class="headerlink" title="2、重要参数介绍"></a>2、重要参数介绍</h3><ul><li>toparty：”2” 这个参数是在企业微信里面部门的id</li><li>Corpid：企业的CorpID标示</li><li>Secret：管理组的密钥凭证</li><li>Agentid：新建应用的id</li><li>只需要求修改以上参数即可</li></ul><p><img src="https://img.xxlaila.cn/image2018-8-23_16-8-30.png" alt="img"></p><ul><li>以上部门没有新建，只是在这个应用中新增加了几个用户。最好的方式是增加一个部门组，用户添加到部门组里面，这种方式最科学</li></ul><h3 id="3、登陆zabbix-进行配置"><a href="#3、登陆zabbix-进行配置" class="headerlink" title="3、登陆zabbix 进行配置"></a>3、登陆zabbix 进行配置</h3><h4 id="3-1、创建一个媒介类型"><a href="#3-1、创建一个媒介类型" class="headerlink" title="3.1、创建一个媒介类型"></a>3.1、创建一个媒介类型</h4><p><img src="https://img.xxlaila.cn/image2018-8-23_16-11-59.png" alt="img"></p><h4 id="3-2、创建一个告警类别"><a href="#3-2、创建一个告警类别" class="headerlink" title="3.2、创建一个告警类别"></a>3.2、创建一个告警类别</h4><p><img src="https://img.xxlaila.cn/image2018-8-23_16-12-53.png" alt="img"><br><img src="https://img.xxlaila.cn/image2018-8-23_16-13-9.png" alt="img"></p><blockquote><p>服务器:{HOST.NAME}发生: {TRIGGER.NAME}故障!</p><p>告警主机:{HOST.NAME}<br>告警地址:{HOST.IP}<br>监控项目:{ITEM.NAME}<br>监控取值:{ITEM.LASTVALUE}<br>告警等级:{TRIGGER.SEVERITY}<br>当前状态:{TRIGGER.STATUS}<br>告警信息:{TRIGGER.NAME}<br>告警时间:{EVENT.DATE} {EVENT.TIME}<br>事件ID:{EVENT.ID}</p></blockquote><p><img src="https://img.xxlaila.cn/image2018-8-23_16-13-20.png" alt="img"></p><blockquote><p>服务器:{HOST.NAME}: {TRIGGER.NAME}已恢复!</p><p>告警主机:{HOST.NAME}<br>告警地址:{HOST.IP}<br>监控项目:{ITEM.NAME}<br>监控取值:{ITEM.LASTVALUE}<br>告警等级:{TRIGGER.SEVERITY}<br>当前状态:{TRIGGER.STATUS}<br>告警信息:{TRIGGER.NAME}<br>告警时间:{EVENT.DATE} {EVENT.TIME}<br>恢复时间:{EVENT.RECOVERY.DATE} {EVENT.RECOVERY.TIME}<br>持续时间:{EVENT.AGE}<br>事件ID:{EVENT.ID}</p></blockquote><p><img src="https://img.xxlaila.cn/image2018-8-23_16-13-28.png" alt="img"></p><blockquote><p>服务器:{HOST.NAME}: 报警确认</p><p>确认人:{USER.FULLNAME}<br>时间:{ACK.DATE} {ACK.TIME}<br>确认信息如下:<br>“{ACK.MESSAGE}”<br>问题服务器IP:{HOSTNAME1}<br>问题ID:{EVENT.ID}<br>当前的问题是: {TRIGGER.NAME}</p></blockquote><h4 id="3-3、为用户添加告警类型"><a href="#3-3、为用户添加告警类型" class="headerlink" title="3.3、为用户添加告警类型"></a>3.3、为用户添加告警类型</h4><p><img src="https://img.xxlaila.cn/image2018-8-23_16-21-58.png" alt="img"><br>这里为admin用户添加的 告警方式。注意一下send to 这个参数，这里一定要是@all。否则不成功</p><h3 id="4、企业微信测试"><a href="#4、企业微信测试" class="headerlink" title="4、企业微信测试"></a>4、企业微信测试</h3><p><img src="https://img.xxlaila.cn/image2018-8-23_16-23-14.png" alt="img"></p><!-- rebuild by neat -->]]></content>
      <categories>
        <category>监控</category>
      </categories>
      <tags>
        <tag>zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title>帧中继配置</title>
    <url>/2019/08/19/%E5%B8%A7%E4%B8%AD%E7%BB%A7%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:23 GMT+0800 (China Standard Time) --><h3 id="点对点配置"><a href="#点对点配置" class="headerlink" title="点对点配置"></a>点对点配置</h3><p><img src="https://img.xxlaila.cn/1566222269423.jpg" alt="img"></p><ul><li><p>RA配置:</p><figure class="highlight lsl"><table><tr><td class="code"><pre><span class="line">[RA]int s0/<span class="number">0</span></span><br><span class="line">[RA-s0/<span class="number">0</span>]encap frame-relay    封装帧中继协议</span><br><span class="line">[RA-s0/<span class="number">0</span>] frame-relay intf dte</span><br><span class="line">[RA-s0/<span class="number">0</span>]frame-relay interface-dlci <span class="number">102</span>      设置本接口对应的INTERFACE-DLCI号</span><br><span class="line">[RA-s0/<span class="number">0</span>]frame-relay map ip <span class="number">172.16</span><span class="number">.1</span><span class="number">.2</span> <span class="number">102</span>  建立对端协议地址与本地INTERFACE-DLCI号的映射关系</span><br><span class="line">[RA-s0/<span class="number">0</span>]ip add <span class="number">172.16</span><span class="number">.1</span><span class="number">.1</span> <span class="number">255.255</span><span class="number">.255</span><span class="number">.0</span>        配置本接口IP地址</span><br><span class="line">[RA-s0/<span class="number">0</span>]no sh                     打开此物理接口</span><br></pre></td></tr></table></figure></li><li><p>RB配置:</p><figure class="highlight lsl"><table><tr><td class="code"><pre><span class="line">[RB]int s0/<span class="number">0</span></span><br><span class="line">[RB-s0/<span class="number">0</span>]encap frame-relay    封装帧中继协议</span><br><span class="line">[RA-s0/<span class="number">0</span>] frame-relay intf dte</span><br><span class="line">[RB-s0/<span class="number">0</span>]frame-relay interface-dlci <span class="number">201</span>       设置本接口对应的INTERFACE-DLCI号</span><br><span class="line">[RB-s0/<span class="number">0</span>]frame-relay map ip <span class="number">172.16</span><span class="number">.1</span><span class="number">.1</span> <span class="number">201</span>  建立对端协议地址与本地INTERFACE-DLCI号的映射关系</span><br><span class="line">[RB-s0/<span class="number">0</span>]ip add <span class="number">172.16</span><span class="number">.1</span><span class="number">.2</span> <span class="number">255.255</span><span class="number">.255</span><span class="number">.0</span>        配置本接口IP地址</span><br><span class="line">[RB-s0/<span class="number">0</span>]no sh                     打开此物理接口</span><br></pre></td></tr></table></figure></li></ul><a id="more"></a><ul><li>FRAME-RELAY配置:<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">[FRAME-RELAY]frame-relay switching                 允许帧中继进行PVC交换</span><br><span class="line">[FRAME-RELAY]int s0/0</span><br><span class="line">[FRAME-RELAY-s0/0] encap frame-relay    封装帧中继协议 </span><br><span class="line">[FRAME-RELAY-s0/0]frame-relay intf dce        设置帧中继接口类型为DCE端</span><br><span class="line">[FRAME-RELAY-s0/0]clock rate 630100            设置DCE时钟频率</span><br><span class="line">[FRAME-RELAY-s0/0]frame-relay<span class="built_in"> route </span>102<span class="built_in"> interface </span>s0/1 201    配置帧中继PVC交换路由</span><br><span class="line">[FRAME-RELAY]int s0/1</span><br><span class="line">[FRAME-RELAY-s0/1] encap frame-relay    封装帧中继协议</span><br><span class="line">[FRAME-RELAY-s0/1]frame-relay intf dce        设置帧中继接口类型为DCE端</span><br><span class="line">[FRAME-RELAY-s0/1]clock rate 630100            设置DCE时钟频率</span><br><span class="line">[FRAME-RELAY-s0/1]frame-relay<span class="built_in"> route </span>pvc 201<span class="built_in"> interface </span>s0/0 102    配置帧中继PVC交换路由</span><br></pre></td></tr></table></figure></li></ul><h3 id="点对多点（星形）"><a href="#点对多点（星形）" class="headerlink" title="点对多点（星形）"></a>点对多点（星形）</h3><p><img src="https://img.xxlaila.cn/1566222529208.jpg" alt="img"></p><ul><li><p>RA配置:</p><figure class="highlight lsl"><table><tr><td class="code"><pre><span class="line">[RA]int s0/<span class="number">0</span></span><br><span class="line">[RA-s0/<span class="number">0</span>] encap frame-relay    封装帧中继协议</span><br><span class="line">[RA-s0/<span class="number">0</span>] frame-relay intf dte</span><br><span class="line">[RA-s0/<span class="number">0</span>]frame-relay interface-dlci <span class="number">102</span>      设置本接口对应的INTERFACE-DLCI号</span><br><span class="line">[RA-s0/<span class="number">0</span>]frame-relay interface-dlci <span class="number">103</span>                 </span><br><span class="line">[RA-s0/<span class="number">0</span>]frame-relay map ip <span class="number">172.16</span><span class="number">.1</span><span class="number">.2</span> <span class="number">102</span>  建立对端协议地址与本地INTERFACE-DLCI号的映射关系</span><br><span class="line">[RA-s0/<span class="number">0</span>]frame-relay map ip <span class="number">172.16</span><span class="number">.1</span><span class="number">.3</span> <span class="number">103</span>  </span><br><span class="line">[RA-s0/<span class="number">0</span>]ip add <span class="number">172.16</span><span class="number">.1</span><span class="number">.1</span> <span class="number">255.255</span><span class="number">.255</span><span class="number">.0</span>     配置本接口IP地址</span><br><span class="line">[RA-s0/<span class="number">0</span>]no sh                     打开此物理接口</span><br></pre></td></tr></table></figure></li><li><p>RB配置:</p><figure class="highlight lsl"><table><tr><td class="code"><pre><span class="line">[RB]int s0/<span class="number">0</span></span><br><span class="line">[RB-s0/<span class="number">0</span>] encap frame-relay    封装帧中继协议</span><br><span class="line">[RA-s0/<span class="number">0</span>] frame-relay intf dte</span><br><span class="line">[RB-s0/<span class="number">0</span>]frame-relay interface-dlci <span class="number">201</span>       设置本接口对应的INTERFACE-DLCI号</span><br><span class="line">[RB-s0/<span class="number">0</span>]frame-relay map ip <span class="number">172.16</span><span class="number">.1</span><span class="number">.1</span> <span class="number">201</span>  建立对端协议地址与本地INTERFACE-DLCI</span><br><span class="line">[RB-s0/<span class="number">0</span>]ip add <span class="number">172.16</span><span class="number">.1</span><span class="number">.2</span> <span class="number">255.255</span><span class="number">.255</span><span class="number">.0</span>       配置本接口IP地址</span><br><span class="line">[RC-s0/<span class="number">0</span>]no sh                     打开此物理接口</span><br></pre></td></tr></table></figure></li><li><p>RC配置:</p><figure class="highlight lsl"><table><tr><td class="code"><pre><span class="line">[RC]int s0/<span class="number">0</span></span><br><span class="line">[RC-s0/<span class="number">0</span>]encap frame-relay    封装帧中继协议</span><br><span class="line">[RC-s0/<span class="number">0</span>]frame-relay interface-dlci <span class="number">301</span>       设置本接口对应的INTERFACE-DLCI号</span><br><span class="line">[RC-s0/<span class="number">0</span>]frame-relay map ip <span class="number">172.16</span><span class="number">.1</span><span class="number">.1</span> <span class="number">301</span>  建立对端协议地址与本地INTERFACE-DLCI号的映射关系</span><br><span class="line">[RC-s0/<span class="number">0</span>]ip add <span class="number">172.16</span><span class="number">.1</span><span class="number">.3</span> <span class="number">255.255</span><span class="number">.255</span><span class="number">.0</span>       配置本接口IP地址</span><br><span class="line">[RC-s0/<span class="number">0</span>]no sh                     打开此物理接口</span><br></pre></td></tr></table></figure></li><li><p>FRAME-RELAY配置:</p><figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">[FRAME-RELAY]frame-relay switching      允许帧中继进行PVC交换</span><br><span class="line">[FRAME-RELAY]int s0/0</span><br><span class="line">[FRAME-RELAY-s0/0]encap frame-relay    封装帧中继协议</span><br><span class="line">[FRAME-RELAY-s0/0]frame-relay intf dce        设置帧中继接口类型为DCE端</span><br><span class="line">[FRAME-RELAY-s0/0]clock rate 630100            设置DCE时钟频率</span><br><span class="line">[FRAME-RELAY-s0/0]frame-relay<span class="built_in"> route </span>102<span class="built_in"> interface </span>s0/1 201   配置帧中继PVC交换路由</span><br><span class="line">[FRAME-RELAY-s0/0]frame-relay<span class="built_in"> route </span>103<span class="built_in"> interface </span>s0/2 301   配置帧中继PVC交换路由</span><br><span class="line">[FRAME-RELAY]int s0/1</span><br><span class="line">[FRAME-RELAY-s0/1]encap frame-relay    封装帧中继协议</span><br><span class="line">[FRAME-RELAY-s0/1]frame-relay intf dce        设置帧中继接口类型为DCE端</span><br><span class="line">[FRAME-RELAY-s0/1]clock rate 630100            设置DCE时钟频率</span><br><span class="line">[FRAME-RELAY-s0/1]frame-relay<span class="built_in"> route </span>201<span class="built_in"> interface </span>s0/0 102   配置帧中继PVC交换路由</span><br><span class="line">[FRAME-RELAY]int s0/2</span><br><span class="line">[FRAME-RELAY-s0/2]encap frame-relay    封装帧中继协议</span><br><span class="line">[FRAME-RELAY-s0/2]frame-relay intf dce        设置帧中继接口类型为DCE端</span><br><span class="line">[FRAME-RELAY-s0/2]clock rate 630100            设置DCE时钟频率</span><br><span class="line">[FRAME-RELAY-s0/2]frame-relay<span class="built_in"> route </span>301<span class="built_in"> interface </span>s0/0 103   配置帧中继PVC交换路由</span><br></pre></td></tr></table></figure></li></ul><h3 id="点对多点子接口（星形）"><a href="#点对多点子接口（星形）" class="headerlink" title="点对多点子接口（星形）"></a>点对多点子接口（星形）</h3><p><img src="https://img.xxlaila.cn/1566222807499.jpg" alt="img"></p><ul><li><p>RA配置:</p><figure class="highlight lsl"><table><tr><td class="code"><pre><span class="line">[RA]int s0/<span class="number">0</span></span><br><span class="line">[RA-s0/<span class="number">0</span>]encap frame-relay    封装帧中继协议</span><br><span class="line">[RA-s0/<span class="number">0</span>]no sh                     打开此物理接口</span><br><span class="line">[RA-s0/<span class="number">0</span>] frame-relay intf dte</span><br><span class="line">[RA]int s0/<span class="number">0.1</span> multipoint</span><br><span class="line">[RA-s0/<span class="number">0.1</span>]frame-relay interface-dlci <span class="number">102</span>     设置本接口对应的INTERFACE-DLCI号</span><br><span class="line">[RA-s0/<span class="number">0.1</span>]frame-relay interface-dlci <span class="number">103</span>                 </span><br><span class="line">[RA-s0/<span class="number">0.1</span>]frame-relay map ip <span class="number">172.16</span><span class="number">.1</span><span class="number">.2</span> <span class="number">102</span> 建立对端协议地址与本地INTERFACE-DLCI号的映射关系</span><br><span class="line">[RA-s0/<span class="number">0.1</span>]frame-relay map ip <span class="number">172.16</span><span class="number">.1</span><span class="number">.3</span> <span class="number">103</span>  </span><br><span class="line">[RA-s0/<span class="number">0.1</span>]ip add <span class="number">172.16</span><span class="number">.1</span><span class="number">.1</span> <span class="number">255.255</span><span class="number">.255</span><span class="number">.0</span>        配置本子接口IP地址</span><br></pre></td></tr></table></figure></li><li><p>RB配置:</p><figure class="highlight lsl"><table><tr><td class="code"><pre><span class="line">[RB]int s0/<span class="number">0</span></span><br><span class="line">[RB-s0/<span class="number">0</span>]encap frame-relay    封装帧中继协议</span><br><span class="line">[RA-s0/<span class="number">0</span>] frame-relay intf dte</span><br><span class="line">[RB-s0/<span class="number">0</span>]frame-relay interface-dlci <span class="number">201</span>       设置本接口对应的INTERFACE-DLCI号</span><br><span class="line">[RB-s0/<span class="number">0</span>]frame-relay map ip <span class="number">172.16</span><span class="number">.1</span><span class="number">.1</span> <span class="number">201</span>  建立对端协议地址与本地INTERFACE-DLCI号的映射关系</span><br><span class="line">[RB-s0/<span class="number">0</span>]ip add <span class="number">172.16</span><span class="number">.1</span><span class="number">.2</span> <span class="number">255.255</span><span class="number">.255</span><span class="number">.0</span>       配置本接口IP地址</span><br><span class="line">[RB-s0/<span class="number">0</span>]no sh                     打开此物理接口</span><br></pre></td></tr></table></figure></li><li><p>RC配置:</p><figure class="highlight lsl"><table><tr><td class="code"><pre><span class="line">[RC]int s0/<span class="number">0</span></span><br><span class="line">[RC-s0/<span class="number">0</span>]encap frame-relay    封装帧中继协议</span><br><span class="line">[RA-s0/<span class="number">0</span>] frame-relay intf dte</span><br><span class="line">[RC-s0/<span class="number">0</span>]frame-relay interface-dlci <span class="number">301</span>       设置本接口对应的INTERFACE-DLCI号</span><br><span class="line">[RC-s0/<span class="number">0</span>]frame-relay map ip <span class="number">172.16</span><span class="number">.1</span><span class="number">.1</span> <span class="number">301</span>  建立对端协议地址与本地INTERFACE-DLCI号的映射关系</span><br><span class="line">[RC-s0/<span class="number">0</span>]ip add <span class="number">172.16</span><span class="number">.1</span><span class="number">.3</span> <span class="number">255.255</span><span class="number">.255</span><span class="number">.0</span>    配置本接口IP地址</span><br><span class="line">[RC-s0/<span class="number">0</span>]no sh                    打开此物理接口</span><br></pre></td></tr></table></figure></li><li><p>FRAME-RELAY配置:</p><figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">[FRAME-RELAY]frame-relay switching    允许帧中继进行PVC交换</span><br><span class="line">[FRAME-RELAY]int s0/0</span><br><span class="line">[FRAME-RELAY-s0/0]encap frame-relay    封装帧中继协议 </span><br><span class="line">[FRAME-RELAY-s0/0]frame-relay intf dce        设置帧中继接口类型为DCE端</span><br><span class="line">[FRAME-RELAY-s0/0]clock rate 630100            设置DCE时钟频率</span><br><span class="line">[FRAME-RELAY-s0/0]frame-relay<span class="built_in"> route </span>102<span class="built_in"> interface </span>s0/1 201   配置帧中继PVC交换路由</span><br><span class="line">[FRAME-RELAY-s0/0]frame-relay<span class="built_in"> route </span>103<span class="built_in"> interface </span>s0/2 301   配置帧中继PVC交换路由</span><br><span class="line">[FRAME-RELAY]int s0/1</span><br><span class="line">[FRAME-RELAY-s0/1]encap frame-relay    封装帧中继协议</span><br><span class="line">[FRAME-RELAY-s0/1]frame-relay intf dce        设置帧中继接口类型为DCE端</span><br><span class="line">[FRAME-RELAY-s0/1]clock rate 630100            设置DCE时钟频率</span><br><span class="line">[FRAME-RELAY-s0/1]frame-relay<span class="built_in"> route </span>201<span class="built_in"> interface </span>s0/0 102   配置帧中继PVC交换路由</span><br><span class="line">[FRAME-RELAY]int s0/2</span><br><span class="line">[FRAME-RELAY-s0/2]encap frame-relay    封装帧中继协议</span><br><span class="line">[FRAME-RELAY-s0/2]frame-relay intf dce        设置帧中继接口类型为DCE端</span><br><span class="line">[FRAME-RELAY-s0/2]clock rate 630100            设置DCE时钟频率</span><br><span class="line">[FRAME-RELAY-s0/2]frame-relay<span class="built_in"> route </span>301<span class="built_in"> interface </span>s0/0 103   配置帧中继PVC交换路由</span><br></pre></td></tr></table></figure></li></ul><h3 id="点对点子接口（全网状）"><a href="#点对点子接口（全网状）" class="headerlink" title="点对点子接口（全网状）"></a>点对点子接口（全网状）</h3><p><img src="https://img.xxlaila.cn/1566222918549.jpg" alt="img"></p><ul><li><p>RA配置:</p><figure class="highlight lsl"><table><tr><td class="code"><pre><span class="line">[RA]int s0/<span class="number">0</span> </span><br><span class="line">[RA-s0/<span class="number">0</span>]encap frame-relay     封装帧中继协议</span><br><span class="line">[RA-s0/<span class="number">0</span>] frame-relay intf dte</span><br><span class="line">[RA-s0/<span class="number">0</span>]no sh               打开此物理接口</span><br><span class="line">[RA]int s0/<span class="number">0.1</span> point-to-point</span><br><span class="line">[RA-s0/<span class="number">0.1</span>]frame-relay interface-dlci <span class="number">102</span>        设置本接口对应的INTERFACE-DLCI号              </span><br><span class="line">[RA-s0/<span class="number">0.1</span>]frame-relay map ip <span class="number">172.16</span><span class="number">.1</span><span class="number">.2</span> <span class="number">102</span> 建立对端协议地址与本地INTERFACE-DLCI号的映射关系</span><br><span class="line">[RA-s0/<span class="number">0.1</span>]ip add <span class="number">172.16</span><span class="number">.1</span><span class="number">.1</span> <span class="number">255.255</span><span class="number">.255</span><span class="number">.0</span>        配置本子接口IP地址</span><br><span class="line">[RA]int s0/<span class="number">0.2</span> point-to-point</span><br><span class="line">[RA-s0/<span class="number">0.2</span>]frame-relay interface-dlci <span class="number">103</span>        设置本接口对应的INTERFACE-DLCI号              </span><br><span class="line">[RA-s0/<span class="number">0.2</span>]frame-relay map ip <span class="number">172.16</span><span class="number">.1</span><span class="number">.1</span> <span class="number">103</span> 建立对端协议地址与本地INTERFACE-DLCI号的映射关系</span><br><span class="line">[RA-s0/<span class="number">0.2</span>]ip add <span class="number">172.16</span><span class="number">.2</span><span class="number">.1</span> <span class="number">255.255</span><span class="number">.255</span><span class="number">.0</span>    配置本子接口IP地址</span><br></pre></td></tr></table></figure></li><li><p>RB配置:</p><figure class="highlight lsl"><table><tr><td class="code"><pre><span class="line">[RB]int s0/<span class="number">0</span></span><br><span class="line">[RB-s0/<span class="number">0</span>]encap frame-relay     封装帧中继协议</span><br><span class="line">[RB-s0/<span class="number">0</span>] frame-relay intf dte</span><br><span class="line">[RB-s0/<span class="number">0</span>]no sh                   打开此物理接口</span><br><span class="line">[RB]int s0/<span class="number">0.1</span> point-to-point</span><br><span class="line">[RB-s0/<span class="number">0.1</span>]frame-relay interface-dlci <span class="number">201</span>     设置本接口对应的INTERFACE-DLCI号             </span><br><span class="line">[RB-s0/<span class="number">0.1</span>]frame-relay map ip <span class="number">172.16</span><span class="number">.1</span><span class="number">.1</span> <span class="number">201</span> 建立对端协议地址与本地INTERFACE-DLCI号的映射关系</span><br><span class="line">[RB-s0/<span class="number">0.1</span>]ip add <span class="number">172.16</span><span class="number">.1</span><span class="number">.2</span> <span class="number">255.255</span><span class="number">.255</span><span class="number">.0</span>     配置本接口IP地址</span><br><span class="line">[RB]int s0/<span class="number">0.2</span> point-to-point</span><br><span class="line">[RB-s0/<span class="number">0.2</span>]frame-relay interface-dlci <span class="number">203</span>      设置本接口对应的INTERFACE-DLCI号             </span><br><span class="line">[RB-s0/<span class="number">0.2</span>]frame-relay map ip <span class="number">172.16</span><span class="number">.1</span><span class="number">.1</span> <span class="number">203</span> 建立对端协议地址与本地INTERFACE-DLCI号的映射关系</span><br><span class="line">[RB-s0/<span class="number">0.2</span>]ip add <span class="number">172.16</span><span class="number">.3</span><span class="number">.1</span> <span class="number">255.255</span><span class="number">.255</span><span class="number">.0</span>        配置本接口IP地址</span><br></pre></td></tr></table></figure></li><li><p>RC配置:</p><figure class="highlight lsl"><table><tr><td class="code"><pre><span class="line">[RC]int s0/<span class="number">0</span></span><br><span class="line">[RC-s0/<span class="number">0</span>]encap frame-relay     封装帧中继协议</span><br><span class="line">[RC-s0/<span class="number">0</span>] frame-relay intf dte</span><br><span class="line">[RC-s0/<span class="number">0</span>]no sh                     打开此物理接口</span><br><span class="line">[RC]int s0/<span class="number">0.1</span> point-to-point</span><br><span class="line">[RC-s0/<span class="number">0.1</span>]frame-relay interface-dlci <span class="number">301</span>         设置本接口对应的INTERFACE-DLCI号                </span><br><span class="line">[RC-s0/<span class="number">0.1</span>]frame-relay map ip <span class="number">172.16</span><span class="number">.3</span><span class="number">.1</span> <span class="number">301</span> 建立对端协议地址与本地INTERFACE-DLCI号的映射关系</span><br><span class="line">[RC-s0/<span class="number">0.1</span>]ip add <span class="number">172.16</span><span class="number">.3</span><span class="number">.2</span> <span class="number">255.255</span><span class="number">.255</span><span class="number">.0</span>       配置本接口IP地址</span><br><span class="line">[RC]int s0/<span class="number">0.2</span> point-to-point</span><br><span class="line">[RC-s0/<span class="number">0.2</span>]frame-relay interface-dlci <span class="number">302</span>         设置本接口对应的INTERFACE-DLCI号                </span><br><span class="line">[RC-s0/<span class="number">0.2</span>]frame-relay map ip <span class="number">172.16</span><span class="number">.2</span><span class="number">.1</span> <span class="number">302</span> 建立对端协议地址与本地INTERFACE-DLCI号的映射关系</span><br><span class="line">[RC-s0/<span class="number">0.2</span>]ip add <span class="number">172.16</span><span class="number">.2</span><span class="number">.2</span> <span class="number">255.255</span><span class="number">.255</span><span class="number">.0</span>        配置本接口IP地址</span><br></pre></td></tr></table></figure></li><li><p>FRAME-RELAY配置:</p><figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">[FRAME-RELAY]frame-relay switching                 允许帧中继进行PVC交换</span><br><span class="line">[FRAME-RELAY]int s0/0</span><br><span class="line">[FRAME-RELAY-s0/0]encap frame-relay    封装帧中继协议 </span><br><span class="line">[FRAME-RELAY-s0/0]frame-relay intf dce        设置帧中继接口类型为DCE端</span><br><span class="line">[FRAME-RELAY-s0/0]clock rate 630100            设置DCE时钟频率</span><br><span class="line">[FRAME-RELAY-s0/0]frame-relay<span class="built_in"> route </span>102<span class="built_in"> interface </span>s0/1 201     配置帧中继PVC交换路由</span><br><span class="line">[FRAME-RELAY-s0/0]frame-relay<span class="built_in"> route </span>103<span class="built_in"> interface </span>s0/2 301     配置帧中继PVC交换路由</span><br><span class="line">[FRAME-RELAY]int s0/1</span><br><span class="line">[FRAME-RELAY-s0/1]encap frame-relay           封装帧中继协议</span><br><span class="line">[FRAME-RELAY-s0/1]frame-relay intf dce        设置帧中继接口类型为DCE端</span><br><span class="line">[FRAME-RELAY-s0/1]clock rate 630100            设置DCE时钟频率</span><br><span class="line">[FRAME-RELAY-s0/1]frame-relay<span class="built_in"> route </span>201<span class="built_in"> interface </span>s0/0 102    配置帧中继PVC交换路由</span><br><span class="line">[FRAME-RELAY-s0/1]frame-relay<span class="built_in"> route </span>pvc 203<span class="built_in"> interface </span>s0/0 302    配置帧中继PVC交换路由</span><br><span class="line">[FRAME-RELAY]int s0/2</span><br><span class="line">[FRAME-RELAY-s0/2]encap frame-relay           封装帧中继协议</span><br><span class="line">[FRAME-RELAY-s0/2]frame-relay intf dce        设置帧中继接口类型为DCE端</span><br><span class="line">[FRAME-RELAY-s0/2]clock rate 630100            设置DCE时钟频率</span><br><span class="line">[FRAME-RELAY-s0/2]frame-relay<span class="built_in"> route </span>301<span class="built_in"> interface </span>s0/0 103    配置帧中继PVC交换路由</span><br><span class="line">[FRAME-RELAY-s0/2]frame-relay<span class="built_in"> route </span>302<span class="built_in"> interface </span>s0/0 203    配置帧中继PVC交换路由</span><br></pre></td></tr></table></figure></li></ul><!-- rebuild by neat -->]]></content>
      <categories>
        <category>网络设备</category>
      </categories>
      <tags>
        <tag>帧中继</tag>
      </tags>
  </entry>
  <entry>
    <title>kubernetes ci/cd(一)</title>
    <url>/2019/08/12/kubernetes-ci-cd-%E4%B8%80/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:23 GMT+0800 (China Standard Time) --><blockquote><p>基于jenkins的CI/CD安装</p></blockquote><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;jenkins一个流行的持续集成/发布工具，在Kubernetes使用,持续构建与发布是我们日常工作中必不可少的一个步骤，目前大多公司都采用 Jenkins 集群来搭建符合需求的 CI/CD 流程，然而传统的 Jenkins Slave 一主多从方式会存在一些痛点，比如：主 Master 发生单点故障时，整个流程都不可用了；每个 Slave 的配置环境不一样，来完成不同语言的编译打包等操作，但是这些差异化的配置导致管理起来非常不方便，维护起来也是比较费劲；资源分配不均衡，有的 Slave 要运行的 job 出现排队等待，而有的 Slave 处于空闲状态；最后资源有浪费，每台 Slave 可能是实体机或者 VM，当 Slave 处于空闲状态时，也不会完全释放掉资源。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;提到基于Kubernete的CI/CD，可以使用的工具有很多，比如Jenkins、Gitlab CI已经新兴的drone之类的，我们这里会使用大家最为熟悉的Jenins来做CI/CD的工具。</p><ul><li>优点:<ul><li>Jenkins 安装完成了，接下来我们不用急着就去使用，我们要了解下在 Kubernetes 环境下面使用 Jenkins 有什么好处。都知道持续构建与发布是我们日常工作中必不可少的一个步骤，目前大多公司都采用 Jenkins 集群来搭建符合需求的 CI/CD 流程，然而传统的 Jenkins Slave 一主多从方式会存在一些痛点，比如:<ul><li>E 主 Master 发生单点故障时，整个流程都不可用了。</li><li>E 每个 Slave 的配置环境不一样，来完成不同语言的编译打包等操作，但是这些差异化的配置导致管理起来非常不方便，维护起来也是比较费劲。</li><li>E 资源分配不均衡，有的 Slave 要运行的 job 出现排队等待，而有的 Slave 处于空闲状态。</li><li>E 资源有浪费，每台 Slave 可能是物理机或者虚拟机，当 Slave 处于空闲状态时，也不会完全释放掉资源。</li></ul></li><li>正因为这些种种痛点，我们渴望一种更高效更可靠的方式来完成这个 CI/CD 流程，而 Docker 虚拟化容器技术能很好的解决这个痛点，又特别是在 Kubernetes 集群环境下面能够更好来解决上面的问题，下图是基于 Kubernetes 搭建 Jenkins 集群的简单示意图<br><img src="https://img.xxlaila.cn/xjfhs84we.png" alt="img"></li></ul></li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;可以看到 Jenkins Master 和 Jenkins Slave 以 Pod 形式运行在 Kubernetes 集群的 Node 上，Master 运行在其中一个节点，并且将其配置数据存储到一个 Volume 上去，Slave 运行在各个节点上，并且它不是一直处于运行状态，它会按照需求动态的创建并自动删除。</p><a id="more"></a><ul><li>这种方式的工作流程大致为<ul><li>当 Jenkins Master 接受到 Build 请求时，会根据配置的 Label 动态创建一个运行在 Pod 中的 Jenkins Slave 并注册到 Master 上，当运行完 Job 后，这个 Slave 会被注销并且这个 Pod 也会自动删除，恢复到最初状态。那么使用这种方式带来了哪些好处呢？</li><li>E 服务高可用，当 Jenkins Master 出现故障时，Kubernetes 会自动创建一个新的 Jenkins Master 容器，并且将 Volume 分配给新创建的容器，保证数据不丢失，从而达到集群服务高可用。</li><li>E 动态伸缩，合理使用资源，每次运行 Job 时，会自动创建一个 Jenkins Slave，Job 完成后，Slave 自动注销并删除容器，资源自动释放，而且 Kubernetes 会根据每个资源的使用情况，动态分配 Slave 到空闲的节点上创建，降低出现因某节点资源利用率高，还排队等待在该节点的情况。</li><li>E 扩展性好，当 Kubernetes 集群的资源严重不足而导致 Job 排队等待时，可以很容易的添加一个 Kubernetes Node 到集群中，从而实现扩展。</li></ul></li></ul><h2 id="1、安装jenkins"><a href="#1、安装jenkins" class="headerlink" title="1、安装jenkins"></a>1、安装jenkins</h2><h3 id="1-1、新建一个-Deployment"><a href="#1-1、新建一个-Deployment" class="headerlink" title="1.1、新建一个 Deployment"></a>1.1、新建一个 Deployment</h3><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment">#cat  jenkins-deployment.yaml</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">jenkins2</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">kube-ops</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">jenkins2</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      terminationGracePeriodSeconds:</span> <span class="number">10</span></span><br><span class="line"><span class="attr">      serviceAccountName:</span> <span class="string">jenkins2</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">jenkins</span></span><br><span class="line"><span class="attr">        image:</span> <span class="string">jenkins/jenkins:lts</span></span><br><span class="line"><span class="attr">        imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">8080</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">web</span></span><br><span class="line"><span class="attr">          protocol:</span> <span class="string">TCP</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">50000</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">agent</span></span><br><span class="line"><span class="attr">          protocol:</span> <span class="string">TCP</span></span><br><span class="line"><span class="attr">        resources:</span></span><br><span class="line"><span class="attr">          limits:</span></span><br><span class="line"><span class="attr">            cpu:</span> <span class="number">1000</span><span class="string">m</span></span><br><span class="line"><span class="attr">            memory:</span> <span class="number">1</span><span class="string">Gi</span></span><br><span class="line"><span class="attr">          requests:</span></span><br><span class="line"><span class="attr">            cpu:</span> <span class="number">500</span><span class="string">m</span></span><br><span class="line"><span class="attr">            memory:</span> <span class="number">512</span><span class="string">Mi</span></span><br><span class="line"><span class="attr">        livenessProbe:</span></span><br><span class="line"><span class="attr">          httpGet:</span></span><br><span class="line"><span class="attr">            path:</span> <span class="string">/login</span></span><br><span class="line"><span class="attr">            port:</span> <span class="number">8080</span></span><br><span class="line"><span class="attr">          initialDelaySeconds:</span> <span class="number">60</span></span><br><span class="line"><span class="attr">          timeoutSeconds:</span> <span class="number">5</span></span><br><span class="line"><span class="attr">          failureThreshold:</span> <span class="number">12</span></span><br><span class="line"><span class="attr">        readinessProbe:</span></span><br><span class="line"><span class="attr">          httpGet:</span></span><br><span class="line"><span class="attr">            path:</span> <span class="string">/login</span></span><br><span class="line"><span class="attr">            port:</span> <span class="number">8080</span></span><br><span class="line"><span class="attr">          initialDelaySeconds:</span> <span class="number">60</span></span><br><span class="line"><span class="attr">          timeoutSeconds:</span> <span class="number">5</span></span><br><span class="line"><span class="attr">          failureThreshold:</span> <span class="number">12</span></span><br><span class="line"><span class="attr">        volumeMounts:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">jenkinshome</span></span><br><span class="line"><span class="attr">          subPath:</span> <span class="string">jenkins2</span></span><br><span class="line"><span class="attr">          mountPath:</span> <span class="string">/var/jenkins_home</span></span><br><span class="line"><span class="attr">        env:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">LIMITS_MEMORY</span></span><br><span class="line"><span class="attr">          valueFrom:</span></span><br><span class="line"><span class="attr">            resourceFieldRef:</span></span><br><span class="line"><span class="attr">              resource:</span> <span class="string">limits.memory</span></span><br><span class="line"><span class="attr">              divisor:</span> <span class="number">1</span><span class="string">Mi</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">JAVA_OPTS</span></span><br><span class="line"><span class="attr">          value:</span> <span class="bullet">-Xmx$(LIMITS_MEMORY)m</span> <span class="attr">-XshowSettings:vm</span> <span class="bullet">-Dhudson.slaves.NodeProvisioner.initialDelay=0</span> <span class="bullet">-Dhudson.slaves.NodeProvisioner.MARGIN=50</span> <span class="bullet">-Dhudson.slaves.NodeProvisioner.MARGIN0=0.85</span> <span class="bullet">-Duser.timezone=Asia/Shanghai</span></span><br><span class="line"><span class="attr">      securityContext:</span></span><br><span class="line"><span class="attr">        fsGroup:</span> <span class="number">1000</span></span><br><span class="line"><span class="attr">      volumes:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">jenkinshome</span></span><br><span class="line"><span class="attr">        persistentVolumeClaim:</span></span><br><span class="line"><span class="attr">          claimName:</span> <span class="string">opspvc</span></span><br><span class="line"> </span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">jenkins2</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">kube-ops</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">jenkins2</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">jenkins2</span></span><br><span class="line"><span class="attr">  type:</span> <span class="string">NodePort</span></span><br><span class="line"><span class="attr">  ports:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">web</span></span><br><span class="line"><span class="attr">    port:</span> <span class="number">8080</span></span><br><span class="line"><span class="attr">    targetPort:</span> <span class="string">web</span></span><br><span class="line"><span class="attr">    nodePort:</span> <span class="number">30002</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">agent</span></span><br><span class="line"><span class="attr">    port:</span> <span class="number">50000</span></span><br><span class="line"><span class="attr">    targetPort:</span> <span class="string">agent</span></span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;对象资源都放置在一个名为 kube-ops 的 namespace 下面，所以我们需要添加创建一个 namespace,namespace 请参考namspace章节的具体介绍</p><figure class="highlight n1ql"><table><tr><td class="code"><pre><span class="line"># kubectl <span class="keyword">create</span> <span class="keyword">namespace</span> kube-ops</span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;这里使用一个名为 jenkins/jenkins:lts 的官方镜像，这是 jenkins 官方的 Docker 镜像，然后也有一些环境变量，当然我们也可以根据自己的需求来定制一个镜像，比如我们可以将一些插件打包在自定义的镜像当中，<a href="https://github.com/jenkinsci/docker" target="_blank" rel="noopener">可以参考文档</a>。我们这里使用默认的官方镜像就行，另外一个还需要注意的是我们将容器的 /var/jenkins_home 目录挂载到了一个名为 opspvc 的 PVC 对象上面，所以我们同样还得提前创建一个对应的 PVC 对象，当然我们也可以使用我们前面的 StorageClass 对象来自动创建：(jenkins-pvc.yaml)</p><h3 id="1-2-Jenkins-StorageClass-创建"><a href="#1-2-Jenkins-StorageClass-创建" class="headerlink" title="1.2 Jenkins StorageClass 创建"></a>1.2 Jenkins StorageClass 创建</h3><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat jenkins-pvc.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">opspv</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">kube-ops</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  capacity:</span></span><br><span class="line"><span class="attr">    storage:</span> <span class="number">20</span><span class="string">Gi</span></span><br><span class="line"><span class="attr">  accessModes:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">ReadWriteMany</span></span><br><span class="line"><span class="attr">  persistentVolumeReclaimPolicy:</span> <span class="string">Delete</span></span><br><span class="line"><span class="attr">  nfs:</span></span><br><span class="line"><span class="attr">    server:</span> <span class="number">172.21</span><span class="number">.16</span><span class="number">.231</span></span><br><span class="line"><span class="attr">    path:</span> <span class="string">/data/jenkins</span></span><br><span class="line"> </span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">opspvc</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">kube-ops</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  accessModes:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">ReadWriteMany</span></span><br><span class="line"><span class="attr">  resources:</span></span><br><span class="line"><span class="attr">    requests:</span></span><br><span class="line"><span class="attr">storage:</span> <span class="number">20</span><span class="string">Gi</span></span><br></pre></td></tr></table></figure><ul><li>创建pvc对象<figure class="highlight vala"><table><tr><td class="code"><pre><span class="line"><span class="meta"># kubectl create -f jenkins-pvc.yaml</span></span><br></pre></td></tr></table></figure></li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;另外这里还需要使用到一个拥有相关权限的 serviceAccount：jenkins2，我们这里只是给jenkins 赋予了一些必要的权限，当然如果你对 serviceAccount 的权限不是很熟悉的话，我们给这个 sa 绑定一个 cluster-admin 的集群角色权限也是可以的，当然这样具有一定的安全风险：（jenkins-rbac.yaml）</p><h3 id="1-3-Jenkins-serviceAccount"><a href="#1-3-Jenkins-serviceAccount" class="headerlink" title="1.3 Jenkins serviceAccount"></a>1.3 Jenkins serviceAccount</h3><figure class="highlight dts"><table><tr><td class="code"><pre><span class="line"><span class="meta"># cat jenkins-rbac.yaml</span></span><br><span class="line"><span class="symbol">apiVersion:</span> v1</span><br><span class="line"><span class="symbol">kind:</span> ServiceAccount</span><br><span class="line"><span class="symbol">metadata:</span></span><br><span class="line"><span class="symbol">  name:</span> jenkins2</span><br><span class="line"><span class="symbol">  namespace:</span> kube-ops</span><br><span class="line"> </span><br><span class="line">---</span><br><span class="line"> </span><br><span class="line"><span class="symbol">kind:</span> Role</span><br><span class="line"><span class="symbol">apiVersion:</span> rbac.authorization.k8s.io/v1beta1</span><br><span class="line"><span class="symbol">metadata:</span></span><br><span class="line"><span class="symbol">  name:</span> jenkins2</span><br><span class="line"><span class="symbol">  namespace:</span> kube-ops</span><br><span class="line"><span class="symbol">rules:</span></span><br><span class="line">  - apiGroups: [<span class="string">""</span>]</span><br><span class="line"><span class="symbol">    resources:</span> [<span class="string">"pods"</span>]</span><br><span class="line"><span class="symbol">    verbs:</span> [<span class="string">"create"</span>,<span class="string">"delete"</span>,<span class="string">"get"</span>,<span class="string">"list"</span>,<span class="string">"patch"</span>,<span class="string">"update"</span>,<span class="string">"watch"</span>]</span><br><span class="line">  - apiGroups: [<span class="string">""</span>]</span><br><span class="line"><span class="symbol">    resources:</span> [<span class="string">"pods/exec"</span>]</span><br><span class="line"><span class="symbol">    verbs:</span> [<span class="string">"create"</span>,<span class="string">"delete"</span>,<span class="string">"get"</span>,<span class="string">"list"</span>,<span class="string">"patch"</span>,<span class="string">"update"</span>,<span class="string">"watch"</span>]</span><br><span class="line">  - apiGroups: [<span class="string">""</span>]</span><br><span class="line"><span class="symbol">    resources:</span> [<span class="string">"pods/log"</span>]</span><br><span class="line"><span class="symbol">    verbs:</span> [<span class="string">"get"</span>,<span class="string">"list"</span>,<span class="string">"watch"</span>]</span><br><span class="line">  - apiGroups: [<span class="string">""</span>]</span><br><span class="line"><span class="symbol">    resources:</span> [<span class="string">"secrets"</span>]</span><br><span class="line"><span class="symbol">    verbs:</span> [<span class="string">"get"</span>]</span><br><span class="line"> </span><br><span class="line">---</span><br><span class="line"><span class="symbol">apiVersion:</span> rbac.authorization.k8s.io/v1beta1</span><br><span class="line"><span class="symbol">kind:</span> RoleBinding</span><br><span class="line"><span class="symbol">metadata:</span></span><br><span class="line"><span class="symbol">  name:</span> jenkins2</span><br><span class="line"><span class="symbol">  namespace:</span> kube-ops</span><br><span class="line"><span class="symbol">roleRef:</span></span><br><span class="line"><span class="symbol">  apiGroup:</span> rbac.authorization.k8s.io</span><br><span class="line"><span class="symbol">  kind:</span> Role</span><br><span class="line"><span class="symbol">  name:</span> jenkins2</span><br><span class="line"><span class="symbol">subjects:</span></span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line"><span class="symbol">    name:</span> jenkins2</span><br><span class="line"><span class="symbol">namespace:</span> kube-ops</span><br></pre></td></tr></table></figure><ul><li>创建 rbac 相关的资源对象<figure class="highlight vala"><table><tr><td class="code"><pre><span class="line"><span class="meta"># kubectl create -f jenkins-rbac.yaml</span></span><br></pre></td></tr></table></figure></li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;这里通过 ingress的形式来访问Jenkins 的 web 服务，Jenkins 服务端口为8080，50000 端口为agent，这个端口主要是用于 Jenkins 的 master 和 slave 之间通信使用的。(jenkins-ingress.yaml)</p><h3 id="1-4-Jenkins-对外提供访问"><a href="#1-4-Jenkins-对外提供访问" class="headerlink" title="1.4 Jenkins 对外提供访问"></a>1.4 Jenkins 对外提供访问</h3><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat jenkins-ingress.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">jenkins-ingress</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">kube-ops</span></span><br><span class="line"><span class="attr">  annotations:</span></span><br><span class="line">    <span class="string">kubernetes.io/ingress.class:</span> <span class="string">"nginx"</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  rules:</span></span><br><span class="line"><span class="attr">  - host:</span> <span class="string">ci.xxlaila.cn</span></span><br><span class="line"><span class="attr">    http:</span></span><br><span class="line"><span class="attr">      paths:</span></span><br><span class="line"><span class="attr">      - backend:</span></span><br><span class="line"><span class="attr">          serviceName:</span> <span class="string">jenkins2</span></span><br><span class="line"><span class="attr">          servicePort:</span> <span class="number">8080</span></span><br></pre></td></tr></table></figure><ul><li>创建 Jenkins 服务<figure class="highlight vala"><table><tr><td class="code"><pre><span class="line"><span class="meta"># kubectl create -f jenkins-deployment.yaml</span></span><br></pre></td></tr></table></figure></li></ul><p>创建完成后docke回去拉去镜像，需要等待一会，我们可以通过命令来进行查看jenkins是否部署成功</p><figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line"># kubectl <span class="keyword">get</span> pods -n kube-ops</span><br><span class="line">NAME                       READY   STATUS    RESTARTS   AGE</span><br><span class="line">jenkins2<span class="number">-84f</span>476cbb-vz4b2   <span class="number">1</span>/<span class="number">1</span>     Running   <span class="number">0</span>          <span class="number">2</span>d19h</span><br></pre></td></tr></table></figure><p>部署完成以后我么可以通过在jenkins-ingress.yaml里面绑定过的域名进行访问，然后进行安装配置：<br><img src="https://img.xxlaila.cn/sfdsfdsf38432.png" alt="img"></p><blockquote><p>初始化的密码我们可以在 jenkins 的容器的日志中进行查看，也可以直接在 nfs 的共享数据目录中查看</p><p>$ cat /data/jenkins/jenkins2/secrets/initialAdminPassword</p></blockquote><p>完成配置，就可以到jenkins的界面，就和我们在vm下安装的jenkins没有任何的区别。<br><img src="https://img.xxlaila.cn/isdy823723894324.png" alt="img"></p><h2 id="2-配置jenkins"><a href="#2-配置jenkins" class="headerlink" title="2 配置jenkins"></a>2 配置jenkins</h2><p>接下来我们需要来配置 Jenkins，让他能够动态的生成 Slave 的 Pod，安装jenkins的插件清单</p><p><code>Kubernetes This plugin integrates Jenkins with Kubernetes</code><br>2.1 Kubernetes和Jenkins的结合<br>&nbsp;&nbsp;&nbsp;&nbsp;点击 系统管理(Manage Jenkins) —&gt; 系统配置(Configure System) —&gt; (拖到最下方)Add a new cloud —&gt; 选择 Kubernetes，然后填写 Kubernetes 和 Jenkins 配置信息。</p><p><img src="https://img.xxlaila.cn/di32sdsf.png" alt="img"></p><blockquote><p>注意 namespace，我们这里填 kube-ops，然后点击Test Connection，如果出现 Connection test successful 的提示信息证明Jenkins 已经可以和 Kubernetes 系统正常通信了，然后下方的 Jenkins URL 地址：<a href="http://jenkins2.kube-ops.svc.cluster.local:8080，这里的格式为服务名.namespace.svc.cluster.local:8080，根据上面创建的jenkins的服务名填写，我这里是之前创建的名为jenkins，如果是用上面我们创建的就应该是jenkins2" target="_blank" rel="noopener">http://jenkins2.kube-ops.svc.cluster.local:8080，这里的格式为服务名.namespace.svc.cluster.local:8080，根据上面创建的jenkins的服务名填写，我这里是之前创建的名为jenkins，如果是用上面我们创建的就应该是jenkins2</a></p></blockquote><h3 id="2-2、配置-Pod-Template"><a href="#2-2、配置-Pod-Template" class="headerlink" title="2.2、配置 Pod Template"></a>2.2、配置 Pod Template</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;配置 Jenkins Slave 运行的 Pod 模板，命名空间我们同样是用kube-ops，Labels 这里也非常重要，对于后面执行 Job 的时候需要用到该值，然后我们这里使用的是 cnych/jenkins:jnlp 这个镜像，这个镜像是在官方的 jnlp 镜像基础上定制的，加入了 kubectl 等一些实用的工具。<br><img src="https://img.xxlaila.cn/897kdfhgdkjb4.png" alt="img"><br><img src="https://img.xxlaila.cn/kjgsadsad8234632.png" alt="img"></p><p>&nbsp;&nbsp;&nbsp;&nbsp;另外需要注意我们这里需要在下面挂载一个主机目录，一个是 /var/run/docker.sock，该文件是用于 Pod 中的容器能够共享宿主机的 Docker，这就是说的 docker in docker 的方式，Docker 二进制文件我们已经打包到上面的镜像中了。如果在slave agent中想要访问kubernetes 集群中其他资源，我们还需要绑定之前创建的Service Account 账号:jenkins2</p><p><img src="https://img.xxlaila.cn/khsdif28734knsdfkds.png" alt="img"><br>&nbsp;&nbsp;&nbsp;&nbsp;另外还有几个参数需要注意，上图有一个pod寿命代理的空闲存活时间（分），意思是当处于空闲状态的时候保留 Slave Pod多长时间，这个参数最好我们保存默认就行了，如果你设置过大的话，Job 任务执行完成后，对应的 Slave Pod 就不会立即被销毁删除。到这里我们的 Kubernetes Plugin插件就算配置完成了</p><h3 id="2-3-Jenkins-测试"><a href="#2-3-Jenkins-测试" class="headerlink" title="2.3 Jenkins 测试"></a>2.3 Jenkins 测试</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;Kubernetes 插件的配置工作完成了，接下来我们就来添加一个 Job 任务，看是否能够在 Slave Pod 中执行，任务执行完成后看 Pod 是否会被销毁在 Jenkins 首页点击create new jobs，创建一个测试的任务，输入任务名称，然后我们选择 Freestyle project 类型的任务<br>&nbsp;&nbsp;&nbsp;&nbsp;新建一个job为simple-test，增加一个shell模块，shell模块里面增加简单的echo来测试slave的动态部署：</p><figure class="highlight vim"><table><tr><td class="code"><pre><span class="line"><span class="keyword">echo</span> <span class="string">"测试 Kubernetes 动态生成 jenkins slave"</span></span><br><span class="line"><span class="keyword">echo</span> <span class="string">"==============docker in docker==========="</span></span><br><span class="line">docker info</span><br><span class="line"><span class="keyword">echo</span> <span class="string">"=============kubectl============="</span></span><br><span class="line">kubectl <span class="built_in">get</span> pods -n kube-<span class="built_in">system</span></span><br></pre></td></tr></table></figure><p><img src="https://img.xxlaila.cn/s94uskljdslfd.png" alt="img"></p><p>现在我们直接在页面点击做成的 Build now 触发构建即可，然后观察 Kubernetes 集群中 Pod 的变化</p><figure class="highlight vala"><table><tr><td class="code"><pre><span class="line"><span class="meta"># kubectl get pods -n kube-ops</span></span><br></pre></td></tr></table></figure><p>Kubernetes 界面也会出现jenkins agent的进行pod的进行部署。部署完成后随及删除pod。</p><p><img src="https://img.xxlaila.cn/sakhd89234klmds.png" alt="img"><br><img src="https://img.xxlaila.cn/nslkfhio3rsd.png" alt="img"></p><h2 id="3、Jenkins错误解决"><a href="#3、Jenkins错误解决" class="headerlink" title="3、Jenkins错误解决"></a>3、Jenkins错误解决</h2><p>第一次学习安装jenkins踩了很多坑，但是同时也学习了很多的，下面是在k8s上安装jenkins遇到的一些错误：</p><ul><li>打开jenkins页面的时候提示dns不能解析，洁面如下图：</li></ul><p><img src="https://img.xxlaila.cn/skajdh823648uesd.png" alt="img"></p><ul><li>查看jenkins的日志提示</li></ul><p><img src="https://img.xxlaila.cn/8243ihkdfnklsads.png" alt="img"></p><p>&nbsp;&nbsp;&nbsp;&nbsp;导致的问题有https、网络连接不通畅，这里我们需要吧https修改为http，需要修改jenkins的配置文件。然后再重新建立jenkins的pod。进入jenkins的目录修改hudson.model.UpdateCenter.xml文件</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">$ cat hudson.model.UpdateCenter.xml</span><br><span class="line"><span class="meta">&lt;?xml version='1.1' encoding='UTF-8'?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">sites</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">site</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">id</span>&gt;</span>default<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://updates.jenkins.io/update-center.json<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">site</span>&gt;</span></span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;在做k8s的时候一定要用证书，不然后期在做各种服务的时候都会遇到错误，因为docker默认去私有registory要https，kuber-api要https。当然没有使用https都可以换成http，在次重新部署jenkins以后提示系列信息。访问目录没有权限。</p><p><img src="https://img.xxlaila.cn/2864jksfhjdsh.png" alt="img"></p><p>&nbsp;&nbsp;&nbsp;&nbsp;进入nfs目录，需要修改下目录权限, 因为当映射本地数据卷时，/home/docker/jenkins目录的拥有者为root用户，而容器中jenkins user的uid为1000</p><p><code>$ sudo chown -R 1000:1000 /data/jenkins</code></p><blockquote><p>这里吧https解决了还是遇到提示网络不通。下图</p></blockquote><p><img src="https://img.xxlaila.cn/xnks94uoildsfs.png" alt="img"></p><blockquote><p>这里是dns的不能解析的问题，以下排错思路：登陆jenkins的容器里面查看路由是否正确</p></blockquote><p><img src="https://img.xxlaila.cn/382468365324.png" alt="img"></p><blockquote><p>然后在确认容器是否可以联通外网，还是dns不能解析<br><img src="https://img.xxlaila.cn/fnijwy4nkdsfkhdsf.png" alt="img"></p></blockquote><blockquote><p>这里ping 114没有问题，ping域名不能解析，说明是dns解析有问题。接着我们在查看容器的dns配置</p></blockquote><p><img src="https://img.xxlaila.cn/dkjfsg328943242.png" alt="img"></p><p>&nbsp;&nbsp;&nbsp;&nbsp;这里是dns问题。这里不阐述dns，参考第二章k8s dns,jenkins 在执行编译的时候提示: <code>‘Jenkins’ doesn’t have label ‘jnlp-agent’</code>,在系统配置配置里面进行测试连接k8s 的api提示如下错误</p><p><img src="https://img.xxlaila.cn/324768ksdjsfhds.png" alt="img"></p><ul><li>添加jenkins的secret认证</li></ul><figure class="highlight stata"><table><tr><td class="code"><pre><span class="line"># kubectl get secret  -<span class="keyword">n</span> kube-ops</span><br><span class="line">NAME                     <span class="keyword">TYPE</span>                                  DATA   AGE</span><br><span class="line">default-<span class="keyword">token</span>-4gzkv      kubernetes.io/service-account-<span class="keyword">token</span>   3      13d</span><br><span class="line">jenkins2-<span class="keyword">token</span>-mjnw4     kubernetes.io/service-account-<span class="keyword">token</span>   3      14m</span><br><span class="line">prometheus-<span class="keyword">token</span>-84p87   kubernetes.io/service-account-<span class="keyword">token</span>   3      13d</span><br><span class="line"># kubectl <span class="keyword">describe</span> secret jenkins2-<span class="keyword">token</span>-mjnw4 -<span class="keyword">n</span> kube-ops</span><br><span class="line">Name:         jenkins2-<span class="keyword">token</span>-mjnw4</span><br><span class="line">Namespace:    kube-ops</span><br><span class="line">Labels:       &lt;none&gt;</span><br><span class="line">Annotations:  kubernetes.io/service-account.name: jenkins2</span><br><span class="line">              kubernetes.io/service-account.uid: ffced652-2f6c-11e9-98a4-fa163e14c5bd</span><br><span class="line"></span><br><span class="line"><span class="keyword">Type</span>:  kubernetes.io/service-account-<span class="keyword">token</span></span><br><span class="line"></span><br><span class="line">Data</span><br><span class="line">====</span><br><span class="line"><span class="keyword">ca</span>.crt:     1025 bytes</span><br><span class="line">namespace:  8 bytes</span><br><span class="line"><span class="keyword">token</span>:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLW9wcyIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJqZW5raW5zMi10b2tlbi1tam53NCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJqZW5raW5zMiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImZmY2VkNjUyLTJmNmMtMTFlOS05OGE0LWZhMTYzZTE0YzViZCIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLW9wczpqZW5raW5zMiJ9.PlPvO_AST4Q6tJJ2i2zGFfufFN1xjWLlHZ5ipTK0aU5CdR49OAropPQhQ0TjLRWf4Z66h847g28OCABmxO1cSG_-8UpwVsohFROTCOjx9Ka3KACmaIkw9Bvihm_lPQlaLykdyXxVDrfI6TobtG0Y5KnKPFj8CjkIFPk5ewTKpOm5pDKVDKu4W_4uOhSnISfLVUvHp8A_ojK_JCVnBBr0Py3UeuEF8vjJES0_yKNxPUtXQq-vkWEZecnAC_x5sfFJTA5aB18sEnxCaeMzgUxzi4IflNxxyVjdZrbq0UdS8llmfnGg5Ur7Zf-lu2ajdOlRdQp6VRPMcQmQaWoHUuoevg</span><br></pre></td></tr></table></figure><p><img src="https://img.xxlaila.cn/2372837934232.png" alt="img"><br><img src="https://img.xxlaila.cn/djfjsr3897493432.png" alt="img"></p><!-- rebuild by neat -->]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>ci/cd</tag>
      </tags>
  </entry>
  <entry>
    <title>kube nfs 动态存储</title>
    <url>/2019/08/12/kube-nfs-%E5%8A%A8%E6%80%81%E5%AD%98%E5%82%A8/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><p>&nbsp;&nbsp;&nbsp;&nbsp;nfs-client-provisioner是一个automatic provisioner，使用NFS作为存储，自动创建PV和对应的PVC，本身不提供NFS存储，需要外部先有一套NFS存储服务。</p><ul><li>PV以 ${namespace}-${pvcName}-${pvName}的命名格式提供（在NFS服务器上）</li><li>PV回收的时候以 archieved-${namespace}-${pvcName}-${pvName} 的命名格式（在NFS服务器上）</li></ul><p><a href="https://github.com/kubernetes-incubator/external-storage/tree/master/nfs-client" target="_blank" rel="noopener">官方访问地址</a></p><h2 id="1、权限体系构建"><a href="#1、权限体系构建" class="headerlink" title="1、权限体系构建"></a>1、权限体系构建</h2><h3 id="1-1、创建serviceaccount"><a href="#1-1、创建serviceaccount" class="headerlink" title="1.1、创建serviceaccount"></a>1.1、创建serviceaccount</h3><p>ServiceAccount也是一种账号, 供运行在pod中的进程使用, 为pod中的进程提供必要的身份证明.</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat serviceaccount.yaml </span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs-client-provisioner</span><br><span class="line">  namespace: kube-test</span><br></pre></td></tr></table></figure><a id="more"></a><h3 id="1-2、创建role"><a href="#1-2、创建role" class="headerlink" title="1.2、创建role"></a>1.2、创建role</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat  clusterrole.yaml</span></span><br><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs-client-provisioner-runner</span><br><span class="line">  namespace: kube-test</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups: [<span class="string">""</span>]</span><br><span class="line">    resources: [<span class="string">"persistentvolumes"</span>]</span><br><span class="line">    verbs: [<span class="string">"get"</span>, <span class="string">"list"</span>, <span class="string">"watch"</span>, <span class="string">"create"</span>, <span class="string">"delete"</span>]</span><br><span class="line">  - apiGroups: [<span class="string">""</span>]</span><br><span class="line">    resources: [<span class="string">"persistentvolumeclaims"</span>]</span><br><span class="line">    verbs: [<span class="string">"get"</span>, <span class="string">"list"</span>, <span class="string">"watch"</span>, <span class="string">"update"</span>]</span><br><span class="line">  - apiGroups: [<span class="string">"storage.k8s.io"</span>]</span><br><span class="line">    resources: [<span class="string">"storageclasses"</span>]</span><br><span class="line">    verbs: [<span class="string">"get"</span>, <span class="string">"list"</span>, <span class="string">"watch"</span>]</span><br><span class="line">  - apiGroups: [<span class="string">""</span>]</span><br><span class="line">    resources: [<span class="string">"events"</span>]</span><br><span class="line">    verbs: [<span class="string">"watch"</span>, <span class="string">"create"</span>, <span class="string">"update"</span>, <span class="string">"patch"</span>]</span><br><span class="line">  - apiGroups: [<span class="string">""</span>]</span><br><span class="line">    resources: [<span class="string">"services"</span>, <span class="string">"endpoints"</span>]</span><br><span class="line">    verbs: [<span class="string">"get"</span>, <span class="string">"create"</span>,<span class="string">"list"</span>, <span class="string">"watch"</span>,<span class="string">"update"</span>]</span><br><span class="line">  - apiGroups: [<span class="string">"extensions"</span>]</span><br><span class="line">    resources: [<span class="string">"podsecuritypolicies"</span>]</span><br><span class="line">    resourceNames: [<span class="string">"nfs-client-provisioner"</span>]</span><br><span class="line">    verbs: [<span class="string">"use"</span>]</span><br></pre></td></tr></table></figure><h3 id="1-3、账户和角色绑定"><a href="#1-3、账户和角色绑定" class="headerlink" title="1.3、账户和角色绑定"></a>1.3、账户和角色绑定</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat clusterrolebinding.yaml </span></span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: run-nfs-client-provisioner</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: nfs-client-provisioner</span><br><span class="line">    namespace: kube-test</span><br><span class="line">roleRef:</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: nfs-client-provisioner-runner</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">执行创建</span><br><span class="line">kubectl create -f serviceaccount.yaml -f clusterrole.yaml -f clusterrolebinding.yaml</span><br></pre></td></tr></table></figure><h2 id="2、安装部署"><a href="#2、安装部署" class="headerlink" title="2、安装部署"></a>2、安装部署</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;下载deployment.yaml文件,需要修改NFS服务器所在的IP地址（10.10.10.60），以及NFS服务器共享的路径（/ifs/kubernetes），两处都需要修改为你实际的NFS服务器和共享目录</p><h3 id="2-1、部署存储供应卷"><a href="#2-1、部署存储供应卷" class="headerlink" title="2.1、部署存储供应卷"></a>2.1、部署存储供应卷</h3><blockquote><p>根据PVC的请求, 动态创建PV存储.</p></blockquote><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat deployment.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs-client-provisioner</span><br><span class="line">---</span><br><span class="line">kind: Deployment</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs-client-provisioner</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  strategy:</span><br><span class="line">    <span class="built_in">type</span>: Recreate</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nfs-client-provisioner</span><br><span class="line">    spec:</span><br><span class="line">      serviceAccountName: nfs-client-provisioner</span><br><span class="line">      containers:</span><br><span class="line">        - name: nfs-client-provisioner</span><br><span class="line">          image: quay.io/external_storage/nfs-client-provisioner:latest</span><br><span class="line">          volumeMounts:</span><br><span class="line">            - name: nfs-client-root</span><br><span class="line">              mountPath: /persistentvolumes</span><br><span class="line">          env:</span><br><span class="line">            - name: PROVISIONER_NAME</span><br><span class="line">              value: fuseim.pri/ifs</span><br><span class="line">            - name: NFS_SERVER</span><br><span class="line">              value: 172.21.16.244</span><br><span class="line">            - name: NFS_PATH</span><br><span class="line">              value: /data</span><br><span class="line">      volumes:</span><br><span class="line">        - name: nfs-client-root</span><br><span class="line">          nfs:</span><br><span class="line">            server: 10.10.10.60</span><br><span class="line">            path: /ifs/kubernetes</span><br></pre></td></tr></table></figure><pre><code>* 修改StorageClass文件并部署class.yaml</code></pre><p>此处可以不修改，或者修改provisioner的名字，需要与上面的deployment的PROVISIONER_NAME名字一致</p><h3 id="2-2、创建storageclass"><a href="#2-2、创建storageclass" class="headerlink" title="2.2、创建storageclass"></a>2.2、创建storageclass</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat class.yaml</span></span><br><span class="line">apiVersion: storage.k8s.io/v1</span><br><span class="line">kind: StorageClass</span><br><span class="line">metadata:</span><br><span class="line">  name: managed-nfs-storage</span><br><span class="line">provisioner: fuseim.pri/ifs <span class="comment"># or choose another name, must match deployment's env PROVISIONER_NAME'</span></span><br><span class="line">parameters:</span><br><span class="line">  archiveOnDelete: <span class="string">"false"</span></span><br></pre></td></tr></table></figure><h4 id="2-2-1、查看StorageClass"><a href="#2-2-1、查看StorageClass" class="headerlink" title="2.2.1、查看StorageClass"></a>2.2.1、查看StorageClass</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get sc</span></span><br><span class="line"><span class="comment"># kubectl get storageclass</span></span><br><span class="line">NAME                  PROVISIONER      AGE</span><br><span class="line">managed-nfs-storage   fuseim.pri/ifs   19h</span><br></pre></td></tr></table></figure><h4 id="2-2-2、设置默认后端存储"><a href="#2-2-2、设置默认后端存储" class="headerlink" title="2.2.2、设置默认后端存储"></a>2.2.2、设置默认后端存储</h4><p>设置这个default名字的SC为Kubernetes的默认存储后端</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl patch storageclass managed-nfs-storage -p '&#123;"metadata": &#123;"annotations":&#123;"storageclass.kubernetes.io/is-default-class":"true"&#125;&#125;&#125;'</span></span><br></pre></td></tr></table></figure><h4 id="2-2-3、测试创建PVC"><a href="#2-2-3、测试创建PVC" class="headerlink" title="2.2.3、测试创建PVC"></a>2.2.3、测试创建PVC</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat test-claim.yaml </span></span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: <span class="built_in">test</span>-claim</span><br><span class="line">  namespace: kube-test</span><br><span class="line">  annotations:</span><br><span class="line">    volume.beta.kubernetes.io/storage-class: <span class="string">"managed-nfs-storage"</span></span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteMany</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 1Mi</span><br></pre></td></tr></table></figure><h4 id="2-2-4、启动测试POD"><a href="#2-2-4、启动测试POD" class="headerlink" title="2.2.4、启动测试POD"></a>2.2.4、启动测试POD</h4><p>POD文件如下，作用就是在test-claim的PV里touch一个SUCCESS文件</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat test-pod.yaml </span></span><br><span class="line">kind: Pod</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: <span class="built_in">test</span>-pod</span><br><span class="line">  namespace: kube-test</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: <span class="built_in">test</span>-pod</span><br><span class="line">    image: docker.io/busybox:1.24</span><br><span class="line">    <span class="built_in">command</span>:</span><br><span class="line">      - <span class="string">"/bin/sh"</span></span><br><span class="line">    args:</span><br><span class="line">      - <span class="string">"-c"</span></span><br><span class="line">      - <span class="string">"touch /mnt/SUCCESS &amp;&amp; exit 0 || exit 1"</span></span><br><span class="line">    volumeMounts:</span><br><span class="line">      - name: nfs-pvc</span><br><span class="line">        mountPath: <span class="string">"/mnt"</span></span><br><span class="line">  restartPolicy: <span class="string">"Never"</span></span><br><span class="line">  volumes:</span><br><span class="line">    - name: nfs-pvc</span><br><span class="line">      persistentVolumeClaim:</span><br><span class="line">        claimName: <span class="built_in">test</span>-claim</span><br></pre></td></tr></table></figure><h4 id="2-2-5、校验是否成功"><a href="#2-2-5、校验是否成功" class="headerlink" title="2.2.5、校验是否成功"></a>2.2.5、校验是否成功</h4><p>去NFS共享目录查看有没有SUCCESS文件<br><img src="https://img.xxlaila.cn/%E6%88%AA%E5%9B%BE.png" alt="img"><br><img src="https://img.xxlaila.cn/8934nsdlsa.png" alt="img"></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get pvc -n kube-test</span></span><br><span class="line">NAME         STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS          AGE</span><br><span class="line"><span class="built_in">test</span>-claim   Bound    pvc-f8e08fa5-2de2-11e9-8991-fa163e14c5bd   1Mi        RWX            managed-nfs-storage   20h</span><br></pre></td></tr></table></figure><h3 id="2-3、更改PersistentVolumes-中的一个回收策略"><a href="#2-3、更改PersistentVolumes-中的一个回收策略" class="headerlink" title="2.3、更改PersistentVolumes 中的一个回收策略"></a>2.3、更改PersistentVolumes 中的一个回收策略</h3><ul><li><p>查看集群中PersistentVolumes</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get pv</span></span><br></pre></td></tr></table></figure></li><li><p>更改PersistentVolumes</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl patch pv pvc-f8e08fa5-2de2-11e9-8991-fa163e14c5bd -p '&#123;"spec":&#123;"persistentVolumeReclaimPolicy":"Retain"&#125;&#125;'</span></span><br><span class="line">persistentvolume/pvc-f8e08fa5-2de2-11e9-8991-fa163e14c5bd patched</span><br></pre></td></tr></table></figure></li><li><p>查看更改</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get pv</span></span><br></pre></td></tr></table></figure></li></ul><!-- rebuild by neat -->]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>nfs</tag>
      </tags>
  </entry>
  <entry>
    <title>pvc pv</title>
    <url>/2019/08/12/pvc-pv/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><h2 id="1、介绍"><a href="#1、介绍" class="headerlink" title="1、介绍"></a>1、介绍</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;PersistentVolume（pv）和PersistentVolumeClaim（pvc）是k8s提供的两种API资源，用于抽象存储细节。管理员关注于如何通过pv提供存储功能而无需关注用户如何使用，同样的用户只需要挂载pvc到容器中而不需要关注存储卷采用何种技术实现。<br>&nbsp;&nbsp;&nbsp;&nbsp;pvc和pv的关系与pod和node关系类似，前者消耗后者的资源。pvc可以向pv申请指定大小的存储资源并设置访问模式,这就可以通过Provision -&gt; Claim 的方式，来对存储资源进行控制。</p><h2 id="2、生命周期"><a href="#2、生命周期" class="headerlink" title="2、生命周期"></a>2、生命周期</h2><p>pv和pvc遵循以下生命周期：</p><ul><li>供应准备。通过集群外的存储系统或者云平台来提供存储持久化支持。</li></ul><ul><li><p>静态提供：管理员手动创建多个PV，供PVC使用。</p></li><li><p>动态提供：动态创建PVC特定的PV，并绑定。</p><ul><li>绑定。用户创建pvc并指定需要的资源和访问模式。在找到可用pv之前，pvc会保持未绑定状态。</li><li>使用。用户可在pod中像volume一样使用pvc。</li><li>释放。用户删除pvc来回收存储资源，pv将变成“released”状态。由于还保留着之前的数据，这些数据需要根据不同的策略来处理，否则这些存储资源无法被其他pvc使用。</li><li>回收(Reclaiming)。pv可以设置三种回收策略：保留（Retain），回收（Recycle）和删除（Delete）。</li></ul></li><li><p>保留策略：允许人工处理保留的数据。</p></li><li><p>删除策略：将删除pv和外部关联的存储资源，需要插件支持。</p></li><li><p>回收策略：将执行清除操作，之后可以被新的pvc使用，需要插件支持。</p></li></ul><blockquote><p><em>目前只有NFS和HostPath类型卷支持回收策略，AWS EBS,GCE PD,Azure Disk和Cinder支持删除(Delete)策略。</em></p></blockquote><h3 id="2-1、Provisioning"><a href="#2-1、Provisioning" class="headerlink" title="2.1、Provisioning"></a>2.1、Provisioning</h3><p>两种方式提供的PV资源供给</p><a id="more"></a><p>static</p><ul><li>通过集群管理者创建多个PV，为集群“使用者”提供存储能力而隐藏真实存储的细节。并且存在于kubenretes api中，可被直接使用。</li></ul><p>dynamic</p><ul><li>动态卷供给是kubernetes独有的功能，这一功能允许按需创建存储建。在此之前，集群管理员需要事先在集群外由存储提供者或者云提供商创建</li><li>存储卷，成功之后再创建PersistentVolume对象，才能够在kubernetes中使用。动态卷供给能让集群管理员不必进行预先创建存储卷，而是随着用户需求进行创建。在1.5版本提高了动态卷的弹性和可用性。</li></ul><h2 id="3、PV类型"><a href="#3、PV类型" class="headerlink" title="3、PV类型"></a>3、PV类型</h2><p>pv支持以下类型:</p><pre><code>* GCEPersistentDisk
* AWSElasticBlockStore
* NFS
* iSCSI
* RBD (Ceph Block Device)
* Glusterfs
* AzureFile
* AzureDisk
* CephFS
* cinder
* FC
* FlexVolume
* Flocker
* PhotonPersistentDisk
* Quobyte
* VsphereVolume
* HostPath (single node testing only – local storage is not supported in any way and WILL NOT WORK in a multi-node cluster)</code></pre><h3 id="3-1、PV属性"><a href="#3-1、PV属性" class="headerlink" title="3.1、PV属性"></a>3.1、PV属性</h3><ul><li>访问模式,与pv的语义相同。在请求资源时使用特定模式。</li><li>资源,申请的存储资源数额。</li></ul><h3 id="3-2、PV卷阶段状态"><a href="#3-2、PV卷阶段状态" class="headerlink" title="3.2、PV卷阶段状态"></a>3.2、PV卷阶段状态</h3><ul><li>Available – 资源尚未被claim使用</li><li>Bound – 卷已经被绑定到claim了</li><li>Released – claim被删除，卷处于释放状态，但未被集群回收。</li><li>Failed – 卷自动回收失败</li></ul><h2 id="4、利用nfs创建pv-pvc"><a href="#4、利用nfs创建pv-pvc" class="headerlink" title="4、利用nfs创建pv_pvc"></a>4、利用nfs创建pv_pvc</h2><p>准备一台机器，搭建NFS服务，nfs搭建这里不阐述，</p><h3 id="4-1、在master节点创建pv"><a href="#4-1、在master节点创建pv" class="headerlink" title="4.1、在master节点创建pv"></a>4.1、在master节点创建pv</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat pv.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolume</span><br><span class="line">metadata:</span><br><span class="line">  name: opspv</span><br><span class="line">  namespace: kube-ops</span><br><span class="line">spec:</span><br><span class="line">  capacity:</span><br><span class="line">    storage: 20Gi</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteMany</span><br><span class="line">  nfs:</span><br><span class="line">    path: /data/jenkins</span><br><span class="line">    server: 172.21.16.236</span><br><span class="line"><span class="comment"># kubectl create -f pv.yaml</span></span><br><span class="line"><span class="comment"># kubectl get pv</span></span><br></pre></td></tr></table></figure><h3 id="4-2、在master节点上创建pvc"><a href="#4-2、在master节点上创建pvc" class="headerlink" title="4.2、在master节点上创建pvc"></a>4.2、在master节点上创建pvc</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat pvc.yaml</span></span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: opspv</span><br><span class="line">  namespace: kube-ops</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteMany</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 20Gi</span><br><span class="line"><span class="comment"># kubectl create -f pvc.yaml</span></span><br><span class="line"><span class="comment"># kubectl get pvc</span></span><br></pre></td></tr></table></figure><h3 id="4-3、创建pod挂载pv-pvc"><a href="#4-3、创建pod挂载pv-pvc" class="headerlink" title="4.3、创建pod挂载pv_pvc"></a>4.3、创建pod挂载pv_pvc</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat jenkins-deployment.yaml </span></span><br><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: jenkins2</span><br><span class="line">  namespace: kube-ops</span><br><span class="line">spec:</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: jenkins2</span><br><span class="line">    spec:</span><br><span class="line">      terminationGracePeriodSeconds: 10</span><br><span class="line">      containers:</span><br><span class="line">      - name: jenkins</span><br><span class="line">        image: jenkins/jenkins:lts</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 8080</span><br><span class="line">          name: web</span><br><span class="line">          protocol: TCP</span><br><span class="line">        - containerPort: 50000</span><br><span class="line">          name: agent</span><br><span class="line">          protocol: TCP</span><br><span class="line">        resources:</span><br><span class="line">          limits:</span><br><span class="line">            cpu: 1000m</span><br><span class="line">            memory: 1Gi</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 500m</span><br><span class="line">            memory: 512Mi</span><br><span class="line">        livenessProbe:</span><br><span class="line">          httpGet:</span><br><span class="line">            path: /login</span><br><span class="line">            port: 8080</span><br><span class="line">          initialDelaySeconds: 60</span><br><span class="line">          timeoutSeconds: 5</span><br><span class="line">          failureThreshold: 12</span><br><span class="line">        readinessProbe:</span><br><span class="line">          httpGet:</span><br><span class="line">            path: /login</span><br><span class="line">            port: 8080</span><br><span class="line">          initialDelaySeconds: 60</span><br><span class="line">          timeoutSeconds: 5</span><br><span class="line">          failureThreshold: 12</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: jenkinshome</span><br><span class="line">          subPath: jenkins2</span><br><span class="line">          mountPath: /var/jenkins_home</span><br><span class="line">        env:</span><br><span class="line">        - name: LIMITS_MEMORY</span><br><span class="line">          valueFrom:</span><br><span class="line">            resourceFieldRef:</span><br><span class="line">              resource: limits.memory</span><br><span class="line">              divisor: 1Mi</span><br><span class="line">        - name: JAVA_OPTS</span><br><span class="line">          value: -Xmx$(LIMITS_MEMORY)m -XshowSettings:vm -Dhudson.slaves.NodeProvisioner.initialDelay=0 -Dhudson.slaves.NodeProvisioner.MARGIN=50 -Dhudson.slaves.NodeProvisioner.MARGIN0=0.85 -Duser.timezone=Asia/Shanghai</span><br><span class="line">      securityContext:</span><br><span class="line">        fsGroup: 1000</span><br><span class="line">      volumes:</span><br><span class="line">      - name: jenkinshome</span><br><span class="line">        persistentVolumeClaim:</span><br><span class="line">          claimName: opspvc</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: jenkins2</span><br><span class="line">  namespace: kube-ops</span><br><span class="line">  labels:</span><br><span class="line">    app: jenkins2</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: jenkins2</span><br><span class="line">  <span class="built_in">type</span>: NodePort</span><br><span class="line">  ports:</span><br><span class="line">  - name: web</span><br><span class="line">    port: 8080</span><br><span class="line">    targetPort: web</span><br><span class="line">    nodePort: 30002</span><br><span class="line">  - name: agent</span><br><span class="line">    port: 50000</span><br><span class="line">    targetPort: agent</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      <tags>
        <tag>pvc,pv,kubernetes,存储</tag>
      </tags>
  </entry>
  <entry>
    <title>kubernetes 单机安装</title>
    <url>/2019/08/12/kubernetes-%E5%8D%95%E6%9C%BA%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:23 GMT+0800 (China Standard Time) --><h2 id="1-环境准备"><a href="#1-环境准备" class="headerlink" title="1.环境准备"></a>1.环境准备</h2><blockquote><p>一个master节点，四个node节点<br>master节点ip</p><ul><li>172.21.16.244<br>node节点ip</li><li>172.21.16.24</li><li>172.21.16.231</li><li>172.21.16.202</li><li>172.21.16.55</li></ul></blockquote><ul><li>以下是每一个节点上均进行操作</li></ul><h2 id="2、服务器添加阿里云yum源"><a href="#2、服务器添加阿里云yum源" class="headerlink" title="2、服务器添加阿里云yum源"></a>2、服务器添加阿里云yum源</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo</span><br><span class="line">[kubernetes]</span><br><span class="line">name=Kubernetes</span><br><span class="line">baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line">repo_gpgcheck=0</span><br><span class="line">gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg</span><br><span class="line">  http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h2 id="3、重新建立yum缓存"><a href="#3、重新建立yum缓存" class="headerlink" title="3、重新建立yum缓存"></a>3、重新建立yum缓存</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># yum -y install epel-release &amp;&amp;yum clean all &amp;&amp;yum makecach</span></span><br></pre></td></tr></table></figure><ul><li>记得同步系统的时间</li></ul><h2 id="3、配置转发请求"><a href="#3、配置转发请求" class="headerlink" title="3、配置转发请求"></a>3、配置转发请求</h2><a id="more"></a><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">关闭swap</span><br><span class="line"><span class="comment"># sudo swapoff -a</span></span><br><span class="line"><span class="comment"># cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.conf</span></span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">vm.swappiness=0</span><br><span class="line">EOF</span><br><span class="line"><span class="comment"># sysctl --system</span></span><br></pre></td></tr></table></figure><h2 id="4、安装docker"><a href="#4、安装docker" class="headerlink" title="4、安装docker"></a>4、安装docker</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># yum -y install docker</span></span><br><span class="line"><span class="comment"># systemctl enable docker &amp;&amp; systemctl start docker</span></span><br></pre></td></tr></table></figure><h2 id="5、安装k8s-需要的插件"><a href="#5、安装k8s-需要的插件" class="headerlink" title="5、安装k8s 需要的插件"></a>5、安装k8s 需要的插件</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># yum -y install kubelet kubeadm kubectl kubernetes-cni</span></span><br><span class="line"><span class="comment"># systemctl enable kubelet &amp;&amp; systemctl start kubelet</span></span><br></pre></td></tr></table></figure><ul><li>修改为 kubelet 为Cgroup模式</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># vim /etc/systemd/system/kubelet.service.d/10-kubeadm.conf</span></span><br><span class="line">Environment=<span class="string">"KUBELET_CGROUP_ARGS=--cgroup-driver=systemd"</span></span><br><span class="line">systemctl daemon-reload &amp;&amp; systemctl restart kubelet</span><br></pre></td></tr></table></figure><h2 id="6、拉取镜像"><a href="#6、拉取镜像" class="headerlink" title="6、拉取镜像"></a>6、拉取镜像</h2><p>新建一个shell 拉取镜像到本地(所有节点均操作)</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">images=(kube-proxy:v1.13.0 kube-scheduler:v1.13.0 kube-controller-manager:v1.13.0 kube-apiserver:v1.13.0 etcd:3.2.24 coredns:1.2.6 pause:3.1 kubernetes-dashboard-amd64:v1.10.0 kubernetes-dashboard-init-amd64:v1.0.1  k8s-dns-sidecar-amd64:1.14.9 k8s-dns-kube-dns-amd64:1.14.9 k8s-dns-dnsmasq-nanny:1.15.0 heapster:v1.5.2 kubernetes-dashboard-arm:v1.10.0)</span><br><span class="line"><span class="keyword">for</span> imageName <span class="keyword">in</span> <span class="variable">$&#123;images[@]&#125;</span> ; <span class="keyword">do</span></span><br><span class="line">docker pull xxlaila/<span class="variable">$imageName</span></span><br><span class="line">docker tag xxlaila/<span class="variable">$imageName</span> k8s.gcr.io/<span class="variable">$imageName</span></span><br><span class="line">docker rmi xxlaila/<span class="variable">$imageName</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><ul><li>以下操作是在k8s的master进行操作</li></ul><h2 id="7、初始化相关镜像"><a href="#7、初始化相关镜像" class="headerlink" title="7、初始化相关镜像"></a>7、初始化相关镜像</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubeadm init --kubernetes-version=v1.13.0 --pod-network-cidr=10.244.0.0/16</span></span><br><span class="line"><span class="comment"># 记下这句话，后面node节点加入需要</span></span><br><span class="line"><span class="comment"># kubeadm join 172.21.17.4:6443 --token 0mdk7x.du3cn19qm1jl2b0e --discovery-token-ca-cert-hash sha256:19bf79b41a931735b1f2f5138e1daa436ab26a4f19781ccf2015cff749ddb4b9</span></span><br></pre></td></tr></table></figure><h3 id="7-1、执行创建目录"><a href="#7-1、执行创建目录" class="headerlink" title="7.1、执行创建目录"></a>7.1、执行创建目录</h3><p>后面在生成证书的时候需要</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkdir -p <span class="variable">$HOME</span>/.kube</span><br><span class="line">sudo cp -i /etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config</span><br><span class="line">sudo chown $(id -u):$(id -g) <span class="variable">$HOME</span>/.kube/config</span><br></pre></td></tr></table></figure><h3 id="7-2、查看验证"><a href="#7-2、查看验证" class="headerlink" title="7.2、查看验证"></a>7.2、查看验证</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get componentstatus</span></span><br><span class="line">NAME                 STATUS    MESSAGE              ERROR</span><br><span class="line">controller-manager   Healthy   ok</span><br><span class="line">scheduler            Healthy   ok</span><br><span class="line">etcd-0               Healthy   &#123;<span class="string">"health"</span>: <span class="string">"true"</span>&#125;</span><br><span class="line"><span class="comment"># kubectl get cs</span></span><br><span class="line">NAME                 STATUS    MESSAGE              ERROR</span><br><span class="line">scheduler            Healthy   ok</span><br><span class="line">controller-manager   Healthy   ok</span><br><span class="line">etcd-0               Healthy   &#123;<span class="string">"health"</span>: <span class="string">"true"</span>&#125;</span><br></pre></td></tr></table></figure><h2 id="8、安装flannel-网络"><a href="#8、安装flannel-网络" class="headerlink" title="8、安装flannel 网络"></a>8、安装flannel 网络</h2><blockquote><p>(配置文件和目录每个node都要建立)</p></blockquote><h3 id="8-1-创建flannel配置文件"><a href="#8-1-创建flannel配置文件" class="headerlink" title="8.1 创建flannel配置文件"></a>8.1 创建flannel配置文件</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># mkdir -p /etc/cni/net.d/</span></span><br><span class="line"><span class="comment"># cat &lt;&lt;EOF&gt; /etc/cni/net.d/10-flannel.conf</span></span><br><span class="line">&#123;</span><br><span class="line">“name”: “cbr0”,</span><br><span class="line">“<span class="built_in">type</span>”: “flannel”,</span><br><span class="line">“delegate”: &#123;</span><br><span class="line">“isDefaultGateway”: <span class="literal">true</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="8-2-创建网络配置"><a href="#8-2-创建网络配置" class="headerlink" title="8.2 创建网络配置"></a>8.2 创建网络配置</h3><blockquote><p>(只需要在主节点操作即可)</p></blockquote><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml</span></span><br><span class="line">clusterrole.rbac.authorization.k8s.io/flannel created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/flannel created</span><br><span class="line">serviceaccount/flannel created</span><br><span class="line">configmap/kube-flannel-cfg created</span><br><span class="line">daemonset.extensions/kube-flannel-ds-amd64 created</span><br><span class="line">daemonset.extensions/kube-flannel-ds-arm64 created</span><br><span class="line">daemonset.extensions/kube-flannel-ds-arm created</span><br><span class="line">daemonset.extensions/kube-flannel-ds-ppc64le created</span><br><span class="line">daemonset.extensions/kube-flannel-ds-s390x created</span><br></pre></td></tr></table></figure><h2 id="9、查看命名空间"><a href="#9、查看命名空间" class="headerlink" title="9、查看命名空间"></a>9、查看命名空间</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get ns</span></span><br><span class="line">NAME          STATUS   AGE</span><br><span class="line">default       Active   27m</span><br><span class="line">kube-public   Active   27m</span><br><span class="line">kube-system   Active   27m</span><br></pre></td></tr></table></figure><h2 id="10、查看system的pod"><a href="#10、查看system的pod" class="headerlink" title="10、查看system的pod"></a>10、查看system的pod</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get pods -n kube-system</span></span><br><span class="line">NAME                                         READY   STATUS    RESTARTS   AGE</span><br><span class="line">coredns-86c58d9df4-4gfvd                     1/1     Running   0          27m</span><br><span class="line">coredns-86c58d9df4-cxtz5                     1/1     Running   0          27m</span><br><span class="line">etcd-k8s-zxc-test-3.kxl                      1/1     Running   0          26m</span><br><span class="line">kube-apiserver-k8s-zxc-test-3.kxl            1/1     Running   0          26m</span><br><span class="line">kube-controller-manager-k8s-zxc-test-3.kxl   1/1     Running   0          26m</span><br><span class="line">kube-flannel-ds-nh95x                        1/1     Running   0          15m</span><br><span class="line">kube-proxy-kvlng                             1/1     Running   0          27m</span><br><span class="line">kube-scheduler-k8s-zxc-test-3.kxl            1/1     Running   0          27m</span><br></pre></td></tr></table></figure><h2 id="11、节点加入到kuberneter"><a href="#11、节点加入到kuberneter" class="headerlink" title="11、节点加入到kuberneter"></a>11、节点加入到kuberneter</h2><ul><li>以下是每个node节点执行</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubeadm join 172.21.17.4:6443 --token 0mdk7x.du3cn19qm1jl2b0e --discovery-token-ca-cert-hash sha256:19bf79b41a931735b1f2f5138e1daa436ab26a4f19781ccf2015cff749ddb4b9</span></span><br></pre></td></tr></table></figure><ul><li>查看节点是否加入</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get nodes</span></span><br><span class="line"><span class="comment"># 使用kubectl get pods命令来查看部署状态</span></span><br><span class="line"><span class="comment"># kubectl get pods --all-namespaces</span></span><br></pre></td></tr></table></figure><h2 id="12、安装kubernetes-dashboard"><a href="#12、安装kubernetes-dashboard" class="headerlink" title="12、安装kubernetes dashboard"></a>12、安装kubernetes dashboard</h2><p>下载官网的dashboard文件修改kubernetes-dashboard.yaml文件,用修改之后的kubernetes-dashboard.yaml</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># git clone https://github.com/xxlaila/kubernetes-yaml.git</span></span><br><span class="line"><span class="comment"># 执行创建dashboard</span></span><br><span class="line"><span class="comment"># cd kubernetes-yaml/kubernetes-dashboard</span></span><br><span class="line"><span class="comment"># kubectl apply -f kubernetes-dashboard.yaml</span></span><br><span class="line">secret/kubernetes-dashboard-certs created</span><br><span class="line">serviceaccount/kubernetes-dashboard created</span><br><span class="line">role.rbac.authorization.k8s.io/kubernetes-dashboard-minimal created</span><br><span class="line">rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard-minimal created</span><br><span class="line">deployment.apps/kubernetes-dashboard created</span><br><span class="line">service/kubernetes-dashboard created</span><br></pre></td></tr></table></figure><h2 id="13、查看dashboard部署是否成功"><a href="#13、查看dashboard部署是否成功" class="headerlink" title="13、查看dashboard部署是否成功"></a>13、查看dashboard部署是否成功</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get pods -n kube-system</span></span><br></pre></td></tr></table></figure><h3 id="13-1、查看dashboard-info"><a href="#13-1、查看dashboard-info" class="headerlink" title="13.1、查看dashboard info"></a>13.1、查看dashboard info</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl describe svc kubernetes-dashboard -n kube-system</span></span><br></pre></td></tr></table></figure><h3 id="13-2、查看dashboard部署在那个节点"><a href="#13-2、查看dashboard部署在那个节点" class="headerlink" title="13.2、查看dashboard部署在那个节点"></a>13.2、查看dashboard部署在那个节点</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl  get pods -n kube-system -o wide</span></span><br></pre></td></tr></table></figure><h3 id="13-3、查看service-节点端口"><a href="#13-3、查看service-节点端口" class="headerlink" title="13.3、查看service 节点端口"></a>13.3、查看service 节点端口</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get service -n kube-system -o wide</span></span><br></pre></td></tr></table></figure><h3 id="13-4、创建dashboard-admin-账户"><a href="#13-4、创建dashboard-admin-账户" class="headerlink" title="13.4、创建dashboard admin 账户"></a>13.4、创建dashboard admin 账户</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl apply -f admin-user.yaml</span></span><br><span class="line">获取tokens</span><br><span class="line"><span class="comment"># kubectl describe serviceaccount admin -n kube-system</span></span><br><span class="line">Name:                admin</span><br><span class="line">Namespace:           kube-system</span><br><span class="line">Labels:              k8s-app=kubernetes-dashboard</span><br><span class="line">Annotations:         kubectl.kubernetes.io/last-applied-configuration:</span><br><span class="line">                       &#123;<span class="string">"apiVersion"</span>:<span class="string">"v1"</span>,<span class="string">"kind"</span>:<span class="string">"ServiceAccount"</span>,<span class="string">"metadata"</span>:&#123;<span class="string">"annotations"</span>:&#123;&#125;,<span class="string">"labels"</span>:&#123;<span class="string">"k8s-app"</span>:<span class="string">"kubernetes-dashboard"</span>&#125;,<span class="string">"name"</span>:<span class="string">"admin"</span>,<span class="string">"namesp...</span></span><br><span class="line"><span class="string">Image pull secrets:  &lt;none&gt;</span></span><br><span class="line"><span class="string">Mountable secrets:   admin-token-kxs6k</span></span><br><span class="line"><span class="string">Tokens:              admin-token-kxs6k</span></span><br><span class="line"><span class="string">Events:              &lt;none&gt;</span></span><br><span class="line"><span class="string">查看token 信息</span></span><br><span class="line"><span class="string"># kubectl describe secret admin-token-kxs6k -n kube-system</span></span><br></pre></td></tr></table></figure><h2 id="14、dashboard-访问"><a href="#14、dashboard-访问" class="headerlink" title="14、dashboard 访问"></a>14、dashboard 访问</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;利用节点ip+30001 端口进行访问。访问之前需要在master节点生成证书，把证书(kubecfg.p12)下载到本地，进行导入到浏览器，这里使用火狐浏览器，google浏览器导入,不成功，生产证书之前记得第9步已操作</p><h3 id="14-1、生成证书"><a href="#14-1、生成证书" class="headerlink" title="14.1、生成证书"></a>14.1、生成证书</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># grep 'client-certificate-data' ~/.kube/config | head -n 1 | awk '&#123;print $2&#125;' | base64 -d &gt;&gt; kubecfg.crt</span></span><br><span class="line"><span class="comment"># grep 'client-key-data' ~/.kube/config | head -n 1 | awk '&#123;print $2&#125;' | base64 -d &gt;&gt; kubecfg.key</span></span><br><span class="line"><span class="comment"># openssl pkcs12 -export -clcerts -inkey kubecfg.key -in kubecfg.crt -out kubecfg.p12 -name "kubernetes-client"</span></span><br></pre></td></tr></table></figure><h3 id="14-2、dashboard-配置修改"><a href="#14-2、dashboard-配置修改" class="headerlink" title="14.2、dashboard 配置修改"></a>14.2、dashboard 配置修改</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;kubernetes dashboard v1.10.0使用的是双因子登陆，默认token失效的时间是900秒，15分钟，每15分钟就要进行一次认证。我们可以功过修改token-ttl参数来设置，主要是修改dashboard.yaml文件，并重新建立即可</p><ul><li>在配置文件里面添加<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ports:</span><br><span class="line">- containerPort: 8443</span><br><span class="line">  protocol: TCP</span><br><span class="line">args:</span><br><span class="line">  - --auto-generate-certificates</span><br><span class="line">  - --token-ttl=43200</span><br></pre></td></tr></table></figure></li></ul><blockquote><p>重建dashboard,通过利用http添加端口30001，然后利用tonken进行验证登陆,安装失败清理环境</p></blockquote><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubeadm reset</span></span><br><span class="line">查看加入集群token</span><br><span class="line"><span class="comment"># kubeadm token list</span></span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>Centos 5.5 安装DRBD</title>
    <url>/2019/08/11/Centos-5-5-%E5%AE%89%E8%A3%85DRBD/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><p>Centos5.5 32bit安装DRBD</p><ul><li>安装前准备</li></ul><table><thead><tr><th>节点类型</th><th>IP地址规划</th><th>主机名</th></tr></thead><tbody><tr><td>主用节点</td><td>192.168.1.101</td><td>node2</td></tr><tr><td>备用节点</td><td>192.168.1.102</td><td>node1</td></tr><tr><td>磁盘</td><td>两台10G磁盘</td><td></td></tr></tbody></table><h2 id="在主节点安装DRBD"><a href="#在主节点安装DRBD" class="headerlink" title="在主节点安装DRBD"></a>在主节点安装DRBD</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@node2 ~]<span class="comment"># yum -y install kmod-drbd83 drbd83</span></span><br></pre></td></tr></table></figure><p>安装成功之后/sbin目录下面有drbdadm，drbdmeta，drbdsetup命令，以及/etc /init.d/drbd启动脚本。</p><ul><li>备用节点安装DRBD</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@node1 ~]<span class="comment"># yum -y install kmod-drbd83 drbd83</span></span><br></pre></td></tr></table></figure><blockquote><p>安装完成后。默认配置文件/etc/drbd.conf，以下是两台的主机配置实例:</p></blockquote><a id="more"></a><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@node2 ~]<span class="comment"># cat /etc/drbd.conf</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># please have a a look at the example configuration file in</span></span><br><span class="line"><span class="comment"># /usr/share/doc/drbd83/drbd.conf</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># drbd.conf</span></span><br><span class="line"><span class="comment"># create by dba.gao@gmail.com at 2010-10-11</span></span><br><span class="line">global &#123;</span><br><span class="line">    <span class="comment"># minor-count 64;</span></span><br><span class="line">    <span class="comment"># dialog-refresh 5; # 5 seconds</span></span><br><span class="line">    <span class="comment"># disable-ip-verification;</span></span><br><span class="line">usage-count no; <span class="comment">#是否参加DRBD使用者统计，默认yes</span></span><br><span class="line">&#125;</span><br><span class="line">common &#123;</span><br><span class="line">    syncer &#123; rate <span class="comment">#设置主备节点同步时的网络速率最大值，单位是字节。</span></span><br><span class="line">        200M; &#125;</span><br><span class="line">&#125;</span><br><span class="line">resource r0 &#123;</span><br><span class="line">protocol C;</span><br><span class="line"><span class="comment"># 使用drbd的第三种同步协议,表示收到远程主机的写入确认后,则认为写入完成</span></span><br><span class="line">handlers &#123;</span><br><span class="line">    pri-on-incon-degr <span class="string">"echo o &gt; /proc/sysrq-trigger ; halt -f"</span>;</span><br><span class="line">    pri-lost-after-sb <span class="string">"echo o &gt; /proc/sysrq-trigger ; halt -f"</span>;</span><br><span class="line">    <span class="built_in">local</span>-io-error <span class="string">"echo o &gt; /proc/sysrq-trigger ; halt -f"</span>;</span><br><span class="line">fence-peer <span class="string">"/usr/lib64/heartbeat/drbd-peer-outdater -t 5"</span>;</span><br><span class="line">pri-lost <span class="string">"echo pri-lost. Have a look at the log files. | mail -s 'DRBD Alert' root"</span>;</span><br><span class="line">    split-brain <span class="string">"/usr/lib/drbd/notify-split-brain.sh root"</span>;</span><br><span class="line">    out-of-sync <span class="string">"/usr/lib/drbd/notify-out-of-sync.sh root"</span>;</span><br><span class="line">  &#125;</span><br><span class="line">net &#123;</span><br><span class="line"><span class="comment"># timeout 60;</span></span><br><span class="line"><span class="comment"># connect-int 10;</span></span><br><span class="line"><span class="comment"># ping-int 10;</span></span><br><span class="line"><span class="comment"># max-buffers 2048;</span></span><br><span class="line"><span class="comment"># max-epoch-size 2048;</span></span><br><span class="line">cram-hmac-alg <span class="string">"sha1"</span>;</span><br><span class="line">shared-secret <span class="string">"MySQL-HA"</span>;</span><br><span class="line"><span class="comment"># DRBD同步时使用的验证方式和密码信息。</span></span><br><span class="line">&#125;</span><br><span class="line">disk &#123;</span><br><span class="line">    on-io-error detach;</span><br><span class="line">fencing resource-only;</span><br><span class="line">&#125;</span><br><span class="line">startup &#123;</span><br><span class="line">    wfc-timeout 120;</span><br><span class="line">    degr-wfc-timeout 120;</span><br><span class="line">  &#125;</span><br><span class="line">  device        /dev/drbd0;</span><br><span class="line"><span class="comment">#这里配置档我们挂在的系统的磁盘标示驱动盘符; on node2 &#123;</span></span><br><span class="line"><span class="comment">#每个主机的说明以on开头,后面是hostname(uname - n)，在后面的&#123;&#125;中为这个主机的配置。</span></span><br><span class="line">disk /dev/sdb5;</span><br><span class="line"><span class="comment">#/dev/drbd0使用的磁盘分区是/dev/sdb5</span></span><br><span class="line">address     192.168.1.101:7788;</span><br><span class="line">IP地址以及DRBD使用的端口 meta-disk internal;</span><br><span class="line">&#125;</span><br><span class="line">on node1 &#123;</span><br><span class="line">disk /dev/sdb5;</span><br><span class="line">address 192.168.1.102:7788; 和上述一样</span><br><span class="line">meta-disk internal; <span class="comment">#drbd的元数据存放方式 &#125;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;配置完成后启动节点，在启动DRBD之前,你需要分别在两台主机的hd分区上,创建供DRBD记录信息的数据块.分别在两台主机上执行(这里注意:在创建分区之前我们需要吧磁盘的分区分好)</p><p><img src alt="img"></p><p>分区分好以后先不要挂在和格式化(挂在以后创建会报错)，然后创建供DRBD记 录信息的数据块</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@node2 ~]<span class="comment"># drbdadm create-md r0</span></span><br><span class="line">[root@node1 ~]<span class="comment"># drbdadm create-md r0</span></span><br><span class="line">或者执行drbdadm create-md all</span><br></pre></td></tr></table></figure><ul><li><p>在两个节点启动服务</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@node2 ~]<span class="comment"># /etc/init.d/drbd start</span></span><br><span class="line">[root@node1 ~]<span class="comment"># /etc/init.d/drbd start</span></span><br></pre></td></tr></table></figure></li><li><p>在任意节点查看节点状态</p></li></ul><blockquote><p>1: cs:Connected ro:Secondary/Secondary ds:Inconsistent/Inconsistent C r—-<br>ns:0 nr:0 dw:0 dr:0 al:0 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:b oos:2007644</p></blockquote><blockquote><p>对输出的含义解释如下:<br>ro表示角色信息，第一次启动drbd时，两个drbd节点默认都处于Secondary状态,<br>ds是磁盘状态信息，“Inconsistent/Inconsisten”，即为“不一致/不一致” 状态，表示两个节点的磁盘数据处于不一致状态。<br>Ns表示网络发送的数据包信息。</p></blockquote><h3 id="这里我设置是node2"><a href="#这里我设置是node2" class="headerlink" title="这里我设置是node2"></a>这里我设置是node2</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@node2 ~]<span class="comment"># drbdsetup /dev/drbd1 primary –o 或者执行下面命令也是可以的</span></span><br><span class="line">[root@node2 ~]<span class="comment">#drbdadm -- --overwrite-data-of-peer primary all</span></span><br></pre></td></tr></table></figure><p>第一次执行完此命令后，在后面如果需要设置哪个是主节点时，就可以使用另 外一个命令:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@node2 ~]<span class="comment">#/sbin/drbdadm primary r0或者/sbin/drbdadm primary all</span></span><br></pre></td></tr></table></figure><blockquote><p>执行此命令后，开始同步两台机器对应磁盘的数据</p></blockquote><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@ node2 ~]<span class="comment">#cat /proc/drbd</span></span><br><span class="line">1: cs:SyncSource ro:Primary/Secondary ds:UpToDate/Inconsistent C r--- -</span><br><span class="line">ns:576224 nr:0 dw:0 dr:581760 al:0 bm:34 lo:84 pe:369 ua:256 ap:0 ep:1 wo:b oos:1443196</span><br><span class="line">[====&gt;...............] sync<span class="string">'ed: 28.4% (1443196/2007644)K delay_probe: 69</span></span><br><span class="line"><span class="string">finish: 0:03:56 speed: 6,024 (5,876) K/sec</span></span><br></pre></td></tr></table></figure><blockquote><p>最后格式化文件系统,由于mount操作只能在主节点进行，所以只有设置了主节点后才能格式化磁盘分 区，然后挂载:</p></blockquote><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@node2 ~]<span class="comment"># mkfs -t ext3 /dev/drbd0</span></span><br><span class="line">[root@node2 ~]<span class="comment"># mount /dev/drbd0 /data/</span></span><br><span class="line">[root@node2 ~]<span class="comment"># df -h</span></span><br><span class="line">Filesystem</span><br><span class="line">/dev/sda3</span><br><span class="line">/dev/sda1</span><br><span class="line">tmpfs</span><br><span class="line">/dev/drbd0</span><br><span class="line">Size  Used Avail Use% Mounted on</span><br><span class="line"> 28G  3.7G   23G  15% /</span><br><span class="line">487M   22M  440M   5% /boot</span><br><span class="line">125M     0  125M   0% /dev/shm</span><br><span class="line">9.9G  151M  9.2G   2% /data</span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;Dw是磁盘写信息;Dr是磁盘读信息;启动DRBD后设置主次节点，选择需要设置主机的主节点，然后执行如下命令: 这里我设置是node2</p><!-- rebuild by neat -->]]></content>
      <categories>
        <category>存储,Centos</category>
      </categories>
      <tags>
        <tag>drdb</tag>
      </tags>
  </entry>
  <entry>
    <title>oracle ORA-12519</title>
    <url>/2019/08/10/oracle-ORA-12519/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><blockquote><p>oracle ORA-12519错误解决<br>今天遇到做系统压力测试的时候，系统报了一个错误<br>OERR: ORA-12519 TNS:no appropriate service handler found</p></blockquote><p><img src="https://img.xxlaila.cn/ORA-12519-error.png" alt="img"></p><p>在网上搜索了一下oralc的错误信息ORA-12519，解决办法挺多的，这里记录一下</p><h3 id="登陆oracle的服务器，在登陆oracle数据库"><a href="#登陆oracle的服务器，在登陆oracle数据库" class="headerlink" title="登陆oracle的服务器，在登陆oracle数据库"></a>登陆oracle的服务器，在登陆oracle数据库</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sqlplus <span class="string">"/as sysdba"</span></span><br></pre></td></tr></table></figure><blockquote><p>首先检查process和session的使用情况</p></blockquote><p><img src="https://img.xxlaila.cn/parameter_%20processes_1.png" alt="img"><br><img src="https://img.xxlaila.cn/parameter_%20session_3.png" alt="img"></p><a id="more"></a><ul><li>这里可以看到process几乎已经满了</li></ul><h3 id="修改oracle的process和session值"><a href="#修改oracle的process和session值" class="headerlink" title="修改oracle的process和session值"></a>修改oracle的process和session值</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">这里我们把这些值修改为1000和1135</span><br><span class="line">SQL&gt; alter system <span class="built_in">set</span> processes=1000 scope=spfile;</span><br><span class="line">系统已更改。</span><br><span class="line">SQL&gt; alter system <span class="built_in">set</span> sessions=1135 scope=spfile;</span><br><span class="line">系统已更改。</span><br></pre></td></tr></table></figure><h3 id="重启数据库后参数修改完成"><a href="#重启数据库后参数修改完成" class="headerlink" title="重启数据库后参数修改完成"></a>重启数据库后参数修改完成</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">SQL&gt; shutdown abort;</span><br><span class="line">ORACLE 例程已经关闭。</span><br><span class="line">SQL&gt; startup;</span><br><span class="line">ORACLE 例程已经启动。</span><br><span class="line">Total System Global Area  534462464 bytes</span><br><span class="line">Fixed Size                  2215064 bytes</span><br><span class="line">Variable Size             234881896 bytes</span><br><span class="line">Database Buffers          289406976 bytes</span><br><span class="line">Redo Buffers                7958528 bytes</span><br><span class="line">数据库装载完毕。</span><br><span class="line">数据库已经打开。</span><br></pre></td></tr></table></figure><h2 id="查看并验证"><a href="#查看并验证" class="headerlink" title="查看并验证"></a>查看并验证</h2><p><img src="https://img.xxlaila.cn/W.png" alt="img"></p><!-- rebuild by neat -->]]></content>
      <categories>
        <category>oracle</category>
      </categories>
      <tags>
        <tag>ORA-12519</tag>
      </tags>
  </entry>
  <entry>
    <title>nginx https</title>
    <url>/2019/08/10/nginx-https/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><p>nginx http 强制跳转到https</p><h2 id="方法一"><a href="#方法一" class="headerlink" title="方法一"></a>方法一</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (<span class="variable">$scheme</span> = http ) &#123;</span><br><span class="line">    <span class="built_in">return</span> 301 https://<span class="variable">$host</span><span class="variable">$request_uri</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>列子</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    listen 80;</span><br><span class="line">    listen 443;</span><br><span class="line">    server_name xxx.test.com;</span><br><span class="line">    index index.html index.php index.htm;</span><br><span class="line">    <span class="keyword">if</span> (<span class="variable">$scheme</span> = http ) &#123;</span><br><span class="line">        <span class="built_in">return</span> 301 https://<span class="variable">$host</span><span class="variable">$request_uri</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="方法二"><a href="#方法二" class="headerlink" title="方法二"></a>方法二</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (<span class="variable">$server_port</span> = 80 ) &#123;</span><br><span class="line">    <span class="built_in">return</span> 301 https://<span class="variable">$host</span><span class="variable">$request_uri</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>列子</li></ul><a id="more"></a><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    listen 80;</span><br><span class="line">    listen 443;</span><br><span class="line">    server_name xxx.test.com;</span><br><span class="line">    index index.html index.php index.htm;</span><br><span class="line">    <span class="keyword">if</span> (<span class="variable">$server_port</span> = 80 ) &#123;</span><br><span class="line">        <span class="built_in">return</span> 301 https://<span class="variable">$host</span><span class="variable">$request_uri</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="方法三"><a href="#方法三" class="headerlink" title="方法三"></a>方法三</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (<span class="variable">$ssl_protocol</span> = <span class="string">""</span>) &#123; <span class="built_in">return</span> 301 https://<span class="variable">$server_name</span><span class="variable">$request_uri</span>; &#125;</span><br><span class="line"><span class="keyword">if</span> (<span class="variable">$host</span> != xxx.test.com) &#123; <span class="built_in">return</span> 301 <span class="variable">$scheme</span>://xxx.test.com<span class="variable">$request_uri</span>; &#125;</span><br></pre></td></tr></table></figure><ul><li>列子</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    listen 80;</span><br><span class="line">    listen 443;</span><br><span class="line">    server_name xxx.test.com;</span><br><span class="line">    index index.html index.php index.htm;</span><br><span class="line">    <span class="keyword">if</span> (<span class="variable">$ssl_protocol</span> = <span class="string">""</span>) &#123; <span class="built_in">return</span> 301 https://<span class="variable">$server_name</span><span class="variable">$request_uri</span>; &#125;</span><br><span class="line">    <span class="keyword">if</span> (<span class="variable">$host</span> != xxx.test.com) &#123; <span class="built_in">return</span> 301 <span class="variable">$scheme</span>://xxx.test.com<span class="variable">$request_uri</span>; &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="方法四"><a href="#方法四" class="headerlink" title="方法四"></a>方法四</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    listen 80;</span><br><span class="line">    server_name test.com www.test.com;</span><br><span class="line">    rewrite ^(.*) https://www.test.com<span class="variable">$1</span> permanent;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>例子</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    listen 80;</span><br><span class="line">    server_name test.com www.test.com;</span><br><span class="line">    rewrite ^(.*) https://www.test.com<span class="variable">$1</span> permanent;</span><br><span class="line">&#125;</span><br><span class="line">server &#123;</span><br><span class="line">    listen 443;</span><br><span class="line">    server_name test.com www.test.com;</span><br><span class="line">    index index.html index.htm index.php;</span><br><span class="line">    root ;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="方法五"><a href="#方法五" class="headerlink" title="方法五"></a>方法五</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    listen 80;</span><br><span class="line">    server_name test.com www.test.com;</span><br><span class="line">    <span class="built_in">return</span> 301 https://<span class="variable">$server_name</span><span class="variable">$request_uri</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      <categories>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>https</tag>
      </tags>
  </entry>
  <entry>
    <title>haproxy keepalived </title>
    <url>/2019/08/10/haproxy-keepalived/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><p>本文主要是代理kubernetes master的高可用。</p><h2 id="安装haproxy和keepalived"><a href="#安装haproxy和keepalived" class="headerlink" title="安装haproxy和keepalived"></a>安装haproxy和keepalived</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># yum -y install keepalived.x86_64</span></span><br><span class="line"><span class="comment"># yum -y install haproxy18u.x86_64</span></span><br></pre></td></tr></table></figure><h2 id="2、配置haproxy"><a href="#2、配置haproxy" class="headerlink" title="2、配置haproxy"></a>2、配置haproxy</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat /etc/haproxy/haproxy.cfg</span></span><br><span class="line">global</span><br><span class="line">  <span class="built_in">log</span> 127.0.0.1 local0 err</span><br><span class="line">  maxconn 50000</span><br><span class="line">  uid 99</span><br><span class="line">  gid 99</span><br><span class="line">  <span class="comment">#daemon</span></span><br><span class="line">  nbproc 1</span><br><span class="line">  pidfile haproxy.pid</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">  mode http</span><br><span class="line">  <span class="built_in">log</span> 127.0.0.1 local0 err</span><br><span class="line">  maxconn 50000</span><br><span class="line">  retries 3</span><br><span class="line">  timeout connect 5s</span><br><span class="line">  timeout client 30s</span><br><span class="line">  timeout server 30s</span><br><span class="line">  timeout check 2s</span><br><span class="line"></span><br><span class="line">listen admin_stats</span><br><span class="line">  mode http</span><br><span class="line">  <span class="built_in">bind</span> 0.0.0.0:1080</span><br><span class="line">  <span class="built_in">log</span> 127.0.0.1 local0 err</span><br><span class="line">  stats refresh 30s</span><br><span class="line">  stats uri     /haproxy-status</span><br><span class="line">  stats realm   Haproxy\ Statistics</span><br><span class="line">  stats auth    admin:admin1</span><br><span class="line">  stats hide-version</span><br><span class="line">  stats admin <span class="keyword">if</span> TRUE</span><br><span class="line"></span><br><span class="line">frontend k8s-https</span><br><span class="line">  <span class="built_in">bind</span> 0.0.0.0:8443</span><br><span class="line">  mode tcp</span><br><span class="line">  default_backend k8s-http</span><br><span class="line"></span><br><span class="line">backend k8s-http</span><br><span class="line">  mode tcp</span><br><span class="line">  balance roundrobin</span><br><span class="line">  server k8s-master-01 172.21.17.4:6443</span><br><span class="line">  server k8s-master-02 172.21.16.230:6443</span><br><span class="line">  server k8s-master-03 172.21.240:6443</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="3、keepalived配置"><a href="#3、keepalived配置" class="headerlink" title="3、keepalived配置"></a>3、keepalived配置</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat keepalived.conf </span></span><br><span class="line">! Configuration File <span class="keyword">for</span> keepalived</span><br><span class="line">global_defs &#123;</span><br><span class="line">  notification_email &#123;</span><br><span class="line">    cq_xxlaila@163.com</span><br><span class="line">    &#125;</span><br><span class="line">  notification_email_from cq_xxlaila@163.com</span><br><span class="line">  smtp_server 127.0.0.1</span><br><span class="line">  smtp_connect_timeout 30</span><br><span class="line">  router_id haproxy-01</span><br><span class="line">&#125;</span><br><span class="line">vrrp_script chk_haproxy &#123;</span><br><span class="line">  script <span class="string">"/etc/keepalived/haproxy_check.sh"</span></span><br><span class="line">  interval 2</span><br><span class="line">  timeout 2</span><br><span class="line">  fall 3</span><br><span class="line">&#125;</span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state MASTER</span><br><span class="line">    interface eth0</span><br><span class="line">    virtual_router_id 51</span><br><span class="line">    priority 99</span><br><span class="line">    advert_int 1</span><br><span class="line">    dont_track_primary</span><br><span class="line">    nopreempt</span><br><span class="line">    authentication &#123;</span><br><span class="line">      auth_type PASS</span><br><span class="line">      auth_pass 57D0BC82E074C9D6</span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">      172.21.16.45</span><br><span class="line">    &#125;</span><br><span class="line">    track_script &#123;</span><br><span class="line">      chk_haproxy</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>监测脚本</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat haproxy_check.sh </span></span><br><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">A=`ps -C haproxy --no-header | wc -l`</span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$A</span> -eq 0 ];<span class="keyword">then</span></span><br><span class="line">    systemctl start haproxy</span><br><span class="line">    sleep 3</span><br><span class="line">    <span class="keyword">if</span> [ `ps -C haproxy --no-header | wc -l ` -eq 0 ];<span class="keyword">then</span></span><br><span class="line">        systemctl stop haproxy</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure><ul><li>启动服务</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># systemctl enable haproxy &amp;&amp;systemctl enable keepalived</span></span><br><span class="line"><span class="comment"># systemctl start keepalived &amp;&amp;systemctl start haproxy</span></span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      <categories>
        <category>haproxy,keepalived,kubernetes</category>
      </categories>
      <tags>
        <tag>haproxy,keepalived</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s-prometheus</title>
    <url>/2019/08/10/k8s-prometheus/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><p><a href="https://prometheus.io/" target="_blank" rel="noopener">Prometheus</a>是一个集数据收集存储、数据查询和数据图表显示于一身的开源监控组件。本文主要讲解如何搭建Prometheus，并使用它监控Kubernetes集群。</p><h2 id="1、下载相关yaml"><a href="#1、下载相关yaml" class="headerlink" title="1、下载相关yaml"></a>1、下载相关yaml</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># https://github.com/xxlaila/kubernetes-yaml/tree/master/prometheus-grafana</span></span><br><span class="line"><span class="comment"># ls -l</span></span><br><span class="line">configmap.yaml</span><br><span class="line">grafana-deploy.yaml</span><br><span class="line">grafana-ingress.yaml</span><br><span class="line">grafana-svc.yaml</span><br><span class="line">node-exporter.yaml</span><br><span class="line">prometheus-deploy.yaml</span><br><span class="line">prometheus-svc.yaml</span><br><span class="line">rbac-setup.yaml</span><br><span class="line">prometheus-ingress.yaml</span><br></pre></td></tr></table></figure><h2 id="2、开始部署"><a href="#2、开始部署" class="headerlink" title="2、开始部署"></a>2、开始部署</h2><h3 id="2-1、采用daemonset方式部署node-exporter组件"><a href="#2-1、采用daemonset方式部署node-exporter组件" class="headerlink" title="2.1、采用daemonset方式部署node-exporter组件"></a>2.1、采用daemonset方式部署node-exporter组件</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl create -f  node-exporter.yaml</span></span><br></pre></td></tr></table></figure><a id="more"></a><h3 id="2-2、部署prometheus组件"><a href="#2-2、部署prometheus组件" class="headerlink" title="2.2、部署prometheus组件"></a>2.2、部署prometheus组件</h3><h4 id="2-2-1、rbac文件"><a href="#2-2-1、rbac文件" class="headerlink" title="2.2.1、rbac文件"></a>2.2.1、rbac文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl create -f rbac-setup.yaml</span></span><br></pre></td></tr></table></figure><h4 id="2-2-2、以configmap的形式管理prometheus组件的配置文件"><a href="#2-2-2、以configmap的形式管理prometheus组件的配置文件" class="headerlink" title="2.2.2、以configmap的形式管理prometheus组件的配置文件"></a>2.2.2、以configmap的形式管理prometheus组件的配置文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl create -f configmap.yaml</span></span><br></pre></td></tr></table></figure><h4 id="2-2-3、Prometheus-deployment-文件"><a href="#2-2-3、Prometheus-deployment-文件" class="headerlink" title="2.2.3、Prometheus deployment 文件"></a>2.2.3、Prometheus deployment 文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl create -f  prometheus-deploy.yaml</span></span><br></pre></td></tr></table></figure><h4 id="2-2-4、Prometheus-service文件"><a href="#2-2-4、Prometheus-service文件" class="headerlink" title="2.2.4、Prometheus service文件"></a>2.2.4、Prometheus service文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl create -f prometheus-svc.yaml</span></span><br></pre></td></tr></table></figure><h4 id="2-2-5、配置Ingress"><a href="#2-2-5、配置Ingress" class="headerlink" title="2.2.5、配置Ingress"></a>2.2.5、配置Ingress</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl create -f prometheus-ingress.yaml</span></span><br></pre></td></tr></table></figure><h3 id="2-3、部署grafana组件"><a href="#2-3、部署grafana组件" class="headerlink" title="2.3、部署grafana组件"></a>2.3、部署grafana组件</h3><h4 id="2-3-1、grafana-deployment配置文件"><a href="#2-3-1、grafana-deployment配置文件" class="headerlink" title="2.3.1、grafana deployment配置文件"></a>2.3.1、grafana deployment配置文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl create -f grafana-deploy.yaml</span></span><br></pre></td></tr></table></figure><h4 id="2-3-2、grafana-service配置文件"><a href="#2-3-2、grafana-service配置文件" class="headerlink" title="2.3.2、grafana service配置文件"></a>2.3.2、grafana service配置文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl create -f grafana-svc.yaml</span></span><br></pre></td></tr></table></figure><h4 id="2-3-3、grafana-ingress配置文件"><a href="#2-3-3、grafana-ingress配置文件" class="headerlink" title="2.3.3、grafana ingress配置文件"></a>2.3.3、grafana ingress配置文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl create -f grafana-ingress.yaml</span></span><br></pre></td></tr></table></figure><h3 id="2-4、WEB界面配置"><a href="#2-4、WEB界面配置" class="headerlink" title="2.4、WEB界面配置"></a>2.4、WEB界面配置</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get svc,pods -n kube-ops</span></span><br></pre></td></tr></table></figure><p><img src="https://img.xxlaila.cn/46ds9824.png" alt="img"></p><h4 id="2-4-1-查看node-exporter"><a href="#2-4-1-查看node-exporter" class="headerlink" title="2.4.1 查看node-exporter"></a>2.4.1 查看node-exporter</h4><p><img src="https://img.xxlaila.cn/8764kjfnks.png" alt="img"></p><h4 id="2-4-2、查看promethues"><a href="#2-4-2、查看promethues" class="headerlink" title="2.4.2、查看promethues"></a>2.4.2、查看promethues</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;prometheus对应的nodeport端口为30005，通过访问<a href="http://172.21.17.4:30005/targets" target="_blank" rel="noopener">http://172.21.17.4:30005/targets</a> 可以看到prometheus已经成功连接上了k8s的apiserver,这里我们前面增加了prometheus的ingress，这里可以直接通过域名进行访问</p><p><img src="https://img.xxlaila.cn/skd9234342.png" alt="img"></p><h4 id="2-4-3、访问grafana"><a href="#2-4-3、访问grafana" class="headerlink" title="2.4.3、访问grafana"></a>2.4.3、访问grafana</h4><p>通过域名访问grafana，默认用户名密码均为admin，配置数据源<br><img src="https://img.xxlaila.cn/sld023423.png" alt="img"></p><ul><li>到grafana官方<a href="https://grafana.com/dashboards/315" target="_blank" rel="noopener">下载模版</a>，导入json模版</li></ul><p><img src="https://img.xxlaila.cn/kf24skdsfds.png" alt="img"></p><!-- rebuild by neat -->]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>Prometheus</tag>
      </tags>
  </entry>
  <entry>
    <title>kubedns插件配置</title>
    <url>/2019/08/10/kubedns%E6%8F%92%E4%BB%B6%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><h1 id="安装和配置kubedns插件"><a href="#安装和配置kubedns插件" class="headerlink" title="安装和配置kubedns插件"></a>安装和配置kubedns插件</h1><h2 id="1、配置文件准备"><a href="#1、配置文件准备" class="headerlink" title="1、配置文件准备"></a>1、配置文件准备</h2><p>下载官方的yaml文件目录：kubernetes/cluster/addons/dns。该插件直接使用kubernetes部署,yaml文件经过修改完成部署</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># git clone https://github.com/xxlaila/kubernetes-yaml.git</span></span><br><span class="line"><span class="comment"># cd kubernetes-yaml/coredns</span></span><br><span class="line"><span class="comment"># sed -i 's/10.96.0.10/10.254.0.2/g' coredns-service.yaml</span></span><br><span class="line"><span class="comment"># kubectl create -f ./</span></span><br><span class="line"><span class="comment"># kubectl get pods,svc,rs -n kube-system</span></span><br><span class="line">NAME                           READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod/coredns-68676b6b88-l7b5g   1/1     Running   0          16m</span><br><span class="line"></span><br><span class="line">NAME              TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)         AGE</span><br><span class="line">service/coredns   ClusterIP   10.254.0.2   &lt;none&gt;        53/UDP,53/TCP   16m</span><br><span class="line"></span><br><span class="line">NAME                                       DESIRED   CURRENT   READY   AGE</span><br><span class="line">replicaset.extensions/coredns-68676b6b88   1         1         1       16m</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl get pods -o wide -n kube-system</span></span><br><span class="line">NAME                                    READY   STATUS    RESTARTS   AGE     IP            NODE            NOMINATED NODE   READINESS GATES</span><br><span class="line">coredns-68676b6b88-l7b5g                1/1     Running   0          40m     10.254.28.2   172.21.16.248   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><h2 id="2、安装和配置dashboard"><a href="#2、安装和配置dashboard" class="headerlink" title="2、安装和配置dashboard"></a>2、安装和配置dashboard</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;官方配置文件kubernetes/cluster/addons/dashboard，这里已经修改过了，经过测试部署，直接进入dashboard目录，修改inages参数进行部署</p><a id="more"></a><h3 id="2-1、安装dashboard"><a href="#2-1、安装dashboard" class="headerlink" title="2.1、安装dashboard"></a>2.1、安装dashboard</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cd ../kubernetes-dashboard</span></span><br><span class="line"><span class="comment"># kubectl create -f kubernetes-dashboard.yaml </span></span><br><span class="line">secret/kubernetes-dashboard-certs created</span><br><span class="line">serviceaccount/kubernetes-dashboard created</span><br><span class="line">role.rbac.authorization.k8s.io/kubernetes-dashboard-minimal created</span><br><span class="line">rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard-minimal created</span><br><span class="line">deployment.apps/kubernetes-dashboard created</span><br><span class="line">service/kubernetes-dashboard created</span><br><span class="line"><span class="comment"># kubectl get pods -o wide -n kube-system</span></span><br><span class="line">NAME                                    READY   STATUS    RESTARTS   AGE     IP            NODE            NOMINATED NODE   READINESS GATES</span><br><span class="line">kubernetes-dashboard-6c655d9445-4557x   1/1     Running   0          6m54s   10.254.90.2   172.21.16.110   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><h3 id="2-2、配置账户授权"><a href="#2-2、配置账户授权" class="headerlink" title="2.2、配置账户授权"></a>2.2、配置账户授权</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl create -f admin-user.yaml </span></span><br><span class="line">serviceaccount/admin created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/admin created</span><br><span class="line"><span class="comment"># kubectl describe serviceaccount admin -n kube-system</span></span><br><span class="line">Name:                admin</span><br><span class="line">Namespace:           kube-system</span><br><span class="line">Labels:              k8s-app=kubernetes-dashboard</span><br><span class="line">Annotations:         &lt;none&gt;</span><br><span class="line">Image pull secrets:  &lt;none&gt;</span><br><span class="line">Mountable secrets:   admin-token-wwjw8</span><br><span class="line">Tokens:              admin-token-wwjw8</span><br><span class="line">Events:              &lt;none&gt;</span><br><span class="line"><span class="comment"># kubectl describe secret admin-token-wwjw8 -n kube-system</span></span><br><span class="line">在浏览器访问任意节点IP地址http://&lt;node_ip&gt;:30001</span><br></pre></td></tr></table></figure><h2 id="3、监控安装"><a href="#3、监控安装" class="headerlink" title="3、监控安装"></a>3、监控安装</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cd ../heapster-influxdb-grafana</span></span><br><span class="line"><span class="comment"># kubectl create -f ./</span></span><br><span class="line"><span class="comment"># kubectl get pods -o wide -n kube-system</span></span><br><span class="line">NAME                                    READY   STATUS    RESTARTS   AGE   IP            NODE            NOMINATED NODE   READINESS GATES</span><br><span class="line">heapster-658646db69-lh5tx               1/1     Running   0          11m   10.254.28.3   172.21.16.248   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">monitoring-grafana-7bfc56ffcd-kgh56     1/1     Running   0          11m   10.254.90.3   172.21.16.110   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">monitoring-influxdb-7478d7675c-9255v    1/1     Running   0          11m   10.254.85.2   172.21.16.244   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;这里遇到一个怪事情；heapster安装以后图始终无法出来，这里折腾差不多大半天。最后在dashboard的yaml文件里面添加了以下参数，图就可以了，</p><p><img src="https://img.xxlaila.cn/4udfs93.png" alt="img"></p><p>args:<br>- –auto-generate-certificates<br>- –token-ttl=43200<br><em>- –heapster-host=<a href="http://heapster" target="_blank" rel="noopener">http://heapster</a></em></p><p><img src="https://img.xxlaila.cn/ds832948dk.png" alt="img"></p><p>Prometheus的安装请参考<a href="http://xxlaila.github.io/2019/08/10/k8s-prometheus/" target="_blank" rel="noopener">《Prometheus 入门》</a>文章，grafana不需要重复部署。只需要在grafana里面增加目录挂在，吧kube-ops 修改kube-system即可</p><!-- rebuild by neat -->]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kube-dns,dashboard</tag>
      </tags>
  </entry>
  <entry>
    <title>kubernetes node节点安装</title>
    <url>/2019/08/10/kubernetes-node%E8%8A%82%E7%82%B9%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:23 GMT+0800 (China Standard Time) --><p>More: <a href="https://xxlaila.github.io/2019/08/09/kubernetes-v1-13-3%E5%AE%89%E8%A3%85/" target="_blank" rel="noopener">master节点安装请参考</a></p><h2 id="1、部署kubernetes-node节点"><a href="#1、部署kubernetes-node节点" class="headerlink" title="1、部署kubernetes node节点"></a>1、部署kubernetes node节点</h2><p>Kubernetes node节点包含如下组件：</p><ul><li><strong>Flanneld</strong>: 之前单机节点安装没有配置TLS，现在需要在service配置文件中增加TLS配置</li><li><strong>Docker</strong>: version 18.06.2-ce</li><li><strong>kubelet</strong></li><li><strong>kube-proxy</strong></li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ls /etc/kubernetes/</span></span><br><span class="line">bootstrap.kubeconfig  kubelet   kube-proxy.kubeconfig  proxy  ssl</span><br><span class="line"><span class="comment"># ls /etc/kubernetes/ssl</span></span><br><span class="line">admin-key.pem  kube-apiserver-key.pem  kube-controller-manager-key.pem  kubelet-api-admin-key.pem   kube-proxy-key.pem  kubernetes-ca-key.pem  kube-scheduler-key.pem</span><br><span class="line">admin.pem      kube-apiserver.pem      kube-controller-manager.pem      kubelet-api-admin.pem       kube-proxy.pem      kubernetes-ca.pem      kube-scheduler.pem</span><br></pre></td></tr></table></figure><h3 id="增加docker-源"><a href="#增加docker-源" class="headerlink" title="增加docker 源"></a>增加docker 源</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># yum-config-manager \</span></span><br><span class="line">  --add-repo \</span><br><span class="line">  https://download.docker.com/linux/centos/docker-ce.repo</span><br></pre></td></tr></table></figure><ul><li><p>根据实际查找当前版本 (可选)</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># yum list docker-ce --showduplicates | sort -r</span></span><br></pre></td></tr></table></figure></li><li><p>如果确定了版本,直接安装,如果要装17。03直接修改下面数字即可</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># yum -y install docker-ce-18.06.2.ce-3.el7  # 主意版本填写包名的格式.</span></span><br></pre></td></tr></table></figure></li><li><p>启docker服务,和开机启动</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># systemctl start docker &amp;&amp; systemctl enable docker</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="1-1、安装flanneld"><a href="#1-1、安装flanneld" class="headerlink" title="1.1、安装flanneld"></a>1.1、安装flanneld</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># mv kubernetes  /etc/ &amp;&amp; chown -R root: /etc/kubernetes</span></span><br><span class="line"><span class="comment"># wget https://github.com/coreos/flannel/releases/download/v0.11.0/flannel-v0.11.0-linux-amd64.tar.gz</span></span><br><span class="line"><span class="comment"># tar zxf flannel-v0.11.0-linux-amd64.tar.gz &amp;&amp; mv flanneld mk-docker-opts.sh /usr/bin/ &amp;&amp; rm -rf flannel-v0.11.0-linux-amd64.tar.gz</span></span><br></pre></td></tr></table></figure><h4 id="1-1-1、flanneld启动配置文件"><a href="#1-1-1、flanneld启动配置文件" class="headerlink" title="1.1.1、flanneld启动配置文件"></a>1.1.1、flanneld启动配置文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat /lib/systemd/system/flanneld.service</span></span><br><span class="line">[Unit]</span><br><span class="line">Description=Flanneld overlay address etcd agent</span><br><span class="line">After=network.target</span><br><span class="line">After=network-online.target</span><br><span class="line">Wants=network-online.target</span><br><span class="line">After=etcd.service</span><br><span class="line">Before=docker.service</span><br><span class="line">[Service]</span><br><span class="line">Type=notify</span><br><span class="line">EnvironmentFile=/etc/sysconfig/flanneld</span><br><span class="line">ExecStart=/usr/bin/flanneld -etcd-endpoints=<span class="variable">$&#123;FLANNEL_ETCD&#125;</span> <span class="variable">$FLANNEL_OPTIONS</span></span><br><span class="line">ExecStartPost=/usr/bin/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/subnet.env</span><br><span class="line">Restart=on-failure</span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">RequiredBy=docker.service</span><br></pre></td></tr></table></figure><a id="more"></a><h4 id="1-1-2、flanneld配置文件"><a href="#1-1-2、flanneld配置文件" class="headerlink" title="1.1.2、flanneld配置文件"></a>1.1.2、flanneld配置文件</h4><p>flanneld 配置文件连接了etcd，而在配置etcd的时候需要证书，所以记的吧证书copy到node节点上去</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat /etc/sysconfig/flanneld</span></span><br><span class="line"><span class="comment"># Flanneld configuration options</span></span><br><span class="line"><span class="comment"># etcd url location.  Point this to the server where etcd runs</span></span><br><span class="line">FLANNEL_ETCD=<span class="string">"https://172.21.17.4:2379,https://172.21.16.230:2379,https://172.21.16.240:2379"</span></span><br><span class="line"><span class="comment"># etcd config key.  This is the configuration key that flannel queries</span></span><br><span class="line"><span class="comment"># For address range assignment</span></span><br><span class="line">FLANNEL_ETCD_PREFIX=<span class="string">"/coreos.com/network"</span></span><br><span class="line"><span class="comment"># Any additional options that you want to pass</span></span><br><span class="line">FLANNEL_OPTIONS=<span class="string">"-etcd-cafile=/etc/etcd/ssl/etcd-ca.pem -etcd-certfile=/etc/etcd/ssl/etcd.pem -etcd-keyfile=/etc/etcd/ssl/etcd-key.pem"</span></span><br></pre></td></tr></table></figure><ul><li>在启动flanneld之前，需要在etcd中添加一条网络配置记录，这个配置将用于flanneld分配给每个docker的虚拟ip地址段,</li><li>在任意一台master执行<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># etcdctl set /coreos.com/network/config '&#123; "Network": "10.254.0.0/16" &#125;'</span></span><br><span class="line">&#123; <span class="string">"Network"</span>: <span class="string">"10.254.0.0/16"</span> &#125;</span><br><span class="line"><span class="comment"># etcdctl get  /coreos.com/network/config </span></span><br><span class="line">&#123; <span class="string">"Network"</span>: <span class="string">"10.254.0.0/16"</span> &#125;</span><br></pre></td></tr></table></figure></li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;在执行的时候我们需要创建一个配置文件，因为前面etcd是启用了https的，否则的话，会报<code>Error: client: etcd cluster is unavailable or misconfigured; error #0: x509: certificate signed by unknown authority</code>的错误。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># etcd.rc</span></span><br><span class="line"><span class="built_in">export</span> ETCDCTL_ENDPOINT=https://172.21.17.4:2379,https://172.21.16.230:2379,https://172.21.16.240:2379</span><br><span class="line"><span class="built_in">export</span> ETCDCTL_CERT_FILE=/etc/etcd/ssl/etcd.pem</span><br><span class="line"><span class="built_in">export</span> ETCDCTL_KEY_FILE=/etc/etcd/ssl/etcd-key.pem</span><br><span class="line"><span class="built_in">export</span> ETCDCTL_CA_FILE=/etc/etcd/ssl/etcd-ca.pem</span><br></pre></td></tr></table></figure><h3 id="1-1-3、启动flanneld"><a href="#1-1-3、启动flanneld" class="headerlink" title="1.1.3、启动flanneld"></a>1.1.3、启动flanneld</h3><p>在启动flanneld之前，我们需要修改docker的配置文件</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat /lib/systemd/system/docker.service</span></span><br><span class="line">[Unit]</span><br><span class="line">Description=Docker Application Container Engine</span><br><span class="line">Documentation=https://docs.docker.com</span><br><span class="line">After=network-online.target firewalld.service</span><br><span class="line">Wants=network-online.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=notify</span><br><span class="line">EnvironmentFile=/run/flannel/subnet.env</span><br><span class="line">ExecStart=/usr/bin/dockerd <span class="variable">$DOCKER_NETWORK_OPTIONS</span></span><br><span class="line">ExecReload=/bin/<span class="built_in">kill</span> -s HUP <span class="variable">$MAINPID</span></span><br><span class="line">LimitNOFILE=infinity</span><br><span class="line">LimitNPROC=infinity</span><br><span class="line">LimitCORE=infinity</span><br><span class="line">TimeoutStartSec=0</span><br><span class="line">Delegate=yes</span><br><span class="line">KillMode=process</span><br><span class="line">Restart=on-failure</span><br><span class="line">StartLimitBurst=3</span><br><span class="line">StartLimitInterval=60s</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line"><span class="comment"># systemctl daemon-reload</span></span><br><span class="line"><span class="comment"># systemctl enable flanneld &amp;&amp;systemctl start flanneld &amp;&amp;systemctl status flanneld</span></span><br></pre></td></tr></table></figure><p>重启了docker和flanneld以后，我们在任意一台node节点上通过ip add s可以查看。flanneld 和docker 网络绑定的情况</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ip add s</span></span><br></pre></td></tr></table></figure><h2 id="2、安装和配置kubelet"><a href="#2、安装和配置kubelet" class="headerlink" title="2、安装和配置kubelet"></a>2、安装和配置kubelet</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;kubelet启动时向kube-apiserver发送tls bootstrapping请求，需要将bootstrap token文件中kube-bootsrap用户授予system:node-bootstrapper cluster角色（role），然后kubelet才能有权限创建认证请求（certificate signing requests）</p><h3 id="2-1、安装kubelet"><a href="#2-1、安装kubelet" class="headerlink" title="2.1、安装kubelet"></a>2.1、安装kubelet</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># wget https://dl.k8s.io/v1.13.3/kubernetes-server-linux-amd64.tar.gz</span></span><br><span class="line"><span class="comment"># tar -xzf kubernetes-server-linux-amd64.tar.gz &amp;&amp;cp -r ./kubernetes/server/bin/&#123;kube-proxy,kubelet&#125; /usr/bin/ &amp;&amp; rm -rf ./kubernetes*</span></span><br></pre></td></tr></table></figure><h3 id="2-2、创建kubelet启动文件"><a href="#2-2、创建kubelet启动文件" class="headerlink" title="2.2、创建kubelet启动文件"></a>2.2、创建kubelet启动文件</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat /usr/lib/systemd/system/kubelet.service</span></span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes Kubelet Server</span><br><span class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</span><br><span class="line">After=docker.service</span><br><span class="line">Requires=docker.service</span><br><span class="line">[Service]</span><br><span class="line">WorkingDirectory=/var/lib/kubelet</span><br><span class="line">EnvironmentFile=-/etc/kubernetes/config</span><br><span class="line">EnvironmentFile=-/etc/kubernetes/kubelet</span><br><span class="line">ExecStart=/usr/bin/kubelet \</span><br><span class="line">           <span class="variable">$KUBE_LOGTOSTDERR</span> \</span><br><span class="line">           <span class="variable">$KUBE_LOG_LEVEL</span> \</span><br><span class="line">           <span class="variable">$KUBELET_API_SERVER</span> \</span><br><span class="line">           <span class="variable">$KUBELET_ADDRESS</span> \</span><br><span class="line">           <span class="variable">$KUBELET_PORT</span> \</span><br><span class="line">           <span class="variable">$KUBELET_HOSTNAME</span> \</span><br><span class="line">           <span class="variable">$KUBE_ALLOW_PRIV</span> \</span><br><span class="line">           <span class="variable">$KUBELET_ARGS</span></span><br><span class="line">Restart=on-failure</span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><h3 id="2-3、kubelet配置文件"><a href="#2-3、kubelet配置文件" class="headerlink" title="2.3、kubelet配置文件"></a>2.3、kubelet配置文件</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat /etc/kubernetes/kubelet</span></span><br><span class="line"><span class="comment">###</span></span><br><span class="line"><span class="comment"># kubernetes kubelet (minion) config</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The address for the info server to serve on (set to 0.0.0.0 or "" for all interfaces)</span></span><br><span class="line">KUBELET_ADDRESS=<span class="string">"--node-ip=&#123;node_ip&#125;"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The port for the info server to serve on</span></span><br><span class="line"><span class="comment"># KUBELET_PORT="--port=10250"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># You may leave this blank to use the actual hostname</span></span><br><span class="line">KUBELET_HOSTNAME=<span class="string">"--hostname-override=&#123;node_ip&#125;"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># location of the api-server</span></span><br><span class="line"><span class="comment"># KUBELET_API_SERVER=""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Add your own!</span></span><br><span class="line">KUBELET_ARGS=<span class="string">"  --address=0.0.0.0 \</span></span><br><span class="line"><span class="string">                --allow-privileged \</span></span><br><span class="line"><span class="string">                --anonymous-auth=false \</span></span><br><span class="line"><span class="string">                --authorization-mode=Webhook \</span></span><br><span class="line"><span class="string">                --bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \</span></span><br><span class="line"><span class="string">                --client-ca-file=/etc/kubernetes/ssl/kubernetes-ca.pem \</span></span><br><span class="line"><span class="string">                --cgroup-driver=cgroupfs \</span></span><br><span class="line"><span class="string">                --cert-dir=/etc/kubernetes/ssl \</span></span><br><span class="line"><span class="string">                --cluster-dns=10.254.0.2 \</span></span><br><span class="line"><span class="string">                --cluster-domain=cluster.local \</span></span><br><span class="line"><span class="string">                --eviction-soft=imagefs.available&lt;15%,memory.available&lt;512Mi,nodefs.available&lt;15%,nodefs.inodesFree&lt;10% \</span></span><br><span class="line"><span class="string">                --eviction-soft-grace-period=imagefs.available=3m,memory.available=1m,nodefs.available=3m,nodefs.inodesFree=1m \</span></span><br><span class="line"><span class="string">                --eviction-hard=imagefs.available&lt;10%,memory.available&lt;256Mi,nodefs.available&lt;10%,nodefs.inodesFree&lt;5% \</span></span><br><span class="line"><span class="string">                --eviction-max-pod-grace-period=30 \</span></span><br><span class="line"><span class="string">                --image-gc-high-threshold=80 \</span></span><br><span class="line"><span class="string">                --image-gc-low-threshold=70 \</span></span><br><span class="line"><span class="string">                --image-pull-progress-deadline=30s \</span></span><br><span class="line"><span class="string">                --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \</span></span><br><span class="line"><span class="string">                --max-pods=100 \</span></span><br><span class="line"><span class="string">                --minimum-image-ttl-duration=720h0m0s \</span></span><br><span class="line"><span class="string">                --node-labels=node.kubernetes.io/k8s-node=true \</span></span><br><span class="line"><span class="string">                --pod-infra-container-image=docker.io/kubernetes/pause:latest \</span></span><br><span class="line"><span class="string">                --port=10250 \</span></span><br><span class="line"><span class="string">                --read-only-port=0 \</span></span><br><span class="line"><span class="string">                --rotate-certificates \</span></span><br><span class="line"><span class="string">                --rotate-server-certificates \</span></span><br><span class="line"><span class="string">                --fail-swap-on=false \</span></span><br><span class="line"><span class="string">                --v=2"</span></span><br></pre></td></tr></table></figure><h3 id="2-4、启动kubelet"><a href="#2-4、启动kubelet" class="headerlink" title="2.4、启动kubelet"></a>2.4、启动kubelet</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># mkdir /var/lib/kubelet -p</span></span><br><span class="line"><span class="comment"># systemctl daemon-reload</span></span><br><span class="line"><span class="comment"># systemctl enable kubelet &amp;&amp;systemctl start kubelet &amp;&amp; systemctl status kubelet</span></span><br><span class="line"><span class="comment"># journalctl -fxeu kubelet</span></span><br></pre></td></tr></table></figure><h2 id="3、通过kubelet的tls请求"><a href="#3、通过kubelet的tls请求" class="headerlink" title="3、通过kubelet的tls请求"></a>3、通过kubelet的tls请求</h2><p>kubelet首次启动时像kube-apiserver发送证书签名请求，必须通过后kubernetes系统才会将该node加入集群：</p><h3 id="3-1、查看未授权csr请求"><a href="#3-1、查看未授权csr请求" class="headerlink" title="3.1、查看未授权csr请求"></a>3.1、查看未授权csr请求</h3><ul><li>任意master节点均可</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get csr</span></span><br><span class="line">NAME                                                   AGE   REQUESTOR                   CONDITION</span><br><span class="line">csr-kxfql                                              78m   system:node:172.21.16.204   Pending</span><br><span class="line">node-csr-QptfMgAu2y4GmUZX1Ph9B0XomA0Rg-fxcgs0Yzd-XRU   79m   system:bootstrap:ff90fd     Approved,Issued</span><br><span class="line"><span class="comment"># kubectl get nodes</span></span><br><span class="line">No resources found.</span><br></pre></td></tr></table></figure><h3 id="3-2、通过csr请求"><a href="#3-2、通过csr请求" class="headerlink" title="3.2、通过csr请求"></a>3.2、通过csr请求</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl certificate approve csr-kxfql</span></span><br><span class="line">certificatesigningrequest.certificates.k8s.io/csr-kxfql approved</span><br></pre></td></tr></table></figure><ul><li>自动生成kubelet kubeconfig文件和公私钥,新版本 kubelet server 的证书自动签发已经被关闭,所以对于 kubelet server 的证书仍需要手动签署</li></ul><h2 id="4、配置kube-proxy"><a href="#4、配置kube-proxy" class="headerlink" title="4、配置kube-proxy"></a>4、配置kube-proxy</h2><h3 id="4-1、kupe-proxy-启动文件"><a href="#4-1、kupe-proxy-启动文件" class="headerlink" title="4.1、kupe-proxy 启动文件"></a>4.1、kupe-proxy 启动文件</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat /usr/lib/systemd/system/kube-proxy.service</span></span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes Kube-Proxy Server</span><br><span class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</span><br><span class="line">After=network.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">EnvironmentFile=-/etc/kubernetes/config</span><br><span class="line">EnvironmentFile=-/etc/kubernetes/proxy</span><br><span class="line">ExecStart=/usr/bin/kube-proxy \</span><br><span class="line">       <span class="variable">$KUBE_LOGTOSTDERR</span> \</span><br><span class="line">       <span class="variable">$KUBE_LOG_LEVEL</span> \</span><br><span class="line">       <span class="variable">$KUBE_MASTER</span> \</span><br><span class="line">       <span class="variable">$KUBE_PROXY_ARGS</span></span><br><span class="line">Restart=on-failure</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><h3 id="4-2、kube-proxy配置文件"><a href="#4-2、kube-proxy配置文件" class="headerlink" title="4.2、kube-proxy配置文件"></a>4.2、kube-proxy配置文件</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat /etc/kubernetes/proxy</span></span><br><span class="line"><span class="comment">###</span></span><br><span class="line"><span class="comment"># kubernetes proxy config</span></span><br><span class="line"><span class="comment"># default config should be adequate</span></span><br><span class="line"><span class="comment"># Add your own!</span></span><br><span class="line">KUBE_PROXY_ARGS=<span class="string">"   --bind-address=0.0.0.0 \</span></span><br><span class="line"><span class="string">                    --cleanup-ipvs=true \</span></span><br><span class="line"><span class="string">                    --cluster-cidr=10.254.0.0/16 \</span></span><br><span class="line"><span class="string">                    --hostname-override=docker4.node \</span></span><br><span class="line"><span class="string">                    --healthz-bind-address=0.0.0.0 \</span></span><br><span class="line"><span class="string">                    --healthz-port=10256 \</span></span><br><span class="line"><span class="string">                    --masquerade-all=true \</span></span><br><span class="line"><span class="string">                    --proxy-mode=ipvs \</span></span><br><span class="line"><span class="string">                    --ipvs-min-sync-period=5s \</span></span><br><span class="line"><span class="string">                    --ipvs-sync-period=5s \</span></span><br><span class="line"><span class="string">                    --ipvs-scheduler=wrr \</span></span><br><span class="line"><span class="string">                    --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \</span></span><br><span class="line"><span class="string">                    --logtostderr=true \</span></span><br><span class="line"><span class="string">                    --v=2"</span></span><br></pre></td></tr></table></figure><h3 id="4-3、启动kube-proxy"><a href="#4-3、启动kube-proxy" class="headerlink" title="4.3、启动kube-proxy"></a>4.3、启动kube-proxy</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># systemctl daemon-reload</span></span><br><span class="line"><span class="comment"># systemctl enable kube-proxy &amp;&amp; systemctl start kube-proxy &amp;&amp; systemctl status kube-proxy</span></span><br></pre></td></tr></table></figure><h3 id="4-4-在kube-proxy和kubelet启动之前"><a href="#4-4-在kube-proxy和kubelet启动之前" class="headerlink" title="4.4 在kube-proxy和kubelet启动之前"></a>4.4 在kube-proxy和kubelet启动之前</h3><p>由于 kubelet 组件是采用 TLS Bootstrap 启动，所以需要预先创建相关配置</p><ul><li>创建用于 tls bootstrap 的 token secret<blockquote><p>master节点操作</p></blockquote><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl create -f bootstrap.secret.yaml</span></span><br></pre></td></tr></table></figure></li></ul><p>为了能让 kubelet 实现自动更新证书，需要配置相关 clusterrolebinding</p><ul><li><p>允许 kubelet tls bootstrap 创建 csr 请求</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl create clusterrolebinding create-csrs-for-bootstrapping \</span><br><span class="line">    --clusterrole=system:node-bootstrapper \</span><br><span class="line">    --group=system:bootstrappers</span><br></pre></td></tr></table></figure></li><li><p>自动批准 system:bootstrappers 组用户 TLS bootstrapping 首次申请证书的 CSR 请求</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl create clusterrolebinding auto-approve-csrs-for-group \</span><br><span class="line">    --clusterrole=system:certificates.k8s.io:certificatesigningrequests:nodeclient \</span><br><span class="line">    --group=system:bootstrappers</span><br></pre></td></tr></table></figure></li><li><p>自动批准 system:nodes 组用户更新 kubelet 自身与 apiserver 通讯证书的 CSR 请求</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl create clusterrolebinding auto-approve-renewals-for-nodes \</span><br><span class="line">    --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeclient \</span><br><span class="line">    --group=system:nodes</span><br></pre></td></tr></table></figure></li><li><p>在 kubelet server 开启 api 认证的情况下，apiserver 反向访问 kubelet 10250 需要此授权(eg: kubectl logs)</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl create clusterrolebinding system:kubelet-api-admin \</span><br><span class="line">    --clusterrole=system:kubelet-api-admin \</span><br><span class="line">    --user=system:kubelet-api-admin</span><br></pre></td></tr></table></figure></li></ul><ul><li><strong>问题</strong>:<br>在启动kubelet的时候，node节点在master节点无法查看，查看kubelet的日志提示如下：</li><li>查看kubelet的日志方式有两种</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># journalctl -f -u kubelet</span></span><br><span class="line"><span class="comment"># systemctl  status kubelet -l</span></span><br></pre></td></tr></table></figure><ul><li>查看nodes<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get nodes</span></span><br><span class="line">NAME            STATUS   ROLES    AGE   VERSION</span><br><span class="line">172.21.16.244   Ready    &lt;none&gt;   12m   v1.13.3</span><br></pre></td></tr></table></figure></li></ul><ul><li><p><strong>验证测试集群</strong><br>创建一个nginx测试集群是否可用</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl run nginx --image=docker.io/nginx:latest --replicas=2 --labels run=nginx</span></span><br><span class="line">kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed <span class="keyword">in</span> a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.</span><br><span class="line">deployment.apps/nginx created</span><br><span class="line"><span class="comment"># kubectl expose deployment nginx --port=80 --type=NodePort</span></span><br><span class="line">service/nginx exposed</span><br></pre></td></tr></table></figure></li><li><p>查看pod情况</p></li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get pod -o wide</span></span><br><span class="line">NAME                     READY   STATUS              RESTARTS   AGE   IP            NODE            NOMINATED NODE   READINESS GATES</span><br><span class="line">nginx-766994fc9f-gcv4n   0/1     ContainerCreating   0          55s   &lt;none&gt;        172.21.16.248   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">nginx-766994fc9f-w2j8p   1/1     Running             0          55s   10.254.45.2   172.21.16.83    &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><ul><li>查看对外的服务</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get svc nginx</span></span><br><span class="line">NAME    TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE</span><br><span class="line">nginx   NodePort   10.254.11.147   &lt;none&gt;        80:48713/TCP   31s</span><br></pre></td></tr></table></figure><p>部署完成后，通过任意node节点IP的地址加端口48713即可访问<br><a href="http://node-ip:48713/" target="_blank" rel="noopener">http://node-ip:48713/</a></p><!-- rebuild by neat -->]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes v13.3 node</tag>
      </tags>
  </entry>
  <entry>
    <title>kubernetes v1.13.3安装</title>
    <url>/2019/08/09/kubernetes-v1-13-3%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:23 GMT+0800 (China Standard Time) --><h2 id="1、-环境准备"><a href="#1、-环境准备" class="headerlink" title="1、 环境准备"></a>1、 环境准备</h2><table><thead><tr><th>ip</th><th>type</th><th>docker</th><th>os</th><th>k8s version</th></tr></thead><tbody><tr><td>172.21.17.4</td><td>master,etcd</td><td></td><td>CentOS Linux release 7.4.1708</td><td>v1.13.3</td></tr><tr><td>172.21.16.230</td><td>master,etcd</td><td></td><td>CentOS Linux release 7.4.1708</td><td></td></tr><tr><td>172.21.16.240</td><td>master,etcd</td><td></td><td>CentOS Linux release 7.4.1708</td><td></td></tr><tr><td>172.21.16.244</td><td>node,flanneld,ha+kee</td><td>18.06.2-ce</td><td>CentOS Linux release 7.4.1708</td><td></td></tr><tr><td>172.21.16.248</td><td>node,flanneld,ha+kee</td><td>18.06.2-ce</td><td>CentOS Linux release 7.4.1708</td><td></td></tr><tr><td>172.21.16.45</td><td>vip</td><td></td><td>CentOS Linux release 7.4.1708</td><td></td></tr></tbody></table><h2 id="2、部署ETC集群"><a href="#2、部署ETC集群" class="headerlink" title="2、部署ETC集群"></a>2、部署ETC集群</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;etcd的正常运行是k8s集群运行的提前条件，因此部署k8s集群首先部署etcd集群。安装CA证书，安装CFSSL证书管理工具。直接下载二进制安装包</p><a id="more"></a><h3 id="2-1、下载cfssl"><a href="#2-1、下载cfssl" class="headerlink" title="2.1、下载cfssl"></a>2.1、下载cfssl</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># curl -o cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64</span></span><br><span class="line"><span class="comment"># curl -o cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64</span></span><br><span class="line"><span class="comment"># curl -o cfssl-certinfo https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64</span></span><br><span class="line"><span class="comment"># chmod +x * &amp;&amp;mv cfssl* /usr/bin/</span></span><br></pre></td></tr></table></figure><h3 id="2-2-、创建etcd证书"><a href="#2-2-、创建etcd证书" class="headerlink" title="2.2 、创建etcd证书"></a>2.2 、创建etcd证书</h3><ul><li><p>etcd-ca-csr.json</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># mkdir etcd_ssl &amp;&amp; cd etcd_ssl</span></span><br><span class="line"><span class="comment"># cat etcd-ca-csr.json</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"CN"</span>: <span class="string">"etcd-ca"</span>,</span><br><span class="line">    <span class="string">"key"</span>: &#123;</span><br><span class="line">        <span class="string">"algo"</span>: <span class="string">"rsa"</span>,</span><br><span class="line">        <span class="string">"size"</span>: 4096</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"names"</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"O"</span>: <span class="string">"etcd"</span>,</span><br><span class="line">            <span class="string">"OU"</span>: <span class="string">"etcd Security"</span>,</span><br><span class="line">            <span class="string">"L"</span>: <span class="string">"Beijing"</span>,</span><br><span class="line">            <span class="string">"ST"</span>: <span class="string">"Beijing"</span>,</span><br><span class="line">            <span class="string">"C"</span>: <span class="string">"CN"</span></span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"ca"</span>: &#123;</span><br><span class="line">        <span class="string">"expiry"</span>: <span class="string">"87600h"</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>etcd-gencert.json</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat etcd-gencert.json</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"signing"</span>: &#123;</span><br><span class="line">    <span class="string">"default"</span>: &#123;</span><br><span class="line">        <span class="string">"usages"</span>: [</span><br><span class="line">          <span class="string">"signing"</span>,</span><br><span class="line">          <span class="string">"key encipherment"</span>,</span><br><span class="line">          <span class="string">"server auth"</span>,</span><br><span class="line">          <span class="string">"client auth"</span></span><br><span class="line">        ],</span><br><span class="line">        <span class="string">"expiry"</span>: <span class="string">"87600h"</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>etcd-csr.json</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat etcd-csr.json</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"key"</span>: &#123;</span><br><span class="line">        <span class="string">"algo"</span>: <span class="string">"rsa"</span>,</span><br><span class="line">        <span class="string">"size"</span>: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"names"</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"O"</span>: <span class="string">"etcd"</span>,</span><br><span class="line">            <span class="string">"OU"</span>: <span class="string">"etcd Security"</span>,</span><br><span class="line">            <span class="string">"L"</span>: <span class="string">"Beijing"</span>,</span><br><span class="line">            <span class="string">"ST"</span>: <span class="string">"Beijing"</span>,</span><br><span class="line">            <span class="string">"C"</span>: <span class="string">"CN"</span></span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"CN"</span>: <span class="string">"etcd"</span>,</span><br><span class="line">    <span class="string">"hosts"</span>: [</span><br><span class="line">        <span class="string">"127.0.0.1"</span>,</span><br><span class="line">        <span class="string">"localhost"</span>,</span><br><span class="line">        <span class="string">"172.21.17.4"</span>,</span><br><span class="line">        <span class="string">"172.21.16.231"</span>,</span><br><span class="line">        <span class="string">"172.21.16.240"</span></span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>接下来执行生成即可</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cfssl gencert --initca=true etcd-ca-csr.json | cfssljson --bare etcd-ca</span></span><br><span class="line"><span class="comment"># cfssl gencert --ca etcd-ca.pem --ca-key etcd-ca-key.pem --config etcd-gencert.json etcd-csr.json | cfssljson --bare etcd</span></span><br><span class="line"><span class="comment"># mkdir -p /etc/etcd/ssl &amp;&amp;mkdir -p /var/lib/etcd</span></span><br><span class="line"><span class="comment"># cp *.pem /etc/etcd/ssl</span></span><br><span class="line"><span class="comment"># ls /etc/etcd/ssl/</span></span><br><span class="line">etcd-ca-key.pem  etcd-ca.pem  etcd-key.pem  etcd.pem</span><br><span class="line"><span class="comment"># scp -r /etc/etcd k8s-master-02:/etc</span></span><br><span class="line"><span class="comment"># scp -r /etc/etcd k8s-master-03:/etc</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="2-3、开始配置etcd"><a href="#2-3、开始配置etcd" class="headerlink" title="2.3、开始配置etcd"></a>2.3、开始配置etcd</h3><h4 id="2-3-1、下载etcd"><a href="#2-3-1、下载etcd" class="headerlink" title="2.3.1、下载etcd"></a>2.3.1、下载etcd</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># wget https://github.com/etcd-io/etcd/releases/download/v3.3.15/etcd-v3.3.15-linux-amd64.tar.gz</span></span><br><span class="line"><span class="comment"># tar zxf etcd-v3.3.15-linux-amd64.tar.gz &amp;&amp;cd etcd-v3.3.15-linux-amd64 &amp;&amp;cp -arp etcd* /usr/bin/</span></span><br></pre></td></tr></table></figure><h4 id="2-3-2、创建etcd的Systemd-unit-文件"><a href="#2-3-2、创建etcd的Systemd-unit-文件" class="headerlink" title="2.3.2、创建etcd的Systemd unit 文件"></a>2.3.2、创建etcd的Systemd unit 文件</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;Etcd 这里采用最新的 3.3.15 版本，安装方式直接复制二进制文件、systemd service 配置即可，不过需要注意相关用户权限问题，以下脚本配置等参考了 etcd rpm 安装包</p><h4 id="2-3-3、配置etcd-conf"><a href="#2-3-3、配置etcd-conf" class="headerlink" title="2.3.3、配置etcd.conf"></a>2.3.3、配置etcd.conf</h4><ul><li>k8s-master-01<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat /etc/etcd/etcd.conf</span></span><br><span class="line"><span class="comment"># [member]</span></span><br><span class="line">ETCD_NAME=etcd1</span><br><span class="line">ETCD_DATA_DIR=<span class="string">"/var/lib/etcd"</span></span><br><span class="line">ETCD_SNAPSHOT_COUNT=<span class="string">"100"</span></span><br><span class="line">ETCD_HEARTBEAT_INTERVAL=<span class="string">"100"</span></span><br><span class="line">ETCD_ELECTION_TIMEOUT=<span class="string">"1000"</span></span><br><span class="line">ETCD_LISTEN_PEER_URLS=<span class="string">"https://172.21.17.4:2380"</span></span><br><span class="line">ETCD_LISTEN_CLIENT_URLS=<span class="string">"https://172.21.17.4:2379,http://127.0.0.1:2379"</span></span><br><span class="line">ETCD_MAX_SNAPSHOTS=<span class="string">"5"</span></span><br><span class="line">ETCD_MAX_WALS=<span class="string">"5"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># [cluster]</span></span><br><span class="line">ETCD_INITIAL_ADVERTISE_PEER_URLS=<span class="string">"https://172.21.17.4:2380"</span></span><br><span class="line">ETCD_INITIAL_CLUSTER=<span class="string">"etcd1=https://172.21.17.4:2380,etcd2=https://172.21.16.231:2380,etcd3=https://172.21.16.240:2380"</span></span><br><span class="line">ETCD_INITIAL_CLUSTER_STATE=<span class="string">"new"</span></span><br><span class="line">ETCD_INITIAL_CLUSTER_TOKEN=<span class="string">"etcd-cluster"</span></span><br><span class="line">ETCD_ADVERTISE_CLIENT_URLS=<span class="string">"https://172.21.17.4:2379"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># [security]</span></span><br><span class="line">ETCD_CERT_FILE=<span class="string">"/etc/etcd/ssl/etcd.pem"</span></span><br><span class="line">ETCD_KEY_FILE=<span class="string">"/etc/etcd/ssl/etcd-key.pem"</span></span><br><span class="line">ETCD_CLIENT_CERT_AUTH=<span class="string">"true"</span></span><br><span class="line">ETCD_TRUSTED_CA_FILE=<span class="string">"/etc/etcd/ssl/etcd-ca.pem"</span></span><br><span class="line">ETCD_AUTO_TLS=<span class="string">"true"</span></span><br><span class="line">ETCD_PEER_CERT_FILE=<span class="string">"/etc/etcd/ssl/etcd.pem"</span></span><br><span class="line">ETCD_PEER_KEY_FILE=<span class="string">"/etc/etcd/ssl/etcd-key.pem"</span></span><br><span class="line">ETCD_PEER_CLIENT_CERT_AUTH=<span class="string">"true"</span></span><br><span class="line">ETCD_PEER_TRUSTED_CA_FILE=<span class="string">"/etc/etcd/ssl/etcd-ca.pem"</span></span><br><span class="line">ETCD_PEER_AUTO_TLS=<span class="string">"true"</span></span><br></pre></td></tr></table></figure></li></ul><ul><li><p>k8s-master-02</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat /etc/etcd/etcd.conf</span></span><br><span class="line"><span class="comment"># [member]</span></span><br><span class="line">ETCD_NAME=etcd2</span><br><span class="line">ETCD_DATA_DIR=<span class="string">"/var/lib/etcd"</span></span><br><span class="line">ETCD_SNAPSHOT_COUNT=<span class="string">"100"</span></span><br><span class="line">ETCD_HEARTBEAT_INTERVAL=<span class="string">"100"</span></span><br><span class="line">ETCD_ELECTION_TIMEOUT=<span class="string">"1000"</span></span><br><span class="line">ETCD_LISTEN_PEER_URLS=<span class="string">"https://172.21.16.231:2380"</span></span><br><span class="line">ETCD_LISTEN_CLIENT_URLS=<span class="string">"https://172.21.16.231:2379,http://127.0.0.1:2379"</span></span><br><span class="line">ETCD_MAX_SNAPSHOTS=<span class="string">"5"</span></span><br><span class="line">ETCD_MAX_WALS=<span class="string">"5"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># [cluster]</span></span><br><span class="line">ETCD_INITIAL_ADVERTISE_PEER_URLS=<span class="string">"https://172.21.16.231:2380"</span></span><br><span class="line">ETCD_INITIAL_CLUSTER=<span class="string">"etcd1=https://172.21.17.4:2380,etcd2=https://172.21.16.231:2380,etcd3=https://172.21.16.240:2380"</span></span><br><span class="line">ETCD_INITIAL_CLUSTER_STATE=<span class="string">"new"</span></span><br><span class="line">ETCD_INITIAL_CLUSTER_TOKEN=<span class="string">"etcd-cluster"</span></span><br><span class="line">ETCD_ADVERTISE_CLIENT_URLS=<span class="string">"https://172.21.16.231:2379"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># [security]</span></span><br><span class="line">ETCD_CERT_FILE=<span class="string">"/etc/etcd/ssl/etcd.pem"</span></span><br><span class="line">ETCD_KEY_FILE=<span class="string">"/etc/etcd/ssl/etcd-key.pem"</span></span><br><span class="line">ETCD_CLIENT_CERT_AUTH=<span class="string">"true"</span></span><br><span class="line">ETCD_TRUSTED_CA_FILE=<span class="string">"/etc/etcd/ssl/etcd-ca.pem"</span></span><br><span class="line">ETCD_AUTO_TLS=<span class="string">"true"</span></span><br><span class="line">ETCD_PEER_CERT_FILE=<span class="string">"/etc/etcd/ssl/etcd.pem"</span></span><br><span class="line">ETCD_PEER_KEY_FILE=<span class="string">"/etc/etcd/ssl/etcd-key.pem"</span></span><br><span class="line">ETCD_PEER_CLIENT_CERT_AUTH=<span class="string">"true"</span></span><br><span class="line">ETCD_PEER_TRUSTED_CA_FILE=<span class="string">"/etc/etcd/ssl/etcd-ca.pem"</span></span><br><span class="line">ETCD_PEER_AUTO_TLS=<span class="string">"true"</span></span><br></pre></td></tr></table></figure></li><li><p>k8s-master-03</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat /etc/etcd/etcd.conf</span></span><br><span class="line"><span class="comment"># [member]</span></span><br><span class="line">ETCD_NAME=etcd3</span><br><span class="line">ETCD_DATA_DIR=<span class="string">"/var/lib/etcd"</span></span><br><span class="line">ETCD_SNAPSHOT_COUNT=<span class="string">"100"</span></span><br><span class="line">ETCD_HEARTBEAT_INTERVAL=<span class="string">"100"</span></span><br><span class="line">ETCD_ELECTION_TIMEOUT=<span class="string">"1000"</span></span><br><span class="line">ETCD_LISTEN_PEER_URLS=<span class="string">"https://172.21.16.240:2380"</span></span><br><span class="line">ETCD_LISTEN_CLIENT_URLS=<span class="string">"https://172.21.16.240:2379,http://127.0.0.1:2379"</span></span><br><span class="line">ETCD_MAX_SNAPSHOTS=<span class="string">"5"</span></span><br><span class="line">ETCD_MAX_WALS=<span class="string">"5"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># [cluster]</span></span><br><span class="line">ETCD_INITIAL_ADVERTISE_PEER_URLS=<span class="string">"https://172.21.16.240:2380"</span></span><br><span class="line">ETCD_INITIAL_CLUSTER=<span class="string">"etcd1=https://172.21.17.4:2380,etcd2=https://172.21.16.231:2380,etcd3=https://172.21.16.240:2380"</span></span><br><span class="line">ETCD_INITIAL_CLUSTER_STATE=<span class="string">"new"</span></span><br><span class="line">ETCD_INITIAL_CLUSTER_TOKEN=<span class="string">"etcd-cluster"</span></span><br><span class="line">ETCD_ADVERTISE_CLIENT_URLS=<span class="string">"https://172.21.16.240:2379"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># [security]</span></span><br><span class="line">ETCD_CERT_FILE=<span class="string">"/etc/etcd/ssl/etcd.pem"</span></span><br><span class="line">ETCD_KEY_FILE=<span class="string">"/etc/etcd/ssl/etcd-key.pem"</span></span><br><span class="line">ETCD_CLIENT_CERT_AUTH=<span class="string">"true"</span></span><br><span class="line">ETCD_TRUSTED_CA_FILE=<span class="string">"/etc/etcd/ssl/etcd-ca.pem"</span></span><br><span class="line">ETCD_AUTO_TLS=<span class="string">"true"</span></span><br><span class="line">ETCD_PEER_CERT_FILE=<span class="string">"/etc/etcd/ssl/etcd.pem"</span></span><br><span class="line">ETCD_PEER_KEY_FILE=<span class="string">"/etc/etcd/ssl/etcd-key.pem"</span></span><br><span class="line">ETCD_PEER_CLIENT_CERT_AUTH=<span class="string">"true"</span></span><br><span class="line">ETCD_PEER_TRUSTED_CA_FILE=<span class="string">"/etc/etcd/ssl/etcd-ca.pem"</span></span><br><span class="line">ETCD_PEER_AUTO_TLS=<span class="string">"true"</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="2-3-4、配置etcd启动文件"><a href="#2-3-4、配置etcd启动文件" class="headerlink" title="2.3.4、配置etcd启动文件"></a>2.3.4、配置etcd启动文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat /lib/systemd/system/etcd.service</span></span><br><span class="line">[Unit]</span><br><span class="line">Description=Etcd Server</span><br><span class="line">After=network.target</span><br><span class="line">After=network-online.target</span><br><span class="line">Wants=network-online.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=notify</span><br><span class="line">WorkingDirectory=/var/lib/etcd/</span><br><span class="line">EnvironmentFile=-/etc/etcd/etcd.conf</span><br><span class="line">User=etcd</span><br><span class="line"><span class="comment"># set GOMAXPROCS to number of processors</span></span><br><span class="line">ExecStart=/bin/bash -c <span class="string">"GOMAXPROCS=<span class="variable">$(nproc)</span> /usr/bin/etcd --name=\"<span class="variable">$&#123;ETCD_NAME&#125;</span>\" --data-dir=\"<span class="variable">$&#123;ETCD_DATA_DIR&#125;</span>\" --listen-client-urls=\"<span class="variable">$&#123;ETCD_LISTEN_CLIENT_URLS&#125;</span>\""</span></span><br><span class="line">Restart=on-failure</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><h4 id="2-3-5、etcd授权"><a href="#2-3-5、etcd授权" class="headerlink" title="2.3.5、etcd授权"></a>2.3.5、etcd授权</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># groupadd -r etcd</span></span><br><span class="line"><span class="comment"># useradd -r -g etcd -d /var/lib/etcd -s /sbin/nologin -c "etcd user" etcd</span></span><br><span class="line"><span class="comment"># chown -R etcd:etcd /etc/etcd &amp;&amp; chmod -R 755 /etc/etcd/ssl &amp;&amp;chown -R etcd:etcd /var/lib/etcd</span></span><br><span class="line"><span class="comment"># systemctl daemon-reload &amp;&amp; systemctl enable etcd &amp;&amp;systemctl start etcd &amp;&amp; systemctl status etcd</span></span><br></pre></td></tr></table></figure><h4 id="2-3-6、验证etcd"><a href="#2-3-6、验证etcd" class="headerlink" title="2.3.6、验证etcd"></a>2.3.6、验证etcd</h4><p>由于etcd使用了证书，所以etcd命令需要带上证书</p><ul><li><p>查看成员列表</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># etcdctl --key-file /etc/etcd/ssl/etcd-key.pem --cert-file /etc/etcd/ssl/etcd.pem --ca-file /etc/etcd/ssl/etcd-ca.pem member list</span></span><br><span class="line">93c04a995ff8aa8: name=etcd3 peerURLs=https://172.21.16.240:2380 clientURLs=https://172.21.16.240:2379 isLeader=<span class="literal">false</span></span><br><span class="line">7cc4daf6e4db3a8a: name=etcd2 peerURLs=https://172.21.16.231:2380 clientURLs=https://172.21.16.231:2379 isLeader=<span class="literal">false</span></span><br><span class="line">ec7ea930930d012e: name=etcd1 peerURLs=https://172.21.17.4:2380 clientURLs=https://172.21.17.4:2379 isLeader=<span class="literal">true</span></span><br></pre></td></tr></table></figure></li><li><p>查看集群状态</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># etcdctl --key-file /etc/etcd/ssl/etcd-key.pem --cert-file /etc/etcd/ssl/etcd.pem --ca-file /etc/etcd/ssl/etcd-ca.pem cluster-health</span></span><br><span class="line">member 93c04a995ff8aa8 is healthy: got healthy result from https://172.21.16.240:2379</span><br><span class="line">member 7cc4daf6e4db3a8a is healthy: got healthy result from https://172.21.16.231:2379</span><br><span class="line">member ec7ea930930d012e is healthy: got healthy result from https://172.21.17.4:2379</span><br><span class="line">cluster is healthy</span><br></pre></td></tr></table></figure></li></ul><h2 id="3、部署kubernetes"><a href="#3、部署kubernetes" class="headerlink" title="3、部署kubernetes"></a>3、部署kubernetes</h2><h3 id="3-1-介绍"><a href="#3-1-介绍" class="headerlink" title="3.1 介绍"></a>3.1 介绍</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;新版本已经越来越趋近全面 TLS + RBAC 配置，所以本次安装将会启动大部分 TLS + RBAC 配置，包括 kube-controler-manager、kube-scheduler 组件不再连接本地 kube-apiserver 的 8080 非认证端口，kubelet 等组件 API 端点关闭匿名访问，启动 RBAC 认证等；为了满足这些认证，需要签署以下证书</p><h3 id="3-2、创建CA"><a href="#3-2、创建CA" class="headerlink" title="3.2、创建CA"></a>3.2、创建CA</h3><h4 id="3-2-1、创建CA配置文件"><a href="#3-2-1、创建CA配置文件" class="headerlink" title="3.2.1、创建CA配置文件"></a>3.2.1、创建CA配置文件</h4><ul><li><p>kubernetes-ca-csr.json集群CA根证书</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># mkdir ssl &amp;&amp; cd ssl/</span></span><br><span class="line"><span class="comment"># cat kubernetes-ca-csr.json </span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"CN"</span>: <span class="string">"kubernetes"</span>,</span><br><span class="line">    <span class="string">"key"</span>: &#123;</span><br><span class="line">        <span class="string">"algo"</span>: <span class="string">"rsa"</span>,</span><br><span class="line">        <span class="string">"size"</span>: 4096</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"names"</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"C"</span>: <span class="string">"CN"</span>,</span><br><span class="line">            <span class="string">"ST"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">            <span class="string">"L"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">            <span class="string">"O"</span>: <span class="string">"kubernetes"</span>,</span><br><span class="line">            <span class="string">"OU"</span>: <span class="string">"System"</span></span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"ca"</span>: &#123;</span><br><span class="line">        <span class="string">"expiry"</span>: <span class="string">"87600h"</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><strong>“CN”</strong>: Common Name，kube-apiserver 从该证书中提取该字段作为请求的用户名（User Name）;浏览器使用该字段验证网站合法性；</li><li><strong>“O”</strong>: Organization，kube-apiserver从该证书中提取该字段作为请求用户所属组（Group）；</li></ul></li><li><p>kubernetes-gencert.json<br>用于生成其他证书的标准</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat kubernetes-gencert.json </span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"signing"</span>: &#123;</span><br><span class="line">        <span class="string">"default"</span>: &#123;</span><br><span class="line">            <span class="string">"expiry"</span>: <span class="string">"87600h"</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">"profiles"</span>: &#123;</span><br><span class="line">            <span class="string">"kubernetes"</span>: &#123;</span><br><span class="line">                <span class="string">"usages"</span>: [</span><br><span class="line">                    <span class="string">"signing"</span>,</span><br><span class="line">                    <span class="string">"key encipherment"</span>,</span><br><span class="line">                    <span class="string">"server auth"</span>,</span><br><span class="line">                    <span class="string">"client auth"</span></span><br><span class="line">                ],</span><br><span class="line">                <span class="string">"expiry"</span>: <span class="string">"87600h"</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>kube-apiserver-csr.json<br>apiserver TLS 认证端口需要的证书</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat kube-apiserver-csr.json </span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"CN"</span>: <span class="string">"kubernetes"</span>,</span><br><span class="line">    <span class="string">"hosts"</span>: [</span><br><span class="line">        <span class="string">"127.0.0.1"</span>,</span><br><span class="line">        <span class="string">"10.254.0.1"</span>,</span><br><span class="line">        <span class="string">"localhost"</span>,</span><br><span class="line">        <span class="string">"172.21.16.45"</span>,</span><br><span class="line">        <span class="string">"*.master.kubernetes.node"</span>,</span><br><span class="line">        <span class="string">"kubernetes"</span>,</span><br><span class="line">        <span class="string">"kubernetes.default"</span>,</span><br><span class="line">        <span class="string">"kubernetes.default.svc"</span>,</span><br><span class="line">        <span class="string">"kubernetes.default.svc.cluster"</span>,</span><br><span class="line">        <span class="string">"kubernetes.default.svc.cluster.local"</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"key"</span>: &#123;</span><br><span class="line">        <span class="string">"algo"</span>: <span class="string">"rsa"</span>,</span><br><span class="line">        <span class="string">"size"</span>: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"names"</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"C"</span>: <span class="string">"CN"</span>,</span><br><span class="line">            <span class="string">"ST"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">            <span class="string">"L"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">            <span class="string">"O"</span>: <span class="string">"kubernetes"</span>,</span><br><span class="line">            <span class="string">"OU"</span>: <span class="string">"System"</span></span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><ul><li><strong>172.21.16.45</strong>: vip地址</li><li>如果hosts字段不为空则需要指定授权使用该证书的ip或域名列表,kube-apiserver指定的service-cluster-ip-range网段的第一个ip，如10.254.0.1</li></ul><ul><li><p>kube-controller-manager-csr.json<br>controller manager 连接 apiserver 需要使用的证书，同时本身 10257 端口也会使用此证书</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat kube-controller-manager-csr.json</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"CN"</span>: <span class="string">"system:kube-controller-manager"</span>,</span><br><span class="line">  <span class="string">"hosts"</span>: [</span><br><span class="line">    <span class="string">"127.0.0.1"</span>,</span><br><span class="line">    <span class="string">"localhost"</span>,</span><br><span class="line">    <span class="string">"*.master.kubernetes.node"</span></span><br><span class="line">  ],</span><br><span class="line">  <span class="string">"key"</span>: &#123;</span><br><span class="line">    <span class="string">"algo"</span>: <span class="string">"rsa"</span>,</span><br><span class="line">    <span class="string">"size"</span>: 2048</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"names"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"C"</span>: <span class="string">"CN"</span>,</span><br><span class="line">      <span class="string">"ST"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">      <span class="string">"L"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">      <span class="string">"O"</span>: <span class="string">"system:kube-controller-manager"</span>,</span><br><span class="line">      <span class="string">"OU"</span>: <span class="string">"System"</span></span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>kube-scheduler-csr.json<br>scheduler连接 apiserver 需要使用的证书，同时本身 10259 端口也会使用此证书</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat kube-scheduler-csr.json</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"CN"</span>: <span class="string">"system:kube-scheduler"</span>,</span><br><span class="line">  <span class="string">"hosts"</span>: [</span><br><span class="line">    <span class="string">"127.0.0.1"</span>,</span><br><span class="line">    <span class="string">"localhost"</span>,</span><br><span class="line">    <span class="string">"*.master.kubernetes.node"</span></span><br><span class="line">  ],</span><br><span class="line">  <span class="string">"key"</span>: &#123;</span><br><span class="line">    <span class="string">"algo"</span>: <span class="string">"rsa"</span>,</span><br><span class="line">    <span class="string">"size"</span>: 2048</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"names"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"C"</span>: <span class="string">"CN"</span>,</span><br><span class="line">      <span class="string">"ST"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">      <span class="string">"L"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">      <span class="string">"O"</span>: <span class="string">"system:kube-scheduler"</span>,</span><br><span class="line">      <span class="string">"OU"</span>: <span class="string">"System"</span></span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>kube-proxy-csr.json<br>proxy 组件连接 apiserver 需要使用的证书</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat kube-proxy-csr.json</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"CN"</span>: <span class="string">"system:kube-proxy"</span>,</span><br><span class="line">    <span class="string">"hosts"</span>: [],</span><br><span class="line">    <span class="string">"key"</span>: &#123;</span><br><span class="line">        <span class="string">"algo"</span>: <span class="string">"rsa"</span>,</span><br><span class="line">        <span class="string">"size"</span>: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"names"</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"C"</span>: <span class="string">"CN"</span>,</span><br><span class="line">            <span class="string">"ST"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">            <span class="string">"L"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">            <span class="string">"O"</span>: <span class="string">"system:kube-proxy"</span>,</span><br><span class="line">            <span class="string">"OU"</span>: <span class="string">"System"</span></span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>kubelet-api-admin-csr.json<br>apiserver 反向连接 kubelet 组件 10250 端口需要使用的证书(例如执行 kubectl logs)</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat kubelet-api-admin-csr.json</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"CN"</span>: <span class="string">"system:kubelet-api-admin"</span>,</span><br><span class="line">    <span class="string">"hosts"</span>: [],</span><br><span class="line">    <span class="string">"key"</span>: &#123;</span><br><span class="line">        <span class="string">"algo"</span>: <span class="string">"rsa"</span>,</span><br><span class="line">        <span class="string">"size"</span>: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"names"</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"C"</span>: <span class="string">"CN"</span>,</span><br><span class="line">            <span class="string">"ST"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">            <span class="string">"L"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">            <span class="string">"O"</span>: <span class="string">"system:kubelet-api-admin"</span>,</span><br><span class="line">            <span class="string">"OU"</span>: <span class="string">"System"</span></span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>admin-csr.json<br>集群管理员(kubectl)连接 apiserver 需要使用的证书</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat admin-csr.json</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"CN"</span>: <span class="string">"system:masters"</span>,</span><br><span class="line">    <span class="string">"hosts"</span>: [],</span><br><span class="line">    <span class="string">"key"</span>: &#123;</span><br><span class="line">        <span class="string">"algo"</span>: <span class="string">"rsa"</span>,</span><br><span class="line">        <span class="string">"size"</span>: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"names"</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"C"</span>: <span class="string">"CN"</span>,</span><br><span class="line">            <span class="string">"ST"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">            <span class="string">"L"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">            <span class="string">"O"</span>: <span class="string">"system:masters"</span>,</span><br><span class="line">            <span class="string">"OU"</span>: <span class="string">"System"</span></span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><p><strong>注意</strong>: 证书文件里面的CN、O字段，两个比较特殊的字段，基本都是system:开头，是为了匹配RBAC规则,<a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/#default-roles-and-role-bindings" target="_blank" rel="noopener">详情参考</a></p><h4 id="3-3、使用命令生成即可"><a href="#3-3、使用命令生成即可" class="headerlink" title="3.3、使用命令生成即可"></a>3.3、使用命令生成即可</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cfssl gencert --initca=true kubernetes-ca-csr.json | cfssljson --bare kubernetes-ca</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># for targetName in kube-apiserver kube-controller-manager kube-scheduler kube-proxy kubelet-api-admin admin; do</span></span><br><span class="line">  cfssl gencert --ca kubernetes-ca.pem --ca-key kubernetes-ca-key.pem --config kubernetes-gencert.json --profile kubernetes <span class="variable">$targetName</span>-csr.json | cfssljson --bare <span class="variable">$targetName</span>; </span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><h4 id="3-4、分发证书"><a href="#3-4、分发证书" class="headerlink" title="3.4、分发证书"></a>3.4、分发证书</h4><p>将生成的证书和秘钥文件（后缀名为.pem）拷贝到所有机器；kubernetes系统的各个组建需要使用tls证书对通信进行加密。</p><h5 id="1）、生成的证书ca证书和秘钥文件如下："><a href="#1）、生成的证书ca证书和秘钥文件如下：" class="headerlink" title="1）、生成的证书ca证书和秘钥文件如下："></a>1）、生成的证书ca证书和秘钥文件如下：</h5><ul><li>admin-key.pem</li><li>admin.pem</li><li>kube-apiserver-key.pem</li><li>kube-apiserver.pem</li><li>kube-controller-manager-key.pem</li><li>kube-controller-manager.pem</li><li>kubelet-api-admin-key.pem</li><li>kubelet-api-admin.pem</li><li>kube-proxy-key.pem</li><li>kube-proxy.pem</li><li>kubernetes-ca-key.pem</li><li>kubernetes-ca.pem</li><li>kube-scheduler-key.pem</li><li>kube-scheduler.pem</li></ul><h5 id="3）、证书拷贝"><a href="#3）、证书拷贝" class="headerlink" title="3）、证书拷贝"></a>3）、证书拷贝</h5><ul><li><p><strong>master 节点拷贝</strong></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># mkdir -p /etc/kubernetes/ssl</span></span><br><span class="line"><span class="comment"># cp *.pem /etc/kubernetes/ssl/</span></span><br><span class="line"><span class="comment"># scp -r /etc/kubernetes k8s-master-02:/etc</span></span><br><span class="line"><span class="comment"># scp -r /etc/kubernetes k8s-master-03:/etc</span></span><br><span class="line"><span class="comment"># scp -r /etc/kubernetes node-01:/etc</span></span><br><span class="line"><span class="comment"># scp -r /etc/kubernetes node-02:/etc</span></span><br></pre></td></tr></table></figure></li><li><p><strong>创建目录</strong></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># mkdir -p /var/log/kube-audit &amp;&amp; mkdir /var/lib/kubelet -p &amp;&amp; mkdir /usr/libexec -p</span></span><br></pre></td></tr></table></figure></li></ul><h2 id="4、创建kube-config文件"><a href="#4、创建kube-config文件" class="headerlink" title="4、创建kube config文件"></a>4、创建kube config文件</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;kubelet、kube-proxy等Node机器上的经常与master机器的kube-apiserver进程通信时需要认证和授权；kubernetes 1.4 开始支持有kube-apiserver为客户端生成tls证书的 TLS Bootstrapping功能，这样就不需要为每个客户端生成证书了，该功能当前仅支持为kuelet生成证书；</p><h3 id="4-1、生成配置文件"><a href="#4-1、生成配置文件" class="headerlink" title="4.1、生成配置文件"></a>4.1、生成配置文件</h3><ul><li>bootstrap.kubeconfig kubelet TLS Bootstarp 引导阶段需要使用的配置文件</li><li>kube-controller-manager.kubeconfig controller manager 组件开启安全端口及 RBAC 认证所需配置</li><li>kube-scheduler.kubeconfig scheduler 组件开启安全端口及 RBAC 认证所需配置</li><li>kube-proxy.kubeconfig proxy 组件连接 apiserver 所需配置文件</li><li>audit-policy.yaml apiserver RBAC 审计日志配置文件</li><li>bootstrap.secret.yaml kubelet TLS Bootstarp 引导阶段使用 Bootstrap Token 方式引导，需要预先创建此 Token</li></ul><h3 id="4-2、创建kubelet-bootstrapping-kubeconfig文件"><a href="#4-2、创建kubelet-bootstrapping-kubeconfig文件" class="headerlink" title="4.2、创建kubelet bootstrapping kubeconfig文件"></a>4.2、创建kubelet bootstrapping kubeconfig文件</h3><p>在这之前我们需要下载kubernetes 相关的二进制包，把对应的工具和命令拷贝到/usr/bin目录下面;下载二进制包</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># wget https://dl.k8s.io/v1.13.3/kubernetes-server-linux-amd64.tar.gz</span></span><br><span class="line"><span class="comment"># tar zxf kubernetes-server-linux-amd64.tar.gz &amp;&amp; cd kubernetes/server/bin</span></span><br></pre></td></tr></table></figure><ul><li>master节点拷贝</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># mv apiextensions-apiserver cloud-controller-manager hyperkube kube-apiserver kube-controller-manager kube-proxy kube-scheduler kubectl kubelet mounter kubeadm /usr/bin/ &amp;&amp; cd &amp;&amp;rm -rf kubernetes kubernetes-server-linux-amd64.tar.gz</span></span><br></pre></td></tr></table></figure><h4 id="4-2-1、生成文件bootstrapping"><a href="#4-2-1、生成文件bootstrapping" class="headerlink" title="4.2.1、生成文件bootstrapping"></a>4.2.1、生成文件bootstrapping</h4><ul><li>master-01<br>&nbsp;&nbsp;&nbsp;&nbsp;config 是一个通用配置文件要连接本地的 6443 加密端口；而这个变量将会覆盖 kubeconfig 中指定的master_vip地址172.21.16.45:6443 地址</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># export KUBE_APISERVER="https://172.21.16.45:6443"</span></span><br></pre></td></tr></table></figure><ul><li>生成 Bootstrap Token<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># BOOTSTRAP_TOKEN_ID=$(head -c 6 /dev/urandom | md5sum | head -c 6)</span></span><br><span class="line"><span class="comment"># BOOTSTRAP_TOKEN_SECRET=$(head -c 16 /dev/urandom | md5sum | head -c 16)</span></span><br><span class="line"><span class="comment"># BOOTSTRAP_TOKEN="$&#123;BOOTSTRAP_TOKEN_ID&#125;.$&#123;BOOTSTRAP_TOKEN_SECRET&#125;"</span></span><br><span class="line"><span class="comment"># echo "Bootstrap Tokne: $&#123;BOOTSTRAP_TOKEN&#125;"</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="4-2-2、生成-kubelet-tls-bootstrap-配置"><a href="#4-2-2、生成-kubelet-tls-bootstrap-配置" class="headerlink" title="4.2.2、生成 kubelet tls bootstrap 配置"></a>4.2.2、生成 kubelet tls bootstrap 配置</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl config set-cluster kubernetes \</span></span><br><span class="line">  --certificate-authority=kubernetes-ca.pem \</span><br><span class="line">  --embed-certs=<span class="literal">true</span> \</span><br><span class="line">  --server=<span class="variable">$&#123;KUBE_APISERVER&#125;</span> \</span><br><span class="line">  --kubeconfig=bootstrap.kubeconfig</span><br><span class="line">  </span><br><span class="line"><span class="comment"># kubectl config set-credentials "system:bootstrap:$&#123;BOOTSTRAP_TOKEN_ID&#125;" \</span></span><br><span class="line">  --token=<span class="variable">$&#123;BOOTSTRAP_TOKEN&#125;</span> \</span><br><span class="line">  --kubeconfig=bootstrap.kubeconfig</span><br><span class="line">  </span><br><span class="line"><span class="comment"># kubectl config set-context default \</span></span><br><span class="line">  --cluster=kubernetes \</span><br><span class="line">  --user=<span class="string">"system:bootstrap:<span class="variable">$&#123;BOOTSTRAP_TOKEN_ID&#125;</span>"</span> \</span><br><span class="line">  --kubeconfig=bootstrap.kubeconfig</span><br><span class="line">  </span><br><span class="line"><span class="comment"># kubectl config use-context default --kubeconfig=bootstrap.kubeconfig</span></span><br></pre></td></tr></table></figure><h4 id="4-2-3、生成-kube-controller-manager-配置文件"><a href="#4-2-3、生成-kube-controller-manager-配置文件" class="headerlink" title="4.2.3、生成 kube-controller-manager 配置文件"></a>4.2.3、生成 kube-controller-manager 配置文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl config set-cluster kubernetes \</span></span><br><span class="line">  --certificate-authority=kubernetes-ca.pem \</span><br><span class="line">  --embed-certs=<span class="literal">true</span> \</span><br><span class="line">  --server=<span class="variable">$&#123;KUBE_APISERVER&#125;</span> \</span><br><span class="line">  --kubeconfig=kube-controller-manager.kubeconfig</span><br><span class="line">  </span><br><span class="line"><span class="comment"># kubectl config set-credentials "system:kube-controller-manager" \</span></span><br><span class="line">  --client-certificate=kube-controller-manager.pem \</span><br><span class="line">  --client-key=kube-controller-manager-key.pem \</span><br><span class="line">  --embed-certs=<span class="literal">true</span> \</span><br><span class="line">  --kubeconfig=kube-controller-manager.kubeconfig</span><br><span class="line">  </span><br><span class="line"><span class="comment"># kubectl config set-context default \</span></span><br><span class="line">  --cluster=kubernetes \</span><br><span class="line">  --user=system:kube-controller-manager \</span><br><span class="line">  --kubeconfig=kube-controller-manager.kubeconfig</span><br><span class="line">  </span><br><span class="line"><span class="comment"># kubectl config use-context default --kubeconfig=kube-controller-manager.kubeconfig</span></span><br></pre></td></tr></table></figure><h4 id="4-2-4、生成-kube-scheduler-配置文件"><a href="#4-2-4、生成-kube-scheduler-配置文件" class="headerlink" title="4.2.4、生成 kube-scheduler 配置文件"></a>4.2.4、生成 kube-scheduler 配置文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl config set-cluster kubernetes \</span></span><br><span class="line">  --certificate-authority=kubernetes-ca.pem \</span><br><span class="line">  --embed-certs=<span class="literal">true</span> \</span><br><span class="line">  --server=<span class="variable">$&#123;KUBE_APISERVER&#125;</span> \</span><br><span class="line">  --kubeconfig=kube-scheduler.kubeconfig</span><br><span class="line">  </span><br><span class="line"><span class="comment"># kubectl config set-credentials "system:kube-scheduler" \</span></span><br><span class="line">  --client-certificate=kube-scheduler.pem \</span><br><span class="line">  --client-key=kube-scheduler-key.pem \</span><br><span class="line">  --embed-certs=<span class="literal">true</span> \</span><br><span class="line">  --kubeconfig=kube-scheduler.kubeconfig</span><br><span class="line">  </span><br><span class="line"><span class="comment"># kubectl config set-context default \</span></span><br><span class="line">  --cluster=kubernetes \</span><br><span class="line">  --user=system:kube-scheduler \</span><br><span class="line">  --kubeconfig=kube-scheduler.kubeconfig</span><br><span class="line">  </span><br><span class="line"><span class="comment"># kubectl config use-context default --kubeconfig=kube-scheduler.kubeconfig</span></span><br></pre></td></tr></table></figure><h4 id="4-2-5、生成-kube-proxy-配置文件"><a href="#4-2-5、生成-kube-proxy-配置文件" class="headerlink" title="4.2.5、生成 kube-proxy 配置文件"></a>4.2.5、生成 kube-proxy 配置文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl config set-cluster kubernetes \</span></span><br><span class="line">  --certificate-authority=kubernetes-ca.pem \</span><br><span class="line">  --embed-certs=<span class="literal">true</span> \</span><br><span class="line">  --server=<span class="variable">$&#123;KUBE_APISERVER&#125;</span> \</span><br><span class="line">  --kubeconfig=kube-proxy.kubeconfig</span><br><span class="line">  </span><br><span class="line"><span class="comment"># kubectl config set-credentials "system:kube-proxy" \</span></span><br><span class="line">  --client-certificate=kube-proxy.pem \</span><br><span class="line">  --client-key=kube-proxy-key.pem \</span><br><span class="line">  --embed-certs=<span class="literal">true</span> \</span><br><span class="line">  --kubeconfig=kube-proxy.kubeconfig</span><br><span class="line">  </span><br><span class="line"><span class="comment"># kubectl config set-context default \</span></span><br><span class="line">  --cluster=kubernetes \</span><br><span class="line">  --user=system:kube-proxy \</span><br><span class="line">  --kubeconfig=kube-proxy.kubeconfig</span><br><span class="line">  </span><br><span class="line"><span class="comment"># kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig</span></span><br></pre></td></tr></table></figure><h4 id="4-2-6、生成-apiserver-RBAC-审计配置文件"><a href="#4-2-6、生成-apiserver-RBAC-审计配置文件" class="headerlink" title="4.2.6、生成 apiserver RBAC 审计配置文件"></a>4.2.6、生成 apiserver RBAC 审计配置文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat &gt;&gt; audit-policy.yaml &lt;&lt;EOF</span></span><br><span class="line"><span class="comment"># Log all requests at the Metadata level.</span></span><br><span class="line">apiVersion: audit.k8s.io/v1</span><br><span class="line">kind: Policy</span><br><span class="line">rules:</span><br><span class="line">- level: Metadata</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h4 id="4-2-7、生成-tls-bootstrap-token-secret-配置文件"><a href="#4-2-7、生成-tls-bootstrap-token-secret-配置文件" class="headerlink" title="4.2.7、生成 tls bootstrap token secret 配置文件"></a>4.2.7、生成 tls bootstrap token secret 配置文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat &gt;&gt; bootstrap.secret.yaml &lt;&lt;EOF</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  <span class="comment"># Name MUST be of form "bootstrap-token-&lt;token id&gt;"</span></span><br><span class="line">  name: bootstrap-token-<span class="variable">$&#123;BOOTSTRAP_TOKEN_ID&#125;</span></span><br><span class="line">  namespace: kube-system</span><br><span class="line"><span class="comment"># Type MUST be 'bootstrap.kubernetes.io/token'</span></span><br><span class="line"><span class="built_in">type</span>: bootstrap.kubernetes.io/token</span><br><span class="line">stringData:</span><br><span class="line">  <span class="comment"># Human readable description. Optional.</span></span><br><span class="line">  description: <span class="string">"The default bootstrap token."</span></span><br><span class="line">  <span class="comment"># Token ID and secret. Required.</span></span><br><span class="line">  token-id: <span class="variable">$&#123;BOOTSTRAP_TOKEN_ID&#125;</span></span><br><span class="line">  token-secret: <span class="variable">$&#123;BOOTSTRAP_TOKEN_SECRET&#125;</span></span><br><span class="line">  <span class="comment"># Expiration. Optional.</span></span><br><span class="line">  expiration: $(date -d<span class="string">'+2 day'</span> -u +<span class="string">"%Y-%m-%dT%H:%M:%SZ"</span>)</span><br><span class="line">  <span class="comment"># Allowed usages.</span></span><br><span class="line">  usage-bootstrap-authentication: <span class="string">"true"</span></span><br><span class="line">  usage-bootstrap-signing: <span class="string">"true"</span></span><br><span class="line">  <span class="comment"># Extra groups to authenticate the token as. Must start with "system:bootstrappers:"</span></span><br><span class="line"><span class="comment">#  auth-extra-groups: system:bootstrappers:worker,system:bootstrappers:ingress</span></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h4 id="4-3、复制文件"><a href="#4-3、复制文件" class="headerlink" title="4.3、复制文件"></a>4.3、复制文件</h4><p>把刚生成的文件复制到<code>/etc/kubernetes</code>目录下面</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># master 节点</span></span><br><span class="line"><span class="comment"># cp audit-policy.yaml bootstrap.kubeconfig  bootstrap.secret.yaml kube-proxy.kubeconfig  kube-scheduler.kubeconfig /etc/kubernetes</span></span><br><span class="line"><span class="comment"># scp -r audit-policy.yaml bootstrap.kubeconfig  bootstrap.secret.yaml kube-proxy.kubeconfig  kube-scheduler.kubeconfig k8s-master-02:/etc/kubernetes</span></span><br><span class="line"><span class="comment"># scp -r audit-policy.yaml bootstrap.kubeconfig  bootstrap.secret.yaml kube-proxy.kubeconfig  kube-scheduler.kubeconfig k8s-master-03:/etc/kubernetes</span></span><br><span class="line"><span class="comment"># node 节点</span></span><br><span class="line"><span class="comment"># scp -r  bootstrap.kubeconfig kube-proxy.kubeconfig node-01:/etc/kubernetes</span></span><br><span class="line"><span class="comment"># scp -r  bootstrap.kubeconfig kube-proxy.kubeconfig node-02:/etc/kubernetes</span></span><br></pre></td></tr></table></figure><h4 id="4-4、处理-ipvs-及依赖"><a href="#4-4、处理-ipvs-及依赖" class="headerlink" title="4.4、处理 ipvs 及依赖"></a>4.4、处理 ipvs 及依赖</h4><p>&nbsp;&nbsp;&nbsp;&nbsp; 新版本目前 kube-proxy 组件全部采用 ipvs 方式负载，所以为了 kube-proxy 能正常工作需要预先处理一下 ipvs 配置以及相关依赖(每台 node 都要处理)</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat &gt;&gt; /etc/sysctl.conf &lt;&lt;EOF</span></span><br><span class="line">net.ipv4.ip_forward=1</span><br><span class="line">net.bridge.bridge-nf-call-iptables=1</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables=1</span><br><span class="line">EOF</span><br><span class="line"><span class="comment"># sysctl -p</span></span><br></pre></td></tr></table></figure><p>kubernetes 中启用 ipvs,详细介绍，<a href="https://github.com/kubernetes/kubernetes/tree/master/pkg/proxy/ipvs" target="_blank" rel="noopener">官方</a>,<a href="https://juejin.im/entry/5b7e409ce51d4538b35c03df" target="_blank" rel="noopener">参考文献</a></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># yum -y install ipvsadm</span></span><br><span class="line"><span class="comment"># cat &gt;&gt; /etc/modules &lt;&lt;EOF</span></span><br><span class="line">ip_vs</span><br><span class="line">ip_vs_lc</span><br><span class="line">ip_vs_wlc</span><br><span class="line">ip_vs_rr</span><br><span class="line">ip_vs_wrr</span><br><span class="line">ip_vs_lblc</span><br><span class="line">ip_vs_lblcr</span><br><span class="line">ip_vs_dh</span><br><span class="line">ip_vs_sh</span><br><span class="line">ip_vs_fo</span><br><span class="line">ip_vs_nq</span><br><span class="line">ip_vs_sed</span><br><span class="line">ip_vs_ftp</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h2 id="5、配置和启动kube-apiserver"><a href="#5、配置和启动kube-apiserver" class="headerlink" title="5、配置和启动kube-apiserver"></a>5、配置和启动kube-apiserver</h2><h3 id="5-1、设置启动文件"><a href="#5-1、设置启动文件" class="headerlink" title="5.1、设置启动文件"></a>5.1、设置启动文件</h3><ul><li><strong>kube-apiserver.service</strong><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat /usr/lib/systemd/system/kube-apiserver.service</span></span><br><span class="line">[Unit]</span><br><span class="line">  Description=Kubernetes API Service</span><br><span class="line">  Documentation=https://github.com/GoogleCloudPlatform/kubernetes</span><br><span class="line">  After=network.target</span><br><span class="line">  After=etcd.service</span><br><span class="line">[Service]</span><br><span class="line">  EnvironmentFile=-/etc/kubernetes/apiserver</span><br><span class="line">  ExecStart=/usr/bin/kube-apiserver \</span><br><span class="line">          <span class="variable">$KUBE_LOGTOSTDERR</span> \</span><br><span class="line">          <span class="variable">$KUBE_LOG_LEVEL</span> \</span><br><span class="line">          <span class="variable">$KUBE_ETCD_SERVERS</span> \</span><br><span class="line">          <span class="variable">$KUBE_API_ADDRESS</span> \</span><br><span class="line">          <span class="variable">$KUBE_API_PORT</span> \</span><br><span class="line">          <span class="variable">$KUBELET_PORT</span> \</span><br><span class="line">          <span class="variable">$KUBE_ALLOW_PRIV</span> \</span><br><span class="line">          <span class="variable">$KUBE_SERVICE_ADDRESSES</span> \</span><br><span class="line">          <span class="variable">$KUBE_ADMISSION_CONTROL</span> \</span><br><span class="line">          <span class="variable">$KUBE_API_ARGS</span></span><br><span class="line">  Restart=on-failure</span><br><span class="line">  Type=notify</span><br><span class="line">  LimitNOFILE=65536</span><br><span class="line">[Install]</span><br></pre></td></tr></table></figure></li></ul><h3 id="5-2、apiserver配置文件"><a href="#5-2、apiserver配置文件" class="headerlink" title="5.2、apiserver配置文件"></a>5.2、apiserver配置文件</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat /etc/kubernetes/apiserver</span></span><br><span class="line"><span class="comment">###</span></span><br><span class="line"><span class="comment"># kubernetes system config</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The following values are used to configure the kube-apiserver</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The address on the local server to listen to.</span></span><br><span class="line">KUBE_API_ADDRESS=<span class="string">"--advertise-address=172.21.17.4 --bind-address=0.0.0.0"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The port on the local server to listen on.</span></span><br><span class="line">KUBE_API_PORT=<span class="string">"--secure-port=6443"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Port minions listen on</span></span><br><span class="line"><span class="comment"># KUBELET_PORT="--kubelet-port=10250"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Comma separated list of nodes in the etcd cluster</span></span><br><span class="line">KUBE_ETCD_SERVERS=<span class="string">"--etcd-servers=https://172.21.17.4:2379,https://172.21.16.230:2379,https://172.21.16.240:2379"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Address range to use for services</span></span><br><span class="line">KUBE_SERVICE_ADDRESSES=<span class="string">"--service-cluster-ip-range=10.254.0.0/16"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># default admission control policies</span></span><br><span class="line">KUBE_ADMISSION_CONTROL=<span class="string">"--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,Priority,ResourceQuota"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Add your own!</span></span><br><span class="line">KUBE_API_ARGS=<span class="string">" --allow-privileged=true \</span></span><br><span class="line"><span class="string">                --anonymous-auth=false \</span></span><br><span class="line"><span class="string">                --alsologtostderr \</span></span><br><span class="line"><span class="string">                --apiserver-count=3 \</span></span><br><span class="line"><span class="string">                --audit-log-maxage=30 \</span></span><br><span class="line"><span class="string">                --audit-log-maxbackup=3 \</span></span><br><span class="line"><span class="string">                --audit-log-maxsize=100 \</span></span><br><span class="line"><span class="string">                --audit-log-path=/var/log/kube-audit/audit.log \</span></span><br><span class="line"><span class="string">                --audit-policy-file=/etc/kubernetes/audit-policy.yaml \</span></span><br><span class="line"><span class="string">                --authorization-mode=Node,RBAC \</span></span><br><span class="line"><span class="string">                --client-ca-file=/etc/kubernetes/ssl/kubernetes-ca.pem \</span></span><br><span class="line"><span class="string">                --enable-bootstrap-token-auth \</span></span><br><span class="line"><span class="string">                --enable-garbage-collector \</span></span><br><span class="line"><span class="string">                --enable-logs-handler \</span></span><br><span class="line"><span class="string">                --endpoint-reconciler-type=lease \</span></span><br><span class="line"><span class="string">                --etcd-cafile=/etc/etcd/ssl/etcd-ca.pem \</span></span><br><span class="line"><span class="string">                --etcd-certfile=/etc/etcd/ssl/etcd.pem \</span></span><br><span class="line"><span class="string">                --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem \</span></span><br><span class="line"><span class="string">                --etcd-compaction-interval=0s \</span></span><br><span class="line"><span class="string">                --event-ttl=168h0m0s \</span></span><br><span class="line"><span class="string">                --kubelet-https=true \</span></span><br><span class="line"><span class="string">                --kubelet-certificate-authority=/etc/kubernetes/ssl/kubernetes-ca.pem \</span></span><br><span class="line"><span class="string">                --kubelet-client-certificate=/etc/kubernetes/ssl/kubelet-api-admin.pem \</span></span><br><span class="line"><span class="string">                --kubelet-client-key=/etc/kubernetes/ssl/kubelet-api-admin-key.pem \</span></span><br><span class="line"><span class="string">                --kubelet-timeout=3s \</span></span><br><span class="line"><span class="string">                --runtime-config=api/all=true \</span></span><br><span class="line"><span class="string">                --service-node-port-range=30000-50000 \</span></span><br><span class="line"><span class="string">                --service-account-key-file=/etc/kubernetes/ssl/kubernetes-ca.pem \</span></span><br><span class="line"><span class="string">                --tls-cert-file=/etc/kubernetes/ssl/kube-apiserver.pem \</span></span><br><span class="line"><span class="string">                --tls-private-key-file=/etc/kubernetes/ssl/kube-apiserver-key.pem \</span></span><br><span class="line"><span class="string">                --v=2"</span></span><br></pre></td></tr></table></figure><ul><li><strong>–client-ca-file</strong>: 定义客户端 CA</li><li><strong>–endpoint-reconciler-type</strong>: master endpoint 策略</li><li><strong>–kubelet-client-certificate、–kubelet-client-key</strong>: master 反向连接 kubelet 使用的证书</li><li><strong>–service-account-key-file</strong>: service account 签名 key(用于有效性验证)</li><li><strong>–tls-cert-file、–tls-private-key-file</strong>: master apiserver 6443 端口证书<br>详细参数<a href="https://www.jianshu.com/p/36ad3028a710" target="_blank" rel="noopener">介绍</a></li></ul><h3 id="5-2-1、启动kube-apiserver"><a href="#5-2-1、启动kube-apiserver" class="headerlink" title="5.2.1、启动kube-apiserver"></a>5.2.1、启动kube-apiserver</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># systemctl daemon-reload</span></span><br><span class="line"><span class="comment"># systemctl enable kube-apiserver &amp;&amp;systemctl start kube-apiserver &amp;&amp;systemctl status kube-apiserver</span></span><br></pre></td></tr></table></figure><h3 id="5-3、配置kube-controller-manager"><a href="#5-3、配置kube-controller-manager" class="headerlink" title="5.3、配置kube-controller-manager"></a>5.3、配置kube-controller-manager</h3><p>创建kube-controller-manager的service配置文件</p><h3 id="5-3-1、配置kube-controller-manager启动文件"><a href="#5-3-1、配置kube-controller-manager启动文件" class="headerlink" title="5.3.1、配置kube-controller-manager启动文件"></a>5.3.1、配置kube-controller-manager启动文件</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat /usr/lib/systemd/system/kube-controller-manager.service</span></span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes Controller Manager</span><br><span class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">EnvironmentFile=-/etc/kubernetes/controller-manager</span><br><span class="line">ExecStart=/usr/bin/kube-controller-manager \</span><br><span class="line">	    <span class="variable">$KUBE_LOGTOSTDERR</span> \</span><br><span class="line">	    <span class="variable">$KUBE_LOG_LEVEL</span> \</span><br><span class="line">	    <span class="variable">$KUBE_MASTER</span> \</span><br><span class="line">	    <span class="variable">$KUBE_CONTROLLER_MANAGER_ARGS</span></span><br><span class="line">Restart=on-failure</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><h3 id="5-3-2、配置controller-manager文件"><a href="#5-3-2、配置controller-manager文件" class="headerlink" title="5.3.2、配置controller-manager文件"></a>5.3.2、配置controller-manager文件</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat /etc/kubernetes/controller-manager</span></span><br><span class="line"><span class="comment">###</span></span><br><span class="line"><span class="comment"># The following values are used to configure the kubernetes controller-manager</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># defaults from config and apiserver should be adequate</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Add your own!</span></span><br><span class="line">KUBE_CONTROLLER_MANAGER_ARGS=<span class="string">"  --address=0.0.0.0 \</span></span><br><span class="line"><span class="string">                                --authentication-kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \</span></span><br><span class="line"><span class="string">                                --authorization-kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \</span></span><br><span class="line"><span class="string">                                --bind-address=0.0.0.0 \</span></span><br><span class="line"><span class="string">                                --cluster-name=kubernetes \</span></span><br><span class="line"><span class="string">                                --cluster-signing-cert-file=/etc/kubernetes/ssl/kubernetes-ca.pem \</span></span><br><span class="line"><span class="string">                                --cluster-signing-key-file=/etc/kubernetes/ssl/kubernetes-ca-key.pem \</span></span><br><span class="line"><span class="string">                                --client-ca-file=/etc/kubernetes/ssl/kubernetes-ca.pem \</span></span><br><span class="line"><span class="string">                                --controllers=*,bootstrapsigner,tokencleaner \</span></span><br><span class="line"><span class="string">                                --deployment-controller-sync-period=10s \</span></span><br><span class="line"><span class="string">                                --experimental-cluster-signing-duration=87600h0m0s \</span></span><br><span class="line"><span class="string">                                --enable-garbage-collector=true \</span></span><br><span class="line"><span class="string">                                --kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \</span></span><br><span class="line"><span class="string">                                --leader-elect=true \</span></span><br><span class="line"><span class="string">                                --node-monitor-grace-period=20s \</span></span><br><span class="line"><span class="string">                                --node-monitor-period=5s \</span></span><br><span class="line"><span class="string">                                --port=10252 \</span></span><br><span class="line"><span class="string">                                --pod-eviction-timeout=2m0s \</span></span><br><span class="line"><span class="string">                                --requestheader-client-ca-file=/etc/kubernetes/ssl/kubernetes-ca.pem \</span></span><br><span class="line"><span class="string">                                --terminated-pod-gc-threshold=50 \</span></span><br><span class="line"><span class="string">                                --tls-cert-file=/etc/kubernetes/ssl/kube-controller-manager.pem \</span></span><br><span class="line"><span class="string">                                --tls-private-key-file=/etc/kubernetes/ssl/kube-controller-manager-key.pem \</span></span><br><span class="line"><span class="string">                                --root-ca-file=/etc/kubernetes/ssl/kubernetes-ca.pem \</span></span><br><span class="line"><span class="string">                                --secure-port=10257 \</span></span><br><span class="line"><span class="string">                                --service-cluster-ip-range=10.254.0.0/16 \</span></span><br><span class="line"><span class="string">                                --service-account-private-key-file=/etc/kubernetes/ssl/kubernetes-ca-key.pem \</span></span><br><span class="line"><span class="string">                                --use-service-account-credentials=true \</span></span><br><span class="line"><span class="string">                                --v=2"</span></span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;controller manager 将不安全端口 10252 绑定到 127.0.0.1 确保 kuebctl get cs 有正确返回；将安全端口 10257 绑定到 0.0.0.0 公开，提供服务调用；由于 controller manager 开始连接 apiserver 的 6443 认证端口，所以需要 –use-service-account-credentials 选项来让 controller manager 创建单独的 service account(默认 system:kube-controller-manager 用户没有那么高权限)</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get componentstatuses</span></span><br><span class="line">NAME                 STATUS      MESSAGE                                                                                     ERROR</span><br><span class="line">controller-manager   Unhealthy   Get http://127.0.0.1:10252/healthz: dial tcp 127.0.0.1:10252: connect: connection refused   </span><br><span class="line">scheduler            Unhealthy   Get http://127.0.0.1:10251/healthz: dial tcp 127.0.0.1:10251: connect: connection refused   </span><br><span class="line">etcd-0               Healthy     &#123;<span class="string">"health"</span>:<span class="string">"true"</span>&#125;                                                                           </span><br><span class="line">etcd-1               Healthy     &#123;<span class="string">"health"</span>:<span class="string">"true"</span>&#125;                                                                           </span><br><span class="line">etcd-2               Healthy     &#123;<span class="string">"health"</span>:<span class="string">"true"</span>&#125;</span><br></pre></td></tr></table></figure><h4 id="5-3-3、启动kube-controller-manager"><a href="#5-3-3、启动kube-controller-manager" class="headerlink" title="5.3.3、启动kube-controller-manager"></a>5.3.3、启动kube-controller-manager</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># systemctl daemon-reload</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># systemctl enable kube-controller-manager &amp;&amp;systemctl start kube-controller-manager &amp;&amp;systemctl status kube-controller-manager</span></span><br></pre></td></tr></table></figure><h3 id="5-4、配置kube-scheduler"><a href="#5-4、配置kube-scheduler" class="headerlink" title="5.4、配置kube-scheduler"></a>5.4、配置kube-scheduler</h3><p>创建kube-scheduler的service配置文件</p><h4 id="5-4-1、创建kube-scheduler启动文件"><a href="#5-4-1、创建kube-scheduler启动文件" class="headerlink" title="5.4.1、创建kube-scheduler启动文件"></a>5.4.1、创建kube-scheduler启动文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat /lib/systemd/system/kube-scheduler.service</span></span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes Scheduler Plugin</span><br><span class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">EnvironmentFile=-/etc/kubernetes/scheduler</span><br><span class="line">ExecStart=/usr/bin/kube-scheduler \</span><br><span class="line">	    <span class="variable">$KUBE_LOGTOSTDERR</span> \</span><br><span class="line">	    <span class="variable">$KUBE_LOG_LEVEL</span> \</span><br><span class="line">	    <span class="variable">$KUBE_MASTER</span> \</span><br><span class="line">	    <span class="variable">$KUBE_SCHEDULER_ARGS</span></span><br><span class="line">Restart=on-failure</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><h4 id="5-4-2、创建scheduler配置文件"><a href="#5-4-2、创建scheduler配置文件" class="headerlink" title="5.4.2、创建scheduler配置文件"></a>5.4.2、创建scheduler配置文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat /etc/kubernetes/scheduler </span></span><br><span class="line"><span class="comment">###</span></span><br><span class="line"><span class="comment"># kubernetes scheduler config</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># default config should be adequate</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Add your own!</span></span><br><span class="line">KUBE_SCHEDULER_ARGS=<span class="string">"   --address=0.0.0.0 \</span></span><br><span class="line"><span class="string">                        --authentication-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \</span></span><br><span class="line"><span class="string">                        --authorization-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \</span></span><br><span class="line"><span class="string">                        --bind-address=0.0.0.0 \</span></span><br><span class="line"><span class="string">                        --client-ca-file=/etc/kubernetes/ssl/kubernetes-ca.pem \</span></span><br><span class="line"><span class="string">                        --kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \</span></span><br><span class="line"><span class="string">                        --requestheader-client-ca-file=/etc/kubernetes/ssl/kubernetes-ca.pem \</span></span><br><span class="line"><span class="string">                        --secure-port=10259 \</span></span><br><span class="line"><span class="string">                        --leader-elect=true \</span></span><br><span class="line"><span class="string">                        --port=10251 \</span></span><br><span class="line"><span class="string">                        --tls-cert-file=/etc/kubernetes/ssl/kube-scheduler.pem \</span></span><br><span class="line"><span class="string">                        --tls-private-key-file=/etc/kubernetes/ssl/kube-scheduler-key.pem \</span></span><br><span class="line"><span class="string">                        --v=2"</span></span><br></pre></td></tr></table></figure><p>shceduler 同 controller manager 一样将不安全端口绑定在本地，安全端口对外公开</p><h4 id="5-4-3、启动kube-scheduler"><a href="#5-4-3、启动kube-scheduler" class="headerlink" title="5.4.3、启动kube-scheduler"></a>5.4.3、启动kube-scheduler</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># systemctl daemon-reload</span></span><br><span class="line"><span class="comment"># systemctl enable kube-scheduler &amp;&amp;systemctl start kube-scheduler &amp;&amp;systemctl status kube-scheduler</span></span><br></pre></td></tr></table></figure><h3 id="5-4、验证master节点"><a href="#5-4、验证master节点" class="headerlink" title="5.4、验证master节点"></a>5.4、验证master节点</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kubectl get componentstatuses</span></span><br><span class="line">NAME                 STATUS    MESSAGE             ERROR</span><br><span class="line">scheduler            Healthy   ok                  </span><br><span class="line">controller-manager   Healthy   ok                  </span><br><span class="line">etcd-2               Healthy   &#123;<span class="string">"health"</span>:<span class="string">"true"</span>&#125;   </span><br><span class="line">etcd-0               Healthy   &#123;<span class="string">"health"</span>:<span class="string">"true"</span>&#125;   </span><br><span class="line">etcd-1               Healthy   &#123;<span class="string">"health"</span>:<span class="string">"true"</span>&#125;</span><br></pre></td></tr></table></figure><p>至此master节点部署完毕</p><p>kubernetes高可用使用haproxy进行代理,<a href="https://xxlaila.github.io/2019/08/10/haproxy-keepalived/" target="_blank" rel="noopener">haproxy</a>代理安装</p><p><a href="https://xxlaila.github.io/2019/08/10/kubernetes-node%E8%8A%82%E7%82%B9%E5%AE%89%E8%A3%85/" target="_blank" rel="noopener">node节点安装</a></p><!-- rebuild by neat -->]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes v1.13.3</tag>
      </tags>
  </entry>
  <entry>
    <title>vsftpd安装</title>
    <url>/2019/08/09/vsftpd%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><p>&nbsp;&nbsp;&nbsp;&nbsp;Centos下ftp的安装一般采用的是vsftpd，但是在ftp的模式中又有几个用户配置项需要注意，有些人喜欢用本地用户去登陆FTP，虽然在建立本地用户的时候加了/sbin/nologin参数，但是这个还是不够安全，而且这样权限控制也不是很好，他们都是统一的控制权限，这里采用虚拟用户前来配置。虚拟用户配合防火墙selinux还有单个用户的权限，这使得FTP有着足够的安全。而且权限控制特别灵活，修改一个用户的权限不会影响到其他用户。<br>centos 系统版本(5.5、5.3、6.0、6.5)<br>centos 7.4 已经验证</p><h3 id="首先我们安装vsftpd"><a href="#首先我们安装vsftpd" class="headerlink" title="首先我们安装vsftpd"></a>首先我们安装vsftpd</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@RAID1 ~]<span class="comment"># yum –y install vsftpd</span></span><br></pre></td></tr></table></figure><h3 id="2、启动和加载vsftp"><a href="#2、启动和加载vsftp" class="headerlink" title="2、启动和加载vsftp"></a>2、启动和加载vsftp</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@RAID1 ~]<span class="comment"># service vsftpd restart</span></span><br><span class="line">[root@RAID1 ~]<span class="comment"># chkconfig –level 35 vsftpd on</span></span><br></pre></td></tr></table></figure><h3 id="3、开始配置vsftpd"><a href="#3、开始配置vsftpd" class="headerlink" title="3、开始配置vsftpd"></a>3、开始配置vsftpd</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;Vsftpd的配置文件在/etc/vsftpd下面，在配置之前我们先cp一份做备份用以免发生意外(做什么都要随手备份，因为没有一万，只有万一。)</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@RAID1 ~]<span class="comment">#  cp /etc/vsftpd/vsftpd.conf /etc/vsftpd/vsftpd.conf.bak</span></span><br><span class="line">[root@RAID1 ~]<span class="comment"># vim /etc/vsftpd/vsftpd.conf</span></span><br></pre></td></tr></table></figure><ul><li><strong>vsftpd的参数介绍</strong></li></ul><a id="more"></a><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">reverse_lookup_enable=NO <span class="comment">#添加此行，解决客户端登陆缓慢问题！重要！默认vsftpd开启了DNS反响解析！这里需要关闭，如果启动有错误，请注销！</span></span><br><span class="line">listen_port=21 <span class="comment">#默认无此行，ftp端口为21，添加listen_port=2222把默认端口修改为2222，注意：防火墙同时要开启2222端口</span></span><br><span class="line">anonymous_enable=NO　　 <span class="comment">#禁止匿名用户</span></span><br><span class="line">local_enable=YES</span><br><span class="line">设定本地用户可以访问。注意：主要是为虚拟宿主用户，如果该项目设定为NO那么所有虚拟用户将无法访问</span><br><span class="line">write_enable=YES <span class="comment">#全局设置，是否容许写入（无论是匿名用户还是本地用户，若要启用上传权限的话，就要开启他）</span></span><br><span class="line">local_umask=022 设定上传后文件的权限掩码。</span><br><span class="line">anon_upload_enable=NO 禁止匿名用户上传。</span><br><span class="line">anon_mkdir_write_enable=NO 禁止匿名用户建立目录。</span><br><span class="line">dirmessage_enable=YES 设定开启目录标语功能。</span><br><span class="line">xferlog_enable=YES 设定开启日志记录功能。</span><br><span class="line">connect_from_port_20=YES 设定端口20进行数据连接。</span><br><span class="line">chown_uploads=NO 设定禁止上传文件更改宿主。</span><br><span class="line">xferlog_file=/var/<span class="built_in">log</span>/vsftpd.log 日志保存路径（先创建好文件）</span><br><span class="line">xferlog_std_format=YES　　 <span class="comment">#使用标准格式</span></span><br><span class="line">async_abor_enable=YES 设定支持异步传输功能。</span><br><span class="line">ascii_upload_enable=YES</span><br><span class="line">ascii_download_enable=YES 设定支持ASCII模式的上传和下载功能。</span><br><span class="line">ftpd_banner=Welcome to Awei FTP servers 设定Vsftpd的登陆标语。</span><br><span class="line">chroot_local_user=YES 禁止本地用户登出自己的FTP主目录。</span><br><span class="line">pam_service_name=vsftpd 设定PAM服务下Vsftpd的验证配置文件名。因此，PAM验证将参考/etc/pam.d/下的vsftpd文件配置。</span><br><span class="line">userlist_enable=YES 设为YES的时候，如果一个用户名是在userlist_file参数指定的文件中，</span><br><span class="line"> 那么在要求他们输入密码之前，会直接拒绝他们登陆。</span><br><span class="line">tcp_wrappers=YES 是否支持tcp_wrappers</span><br></pre></td></tr></table></figure><ul><li><strong>以下是我使用的参数,使用的是被动模式</strong><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">anonymous_enable=No</span><br><span class="line">listen_port=21</span><br><span class="line">local_enable=YES</span><br><span class="line">write_enable=YES</span><br><span class="line">local_umask=022</span><br><span class="line">dirmessage_enable=YES</span><br><span class="line">xferlog_enable=YES</span><br><span class="line">connect_from_port_20=YES</span><br><span class="line"><span class="comment">#chown_uploads=YES</span></span><br><span class="line">xferlog_file=/var/<span class="built_in">log</span>/vsftpd.log</span><br><span class="line">xferlog_std_format=YES</span><br><span class="line">async_abor_enable=YES</span><br><span class="line">ascii_upload_enable=YES</span><br><span class="line">ascii_download_enable=YES</span><br><span class="line">ftpd_banner=Welcome to blah FTP service.</span><br><span class="line">chroot_list_enable=YES</span><br><span class="line">listen=YES</span><br><span class="line">pam_service_name=vsftpd</span><br><span class="line">userlist_enable=YES</span><br><span class="line">tcp_wrappers=YES</span><br><span class="line">reverse_lookup_enable=No</span><br><span class="line">guest_enable=YES</span><br><span class="line">guest_username=vsftpd</span><br><span class="line">user_config_dir=/etc/vsftpd/vconf</span><br><span class="line">virtual_use_local_privs=YES</span><br><span class="line">pasv_min_port=9000</span><br><span class="line">pasv_max_port=9045</span><br><span class="line">chroot_local_user=YES</span><br><span class="line">chroot_list_enable=NO</span><br><span class="line">allow_writeable_chroot=YES</span><br><span class="line"><span class="comment">#port_enable=YES</span></span><br><span class="line"><span class="comment">#connect_from_port_20=YES</span></span><br><span class="line">pasv_enable=yes</span><br></pre></td></tr></table></figure></li></ul><ul><li>备注: 这里vsftp采用的被动模式，被动模式开放了一个端口段，公司路由器上需要开放这一个端口端，<a href="https://xxlaila.github.io/2019/09/10/路由器端口映射/" target="_blank" rel="noopener">路由器端口映射</a></li></ul><h3 id="4、建立虚拟用户名单文件"><a href="#4、建立虚拟用户名单文件" class="headerlink" title="4、建立虚拟用户名单文件"></a>4、建立虚拟用户名单文件</h3><p>编辑虚拟用户的名单：（第一行用户名。第二行密码。不能使用root）</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@RAID1 ~]<span class="comment"># vim /etc/vsftpd/xuniusers</span></span><br><span class="line"><span class="built_in">test</span></span><br><span class="line">23123213</span><br><span class="line">test1</span><br><span class="line">34dsfds</span><br><span class="line">test2</span><br><span class="line">df43sd</span><br></pre></td></tr></table></figure><h3 id="5、开始建立生成虚拟用户数据文件"><a href="#5、开始建立生成虚拟用户数据文件" class="headerlink" title="5、开始建立生成虚拟用户数据文件"></a>5、开始建立生成虚拟用户数据文件</h3><p>这里需要安装db4,设置PAM文件权限，并制定虚拟用户数据库文件读取</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@RAID1 ~]<span class="comment"># yum –y install db4-utils</span></span><br><span class="line">[root@RAID1 ~]<span class="comment"># db_load -T -t hash -f /etc/vsftpd/xuniusers /etc/vsftpd/xuniusers.db</span></span><br><span class="line">[root@RAID1 ~]<span class="comment"># chmod 600 /etc/vsftpd/xuniusers.db</span></span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;在/etc/pam.d/vsftpd的文件头部加入以下信息（注*这里一定要在前面，不能再后面，刚开始我也加载到后面登陆的时候提示错误）,修改前先备份</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@RAID1 ~]<span class="comment"># cp /etc/pam.d/vsftpd /etc/pam.d/vsftpd.bak</span></span><br><span class="line">[root@RAID1 ~]<span class="comment"># vi /etc/pam.d/vsftpd</span></span><br><span class="line">auth sufficient /lib/security/pam_userdb.so db=/etc/vsftpd/xuniusers</span><br><span class="line">account sufficient /lib/security/pam_userdb.so db=/etc/vsftpd/xuniusers</span><br></pre></td></tr></table></figure><p><img src="https://img.xxlaila.cn/%E5%9B%BE%E7%89%87%201.png" alt="img"></p><ul><li>注*64位的操作系统，则上面lib改为64。不然配置也会无效</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">auth sufficient /lib64/security/pam_userdb.so db=/etc/vsftpd/xuniusers</span><br><span class="line">account sufficient /lib64/security/pam_userdb.so db=/etc/vsftpd/xuniusers</span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;建立一个系统用户vsftpd，用户的主目录可以自己设置，/home/wwwroot，设置用户登陆的终端为/bin/false</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@RAID1 ~]<span class="comment"># useradd vsftpd -d /home/wwwroot -s /bin/false</span></span><br><span class="line">[root@RAID1 ~]<span class="comment"># chown vsftpd:vsftpd /home/wwwroot -R</span></span><br><span class="line">[root@RAID1 ~]<span class="comment"># chown www:www /home/wwwroot –R #如果虚拟用户的宿主用户为nginx，需要这样设置。</span></span><br></pre></td></tr></table></figure><h3 id="6、建立虚拟用户个人vsftp的配置文件"><a href="#6、建立虚拟用户个人vsftp的配置文件" class="headerlink" title="6、建立虚拟用户个人vsftp的配置文件"></a>6、建立虚拟用户个人vsftp的配置文件</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@RAID1 ~]<span class="comment"># mkdir /etc/vsftpd/vconf</span></span><br><span class="line">[root@RAID1 ~]<span class="comment"># cd /etc/vsftpd/vconf</span></span><br><span class="line">touch test1 test2 test3 <span class="comment">#这里创建三个虚拟用户配置文件</span></span><br><span class="line">vi web1 <span class="comment">#编辑用户test1配置文件，其他的跟这个配置文件类似</span></span><br><span class="line">[root@RAID1 ~]<span class="comment"># vim test1</span></span><br><span class="line">local_root=/home/wwwroot/test1/</span><br><span class="line">write_enable=YES</span><br><span class="line">anon_umask=022</span><br><span class="line">anon_world_readable_only=NO</span><br><span class="line">anon_upload_enable=YES</span><br><span class="line">anon_mkdir_write_enable=YES</span><br><span class="line">anon_other_write_enable=YES</span><br><span class="line"></span><br><span class="line"><span class="comment"># chmod u-w /home/wwwroot</span></span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;&nbsp;&nbsp;最后重启vsftpd服务,不关闭Selinux可以执行以下命令通过FTP。防火墙开放端口<code>setsebool -P ftpd_disable_trans 1</code><br>&nbsp;&nbsp;&nbsp;&nbsp;上述配置完成后还可以通过#adduser -d /目录路径 -g vsftpd -s /sbin/nologin 用户名 这个命令来添加一个用户，不需要配置任何权限都可以进行FTP的访问,最后补充说明，需要安装的其他插件</p><ul><li><p>需要安装的的是pan</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@RAID1 ~]<span class="comment"># yum install -y pam</span></span><br></pre></td></tr></table></figure></li><li><p>这里我们还可以查看日志，可以根据提示的提示来判断。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@RAID1 ~]<span class="comment"># cat /var/log/secure</span></span><br></pre></td></tr></table></figure></li></ul><p>测试用户登录虚拟用户只能看到自己本身的目录 ，不能去其他目录查看(到这里vsftpd配置结束)</p><!-- rebuild by neat -->]]></content>
      <categories>
        <category>Centos</category>
      </categories>
      <tags>
        <tag>vsftpd</tag>
      </tags>
  </entry>
  <entry>
    <title>TeamViewer mac破解</title>
    <url>/2019/08/09/TeamViewer-mac%E7%A0%B4%E8%A7%A3/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><h2 id="TeamViewer14-4-MAC破解"><a href="#TeamViewer14-4-MAC破解" class="headerlink" title="TeamViewer14.4 MAC破解"></a>TeamViewer14.4 MAC破解</h2><h3 id="在终端执行以下命令"><a href="#在终端执行以下命令" class="headerlink" title="在终端执行以下命令"></a>在终端执行以下命令</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo python TeamViewer-id-changer.py</span><br><span class="line">使用mac自带python2.7 执行即可</span><br></pre></td></tr></table></figure><a id="more"></a><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ vim TeamViewer-id-changer.py</span><br><span class="line"></span><br><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time    : 2019/8/1 14:57</span></span><br><span class="line"><span class="comment"># @Author  : xxlaila</span></span><br><span class="line"><span class="comment"># @Site    : </span></span><br><span class="line"><span class="comment"># @File    : TeamViewer-id-changer.py</span></span><br><span class="line"><span class="comment"># @Software: PyCharm</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line">import platform</span><br><span class="line">import random</span><br><span class="line">import re</span><br><span class="line">import string</span><br><span class="line">import sys</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">''</span><span class="string">'</span></span><br><span class="line"><span class="string">--------------------------------</span></span><br><span class="line"><span class="string">TeamViewer 14 ID Changer for MAC OS</span></span><br><span class="line"><span class="string">Version: 0.2 2019</span></span><br><span class="line"><span class="string">--------------------------------</span></span><br><span class="line"><span class="string">'</span><span class="string">''</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> platform.system() != <span class="string">"Darwin"</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"This script can be run only on MAC OS."</span>)</span><br><span class="line">    sys.exit()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> os.geteuid() != 0:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"This script must be run form root."</span>)</span><br><span class="line">    sys.exit()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="string">"SUDO_USER"</span> <span class="keyword">in</span> os.environ:</span><br><span class="line">    USERNAME = os.environ[<span class="string">"SUDO_USER"</span>]</span><br><span class="line">    <span class="keyword">if</span> USERNAME == <span class="string">"root"</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"Can not find user name. Run this script via sudo from regular user"</span>)</span><br><span class="line">        sys.exit()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"Can not find user name. Run this script via sudo from regular user"</span>)</span><br><span class="line">    sys.exit()</span><br><span class="line"></span><br><span class="line">HOMEDIRLIB = <span class="string">"/Users/"</span> + USERNAME + <span class="string">"/library/preferences/"</span></span><br><span class="line">GLOBALLIB = <span class="string">"/library/preferences/"</span></span><br><span class="line"></span><br><span class="line">CONFIGS = []</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Find config files</span></span><br><span class="line"></span><br><span class="line">def listdir_fullpath(d):</span><br><span class="line">    <span class="built_in">return</span> [os.path.join(d, f) <span class="keyword">for</span> f <span class="keyword">in</span> os.listdir(d)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> listdir_fullpath(HOMEDIRLIB):</span><br><span class="line">    <span class="keyword">if</span> <span class="string">'teamviewer'</span> <span class="keyword">in</span> file.lower():</span><br><span class="line">        CONFIGS.append(file)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> listdir_fullpath(GLOBALLIB):</span><br><span class="line">    <span class="keyword">if</span> <span class="string">'teamviewer'</span> <span class="keyword">in</span> file.lower():</span><br><span class="line">        CONFIGS.append(file)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> not CONFIGS:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">''</span><span class="string">'</span></span><br><span class="line"><span class="string">            There is no TemViewer configs found.</span></span><br><span class="line"><span class="string">            Maybe you have deleted it manualy or never run TeamViewer after installation.</span></span><br><span class="line"><span class="string">            Nothing to delete.</span></span><br><span class="line"><span class="string">            '</span><span class="string">''</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># Delete config files</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"Configs found:\n"</span>)</span><br><span class="line">    <span class="keyword">for</span> file <span class="keyword">in</span> CONFIGS:</span><br><span class="line">        <span class="built_in">print</span>(file)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">''</span><span class="string">'</span></span><br><span class="line"><span class="string">        This files will be DELETED permanently.</span></span><br><span class="line"><span class="string">        All TeamViewer settings will be lost</span></span><br><span class="line"><span class="string">        '</span><span class="string">''</span>)</span><br><span class="line">        raw_input(<span class="string">"Press Enter to continue or CTR+C to abort..."</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> file <span class="keyword">in</span> CONFIGS:</span><br><span class="line">        try:</span><br><span class="line">            os.remove(file)</span><br><span class="line">        except:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">"Cannot delete config files. Permission denied?"</span>)</span><br><span class="line">            sys.exit()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"Done."</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Find binaryes</span></span><br><span class="line"></span><br><span class="line">TMBINARYES = [</span><br><span class="line">    <span class="string">'/Applications/TeamViewer.app/Contents/MacOS/TeamViewer'</span>,</span><br><span class="line">    <span class="string">'/Applications/TeamViewer.app/Contents/MacOS/TeamViewer_Service'</span>,</span><br><span class="line">    <span class="string">'/Applications/TeamViewer.app/Contents/Helpers/TeamViewer_Desktop'</span>,</span><br><span class="line">    <span class="string">'/Applications/TeamViewer.app/Contents/Helpers/TeamViewer_Assignment'</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> TMBINARYES:</span><br><span class="line">    <span class="keyword">if</span> os.path.exists(file):</span><br><span class="line">        pass</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"File not found: "</span> + file)</span><br><span class="line">        <span class="built_in">print</span> (<span class="string">"Install TeamViewer correctly"</span>)</span><br><span class="line">        sys.exit()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Patch files</span></span><br><span class="line"></span><br><span class="line">def idpatch(fpath, platf, serial):</span><br><span class="line">    file = open(fpath, <span class="string">'r+b'</span>)</span><br><span class="line">    binary = file.read()</span><br><span class="line">    PlatformPattern = <span class="string">"IOPlatformExpert.&#123;6&#125;"</span></span><br><span class="line">    SerialPattern = <span class="string">"IOPlatformSerialNumber%s%s%s"</span></span><br><span class="line"></span><br><span class="line">    binary = re.sub(PlatformPattern, platf, binary)</span><br><span class="line">    binary = re.sub(SerialPattern % (chr(0), <span class="string">"[0-9a-zA-Z]&#123;8,8&#125;"</span>, chr(0)), SerialPattern % (chr(0), serial, chr(0)), binary)</span><br><span class="line"></span><br><span class="line">    file = open(fpath, <span class="string">'wb'</span>).write(binary)</span><br><span class="line">    <span class="built_in">return</span> True</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def random_generator(size=8, chars=string.ascii_uppercase + string.ascii_lowercase + string.digits):</span><br><span class="line">    <span class="built_in">return</span> <span class="string">''</span>.join(random.choice(chars) <span class="keyword">for</span> _ <span class="keyword">in</span> range(size))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">RANDOMSERIAL = random_generator(8)</span><br><span class="line">RANDOMPLATFORM = <span class="string">"IOPlatformExpert"</span> + random_generator(6)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> TMBINARYES:</span><br><span class="line">    try:</span><br><span class="line">        idpatch(file, RANDOMPLATFORM, RANDOMSERIAL)</span><br><span class="line">    except:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"Error: can not patch file "</span> + file)</span><br><span class="line">        sys.exit()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"PlatformDevice: "</span> + RANDOMPLATFORM)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"PlatformSerial: "</span> + RANDOMSERIAL)</span><br><span class="line"></span><br><span class="line">os.system(<span class="string">"sudo codesign -f -s - /Applications/TeamViewer.app/"</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">''</span><span class="string">'</span></span><br><span class="line"><span class="string">ID changed sucessfully.</span></span><br><span class="line"><span class="string">!!! Restart computer before using TeamViewer !!!!</span></span><br><span class="line"><span class="string">'</span><span class="string">''</span>)</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
      <categories>
        <category>TeamViewer</category>
      </categories>
      <tags>
        <tag>TeamViewer</tag>
      </tags>
  </entry>
  <entry>
    <title>jenkins job管理</title>
    <url>/2019/08/09/jenkins-job%E7%AE%A1%E7%90%86/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><h1 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h1><ul><li><strong>介绍</strong>: 由于公司的ci用于编译的环境比较多，为了更好的区分，为每一个环境建立了一个view</li><li><strong>痛点</strong>: 运维人员在建立job的时候需要到对应的view下面建立，虽然这不是狠痛苦，但是还是不太方便。</li><li><strong>解决</strong>: 人员登陆默认是在all view下面，每个运维人员在这下面建立job，然后每个view根据自己的规则吧对应的job添加进来。job规则自己提前定义好</li></ul><h2 id="1、安装jenkins插件"><a href="#1、安装jenkins插件" class="headerlink" title="1、安装jenkins插件"></a>1、安装jenkins插件</h2><p>view job 过滤插件view-job-filters，安装过程不累赘</p><h2 id="2、配置view规则"><a href="#2、配置view规则" class="headerlink" title="2、配置view规则"></a>2、配置view规则</h2><p>这里设置两个前端和一个后端实例</p><a id="more"></a><h3 id="2-1、前端1"><a href="#2-1、前端1" class="headerlink" title="2.1、前端1"></a>2.1、前端1</h3><p><img src="https://img.xxlaila.cn/image2018-5-29_10-47-20.png" alt="img"></p><h3 id="2-2、前端test"><a href="#2-2、前端test" class="headerlink" title="2.2、前端test"></a>2.2、前端test</h3><p>test 我们用安装的这个插件来进行配置,点击Add Job Filter——&gt;会有很多的规则，可以根据不同的状态、栏目来进行却分，这里我们选择</p><p><img src="https://img.xxlaila.cn/image2018-5-29_10-49-46.png" alt="img"><br><img src="https://img.xxlaila.cn/image2018-5-29_10-52-49.png" alt="img"></p><p>&nbsp;&nbsp;&nbsp;&nbsp;这里添加了两条规则，这个是建立job的时候有点特殊性，用第一种方式实现就会有问题，第一条规则是现实所有test类的job，但是吧下面的一条给加进来了，不现实这类job。保持即可</p><h2 id="后端java程序"><a href="#后端java程序" class="headerlink" title="后端java程序"></a>后端java程序</h2><p>dev环境为例子</p><p><img src="https://img.xxlaila.cn/image2018-5-29_10-54-54.png" alt="img"></p><!-- rebuild by neat -->]]></content>
      <categories>
        <category>jenkins</category>
      </categories>
      <tags>
        <tag>jenkins job</tag>
      </tags>
  </entry>
  <entry>
    <title>jenkins用户权限配置</title>
    <url>/2019/08/09/jenkins%E7%94%A8%E6%88%B7%E6%9D%83%E9%99%90%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><h3 id="1、jenkins用户权限"><a href="#1、jenkins用户权限" class="headerlink" title="1、jenkins用户权限"></a>1、jenkins用户权限</h3><ul><li>可以集成gitlab、jenkins专有账户、LDAP、Servlet容器代理、Unix用户/组数据库</li></ul><h3 id="2、授权策略"><a href="#2、授权策略" class="headerlink" title="2、授权策略"></a>2、授权策略</h3><ul><li>Gitlab Commiter Authorization Strategy</li><li>Role-Based Strategy</li><li>任何用户可以做任何事(没有任何限制)</li><li>安全矩阵</li><li>登录用户可以做任何事</li><li>遗留模式</li><li>项目矩阵授权策略</li></ul><h3 id="3、插件安装"><a href="#3、插件安装" class="headerlink" title="3、插件安装"></a>3、插件安装</h3><p>安装插件：Role-based Authorization Strategy</p><a id="more"></a><h3 id="4、jenkins设置"><a href="#4、jenkins设置" class="headerlink" title="4、jenkins设置"></a>4、jenkins设置</h3><p>系统管理——&gt;全局安全配置——&gt;<br><img src="https://img.xxlaila.cn/image2018-5-11_14-26-37.png" alt="img"></p><p>回到系统管理界面，就可以看到多出来一个插件: Mangge and Assing Roles</p><h3 id="5、权限设置"><a href="#5、权限设置" class="headerlink" title="5、权限设置"></a>5、权限设置</h3><p>进入Manager and Assign Roles——&gt;Manage Roles,这里建立了四个权限，分别来对应不同的人员<br><img src="https://img.xxlaila.cn/image2018-5-24_11-35-15.png" alt="img"></p><ul><li>创建项目角色:</li></ul><p><img src="https://img.xxlaila.cn/image2018-5-24_11-35-27.png" alt="img"></p><p>回到Manage and Assign Roles界面</p><h3 id="6、配置角色"><a href="#6、配置角色" class="headerlink" title="6、配置角色"></a>6、配置角色</h3><p>选择Assign Roles,用户新建以后，根据用户不同类型的勾选不同的权限,</p><!-- rebuild by neat -->]]></content>
      <categories>
        <category>jenkins</category>
      </categories>
      <tags>
        <tag>jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title>mongodb副本集</title>
    <url>/2019/08/09/mongodb%E5%89%AF%E6%9C%AC%E9%9B%86/</url>
    <content><![CDATA[<!-- build time:Fri Nov 01 2019 20:14:22 GMT+0800 (China Standard Time) --><p>&nbsp;&nbsp;&nbsp;&nbsp;Mongodb replica set安装加认证，这里使用的是keyFile进行认证，之前看过很多文章，坑一大堆，这里是看了两天的官方文档进行的安装，并用户生产，配置文件参数贴一部分,三个带有数据集的节点组成的复制集拥有，架构图如下，参考官方</p><p><img src="https://img.xxlaila.cn/image2018-8-13_15-27-53.png" alt="img"><br>一个主节点，两个从节点，这两个从节点都可以在选举中升级为主节点</p><h3 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h3><p>三台服务器</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">primary: 192.168.32.7</span><br><span class="line">secaodray: 192.168.32.11</span><br><span class="line">secondary: 192.168.32.14</span><br></pre></td></tr></table></figure><h2 id="1、安装mongodb"><a href="#1、安装mongodb" class="headerlink" title="1、安装mongodb"></a>1、安装mongodb</h2><h3 id="1-1、每个节点都需要操作"><a href="#1-1、每个节点都需要操作" class="headerlink" title="1.1、每个节点都需要操作"></a>1.1、每个节点都需要操作</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># sudo vim /etc/yum.repos.d/mongodb-enterprise.repo</span></span><br><span class="line">[mongodb-enterprise]</span><br><span class="line">name=MongoDB Enterprise Repository</span><br><span class="line">baseurl=https://repo.mongodb.com/yum/redhat/<span class="variable">$releasever</span>/mongodb-enterprise/3.4/<span class="variable">$basearch</span>/</span><br><span class="line">gpgcheck=1</span><br><span class="line">enabled=1</span><br><span class="line">gpgkey=https://www.mongodb.org/static/pgp/server-3.4.asc   </span><br><span class="line"></span><br><span class="line"><span class="comment"># sudo yum install -y mongodb-enterprise</span></span><br></pre></td></tr></table></figure><p>注意：如果采用源码包方式安装需要安装一下插件</p><a id="more"></a><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># sudo yum install cyrus-sasl cyrus-sasl-plain cyrus-sasl-gssapi krb5-libs lm_sensors-libs net-snmp-agent-libs net-snmp openssl rpm-libs tcp_wrappers-libs libcurl</span></span><br></pre></td></tr></table></figure><h2 id="2、修改mongodb的配置文件-每个节点均操作"><a href="#2、修改mongodb的配置文件-每个节点均操作" class="headerlink" title="2、修改mongodb的配置文件(每个节点均操作)"></a>2、修改mongodb的配置文件(每个节点均操作)</h2><h3 id="自定义mongodb的目录"><a href="#自定义mongodb的目录" class="headerlink" title="自定义mongodb的目录"></a>自定义mongodb的目录</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># mkdir /opt/mongodb/&#123;data,conf,logs&#125; -p</span></span><br><span class="line"><span class="comment"># sudo vim /etc/mongod.conf</span></span><br><span class="line"> systemLog:</span><br><span class="line">  destination: file</span><br><span class="line">  logAppend: <span class="literal">true</span></span><br><span class="line">  path: /opt/mongodb/logs/mongod.log</span><br><span class="line">storage:</span><br><span class="line">  dbPath: /opt/mongodb/data</span><br><span class="line">  journal:</span><br><span class="line">    enabled: <span class="literal">true</span></span><br><span class="line">processManagement:</span><br><span class="line">  fork: <span class="literal">true</span></span><br><span class="line">  pidFilePath: /opt/mongodb/logs/mongod.pid</span><br><span class="line">net:</span><br><span class="line">  port: 27017</span><br><span class="line">  bindIp: 0.0.0.0</span><br></pre></td></tr></table></figure><h3 id="3、生成密钥文件-在mong01上操作"><a href="#3、生成密钥文件-在mong01上操作" class="headerlink" title="3、生成密钥文件(在mong01上操作)"></a>3、生成密钥文件(在mong01上操作)</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># openssl rand -base64 756 &gt; ／opt/mongodb/conf/mongo-keyfile</span></span><br><span class="line"><span class="comment"># sudo chmod 400 /opt/mongo/mongo-keyfile</span></span><br><span class="line"><span class="comment"># scp –r mongo-keyfile user@192.168.32.11:/opt/mongodb/conf</span></span><br><span class="line"><span class="comment"># scp –r mongo-keyfile user@192.168.32.14:/opt/mongodb/conf</span></span><br></pre></td></tr></table></figure><h3 id="4、修改mongodb的配置"><a href="#4、修改mongodb的配置" class="headerlink" title="4、修改mongodb的配置"></a>4、修改mongodb的配置</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">security:</span><br><span class="line">  keyFile: /opt/mongodb/conf/mongo-keyfile</span><br><span class="line">replication:</span><br><span class="line">  replSetName: xxlaila01（可变化，自定义）</span><br></pre></td></tr></table></figure><p>分别在三台服务器上启动mongodb</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># mongod --config /etc/mongod.conf</span></span><br></pre></td></tr></table></figure><h3 id="5、建立集群"><a href="#5、建立集群" class="headerlink" title="5、建立集群"></a>5、建立集群</h3><p>在你需要认为是主节点的服务器进行mongodb的登陆，和账户权限的建立，这里我选择的192.168.32.7</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mongo --shell --host 127.0.0.1</span><br></pre></td></tr></table></figure><p>登陆进去以后可以进行一个简单的命令进行查看</p><p><img src="https://img.xxlaila.cn/image2018-8-13_15-37-2.png" alt="img"></p><h3 id="6、把服务器加入副本集"><a href="#6、把服务器加入副本集" class="headerlink" title="6、把服务器加入副本集"></a>6、把服务器加入副本集</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">MongoDB Enterprise &gt; config = &#123; _id:<span class="string">"kxlprod01"</span>,members:[ &#123;_id:0,host:<span class="string">"192.168.32.7:27017"</span>&#125;,</span><br><span class="line">...   &#123;_id:1,host:<span class="string">"192.168.32.11:27017"</span>&#125; ,&#123;_id:2,host:<span class="string">"192.168.32.14:27017"</span>&#125;] &#125;</span><br></pre></td></tr></table></figure><p>config = { _id:”kxlprod01”,members:[ {_id:0,host:”192.168.32.7:27017”},{_id:1,host:”192.168.32.11:27017”} ,{_id:2,host:”192.168.32.14:27017”}] }，增加内容</p><h4 id="6-1-看当前副本集的状态"><a href="#6-1-看当前副本集的状态" class="headerlink" title="6.1 看当前副本集的状态"></a>6.1 看当前副本集的状态</h4><p>利用rs.status()命令可以查看当前副本集的状态</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&gt; rs.status()</span><br></pre></td></tr></table></figure><p>这里提示配置还没有加载到mongodb副本里面</p><h4 id="6-2、加载配置到副本集"><a href="#6-2、加载配置到副本集" class="headerlink" title="6.2、加载配置到副本集"></a>6.2、加载配置到副本集</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&gt; rs.initiate(config)</span><br></pre></td></tr></table></figure><p>再次查看副本的状态就可以看到mongodb的副本集已建立，如果此时主节点未被选举出来，稍微等一会就成功</p><h3 id="7、创建mongodb副本集认证"><a href="#7、创建mongodb副本集认证" class="headerlink" title="7、创建mongodb副本集认证"></a>7、创建mongodb副本集认证</h3><p>下面两行我们可以看到第一次主节点没有选举成功，随即我们在回车PRIMARY节点选举成功了，下面我们创建一个管理员账户</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">MongoDB Enterprise xxlaila01:SECONDARY&gt; admin = db.getSiblingDB(<span class="string">"admin"</span>)</span><br><span class="line">MongoDB Enterprise xxlaila01:PRIMARY&gt;</span><br><span class="line">admin.createUser(</span><br><span class="line"></span><br><span class="line">  &#123;</span><br><span class="line"></span><br><span class="line">    user: <span class="string">"root"</span>,</span><br><span class="line"></span><br><span class="line">    <span class="built_in">pwd</span>: <span class="string">"123456"</span>,</span><br><span class="line"></span><br><span class="line">    roles: [ &#123; role: <span class="string">"userAdminAnyDatabase"</span>, db: <span class="string">"admin"</span> &#125; ]</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">)</span><br><span class="line">db.getSiblingDB(<span class="string">"admin"</span>).auth(<span class="string">"root"</span>, <span class="string">"123456"</span> )</span><br></pre></td></tr></table></figure><h4 id="7-1、创建集群账户"><a href="#7-1、创建集群账户" class="headerlink" title="7.1、创建集群账户"></a>7.1、创建集群账户</h4><p>创建一个集群管理账户，集群账户具有管理整个副本集的</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ mongo -u <span class="string">"root"</span> -p <span class="string">"123456"</span> --authenticationDatabase <span class="string">"admin"</span></span><br><span class="line"></span><br><span class="line">db.getSiblingDB(<span class="string">"admin"</span>).createUser(</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="string">"user"</span> : <span class="string">"manger"</span>,</span><br><span class="line">    <span class="string">"pwd"</span> : <span class="string">"123456"</span>,</span><br><span class="line">    roles: [ &#123; <span class="string">"role"</span> : <span class="string">"clusterAdmin"</span>, <span class="string">"db"</span> : <span class="string">"admin"</span> &#125; ]</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="7-2-创建一个程序连接的账户"><a href="#7-2-创建一个程序连接的账户" class="headerlink" title="7.2 创建一个程序连接的账户"></a>7.2 创建一个程序连接的账户</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">db.getSiblingDB(<span class="string">"admin"</span>).createUser(</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="string">"user"</span> : <span class="string">"systemprod"</span>,</span><br><span class="line">    <span class="string">"pwd"</span> : <span class="string">"123456"</span>,</span><br><span class="line">    roles: [ &#123; <span class="string">"role"</span> : <span class="string">"root"</span>, <span class="string">"db"</span> : <span class="string">"admin"</span> &#125; ]</span><br><span class="line">  &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>至此mongodb的副本集创建完成。测试没有问题,登陆其中一台SECONDARY服务器进行测试,这里测试192.168.32.11服务器</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ mongo</span><br><span class="line"></span><br><span class="line">&gt; rs.status()    <span class="comment"># 这里提示没有权限（登录进来以后如果不是主几点，mognodb就会默认显示未secondary）</span></span><br><span class="line"></span><br><span class="line">&gt; use admin 	<span class="comment">#</span></span><br><span class="line"></span><br><span class="line">&gt; db.getSiblingDB(<span class="string">"admin"</span>).auth(<span class="string">"manger"</span>, <span class="string">"123456"</span>)</span><br></pre></td></tr></table></figure><p>完成后我们在执行rs.status()就可以看到副本集的信息</p><h4 id="7-3-测试程序连接账户"><a href="#7-3-测试程序连接账户" class="headerlink" title="7.3 测试程序连接账户"></a>7.3 测试程序连接账户</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">MongoDB Enterprise xxlaila01:SECONDARY&gt; db.getSiblingDB(<span class="string">"admin"</span>).auth(<span class="string">"systemprod"</span>, <span class="string">"123456"</span> )</span><br><span class="line">&gt; show dbs;	<span class="comment">#会提示 “not master and slaveok=false”</span></span><br><span class="line"></span><br><span class="line">&gt; db.getMongo().setSlaveOk()</span><br><span class="line">&gt; show dbs;	<span class="comment">#在次执行会显示出结果</span></span><br></pre></td></tr></table></figure><p>副本集没有读的权限，需要执行db.getMongo().setSlaveOk()</p><!-- rebuild by neat -->]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>mongodb,mongodb副本集,mongodb高可用</tag>
      </tags>
  </entry>
</search>
